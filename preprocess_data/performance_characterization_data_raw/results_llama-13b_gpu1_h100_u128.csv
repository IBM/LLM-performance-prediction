,Unnamed: 0,smpnum,reqnum,errors,status,model,num_users,requests,latency_ms,records,n_gpus,gpu_type,start_timestamp,end_timestamp,experiment_duration_s,n_input_tokens,n_output_tokens,latency_ms_per_token,timestamps_per_token
0,0,855,0,[],200,llama-13b,128,1,5532.0,1.0,1,H100,1697101590985,1697101596517.0,120,83.0,20.0,"[507, 1511, 271, 84, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 95, 95, 72, 93, 90, 527]","[1697101591492, 1697101593003, 1697101593274, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516]"
1,1,859,0,[],200,llama-13b,128,1,2022.0,1.0,1,H100,1697101590981,1697101593003.0,120,23.0,1.0,"[228, 1793]","[1697101591209, 1697101593002]"
2,2,175,0,[],200,llama-13b,128,1,3456.0,1.0,1,H100,1697101590981,1697101594437.0,120,140.0,8.0,"[234, 2060, 83, 60, 80, 749, 98, 92]","[1697101591215, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437]"
3,3,672,0,[],200,llama-13b,128,1,5498.0,1.0,1,H100,1697101591019,1697101596517.0,120,93.0,20.0,"[579, 1677, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 92, 90, 528]","[1697101591598, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596517]"
4,4,43,0,[],200,llama-13b,128,1,3366.0,1.0,1,H100,1697101590979,1697101594345.0,120,732.0,8.0,"[112, 882, 43, 1259, 83, 60, 79, 750, 98]","[1697101591091, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345]"
5,5,202,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590979,1697101601193.0,120,,,"[18, 174, 23, 822, 1258, 83, 61, 79, 750, 98, 91, 90, 89, 85, 643, 102, 98, 96, 95, 72, 92, 90, 528, 81, 75, 74, 905, 94, 89, 88, 88, 85, 730, 96, 90, 87, 83, 819, 100, 99, 102, 90, 89]","[1697101590997, 1697101591171, 1697101591194, 1697101592016, 1697101593274, 1697101593357, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594436, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595898, 1697101595988, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597922, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600391, 1697101600480]"
6,6,641,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590986,1697101601193.0,120,,,"[536, 1753, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 92, 90, 528, 80, 75, 74, 905, 94, 89, 89, 87, 85, 730, 96, 90, 87, 83, 819, 100, 99, 102, 90, 89]","[1697101591522, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600391, 1697101600480]"
7,7,69,0,[],200,llama-13b,128,1,5498.0,1.0,1,H100,1697101591019,1697101596517.0,120,85.0,20.0,"[560, 1424, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 92, 90, 528]","[1697101591579, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596517]"
8,8,344,0,[],200,llama-13b,128,1,2020.0,1.0,1,H100,1697101590983,1697101593003.0,120,13.0,1.0,"[325, 1695]","[1697101591308, 1697101593003]"
9,9,450,0,[],200,llama-13b,128,1,5011.0,1.0,1,H100,1697101590977,1697101595988.0,120,91.0,20.0,"[191, 805, 43, 1259, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591168, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
10,10,533,0,[],200,llama-13b,128,1,2293.0,1.0,1,H100,1697101590982,1697101593275.0,120,216.0,2.0,"[304, 1989]","[1697101591286, 1697101593275]"
11,11,698,0,[],200,llama-13b,128,1,2441.0,1.0,1,H100,1697101590977,1697101593418.0,120,182.0,6.0,"[13, 25, 179, 822, 1258, 83, 61]","[1697101590990, 1697101591015, 1697101591194, 1697101592016, 1697101593274, 1697101593357, 1697101593418]"
12,12,554,0,[],200,llama-13b,128,1,992.0,1.0,1,H100,1697101590981,1697101591973.0,120,26.0,1.0,"[204, 788]","[1697101591185, 1697101591973]"
13,13,448,0,[],200,llama-13b,128,1,4360.0,1.0,1,H100,1697101590984,1697101595344.0,120,335.0,12.0,"[414, 1605, 271, 83, 61, 79, 750, 98, 92, 89, 89, 85, 644]","[1697101591398, 1697101593003, 1697101593274, 1697101593357, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344]"
14,14,158,0,[],200,llama-13b,128,1,5009.0,1.0,1,H100,1697101590980,1697101595989.0,120,85.0,20.0,"[141, 852, 43, 1259, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591121, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
15,15,533,0,[],200,llama-13b,128,1,2256.0,1.0,1,H100,1697101591019,1697101593275.0,120,216.0,2.0,"[585, 1671]","[1697101591604, 1697101593275]"
16,16,231,0,[],200,llama-13b,128,1,993.0,1.0,1,H100,1697101590980,1697101591973.0,120,13.0,1.0,"[194, 799]","[1697101591174, 1697101591973]"
17,17,205,0,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101590979,1697101595899.0,120,87.0,20.0,"[19, 173, 23, 822, 1258, 83, 61, 79, 750, 98, 91, 90, 89, 85, 643, 102, 98, 96, 95, 72, 92]","[1697101590998, 1697101591171, 1697101591194, 1697101592016, 1697101593274, 1697101593357, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594436, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595898]"
18,18,673,0,[],200,llama-13b,128,1,5534.0,1.0,1,H100,1697101590982,1697101596516.0,120,93.0,20.0,"[297, 1996, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 90, 527]","[1697101591279, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516]"
19,19,365,0,[],200,llama-13b,128,1,994.0,1.0,1,H100,1697101590979,1697101591973.0,120,23.0,1.0,"[106, 888]","[1697101591085, 1697101591973]"
20,20,861,0,[],200,llama-13b,128,1,2026.0,1.0,1,H100,1697101590977,1697101593003.0,120,10.0,1.0,"[425, 1601]","[1697101591402, 1697101593003]"
21,21,530,0,[],200,llama-13b,128,1,2026.0,1.0,1,H100,1697101590977,1697101593003.0,120,26.0,1.0,"[303, 1723]","[1697101591280, 1697101593003]"
22,22,852,0,[],200,llama-13b,128,1,5532.0,1.0,1,H100,1697101590985,1697101596517.0,120,100.0,20.0,"[505, 1512, 272, 84, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 95, 95, 72, 93, 90, 527]","[1697101591490, 1697101593002, 1697101593274, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516]"
23,23,371,0,[],200,llama-13b,128,1,2020.0,1.0,1,H100,1697101590983,1697101593003.0,120,13.0,1.0,"[391, 1629]","[1697101591374, 1697101593003]"
24,24,613,0,[],200,llama-13b,128,1,5533.0,1.0,1,H100,1697101590983,1697101596516.0,120,90.0,20.0,"[321, 1699, 272, 82, 61, 79, 750, 98, 92, 89, 89, 85, 644, 101, 98, 96, 95, 72, 93, 90, 527]","[1697101591304, 1697101593003, 1697101593275, 1697101593357, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516]"
25,25,444,0,[],200,llama-13b,128,1,2518.0,1.0,1,H100,1697101590979,1697101593497.0,120,457.0,6.0,"[49, 945, 43, 1258, 84, 60, 79]","[1697101591028, 1697101591973, 1697101592016, 1697101593274, 1697101593358, 1697101593418, 1697101593497]"
26,26,778,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590990,1697101601193.0,120,,,"[536, 1749, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 72, 93, 90, 528, 80, 75, 74, 905, 94, 89, 89, 87, 85, 730, 96, 90, 87, 83, 819, 100, 99, 102, 91, 88]","[1697101591526, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600392, 1697101600480]"
27,27,526,0,[],200,llama-13b,128,1,5009.0,1.0,1,H100,1697101590979,1697101595988.0,120,89.0,20.0,"[54, 940, 43, 1258, 84, 60, 79, 750, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591033, 1697101591973, 1697101592016, 1697101593274, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
28,28,590,0,[],200,llama-13b,128,1,5540.0,1.0,1,H100,1697101590977,1697101596517.0,120,88.0,20.0,"[527, 1499, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 95, 95, 72, 93, 90, 528]","[1697101591504, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596517]"
29,29,284,0,[],200,llama-13b,128,1,7939.0,1.0,1,H100,1697101590983,1697101598922.0,120,90.0,31.0,"[395, 1897, 82, 61, 79, 750, 98, 92, 89, 89, 85, 644, 101, 98, 96, 95, 72, 93, 90, 527, 81, 75, 74, 905, 94, 89, 89, 87, 85, 730, 96]","[1697101591378, 1697101593275, 1697101593357, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598921]"
30,30,424,0,[],200,llama-13b,128,1,5009.0,1.0,1,H100,1697101590979,1697101595988.0,120,88.0,20.0,"[95, 899, 43, 1258, 84, 60, 79, 750, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591074, 1697101591973, 1697101592016, 1697101593274, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
31,31,308,0,[],200,llama-13b,128,1,5009.0,1.0,1,H100,1697101590979,1697101595988.0,120,87.0,20.0,"[100, 894, 43, 1258, 84, 60, 79, 750, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591079, 1697101591973, 1697101592016, 1697101593274, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
32,32,96,0,[],200,llama-13b,128,1,993.0,1.0,1,H100,1697101590980,1697101591973.0,120,31.0,1.0,"[136, 857]","[1697101591116, 1697101591973]"
33,33,452,1,[],200,llama-13b,128,1,2462.0,1.0,1,H100,1697101591975,1697101594437.0,120,216.0,4.0,"[42, 1941, 289, 98, 92]","[1697101592017, 1697101593958, 1697101594247, 1697101594345, 1697101594437]"
34,34,664,1,[],200,llama-13b,128,1,2932.0,1.0,1,H100,1697101595990,1697101598922.0,120,364.0,9.0,"[31, 1412, 219, 93, 89, 89, 88, 84, 730, 97]","[1697101596021, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922]"
35,35,13,0,[],200,llama-13b,128,1,5532.0,1.0,1,H100,1697101590985,1697101596517.0,120,90.0,20.0,"[517, 1501, 272, 83, 60, 80, 749, 98, 92, 89, 89, 86, 643, 101, 99, 95, 95, 72, 93, 90, 528]","[1697101591502, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596517]"
36,36,781,1,[],200,llama-13b,128,1,3022.0,1.0,1,H100,1697101595990,1697101599012.0,120,335.0,10.0,"[13, 1430, 218, 94, 89, 89, 87, 85, 731, 96, 90]","[1697101596003, 1697101597433, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598826, 1697101598922, 1697101599012]"
37,37,212,2,[],200,llama-13b,128,1,753.0,1.0,1,H100,1697101599013,1697101599766.0,120,31.0,1.0,"[12, 741]","[1697101599025, 1697101599766]"
38,38,93,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598923,1697101601191.0,120,,,"[6, 837, 234, 100, 100, 101, 91, 89]","[1697101598929, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
39,39,540,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601191.0,120,,,"[18, 1308]","[1697101599785, 1697101601093]"
40,40,225,0,[],200,llama-13b,128,1,2019.0,1.0,1,H100,1697101590984,1697101593003.0,120,23.0,1.0,"[424, 1595]","[1697101591408, 1697101593003]"
41,41,557,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,31.0,1.0,"[48, 905]","[1697101593053, 1697101593958]"
42,42,281,0,[],200,llama-13b,128,1,993.0,1.0,1,H100,1697101590980,1697101591973.0,120,23.0,1.0,"[100, 893]","[1697101591080, 1697101591973]"
43,43,916,2,[],200,llama-13b,128,1,1261.0,1.0,1,H100,1697101593959,1697101595220.0,120,8.0,1.0,"[42, 1219]","[1697101594001, 1697101595220]"
44,44,642,1,[],200,llama-13b,128,1,4543.0,1.0,1,H100,1697101591974,1697101596517.0,120,89.0,20.0,"[19, 1010, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 92, 90, 528]","[1697101591993, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596517]"
45,45,348,3,[],200,llama-13b,128,1,5080.0,1.0,1,H100,1697101595221,1697101600301.0,120,91.0,20.0,"[24, 1165, 107, 80, 75, 74, 905, 94, 90, 88, 88, 85, 730, 96, 89, 87, 84, 818, 100, 100, 101]","[1697101595245, 1697101596410, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597835, 1697101597923, 1697101598011, 1697101598096, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301]"
46,46,707,4,[],200,llama-13b,128,1,787.0,1.0,1,H100,1697101600306,1697101601093.0,120,8.0,1.0,"[35, 752]","[1697101600341, 1697101601093]"
47,47,132,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601094,1697101604270.0,120,,,"[12, 266, 783, 1253, 101, 74]","[1697101601106, 1697101601372, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
48,48,563,0,[],200,llama-13b,128,1,4917.0,1.0,1,H100,1697101590982,1697101595899.0,120,874.0,18.0,"[303, 1718, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93]","[1697101591285, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899]"
49,49,229,0,[],200,llama-13b,128,1,1984.0,1.0,1,H100,1697101591019,1697101593003.0,120,15.0,1.0,"[572, 1412]","[1697101591591, 1697101593003]"
50,50,372,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601192.0,120,,,"[37, 878, 219, 93, 89, 89, 88, 85, 729, 97, 89, 87, 84, 818, 100, 100, 101, 91, 89]","[1697101596555, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
51,51,586,1,[],200,llama-13b,128,1,4647.0,1.0,1,H100,1697101593005,1697101597652.0,120,85.0,20.0,"[42, 911, 289, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89, 529, 80, 75, 74, 906]","[1697101593047, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
52,52,573,6,[],200,llama-13b,128,1,974.0,1.0,1,H100,1697101604274,1697101605248.0,120,874.0,2.0,"[16, 958]","[1697101604290, 1697101605248]"
53,53,444,0,[],200,llama-13b,128,1,3270.0,1.0,1,H100,1697101590977,1697101594247.0,120,457.0,6.0,"[590, 1436, 272, 83, 60, 80, 749]","[1697101591567, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247]"
54,54,807,1,[],200,llama-13b,128,1,4578.0,1.0,1,H100,1697101594248,1697101598826.0,120,90.0,20.0,"[18, 954, 124, 101, 99, 96, 94, 72, 93, 90, 527, 81, 75, 74, 906, 93, 89, 89, 88, 84, 731]","[1697101594266, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598826]"
55,55,73,2,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101596518,1697101597433.0,120,9.0,1.0,"[54, 861]","[1697101596572, 1697101597433]"
56,56,405,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597434,1697101601192.0,120,,,"[13, 1142, 236, 97, 89, 87, 84, 818, 100, 100, 101, 91, 88]","[1697101597447, 1697101598589, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
57,57,718,0,[],200,llama-13b,128,1,994.0,1.0,1,H100,1697101590979,1697101591973.0,120,13.0,1.0,"[38, 956]","[1697101591017, 1697101591973]"
58,58,239,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598827,1697101601193.0,120,,,"[36, 903, 234, 100, 100, 101, 91, 89]","[1697101598863, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
59,59,63,0,[],200,llama-13b,128,1,2020.0,1.0,1,H100,1697101590983,1697101593003.0,120,39.0,1.0,"[401, 1619]","[1697101591384, 1697101593003]"
60,60,758,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601194,1697101604270.0,120,,,"[15, 902, 44, 1253, 101, 74]","[1697101601209, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
61,61,393,1,[],200,llama-13b,128,1,4829.0,1.0,1,H100,1697101593005,1697101597834.0,120,182.0,22.0,"[92, 861, 289, 98, 92, 89, 89, 86, 642, 102, 98, 96, 95, 72, 93, 89, 529, 80, 75, 74, 906, 93, 89]","[1697101593097, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834]"
62,62,882,0,[],200,llama-13b,128,1,3635.0,1.0,1,H100,1697101590980,1697101594615.0,120,345.0,11.0,"[199, 794, 43, 1259, 83, 60, 80, 749, 98, 92, 89, 89]","[1697101591179, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615]"
63,63,567,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601198,1697101604271.0,120,,,"[572, 1523, 115, 101, 74]","[1697101601770, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
64,64,729,2,[],200,llama-13b,128,1,2211.0,1.0,1,H100,1697101601197,1697101603408.0,120,874.0,2.0,"[525, 1686]","[1697101601722, 1697101603408]"
65,65,673,0,[],200,llama-13b,128,1,5009.0,1.0,1,H100,1697101590979,1697101595988.0,120,93.0,20.0,"[107, 930, 1259, 83, 60, 79, 750, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591086, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
66,66,591,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590984,1697101601193.0,120,,,"[406, 1613, 271, 84, 60, 79, 750, 98, 92, 89, 89, 85, 644, 101, 99, 95, 95, 72, 93, 90, 527, 81, 75, 74, 905, 94, 89, 89, 87, 85, 730, 96, 90, 87, 83, 819, 100, 99, 102, 90, 89]","[1697101591390, 1697101593003, 1697101593274, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600391, 1697101600480]"
67,67,885,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590990,1697101601192.0,120,,,"[530, 1483, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 72, 93, 90, 528, 80, 75, 74, 905, 94, 89, 89, 87, 85, 730, 96, 90, 87, 83, 819, 100, 99, 102, 90, 89]","[1697101591520, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600391, 1697101600480]"
68,68,927,4,[],200,llama-13b,128,1,5173.0,1.0,1,H100,1697101604275,1697101609448.0,120,83.0,20.0,"[323, 1609, 238, 81, 76, 74, 772, 90, 86, 84, 65, 83, 80, 491, 95, 73, 73, 93, 90, 87, 510]","[1697101604598, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609448]"
69,69,664,0,[],200,llama-13b,128,1,3457.0,1.0,1,H100,1697101590980,1697101594437.0,120,364.0,9.0,"[130, 863, 43, 1259, 83, 60, 80, 749, 98, 92]","[1697101591110, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437]"
70,70,129,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603411,1697101604272.0,120,,,"[77, 706]","[1697101603488, 1697101604194]"
71,71,488,4,[],200,llama-13b,128,1,1927.0,1.0,1,H100,1697101604280,1697101606207.0,120,6.0,1.0,"[514, 1413]","[1697101604794, 1697101606207]"
72,72,189,5,[],200,llama-13b,128,1,4664.0,1.0,1,H100,1697101604274,1697101608938.0,120,88.0,20.0,"[207, 713, 54, 1198, 80, 76, 74, 772, 90, 86, 84, 64, 84, 80, 491, 95, 73, 72, 94, 89, 88]","[1697101604481, 1697101605194, 1697101605248, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608938]"
73,73,554,0,[],200,llama-13b,128,1,2020.0,1.0,1,H100,1697101590983,1697101593003.0,120,26.0,1.0,"[331, 1689]","[1697101591314, 1697101593003]"
74,74,913,1,[],200,llama-13b,128,1,4647.0,1.0,1,H100,1697101593005,1697101597652.0,120,88.0,20.0,"[98, 855, 289, 98, 92, 89, 89, 86, 642, 102, 98, 96, 95, 72, 93, 89, 528, 81, 75, 74, 906]","[1697101593103, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
75,75,39,0,[],200,llama-13b,128,1,2013.0,1.0,1,H100,1697101590990,1697101593003.0,120,8.0,1.0,"[584, 1429]","[1697101591574, 1697101593003]"
76,76,400,1,[],200,llama-13b,128,1,1696.0,1.0,1,H100,1697101593005,1697101594701.0,120,123.0,7.0,"[18, 935, 289, 98, 92, 89, 89, 85]","[1697101593023, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700]"
77,77,840,2,[],200,llama-13b,128,1,1708.0,1.0,1,H100,1697101594702,1697101596410.0,120,17.0,1.0,"[6, 1702]","[1697101594708, 1697101596410]"
78,78,271,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596411,1697101601191.0,120,,,"[13, 1009, 219, 93, 89, 89, 88, 84, 730, 97, 90, 86, 84, 818, 100, 99, 102, 91, 89]","[1697101596424, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600392, 1697101600481]"
79,79,427,0,[],200,llama-13b,128,1,2521.0,1.0,1,H100,1697101590977,1697101593498.0,120,58.0,5.0,"[321, 1705, 272, 82, 61, 80]","[1697101591298, 1697101593003, 1697101593275, 1697101593357, 1697101593418, 1697101593498]"
80,80,879,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590979,1697101601193.0,120,,,"[89, 905, 43, 1258, 84, 60, 79, 750, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89, 528, 81, 75, 74, 905, 93, 90, 88, 88, 85, 730, 96, 90, 87, 83, 819, 100, 99, 102, 90, 89]","[1697101591068, 1697101591973, 1697101592016, 1697101593274, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597744, 1697101597834, 1697101597922, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600391, 1697101600480]"
81,81,65,1,[],200,llama-13b,128,1,6043.0,1.0,1,H100,1697101594438,1697101600481.0,120,39.0,30.0,"[12, 770, 124, 101, 99, 96, 94, 73, 92, 90, 527, 81, 75, 74, 906, 93, 89, 89, 88, 85, 730, 96, 89, 87, 84, 818, 100, 100, 101, 91, 88]","[1697101594450, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
82,82,781,1,[],200,llama-13b,128,1,3017.0,1.0,1,H100,1697101593499,1697101596516.0,120,335.0,10.0,"[6, 1714, 125, 101, 98, 96, 95, 72, 93, 89, 528]","[1697101593505, 1697101595219, 1697101595344, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516]"
83,83,211,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596529,1697101601192.0,120,,,"[61, 843, 219, 93, 89, 89, 88, 85, 729, 97, 89, 87, 84, 818, 100, 100, 101, 91, 89]","[1697101596590, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
84,84,609,0,[],200,llama-13b,128,1,5532.0,1.0,1,H100,1697101590984,1697101596516.0,120,88.0,20.0,"[488, 1532, 270, 84, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 95, 95, 72, 93, 90, 527]","[1697101591472, 1697101593004, 1697101593274, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516]"
85,85,565,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604271.0,120,,,"[531, 1565, 115, 101, 74]","[1697101601728, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
86,86,894,4,[],200,llama-13b,128,1,1931.0,1.0,1,H100,1697101604276,1697101606207.0,120,14.0,1.0,"[518, 1413]","[1697101604794, 1697101606207]"
87,87,861,1,[],200,llama-13b,128,1,682.0,1.0,1,H100,1697101593276,1697101593958.0,120,10.0,1.0,"[12, 670]","[1697101593288, 1697101593958]"
88,88,328,5,[],200,llama-13b,128,1,1565.0,1.0,1,H100,1697101606208,1697101607773.0,120,109.0,6.0,"[19, 1042, 179, 90, 86, 84, 64]","[1697101606227, 1697101607269, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772]"
89,89,292,2,[],200,llama-13b,128,1,1261.0,1.0,1,H100,1697101593959,1697101595220.0,120,286.0,1.0,"[42, 1219]","[1697101594001, 1697101595220]"
90,90,649,3,[],200,llama-13b,128,1,5080.0,1.0,1,H100,1697101595221,1697101600301.0,120,244.0,20.0,"[12, 1177, 107, 80, 75, 74, 905, 94, 89, 89, 88, 85, 730, 96, 89, 87, 84, 818, 100, 100, 101]","[1697101595233, 1697101596410, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301]"
91,91,689,6,[],200,llama-13b,128,1,559.0,1.0,1,H100,1697101607774,1697101608333.0,120,15.0,1.0,"[6, 553]","[1697101607780, 1697101608333]"
92,92,95,0,[],200,llama-13b,128,1,2013.0,1.0,1,H100,1697101590990,1697101593003.0,120,12.0,1.0,"[578, 1435]","[1697101591568, 1697101593003]"
93,93,453,1,[],200,llama-13b,128,1,954.0,1.0,1,H100,1697101593004,1697101593958.0,120,26.0,1.0,"[7, 947]","[1697101593011, 1697101593958]"
94,94,494,0,[],200,llama-13b,128,1,3630.0,1.0,1,H100,1697101590986,1697101594616.0,120,6.0,10.0,"[530, 1487, 272, 83, 60, 80, 749, 98, 92, 89, 89]","[1697101591516, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615]"
95,95,202,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590986,1697101601193.0,120,,,"[587, 1430, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 92, 90, 528, 80, 75, 74, 905, 94, 89, 89, 87, 85, 730, 96, 90, 87, 83, 819, 100, 100, 101, 90, 89]","[1697101591573, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600391, 1697101600480]"
96,96,806,2,[],200,llama-13b,128,1,4866.0,1.0,1,H100,1697101593959,1697101598825.0,120,89.0,20.0,"[6, 1254, 125, 101, 99, 95, 95, 72, 93, 89, 528, 82, 74, 74, 906, 93, 89, 89, 88, 84, 730]","[1697101593965, 1697101595219, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596598, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825]"
97,97,117,7,[],200,llama-13b,128,1,1115.0,1.0,1,H100,1697101608334,1697101609449.0,120,364.0,2.0,"[18, 1006, 91]","[1697101608352, 1697101609358, 1697101609449]"
98,98,444,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609450,1697101610267.0,120,,,"[19, 635]","[1697101609469, 1697101610104]"
99,99,798,9,[],200,llama-13b,128,1,2158.0,1.0,1,H100,1697101610274,1697101612432.0,120,79.0,6.0,"[24, 793, 1095, 84, 82, 80]","[1697101610298, 1697101611091, 1697101612186, 1697101612270, 1697101612352, 1697101612432]"
100,100,754,2,[],200,llama-13b,128,1,2165.0,1.0,1,H100,1697101597835,1697101600000.0,120,88.0,7.0,"[7, 984, 96, 90, 86, 84, 818]","[1697101597842, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000]"
101,101,508,0,[],200,llama-13b,128,1,5008.0,1.0,1,H100,1697101590980,1697101595988.0,120,86.0,20.0,"[135, 858, 43, 1259, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591115, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
102,102,93,0,[],200,llama-13b,128,1,5009.0,1.0,1,H100,1697101590979,1697101595988.0,120,88.0,20.0,"[55, 939, 43, 1258, 84, 60, 79, 750, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591034, 1697101591973, 1697101592016, 1697101593274, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
103,103,184,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600001,1697101601192.0,120,,,"[6, 1086]","[1697101600007, 1697101601093]"
104,104,857,1,[],200,llama-13b,128,1,603.0,1.0,1,H100,1697101594617,1697101595220.0,120,18.0,1.0,"[12, 591]","[1697101594629, 1697101595220]"
105,105,145,0,[],200,llama-13b,128,1,3542.0,1.0,1,H100,1697101590985,1697101594527.0,120,161.0,9.0,"[529, 1489, 272, 83, 60, 80, 749, 98, 92, 90]","[1697101591514, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594527]"
106,106,538,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601194,1697101604270.0,120,,,"[15, 902, 44, 1253, 101, 74]","[1697101601209, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
107,107,373,1,[],200,llama-13b,128,1,874.0,1.0,1,H100,1697101594346,1697101595220.0,120,15.0,1.0,"[12, 862]","[1697101594358, 1697101595220]"
108,108,896,5,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101604275,1697101605195.0,120,15.0,1.0,"[211, 708]","[1697101604486, 1697101605194]"
109,109,727,2,[],200,llama-13b,128,1,1526.0,1.0,1,H100,1697101595220,1697101596746.0,120,58.0,5.0,"[7, 1183, 107, 80, 75, 74]","[1697101595227, 1697101596410, 1697101596517, 1697101596597, 1697101596672, 1697101596746]"
110,110,410,6,[],200,llama-13b,128,1,2741.0,1.0,1,H100,1697101605195,1697101607936.0,120,364.0,12.0,"[7, 1005, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80]","[1697101605202, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936]"
111,111,152,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596747,1697101601192.0,120,,,"[7, 1835, 236, 97, 89, 87, 84, 818, 100, 100, 101, 91, 88]","[1697101596754, 1697101598589, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
112,112,228,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612434,1697101616321.0,120,,,"[6, 574, 107, 90, 86, 83, 81, 63, 80, 719, 95, 93, 92, 90, 69, 87, 634, 80, 78, 75]","[1697101612440, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613524, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615563, 1697101615641, 1697101615716]"
113,113,828,0,[],200,llama-13b,128,1,3264.0,1.0,1,H100,1697101590983,1697101594247.0,120,182.0,6.0,"[385, 1907, 82, 61, 79, 750]","[1697101591368, 1697101593275, 1697101593357, 1697101593418, 1697101593497, 1697101594247]"
114,114,699,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,39.0,1.0,"[36, 917]","[1697101593041, 1697101593958]"
115,115,127,2,[],200,llama-13b,128,1,1681.0,1.0,1,H100,1697101593959,1697101595640.0,120,100.0,5.0,"[18, 1242, 125, 101, 99, 95]","[1697101593977, 1697101595219, 1697101595344, 1697101595445, 1697101595544, 1697101595639]"
116,116,659,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590984,1697101601193.0,120,,,"[484, 1536, 270, 84, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 95, 95, 72, 93, 90, 527, 81, 75, 74, 905, 94, 89, 89, 87, 85, 730, 96, 90, 87, 83, 819, 100, 99, 102, 90, 89]","[1697101591468, 1697101593004, 1697101593274, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600391, 1697101600480]"
117,117,690,0,[],200,llama-13b,128,1,1984.0,1.0,1,H100,1697101591019,1697101593003.0,120,39.0,1.0,"[591, 1393]","[1697101591610, 1697101593003]"
118,118,389,0,[],200,llama-13b,128,1,2022.0,1.0,1,H100,1697101590981,1697101593003.0,120,8.0,1.0,"[235, 1787]","[1697101591216, 1697101593003]"
119,119,121,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,13.0,1.0,"[122, 831]","[1697101593127, 1697101593958]"
120,120,771,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607937,1697101610278.0,120,,,"[7, 1414, 91, 79, 77]","[1697101607944, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
121,121,482,2,[],200,llama-13b,128,1,4866.0,1.0,1,H100,1697101593959,1697101598825.0,120,91.0,20.0,"[30, 1231, 124, 101, 99, 95, 95, 72, 93, 89, 528, 81, 75, 74, 906, 93, 89, 89, 88, 84, 730]","[1697101593989, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825]"
122,122,718,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,13.0,1.0,"[24, 929]","[1697101593029, 1697101593958]"
123,123,150,2,[],200,llama-13b,128,1,1385.0,1.0,1,H100,1697101593959,1697101595344.0,120,216.0,2.0,"[6, 1379]","[1697101593965, 1697101595344]"
124,124,513,3,[],200,llama-13b,128,1,4956.0,1.0,1,H100,1697101595345,1697101600301.0,120,83.0,20.0,"[6, 1059, 107, 80, 75, 74, 905, 94, 89, 89, 88, 85, 730, 96, 89, 87, 84, 818, 100, 100, 101]","[1697101595351, 1697101596410, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301]"
125,125,200,8,[],200,llama-13b,128,1,3013.0,1.0,1,H100,1697101610283,1697101613296.0,120,6.0,9.0,"[592, 1194, 117, 84, 82, 81, 78, 609, 90, 86]","[1697101610875, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613120, 1697101613210, 1697101613296]"
126,126,562,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613297,1697101616319.0,120,,,"[7, 833, 186, 95, 93, 92, 90, 69, 88, 633, 79, 78, 76]","[1697101613304, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
127,127,842,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598827,1697101601193.0,120,,,"[12, 927, 234, 100, 100, 101, 91, 89]","[1697101598839, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
128,128,511,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[83, 832, 44, 1253, 101, 74]","[1697101601279, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
129,129,99,0,[],200,llama-13b,128,1,2022.0,1.0,1,H100,1697101590981,1697101593003.0,120,10.0,1.0,"[215, 1807]","[1697101591196, 1697101593003]"
130,130,536,1,[],200,llama-13b,128,1,4388.0,1.0,1,H100,1697101594438,1697101598826.0,120,83.0,20.0,"[6, 776, 124, 101, 99, 96, 94, 73, 92, 90, 527, 81, 75, 74, 906, 93, 89, 89, 88, 84, 731]","[1697101594444, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598826]"
131,131,869,5,[],200,llama-13b,128,1,3661.0,1.0,1,H100,1697101604275,1697101607936.0,120,244.0,12.0,"[259, 1673, 239, 80, 76, 74, 772, 90, 86, 84, 64, 84, 80]","[1697101604534, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936]"
132,132,449,1,[],200,llama-13b,128,1,4648.0,1.0,1,H100,1697101593004,1697101597652.0,120,86.0,20.0,"[25, 929, 289, 98, 92, 89, 89, 85, 643, 102, 99, 96, 94, 72, 93, 89, 529, 80, 75, 74, 906]","[1697101593029, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
133,133,811,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101595990,1697101601191.0,120,,,"[37, 1406, 219, 93, 89, 89, 88, 84, 730, 97, 90, 86, 84, 818, 100, 100, 101, 91, 88]","[1697101596027, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
134,134,69,0,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101590979,1697101595899.0,120,85.0,20.0,"[31, 161, 23, 822, 1258, 83, 61, 79, 750, 98, 91, 90, 89, 85, 643, 102, 98, 96, 95, 72, 93]","[1697101591010, 1697101591171, 1697101591194, 1697101592016, 1697101593274, 1697101593357, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594436, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899]"
135,135,392,0,[],200,llama-13b,128,1,2021.0,1.0,1,H100,1697101590982,1697101593003.0,120,20.0,1.0,"[245, 1776]","[1697101591227, 1697101593003]"
136,136,368,0,[],200,llama-13b,128,1,5009.0,1.0,1,H100,1697101590979,1697101595988.0,120,88.0,20.0,"[43, 951, 43, 1258, 84, 60, 79, 750, 98, 91, 90, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591022, 1697101591973, 1697101592016, 1697101593274, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594436, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
137,137,752,0,[],200,llama-13b,128,1,1037.0,1.0,1,H100,1697101590979,1697101592016.0,120,39.0,3.0,"[7, 186, 22, 822]","[1697101590986, 1697101591172, 1697101591194, 1697101592016]"
138,138,553,0,[],200,llama-13b,128,1,5009.0,1.0,1,H100,1697101590980,1697101595989.0,120,88.0,20.0,"[200, 793, 43, 1259, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591180, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
139,139,778,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590981,1697101601193.0,120,,,"[210, 825, 1259, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89, 528, 81, 75, 74, 905, 94, 89, 89, 87, 85, 730, 96, 90, 87, 83, 819, 100, 99, 102, 90, 89]","[1697101591191, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600391, 1697101600480]"
140,140,866,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598827,1697101601193.0,120,,,"[30, 1143, 100, 100, 101, 91, 89]","[1697101598857, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
141,141,236,2,[],200,llama-13b,128,1,2097.0,1.0,1,H100,1697101601196,1697101603293.0,120,8.0,1.0,"[298, 1799]","[1697101601494, 1697101603293]"
142,142,567,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[107, 792]","[1697101603402, 1697101604194]"
143,143,806,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597653,1697101601193.0,120,,,"[19, 917, 237, 96, 89, 87, 84, 818, 101, 99, 101, 91, 89]","[1697101597672, 1697101598589, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600101, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
144,144,298,3,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101601196,1697101602111.0,120,17.0,1.0,"[95, 820]","[1697101601291, 1697101602111]"
145,145,924,4,[],200,llama-13b,128,1,1927.0,1.0,1,H100,1697101604280,1697101606207.0,120,9.0,1.0,"[508, 1419]","[1697101604788, 1697101606207]"
146,146,884,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101595990,1697101601191.0,120,,,"[31, 1412, 219, 93, 89, 89, 88, 84, 730, 97, 90, 86, 84, 818, 100, 100, 101, 91, 88]","[1697101596021, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
147,147,652,4,[],200,llama-13b,128,1,1181.0,1.0,1,H100,1697101602112,1697101603293.0,120,14.0,1.0,"[18, 1163]","[1697101602130, 1697101603293]"
148,148,82,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[83, 816]","[1697101603378, 1697101604194]"
149,149,237,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[95, 820, 44, 1253, 101, 74]","[1697101601291, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
150,150,353,5,[],200,llama-13b,128,1,1416.0,1.0,1,H100,1697101606208,1697101607624.0,120,52.0,4.0,"[19, 1042, 179, 90, 86]","[1697101606227, 1697101607269, 1697101607448, 1697101607538, 1697101607624]"
151,151,436,6,[],200,llama-13b,128,1,5167.0,1.0,1,H100,1697101604281,1697101609448.0,120,86.0,20.0,"[592, 1334, 238, 81, 76, 74, 772, 90, 86, 84, 65, 84, 79, 492, 95, 72, 73, 93, 90, 87, 510]","[1697101604873, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607857, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609448]"
152,152,203,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[184, 1912, 116, 101, 74]","[1697101601380, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
153,153,394,0,[],200,llama-13b,128,1,2019.0,1.0,1,H100,1697101590984,1697101593003.0,120,11.0,1.0,"[402, 1617]","[1697101591386, 1697101593003]"
154,154,828,0,[],200,llama-13b,128,1,3261.0,1.0,1,H100,1697101590986,1697101594247.0,120,182.0,6.0,"[524, 1765, 83, 60, 80, 749]","[1697101591510, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247]"
155,155,185,1,[],200,llama-13b,128,1,5635.0,1.0,1,H100,1697101592017,1697101597652.0,120,93.0,20.0,"[6, 2224, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 92, 89, 529, 80, 75, 74, 905]","[1697101592023, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651]"
156,156,757,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,20.0,1.0,"[54, 899]","[1697101593059, 1697101593958]"
157,157,312,2,[],200,llama-13b,128,1,2097.0,1.0,1,H100,1697101601196,1697101603293.0,120,23.0,1.0,"[377, 1720]","[1697101601573, 1697101603293]"
158,158,186,2,[],200,llama-13b,128,1,5052.0,1.0,1,H100,1697101593959,1697101599011.0,120,123.0,22.0,"[48, 1213, 124, 101, 99, 95, 95, 72, 93, 90, 527, 81, 75, 74, 906, 93, 89, 89, 88, 84, 730, 97, 89]","[1697101594007, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922, 1697101599011]"
159,159,572,4,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101604275,1697101605195.0,120,16.0,1.0,"[126, 794]","[1697101604401, 1697101605195]"
160,160,677,3,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101603295,1697101604194.0,120,9.0,1.0,"[60, 839]","[1697101603355, 1697101604194]"
161,161,1,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101605196,1697101610266.0,120,,,"[36, 975, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 72, 94, 89, 87, 512, 79, 77]","[1697101605232, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608937, 1697101609449, 1697101609528, 1697101609605]"
162,162,712,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607625,1697101610274.0,120,,,"[6, 702, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607631, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
163,163,228,1,[],200,llama-13b,128,1,4578.0,1.0,1,H100,1697101594248,1697101598826.0,120,100.0,20.0,"[6, 965, 125, 101, 99, 96, 94, 72, 93, 90, 527, 81, 75, 74, 906, 93, 89, 89, 88, 84, 731]","[1697101594254, 1697101595219, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598826]"
164,164,107,4,[],200,llama-13b,128,1,1053.0,1.0,1,H100,1697101604195,1697101605248.0,120,216.0,2.0,"[36, 1017]","[1697101604231, 1697101605248]"
165,165,339,0,[],200,llama-13b,128,1,5011.0,1.0,1,H100,1697101590977,1697101595988.0,120,87.0,20.0,"[127, 869, 43, 1259, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591104, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
166,166,231,0,[],200,llama-13b,128,1,2022.0,1.0,1,H100,1697101590981,1697101593003.0,120,13.0,1.0,"[229, 1793]","[1697101591210, 1697101593003]"
167,167,111,7,[],200,llama-13b,128,1,2155.0,1.0,1,H100,1697101610278,1697101612433.0,120,79.0,5.0,"[538, 1253, 117, 84, 82, 81]","[1697101610816, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433]"
168,168,596,1,[],200,llama-13b,128,1,4648.0,1.0,1,H100,1697101593004,1697101597652.0,120,87.0,20.0,"[19, 935, 289, 98, 92, 89, 89, 85, 643, 102, 99, 96, 94, 72, 93, 89, 529, 80, 75, 74, 906]","[1697101593023, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
169,169,552,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101605249,1697101610266.0,120,,,"[6, 2014, 179, 89, 86, 85, 64, 84, 80, 492, 95, 72, 73, 93, 89, 87, 512, 79, 77]","[1697101605255, 1697101607269, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608761, 1697101608850, 1697101608937, 1697101609449, 1697101609528, 1697101609605]"
170,170,474,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612434,1697101616321.0,120,,,"[12, 675, 90, 86, 83, 81, 63, 80, 719, 95, 93, 92, 90, 69, 88, 633, 80, 78, 75]","[1697101612446, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613524, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615563, 1697101615641, 1697101615716]"
171,171,11,0,[],200,llama-13b,128,1,4822.0,1.0,1,H100,1697101590985,1697101595807.0,120,732.0,17.0,"[501, 1516, 272, 84, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 95, 95, 72]","[1697101591486, 1697101593002, 1697101593274, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806]"
172,172,515,1,[],200,llama-13b,128,1,1443.0,1.0,1,H100,1697101595990,1697101597433.0,120,11.0,1.0,"[43, 1400]","[1697101596033, 1697101597433]"
173,173,61,0,[],200,llama-13b,128,1,2021.0,1.0,1,H100,1697101590982,1697101593003.0,120,9.0,1.0,"[314, 1707]","[1697101591296, 1697101593003]"
174,174,89,0,[],200,llama-13b,128,1,5009.0,1.0,1,H100,1697101590979,1697101595988.0,120,52.0,20.0,"[48, 946, 43, 1258, 84, 60, 79, 750, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591027, 1697101591973, 1697101592016, 1697101593274, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
175,175,828,9,[],200,llama-13b,128,1,2349.0,1.0,1,H100,1697101616334,1697101618683.0,120,182.0,6.0,"[578, 1452, 81, 81, 78, 79]","[1697101616912, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618683]"
176,176,591,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598827,1697101601191.0,120,,,"[54, 885, 234, 100, 100, 101, 91, 89]","[1697101598881, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
177,177,775,0,[],200,llama-13b,128,1,993.0,1.0,1,H100,1697101590980,1697101591973.0,120,17.0,1.0,"[147, 846]","[1697101591127, 1697101591973]"
178,178,536,0,[],200,llama-13b,128,1,5534.0,1.0,1,H100,1697101590982,1697101596516.0,120,83.0,20.0,"[292, 1729, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 90, 527]","[1697101591274, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516]"
179,179,865,2,[],200,llama-13b,128,1,1154.0,1.0,1,H100,1697101597435,1697101598589.0,120,9.0,1.0,"[29, 1125]","[1697101597464, 1697101598589]"
180,180,266,3,[],200,llama-13b,128,1,1176.0,1.0,1,H100,1697101598590,1697101599766.0,120,9.0,1.0,"[25, 1151]","[1697101598615, 1697101599766]"
181,181,624,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601192.0,120,,,"[48, 1278]","[1697101599815, 1697101601093]"
182,182,427,0,[],200,llama-13b,128,1,2514.0,1.0,1,H100,1697101590984,1697101593498.0,120,58.0,5.0,"[420, 1599, 271, 84, 60, 80]","[1697101591404, 1697101593003, 1697101593274, 1697101593358, 1697101593418, 1697101593498]"
183,183,56,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601194,1697101604271.0,120,,,"[45, 872, 44, 1253, 101, 74]","[1697101601239, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
184,184,777,1,[],200,llama-13b,128,1,1721.0,1.0,1,H100,1697101593499,1697101595220.0,120,9.0,1.0,"[12, 1708]","[1697101593511, 1697101595219]"
185,185,537,1,[],200,llama-13b,128,1,4401.0,1.0,1,H100,1697101595900,1697101600301.0,120,83.0,20.0,"[6, 504, 107, 81, 74, 74, 905, 94, 89, 89, 87, 85, 731, 96, 89, 87, 84, 818, 100, 100, 101]","[1697101595906, 1697101596410, 1697101596517, 1697101596598, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301]"
186,186,865,1,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101596518,1697101597433.0,120,9.0,1.0,"[55, 860]","[1697101596573, 1697101597433]"
187,187,16,3,[],200,llama-13b,128,1,914.0,1.0,1,H100,1697101601197,1697101602111.0,120,9.0,1.0,"[82, 832]","[1697101601279, 1697101602111]"
188,188,207,1,[],200,llama-13b,128,1,1029.0,1.0,1,H100,1697101591974,1697101593003.0,120,10.0,1.0,"[25, 1004]","[1697101591999, 1697101593003]"
189,189,289,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597434,1697101601192.0,120,,,"[18, 1137, 236, 97, 89, 87, 84, 818, 101, 99, 101, 91, 88]","[1697101597452, 1697101598589, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600101, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
190,190,206,2,[],200,llama-13b,128,1,1189.0,1.0,1,H100,1697101595221,1697101596410.0,120,16.0,1.0,"[18, 1171]","[1697101595239, 1697101596410]"
191,191,373,4,[],200,llama-13b,128,1,1181.0,1.0,1,H100,1697101602112,1697101603293.0,120,15.0,1.0,"[30, 1151]","[1697101602142, 1697101603293]"
192,192,541,2,[],200,llama-13b,128,1,4647.0,1.0,1,H100,1697101593005,1697101597652.0,120,90.0,20.0,"[64, 889, 289, 98, 92, 89, 89, 86, 642, 102, 98, 96, 95, 72, 93, 89, 529, 80, 75, 74, 906]","[1697101593069, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
193,193,892,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600305,1697101601192.0,120,,,"[24, 764]","[1697101600329, 1697101601093]"
194,194,565,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596411,1697101601191.0,120,,,"[7, 1015, 219, 93, 89, 89, 88, 84, 730, 97, 90, 86, 84, 818, 100, 99, 102, 91, 88]","[1697101596418, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600392, 1697101600480]"
195,195,343,0,[],200,llama-13b,128,1,5534.0,1.0,1,H100,1697101590982,1697101596516.0,120,84.0,20.0,"[310, 1711, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 90, 527]","[1697101591292, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516]"
196,196,320,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604272.0,120,,,"[322, 1890, 101, 74]","[1697101601518, 1697101603408, 1697101603509, 1697101603583]"
197,197,899,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597653,1697101601192.0,120,,,"[18, 918, 237, 96, 89, 87, 84, 818, 101, 99, 101, 90, 90]","[1697101597671, 1697101598589, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600101, 1697101600200, 1697101600301, 1697101600391, 1697101600481]"
198,198,609,0,[],200,llama-13b,128,1,5009.0,1.0,1,H100,1697101590979,1697101595988.0,120,88.0,20.0,"[119, 875, 43, 1259, 83, 60, 79, 750, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89]","[1697101591098, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988]"
199,199,733,5,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101603295,1697101604194.0,120,31.0,1.0,"[77, 822]","[1697101603372, 1697101604194]"
200,200,13,0,[],200,llama-13b,128,1,5535.0,1.0,1,H100,1697101590981,1697101596516.0,120,90.0,20.0,"[217, 1804, 273, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89, 528]","[1697101591198, 1697101593002, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516]"
201,201,477,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590981,1697101601192.0,120,,,"[205, 787, 43, 1259, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89, 528, 81, 75, 74, 905, 93, 90, 88, 88, 85, 730, 96, 90, 87, 83, 819, 100, 99, 102, 90, 89]","[1697101591186, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597744, 1697101597834, 1697101597922, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600391, 1697101600480]"
202,202,697,1,[],200,llama-13b,128,1,2494.0,1.0,1,H100,1697101596518,1697101599012.0,120,123.0,10.0,"[10, 905, 219, 93, 89, 89, 88, 84, 730, 96, 91]","[1697101596528, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598921, 1697101599012]"
203,203,553,0,[],200,llama-13b,128,1,5534.0,1.0,1,H100,1697101590982,1697101596516.0,120,88.0,20.0,"[309, 1712, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 90, 527]","[1697101591291, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516]"
204,204,674,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604274,1697101610275.0,120,,,"[141, 779, 54, 1197, 81, 76, 74, 772, 89, 87, 84, 64, 84, 80, 491, 95, 73, 72, 94, 89, 88, 510, 79, 77]","[1697101604415, 1697101605194, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608938, 1697101609448, 1697101609527, 1697101609604]"
205,205,99,2,[],200,llama-13b,128,1,753.0,1.0,1,H100,1697101599013,1697101599766.0,120,10.0,1.0,"[12, 741]","[1697101599025, 1697101599766]"
206,206,802,0,[],200,llama-13b,128,1,2021.0,1.0,1,H100,1697101590982,1697101593003.0,120,9.0,1.0,"[240, 1781]","[1697101591222, 1697101593003]"
207,207,233,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,6.0,1.0,"[42, 911]","[1697101593047, 1697101593958]"
208,208,808,0,[],200,llama-13b,128,1,1037.0,1.0,1,H100,1697101590979,1697101592016.0,120,286.0,2.0,"[118, 876, 43]","[1697101591097, 1697101591973, 1697101592016]"
209,209,41,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101595990,1697101601191.0,120,,,"[25, 1418, 219, 93, 89, 89, 88, 84, 730, 97, 90, 86, 84, 818, 100, 100, 101, 91, 88]","[1697101596015, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
210,210,887,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101593005,1697101601193.0,120,,,"[12, 941, 289, 98, 92, 89, 89, 85, 643, 102, 99, 96, 94, 73, 92, 89, 529, 80, 75, 74, 906, 93, 89, 89, 87, 85, 730, 97, 89, 87, 83, 819, 100, 100, 101, 91, 88]","[1697101593017, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
211,211,404,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604270.0,120,,,"[37, 878, 44, 1253, 101, 74]","[1697101601233, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
212,212,854,0,[],200,llama-13b,128,1,6944.0,1.0,1,H100,1697101590979,1697101597923.0,120,67.0,29.0,"[30, 163, 22, 822, 1258, 83, 61, 79, 750, 98, 91, 90, 89, 85, 643, 102, 98, 96, 95, 72, 92, 90, 528, 81, 75, 74, 905, 93, 90, 89]","[1697101591009, 1697101591172, 1697101591194, 1697101592016, 1697101593274, 1697101593357, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594436, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595898, 1697101595988, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597744, 1697101597834, 1697101597923]"
213,213,477,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590981,1697101601193.0,120,,,"[222, 1799, 273, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 90, 527, 81, 75, 74, 905, 94, 89, 88, 88, 85, 730, 96, 90, 87, 83, 819, 100, 99, 102, 90, 89]","[1697101591203, 1697101593002, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597922, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600391, 1697101600480]"
214,214,561,2,[],200,llama-13b,128,1,4866.0,1.0,1,H100,1697101593959,1697101598825.0,120,87.0,20.0,"[18, 1243, 124, 101, 99, 95, 95, 72, 93, 89, 528, 81, 75, 74, 906, 93, 89, 89, 88, 84, 730]","[1697101593977, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825]"
215,215,225,0,[],200,llama-13b,128,1,1984.0,1.0,1,H100,1697101591019,1697101593003.0,120,23.0,1.0,"[590, 1394]","[1697101591609, 1697101593003]"
216,216,556,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,9.0,1.0,"[68, 885]","[1697101593073, 1697101593958]"
217,217,365,0,[],200,llama-13b,128,1,1984.0,1.0,1,H100,1697101591019,1697101593003.0,120,23.0,1.0,"[567, 1417]","[1697101591586, 1697101593003]"
218,218,149,0,[],200,llama-13b,128,1,3632.0,1.0,1,H100,1697101590983,1697101594615.0,120,563.0,10.0,"[389, 1631, 272, 83, 60, 79, 750, 98, 92, 89, 89]","[1697101591372, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615]"
219,219,4,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101605249,1697101610266.0,120,,,"[20, 2000, 179, 89, 86, 85, 64, 84, 80, 491, 96, 72, 72, 94, 90, 86, 512, 79, 77]","[1697101605269, 1697101607269, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608937, 1697101609449, 1697101609528, 1697101609605]"
220,220,723,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,14.0,1.0,"[30, 923]","[1697101593035, 1697101593958]"
221,221,153,2,[],200,llama-13b,128,1,1585.0,1.0,1,H100,1697101593959,1697101595544.0,120,335.0,4.0,"[12, 1248, 125, 101, 99]","[1697101593971, 1697101595219, 1697101595344, 1697101595445, 1697101595544]"
222,222,313,1,[],200,llama-13b,128,1,604.0,1.0,1,H100,1697101594616,1697101595220.0,120,20.0,1.0,"[6, 598]","[1697101594622, 1697101595220]"
223,223,39,0,[],200,llama-13b,128,1,2020.0,1.0,1,H100,1697101590984,1697101593004.0,120,8.0,1.0,"[426, 1593]","[1697101591410, 1697101593003]"
224,224,753,0,[],200,llama-13b,128,1,5533.0,1.0,1,H100,1697101590984,1697101596517.0,120,83.0,20.0,"[490, 1529, 271, 84, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 95, 95, 72, 93, 90, 527]","[1697101591474, 1697101593003, 1697101593274, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516]"
225,225,63,0,[],200,llama-13b,128,1,1984.0,1.0,1,H100,1697101591019,1697101593003.0,120,39.0,1.0,"[584, 1400]","[1697101591603, 1697101593003]"
226,226,284,1,[],200,llama-13b,128,1,6996.0,1.0,1,H100,1697101593004,1697101600000.0,120,90.0,31.0,"[7, 1236, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 92, 89, 529, 80, 75, 74, 905, 94, 89, 89, 88, 84, 730, 97, 89, 87, 83, 819]","[1697101593011, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599181, 1697101600000]"
227,227,624,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601194,1697101604270.0,120,,,"[27, 890, 44, 1253, 101, 74]","[1697101601221, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
228,228,509,1,[],200,llama-13b,128,1,1340.0,1.0,1,H100,1697101593005,1697101594345.0,120,286.0,3.0,"[30, 923, 289, 98]","[1697101593035, 1697101593958, 1697101594247, 1697101594345]"
229,229,866,2,[],200,llama-13b,128,1,4480.0,1.0,1,H100,1697101594346,1697101598826.0,120,93.0,20.0,"[6, 992, 101, 99, 96, 94, 73, 92, 90, 527, 82, 74, 74, 906, 93, 89, 89, 88, 84, 731]","[1697101594352, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596516, 1697101596598, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598826]"
230,230,183,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601191.0,120,,,"[25, 890, 219, 93, 89, 89, 88, 85, 729, 96, 91, 86, 83, 819, 100, 100, 101, 91, 89]","[1697101596543, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598921, 1697101599012, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
231,231,376,2,[],200,llama-13b,128,1,5080.0,1.0,1,H100,1697101595221,1697101600301.0,120,87.0,20.0,"[24, 1165, 107, 80, 75, 74, 905, 94, 90, 88, 88, 85, 730, 96, 89, 87, 84, 818, 100, 100, 101]","[1697101595245, 1697101596410, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597835, 1697101597923, 1697101598011, 1697101598096, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301]"
232,232,499,1,[],200,llama-13b,128,1,4299.0,1.0,1,H100,1697101594527,1697101598826.0,120,88.0,20.0,"[7, 686, 124, 101, 99, 96, 94, 73, 92, 90, 527, 81, 75, 74, 905, 94, 89, 89, 88, 85, 730]","[1697101594534, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598826]"
233,233,449,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101595990,1697101601191.0,120,,,"[13, 1430, 218, 94, 89, 89, 87, 85, 731, 96, 90, 86, 84, 818, 100, 100, 101, 90, 89]","[1697101596003, 1697101597433, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600391, 1697101600480]"
234,234,263,0,[],200,llama-13b,128,1,2019.0,1.0,1,H100,1697101590984,1697101593003.0,120,15.0,1.0,"[412, 1607]","[1697101591396, 1697101593003]"
235,235,618,1,[],200,llama-13b,128,1,952.0,1.0,1,H100,1697101593006,1697101593958.0,120,9.0,1.0,"[122, 830]","[1697101593128, 1697101593958]"
236,236,802,0,[],200,llama-13b,128,1,994.0,1.0,1,H100,1697101590979,1697101591973.0,120,9.0,1.0,"[42, 952]","[1697101591021, 1697101591973]"
237,237,228,0,[],200,llama-13b,128,1,5534.0,1.0,1,H100,1697101590982,1697101596516.0,120,100.0,20.0,"[286, 1735, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 90, 527]","[1697101591268, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516]"
238,238,295,3,[],200,llama-13b,128,1,1173.0,1.0,1,H100,1697101598827,1697101600000.0,120,52.0,2.0,"[42, 897, 234]","[1697101598869, 1697101599766, 1697101600000]"
239,239,47,2,[],200,llama-13b,128,1,4866.0,1.0,1,H100,1697101593959,1697101598825.0,120,90.0,20.0,"[36, 1225, 124, 101, 99, 96, 94, 72, 93, 90, 527, 81, 75, 74, 906, 93, 89, 89, 88, 84, 730]","[1697101593995, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825]"
240,240,253,1,[],200,llama-13b,128,1,4578.0,1.0,1,H100,1697101594248,1697101598826.0,120,67.0,20.0,"[12, 959, 125, 101, 99, 96, 94, 73, 92, 90, 527, 81, 75, 74, 906, 93, 89, 89, 88, 84, 731]","[1697101594260, 1697101595219, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598826]"
241,241,809,2,[],200,llama-13b,128,1,2097.0,1.0,1,H100,1697101601196,1697101603293.0,120,16.0,1.0,"[377, 1720]","[1697101601573, 1697101603293]"
242,242,654,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600001,1697101601192.0,120,,,"[19, 1073]","[1697101600020, 1697101601093]"
243,243,240,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[119, 780]","[1697101603414, 1697101604194]"
244,244,483,3,[],200,llama-13b,128,1,4661.0,1.0,1,H100,1697101595640,1697101600301.0,120,84.0,20.0,"[7, 763, 107, 81, 74, 74, 905, 94, 89, 89, 87, 85, 731, 96, 89, 87, 84, 818, 100, 100, 101]","[1697101595647, 1697101596410, 1697101596517, 1697101596598, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301]"
245,245,905,0,[],200,llama-13b,128,1,994.0,1.0,1,H100,1697101590979,1697101591973.0,120,11.0,1.0,"[88, 906]","[1697101591067, 1697101591973]"
246,246,86,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604271.0,120,,,"[514, 1582, 115, 101, 74]","[1697101601711, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
247,247,337,1,[],200,llama-13b,128,1,1029.0,1.0,1,H100,1697101591974,1697101593003.0,120,12.0,1.0,"[13, 1016]","[1697101591987, 1697101593003]"
248,248,696,2,[],200,llama-13b,128,1,4647.0,1.0,1,H100,1697101593005,1697101597652.0,120,83.0,20.0,"[117, 836, 289, 98, 92, 89, 89, 86, 643, 101, 98, 96, 95, 72, 93, 89, 528, 82, 74, 74, 906]","[1697101593122, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595344, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596598, 1697101596672, 1697101596746, 1697101597652]"
249,249,572,4,[],200,llama-13b,128,1,1927.0,1.0,1,H100,1697101604280,1697101606207.0,120,16.0,1.0,"[508, 1419]","[1697101604788, 1697101606207]"
250,250,926,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606208,1697101610267.0,120,,,"[13, 1048, 179, 90, 86, 84, 64, 84, 80, 491, 96, 72, 72, 94, 90, 86, 512, 78, 78]","[1697101606221, 1697101607269, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608937, 1697101609449, 1697101609527, 1697101609605]"
251,251,413,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604275,1697101610275.0,120,,,"[193, 726, 54, 1198, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 73, 73, 93, 89, 88, 510, 79, 77]","[1697101604468, 1697101605194, 1697101605248, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608668, 1697101608761, 1697101608850, 1697101608938, 1697101609448, 1697101609527, 1697101609604]"
252,252,262,0,[],200,llama-13b,128,1,994.0,1.0,1,H100,1697101590979,1697101591973.0,120,39.0,1.0,"[38, 956]","[1697101591017, 1697101591973]"
253,253,751,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101593005,1697101601193.0,120,,,"[87, 1155, 98, 92, 89, 89, 86, 642, 102, 98, 96, 95, 72, 93, 89, 529, 80, 75, 74, 906, 93, 89, 89, 88, 84, 730, 97, 89, 87, 84, 818, 100, 100, 101, 90, 89]","[1697101593092, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600391, 1697101600480]"
254,254,276,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607937,1697101610270.0,120,,,"[13, 1499, 79, 77]","[1697101607950, 1697101609449, 1697101609528, 1697101609605]"
255,255,356,6,[],200,llama-13b,128,1,819.0,1.0,1,H100,1697101610273,1697101611092.0,120,874.0,2.0,"[153, 620, 46]","[1697101610426, 1697101611046, 1697101611092]"
256,256,212,3,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101597653,1697101598589.0,120,31.0,1.0,"[61, 875]","[1697101597714, 1697101598589]"
257,257,618,1,[],200,llama-13b,128,1,1029.0,1.0,1,H100,1697101591975,1697101593004.0,120,9.0,1.0,"[36, 992]","[1697101592011, 1697101593003]"
258,258,639,0,[],200,llama-13b,128,1,2518.0,1.0,1,H100,1697101590980,1697101593498.0,120,100.0,6.0,"[129, 864, 43, 1259, 83, 60, 80]","[1697101591109, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593498]"
259,259,714,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101611093,1697101616320.0,120,,,"[6, 1915, 107, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 88, 633, 79, 78, 76]","[1697101611099, 1697101613014, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
260,260,43,2,[],200,llama-13b,128,1,2338.0,1.0,1,H100,1697101593005,1697101595343.0,120,732.0,8.0,"[62, 891, 289, 98, 92, 89, 89, 85, 643]","[1697101593067, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343]"
261,261,570,4,[],200,llama-13b,128,1,1176.0,1.0,1,H100,1697101598590,1697101599766.0,120,18.0,1.0,"[30, 1146]","[1697101598620, 1697101599766]"
262,262,634,7,[],200,llama-13b,128,1,1793.0,1.0,1,H100,1697101610276,1697101612069.0,120,13.0,1.0,"[418, 1375]","[1697101610694, 1697101612069]"
263,263,0,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601192.0,120,,,"[30, 1296]","[1697101599797, 1697101601093]"
264,264,836,0,[],200,llama-13b,128,1,1984.0,1.0,1,H100,1697101591019,1697101593003.0,120,11.0,1.0,"[561, 1423]","[1697101591580, 1697101593003]"
265,265,358,6,[],200,llama-13b,128,1,2312.0,1.0,1,H100,1697101601197,1697101603509.0,120,216.0,3.0,"[418, 1677, 116, 101]","[1697101601615, 1697101603292, 1697101603408, 1697101603509]"
266,266,719,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603510,1697101604272.0,120,,,[6],[1697101603516]
267,267,68,1,[],200,llama-13b,128,1,1721.0,1.0,1,H100,1697101593499,1697101595220.0,120,12.0,1.0,"[6, 1714]","[1697101593505, 1697101595219]"
268,268,116,8,[],200,llama-13b,128,1,1927.0,1.0,1,H100,1697101604280,1697101606207.0,120,23.0,1.0,"[402, 1525]","[1697101604682, 1697101606207]"
269,269,396,2,[],200,llama-13b,128,1,5081.0,1.0,1,H100,1697101595220,1697101600301.0,120,89.0,20.0,"[7, 1183, 106, 81, 75, 74, 905, 94, 89, 89, 88, 85, 730, 96, 89, 87, 84, 818, 100, 100, 101]","[1697101595227, 1697101596410, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301]"
270,270,474,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606209,1697101610267.0,120,,,"[54, 1185, 90, 86, 84, 65, 83, 80, 491, 96, 72, 72, 94, 90, 87, 511, 78, 78]","[1697101606263, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609605]"
271,271,267,1,[],200,llama-13b,128,1,4647.0,1.0,1,H100,1697101593005,1697101597652.0,120,83.0,20.0,"[36, 917, 289, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89, 529, 80, 75, 74, 906]","[1697101593041, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
272,272,371,0,[],200,llama-13b,128,1,2020.0,1.0,1,H100,1697101590983,1697101593003.0,120,13.0,1.0,"[327, 1693]","[1697101591310, 1697101593003]"
273,273,915,1,[],200,llama-13b,128,1,1029.0,1.0,1,H100,1697101591974,1697101593003.0,120,182.0,1.0,"[7, 1022]","[1697101591981, 1697101593003]"
274,274,63,8,[],200,llama-13b,128,1,943.0,1.0,1,H100,1697101612071,1697101613014.0,120,39.0,1.0,"[99, 844]","[1697101612170, 1697101613014]"
275,275,833,10,[],200,llama-13b,128,1,2849.0,1.0,1,H100,1697101610272,1697101613121.0,120,563.0,8.0,"[154, 620, 46, 1094, 84, 82, 81, 77, 611]","[1697101610426, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121]"
276,276,724,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,11.0,1.0,"[111, 842]","[1697101593116, 1697101593958]"
277,277,414,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613015,1697101616319.0,120,,,"[18, 1104, 186, 95, 93, 92, 90, 69, 87, 634, 80, 77, 77]","[1697101613033, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615563, 1697101615640, 1697101615717]"
278,278,345,2,[],200,llama-13b,128,1,4647.0,1.0,1,H100,1697101593005,1697101597652.0,120,39.0,20.0,"[93, 860, 289, 98, 92, 89, 89, 86, 642, 102, 98, 96, 95, 72, 93, 89, 529, 80, 75, 74, 906]","[1697101593098, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
279,279,126,2,[],200,llama-13b,128,1,1261.0,1.0,1,H100,1697101593959,1697101595220.0,120,19.0,1.0,"[36, 1225]","[1697101593995, 1697101595220]"
280,280,143,8,[],200,llama-13b,128,1,3440.0,1.0,1,H100,1697101616332,1697101619772.0,120,6.0,12.0,"[280, 1643, 109, 81, 81, 78, 78, 669, 95, 88, 67, 86, 85]","[1697101616612, 1697101618255, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772]"
281,281,743,0,[],200,llama-13b,128,1,2439.0,1.0,1,H100,1697101590979,1697101593418.0,120,123.0,6.0,"[13, 179, 23, 822, 1258, 83, 61]","[1697101590992, 1697101591171, 1697101591194, 1697101592016, 1697101593274, 1697101593357, 1697101593418]"
282,282,480,3,[],200,llama-13b,128,1,1189.0,1.0,1,H100,1697101595221,1697101596410.0,120,26.0,1.0,"[30, 1159]","[1697101595251, 1697101596410]"
283,283,280,0,[],200,llama-13b,128,1,5008.0,1.0,1,H100,1697101590981,1697101595989.0,120,91.0,20.0,"[211, 781, 43, 1259, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 90]","[1697101591192, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989]"
284,284,42,0,[],200,llama-13b,128,1,2020.0,1.0,1,H100,1697101590984,1697101593004.0,120,10.0,1.0,"[494, 1525]","[1697101591478, 1697101593003]"
285,285,697,1,[],200,llama-13b,128,1,3022.0,1.0,1,H100,1697101595990,1697101599012.0,120,123.0,10.0,"[19, 1424, 219, 93, 89, 89, 88, 84, 731, 96, 90]","[1697101596009, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598826, 1697101598922, 1697101599012]"
286,286,840,4,[],200,llama-13b,128,1,1022.0,1.0,1,H100,1697101596411,1697101597433.0,120,17.0,1.0,"[18, 1004]","[1697101596429, 1697101597433]"
287,287,691,0,[],200,llama-13b,128,1,2020.0,1.0,1,H100,1697101590984,1697101593004.0,120,47.0,1.0,"[430, 1589]","[1697101591414, 1697101593003]"
288,288,121,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,13.0,1.0,"[110, 843]","[1697101593115, 1697101593958]"
289,289,389,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,8.0,1.0,"[76, 877]","[1697101593081, 1697101593958]"
290,290,396,1,[],200,llama-13b,128,1,4647.0,1.0,1,H100,1697101593005,1697101597652.0,120,89.0,20.0,"[105, 848, 289, 98, 92, 89, 89, 86, 643, 101, 98, 96, 95, 72, 93, 89, 528, 82, 74, 74, 906]","[1697101593110, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595344, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596598, 1697101596672, 1697101596746, 1697101597652]"
291,291,507,0,[],200,llama-13b,128,1,5498.0,1.0,1,H100,1697101591019,1697101596517.0,120,83.0,20.0,"[578, 1406, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 92, 90, 528]","[1697101591597, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596517]"
292,292,533,1,[],200,llama-13b,128,1,1662.0,1.0,1,H100,1697101595990,1697101597652.0,120,216.0,2.0,"[7, 1655]","[1697101595997, 1697101597652]"
293,293,479,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101593959,1697101601191.0,120,,,"[24, 1237, 124, 101, 99, 95, 95, 72, 93, 90, 527, 81, 75, 74, 906, 93, 89, 89, 88, 84, 730, 97, 89, 87, 84, 818, 100, 100, 101, 91, 88]","[1697101593983, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
294,294,272,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597434,1697101601192.0,120,,,"[30, 1125, 237, 96, 89, 87, 84, 818, 101, 99, 101, 90, 89]","[1697101597464, 1697101598589, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600101, 1697101600200, 1697101600301, 1697101600391, 1697101600480]"
295,295,718,1,[],200,llama-13b,128,1,1030.0,1.0,1,H100,1697101591974,1697101593004.0,120,13.0,1.0,"[31, 998]","[1697101592005, 1697101593003]"
296,296,149,2,[],200,llama-13b,128,1,2539.0,1.0,1,H100,1697101593005,1697101595544.0,120,563.0,10.0,"[104, 849, 289, 98, 92, 89, 89, 86, 643, 101, 98]","[1697101593109, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595344, 1697101595445, 1697101595543]"
297,297,510,3,[],200,llama-13b,128,1,972.0,1.0,1,H100,1697101595545,1697101596517.0,120,79.0,2.0,"[6, 859, 107]","[1697101595551, 1697101596410, 1697101596517]"
298,298,889,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597653,1697101601192.0,120,,,"[31, 905, 237, 96, 89, 87, 84, 818, 100, 100, 101, 91, 89]","[1697101597684, 1697101598589, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
299,299,871,4,[],200,llama-13b,128,1,1493.0,1.0,1,H100,1697101596518,1697101598011.0,120,123.0,6.0,"[43, 872, 219, 93, 89, 89, 88]","[1697101596561, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011]"
300,300,273,5,[],200,llama-13b,128,1,578.0,1.0,1,H100,1697101598012,1697101598590.0,120,19.0,1.0,"[12, 565]","[1697101598024, 1697101598589]"
301,301,626,6,[],200,llama-13b,128,1,1175.0,1.0,1,H100,1697101598591,1697101599766.0,120,10.0,1.0,"[30, 1145]","[1697101598621, 1697101599766]"
302,302,51,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601191.0,120,,,"[12, 1314]","[1697101599779, 1697101601093]"
303,303,410,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[186, 1910, 116, 101, 74]","[1697101601382, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
304,304,768,9,[],200,llama-13b,128,1,2401.0,1.0,1,H100,1697101604275,1697101606676.0,120,47.0,6.0,"[122, 798, 53, 1197, 81, 76, 74]","[1697101604397, 1697101605195, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676]"
305,305,754,2,[],200,llama-13b,128,1,1848.0,1.0,1,H100,1697101593959,1697101595807.0,120,88.0,7.0,"[54, 1331, 101, 99, 96, 94, 72]","[1697101594013, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595806]"
306,306,693,0,[],200,llama-13b,128,1,2256.0,1.0,1,H100,1697101591019,1697101593275.0,120,67.0,2.0,"[597, 1387, 272]","[1697101591616, 1697101593003, 1697101593275]"
307,307,285,0,[],200,llama-13b,128,1,6941.0,1.0,1,H100,1697101590982,1697101597923.0,120,100.0,27.0,"[321, 1700, 272, 82, 61, 79, 750, 98, 92, 89, 89, 85, 644, 101, 98, 96, 95, 72, 93, 90, 527, 81, 75, 74, 905, 94, 89, 89]","[1697101591303, 1697101593003, 1697101593275, 1697101593357, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923]"
308,308,14,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601192.0,120,,,"[49, 866, 219, 93, 89, 89, 88, 85, 729, 97, 89, 87, 84, 818, 100, 100, 101, 91, 89]","[1697101596567, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
309,309,810,2,[],200,llama-13b,128,1,4388.0,1.0,1,H100,1697101594438,1697101598826.0,120,91.0,20.0,"[6, 776, 124, 101, 99, 96, 94, 73, 92, 90, 527, 81, 75, 74, 906, 93, 89, 89, 88, 84, 731]","[1697101594444, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598826]"
310,310,913,2,[],200,llama-13b,128,1,4867.0,1.0,1,H100,1697101593959,1697101598826.0,120,88.0,20.0,"[48, 1213, 124, 101, 99, 95, 95, 72, 93, 90, 527, 81, 75, 74, 906, 93, 89, 89, 88, 84, 730]","[1697101594007, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825]"
311,311,185,3,[],200,llama-13b,128,1,4493.0,1.0,1,H100,1697101595808,1697101600301.0,120,93.0,20.0,"[6, 703, 80, 75, 74, 905, 94, 89, 89, 87, 85, 731, 96, 89, 87, 84, 818, 100, 100, 101]","[1697101595814, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301]"
312,312,639,0,[],200,llama-13b,128,1,3270.0,1.0,1,H100,1697101590977,1697101594247.0,120,100.0,6.0,"[531, 1495, 272, 83, 60, 80, 749]","[1697101591508, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247]"
313,313,71,1,[],200,llama-13b,128,1,2349.0,1.0,1,H100,1697101594248,1697101596597.0,120,364.0,11.0,"[7, 964, 125, 101, 99, 96, 94, 72, 93, 90, 527, 81]","[1697101594255, 1697101595219, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597]"
314,314,701,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101593005,1697101601193.0,120,,,"[81, 872, 289, 98, 92, 89, 89, 86, 642, 102, 98, 96, 95, 72, 93, 89, 529, 80, 75, 74, 906, 93, 89, 89, 88, 84, 730, 97, 89, 87, 83, 819, 100, 100, 101, 91, 88]","[1697101593086, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
315,315,543,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600308,1697101601192.0,120,,,"[33, 752]","[1697101600341, 1697101601093]"
316,316,396,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596598,1697101601192.0,120,,,"[6, 829, 219, 93, 89, 89, 88, 85, 729, 97, 89, 87, 84, 818, 100, 100, 101, 91, 88]","[1697101596604, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
317,317,897,5,[],200,llama-13b,128,1,2095.0,1.0,1,H100,1697101601198,1697101603293.0,120,9.0,1.0,"[495, 1599]","[1697101601693, 1697101603292]"
318,318,294,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603294,1697101604272.0,120,,,[55],[1697101603349]
319,319,652,7,[],200,llama-13b,128,1,1926.0,1.0,1,H100,1697101604281,1697101606207.0,120,14.0,1.0,"[530, 1396]","[1697101604811, 1697101606207]"
320,320,81,8,[],200,llama-13b,128,1,2553.0,1.0,1,H100,1697101606208,1697101608761.0,120,732.0,13.0,"[36, 1026, 178, 90, 86, 84, 65, 83, 80, 491, 96, 72, 73, 93]","[1697101606244, 1697101607270, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608668, 1697101608761]"
321,321,446,9,[],200,llama-13b,128,1,596.0,1.0,1,H100,1697101608762,1697101609358.0,120,26.0,1.0,"[7, 589]","[1697101608769, 1697101609358]"
322,322,891,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609358,1697101610267.0,120,,,"[7, 739]","[1697101609365, 1697101610104]"
323,323,322,11,[],200,llama-13b,128,1,4576.0,1.0,1,H100,1697101610273,1697101614849.0,120,93.0,20.0,"[207, 1706, 84, 82, 81, 77, 611, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87]","[1697101610480, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
324,324,38,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601192.0,120,,,"[18, 897, 219, 93, 89, 89, 88, 84, 730, 96, 91, 86, 83, 819, 100, 100, 101, 91, 89]","[1697101596536, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598921, 1697101599012, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
325,325,80,4,[],200,llama-13b,128,1,788.0,1.0,1,H100,1697101600305,1697101601093.0,120,13.0,1.0,"[18, 770]","[1697101600323, 1697101601093]"
326,326,408,5,[],200,llama-13b,128,1,278.0,1.0,1,H100,1697101601094,1697101601372.0,120,16.0,1.0,"[24, 254]","[1697101601118, 1697101601372]"
327,327,762,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601373,1697101604271.0,120,,,"[421, 1499, 115, 101, 75]","[1697101601794, 1697101603293, 1697101603408, 1697101603509, 1697101603584]"
328,328,196,7,[],200,llama-13b,128,1,1932.0,1.0,1,H100,1697101604275,1697101606207.0,120,13.0,1.0,"[317, 1615]","[1697101604592, 1697101606207]"
329,329,755,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604270.0,120,,,"[31, 884, 44, 1253, 101, 74]","[1697101601227, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
330,330,553,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606208,1697101610267.0,120,,,"[30, 1032, 178, 90, 86, 84, 64, 84, 80, 491, 96, 72, 72, 94, 90, 86, 512, 78, 78]","[1697101606238, 1697101607270, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608937, 1697101609449, 1697101609527, 1697101609605]"
331,331,675,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616322.0,120,,,"[19, 1424]","[1697101614871, 1697101616295]"
332,332,180,4,[],200,llama-13b,128,1,3581.0,1.0,1,H100,1697101604275,1697101607856.0,120,123.0,12.0,"[38, 881, 54, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84]","[1697101604313, 1697101605194, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856]"
333,333,100,13,[],200,llama-13b,128,1,3522.0,1.0,1,H100,1697101616332,1697101619854.0,120,732.0,14.0,"[153, 892, 48, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82]","[1697101616485, 1697101617377, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854]"
334,334,537,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604272.0,120,,,"[279, 1816, 116, 101, 74]","[1697101601476, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
335,335,614,0,[],200,llama-13b,128,1,2022.0,1.0,1,H100,1697101590981,1697101593003.0,120,15.0,1.0,"[223, 1799]","[1697101591204, 1697101593003]"
336,336,47,1,[],200,llama-13b,128,1,4648.0,1.0,1,H100,1697101593004,1697101597652.0,120,90.0,20.0,"[71, 883, 289, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89, 529, 80, 75, 74, 906]","[1697101593075, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
337,337,869,3,[],200,llama-13b,128,1,3581.0,1.0,1,H100,1697101604275,1697101607856.0,120,244.0,12.0,"[120, 800, 53, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84]","[1697101604395, 1697101605195, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856]"
338,338,644,2,[],200,llama-13b,128,1,1092.0,1.0,1,H100,1697101600001,1697101601093.0,120,19.0,1.0,"[18, 1074]","[1697101600019, 1697101601093]"
339,339,72,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601094,1697101604270.0,120,,,"[6, 272, 783, 1253, 101, 74]","[1697101601100, 1697101601372, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
340,340,430,1,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101596518,1697101597433.0,120,15.0,1.0,"[42, 873]","[1697101596560, 1697101597433]"
341,341,789,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597434,1697101601192.0,120,,,"[12, 1143, 236, 97, 89, 87, 84, 818, 100, 100, 101, 91, 88]","[1697101597446, 1697101598589, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
342,342,460,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619855,1697101623659.0,120,,,"[7, 1811, 85, 81, 79, 79, 60, 614, 88, 86, 84, 83]","[1697101619862, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
343,343,214,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[37, 878, 44, 1253, 101, 74]","[1697101601233, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
344,344,568,4,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101604275,1697101605195.0,120,11.0,1.0,"[92, 827]","[1697101604367, 1697101605194]"
345,345,899,5,[],200,llama-13b,128,1,4253.0,1.0,1,H100,1697101605196,1697101609449.0,120,100.0,20.0,"[24, 987, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 72, 94, 89, 87, 512]","[1697101605220, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608937, 1697101609449]"
346,346,400,3,[],200,llama-13b,128,1,1654.0,1.0,1,H100,1697101598827,1697101600481.0,120,123.0,7.0,"[6, 933, 234, 100, 100, 101, 91, 89]","[1697101598833, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
347,347,719,0,[],200,llama-13b,128,1,3270.0,1.0,1,H100,1697101590977,1697101594247.0,120,182.0,6.0,"[503, 1794, 84, 60, 80, 749]","[1697101591480, 1697101593274, 1697101593358, 1697101593418, 1697101593498, 1697101594247]"
348,348,734,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600482,1697101604270.0,120,,,"[6, 884, 783, 1253, 101, 74]","[1697101600488, 1697101601372, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
349,349,331,6,[],200,llama-13b,128,1,653.0,1.0,1,H100,1697101609451,1697101610104.0,120,26.0,1.0,"[42, 611]","[1697101609493, 1697101610104]"
350,350,163,5,[],200,llama-13b,128,1,2402.0,1.0,1,H100,1697101604274,1697101606676.0,120,67.0,6.0,"[195, 725, 54, 1198, 80, 76, 74]","[1697101604469, 1697101605194, 1697101605248, 1697101606446, 1697101606526, 1697101606602, 1697101606676]"
351,351,148,1,[],200,llama-13b,128,1,972.0,1.0,1,H100,1697101594248,1697101595220.0,120,16.0,1.0,"[18, 954]","[1697101594266, 1697101595220]"
352,352,690,7,[],200,llama-13b,128,1,375.0,1.0,1,H100,1697101610105,1697101610480.0,120,39.0,1.0,"[36, 339]","[1697101610141, 1697101610480]"
353,353,509,2,[],200,llama-13b,128,1,1376.0,1.0,1,H100,1697101595221,1697101596597.0,120,286.0,3.0,"[12, 1177, 107, 80]","[1697101595233, 1697101596410, 1697101596517, 1697101596597]"
354,354,868,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596598,1697101601192.0,120,,,"[7, 828, 219, 93, 89, 89, 88, 85, 729, 97, 89, 87, 84, 818, 100, 100, 101, 91, 88]","[1697101596605, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
355,355,771,7,[],200,llama-13b,128,1,4570.0,1.0,1,H100,1697101610279,1697101614849.0,120,47.0,20.0,"[525, 1264, 118, 84, 82, 81, 77, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 70, 86]","[1697101610804, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614849]"
356,356,98,1,[],200,llama-13b,128,1,539.0,1.0,1,H100,1697101593419,1697101593958.0,120,14.0,1.0,"[6, 533]","[1697101593425, 1697101593958]"
357,357,522,6,[],200,llama-13b,128,1,1655.0,1.0,1,H100,1697101606678,1697101608333.0,120,20.0,1.0,"[24, 1631]","[1697101606702, 1697101608333]"
358,358,805,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590985,1697101601193.0,120,,,"[513, 1505, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 95, 95, 72, 93, 90, 528, 80, 75, 74, 905, 94, 89, 89, 87, 85, 730, 96, 90, 87, 83, 819, 100, 99, 102, 90, 89]","[1697101591498, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600391, 1697101600480]"
359,359,884,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608334,1697101610266.0,120,,,"[12, 1012, 91, 79, 77]","[1697101608346, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
360,360,122,8,[],200,llama-13b,128,1,4369.0,1.0,1,H100,1697101610481,1697101614850.0,120,88.0,20.0,"[406, 1182, 117, 84, 82, 81, 78, 609, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 70, 87]","[1697101610887, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614850]"
361,361,459,2,[],200,llama-13b,128,1,1681.0,1.0,1,H100,1697101593959,1697101595640.0,120,58.0,5.0,"[54, 1207, 124, 101, 99, 96]","[1697101594013, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595640]"
362,362,818,3,[],200,llama-13b,128,1,769.0,1.0,1,H100,1697101595641,1697101596410.0,120,13.0,1.0,"[12, 757]","[1697101595653, 1697101596410]"
363,363,242,4,[],200,llama-13b,128,1,2511.0,1.0,1,H100,1697101596411,1697101598922.0,120,345.0,9.0,"[7, 1015, 219, 93, 89, 89, 88, 84, 730, 97]","[1697101596418, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922]"
364,364,597,5,[],200,llama-13b,128,1,843.0,1.0,1,H100,1697101598923,1697101599766.0,120,39.0,1.0,"[12, 831]","[1697101598935, 1697101599766]"
365,365,927,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601191.0,120,,,"[12, 1314]","[1697101599779, 1697101601093]"
366,366,359,7,[],200,llama-13b,128,1,2096.0,1.0,1,H100,1697101601197,1697101603293.0,120,10.0,1.0,"[285, 1811]","[1697101601482, 1697101603293]"
367,367,721,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603294,1697101604272.0,120,,,"[90, 810]","[1697101603384, 1697101604194]"
368,368,151,9,[],200,llama-13b,128,1,1926.0,1.0,1,H100,1697101604281,1697101606207.0,120,39.0,1.0,"[610, 1316]","[1697101604891, 1697101606207]"
369,369,595,10,[],200,llama-13b,128,1,1061.0,1.0,1,H100,1697101606209,1697101607270.0,120,8.0,1.0,"[61, 1000]","[1697101606270, 1697101607270]"
370,370,22,11,[],200,llama-13b,128,1,1062.0,1.0,1,H100,1697101607271,1697101608333.0,120,16.0,1.0,"[41, 1021]","[1697101607312, 1697101608333]"
371,371,376,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608334,1697101610266.0,120,,,"[36, 988, 91, 79, 77]","[1697101608370, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
372,372,698,1,[],200,llama-13b,128,1,2021.0,1.0,1,H100,1697101595990,1697101598011.0,120,182.0,6.0,"[7, 1436, 218, 94, 89, 89, 87]","[1697101595997, 1697101597433, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010]"
373,373,766,7,[],200,llama-13b,128,1,653.0,1.0,1,H100,1697101609451,1697101610104.0,120,11.0,1.0,"[48, 605]","[1697101609499, 1697101610104]"
374,374,733,13,[],200,llama-13b,128,1,775.0,1.0,1,H100,1697101610271,1697101611046.0,120,31.0,1.0,"[107, 668]","[1697101610378, 1697101611046]"
375,375,24,2,[],200,llama-13b,128,1,2547.0,1.0,1,H100,1697101597653,1697101600200.0,120,79.0,9.0,"[36, 1137, 96, 90, 86, 84, 818, 100, 100]","[1697101597689, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200]"
376,376,167,14,[],200,llama-13b,128,1,3803.0,1.0,1,H100,1697101611047,1697101614850.0,120,88.0,20.0,"[24, 998, 117, 84, 82, 81, 78, 609, 90, 86, 84, 81, 62, 80, 721, 94, 93, 92, 90, 70, 87]","[1697101611071, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613120, 1697101613210, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614324, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614850]"
377,377,858,0,[],200,llama-13b,128,1,3636.0,1.0,1,H100,1697101590979,1697101594615.0,120,182.0,12.0,"[13, 202, 822, 1258, 83, 61, 79, 750, 98, 91, 90, 89]","[1697101590992, 1697101591194, 1697101592016, 1697101593274, 1697101593357, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594436, 1697101594526, 1697101594615]"
378,378,384,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600201,1697101601192.0,120,,,"[6, 886]","[1697101600207, 1697101601093]"
379,379,733,4,[],200,llama-13b,128,1,2099.0,1.0,1,H100,1697101601194,1697101603293.0,120,31.0,1.0,"[403, 1696]","[1697101601597, 1697101603293]"
380,380,133,5,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101603295,1697101604194.0,120,15.0,1.0,"[101, 798]","[1697101603396, 1697101604194]"
381,381,589,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619773,1697101623658.0,120,,,"[6, 600, 196, 95, 94, 93, 92, 92, 73, 644, 81, 79, 79, 60, 614, 88, 86, 84, 83]","[1697101619779, 1697101620379, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620949, 1697101621041, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
382,382,492,6,[],200,llama-13b,128,1,4742.0,1.0,1,H100,1697101604195,1697101608937.0,120,47.0,20.0,"[6, 311, 736, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73, 94, 89, 87]","[1697101604201, 1697101604512, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667, 1697101608761, 1697101608850, 1697101608937]"
383,383,130,2,[],200,llama-13b,128,1,577.0,1.0,1,H100,1697101598012,1697101598589.0,120,14.0,1.0,"[6, 571]","[1697101598018, 1697101598589]"
384,384,486,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598591,1697101601193.0,120,,,"[35, 1140, 234, 100, 100, 101, 91, 89]","[1697101598626, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
385,385,621,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597653,1697101601192.0,120,,,"[25, 911, 237, 96, 90, 86, 84, 818, 101, 99, 101, 91, 89]","[1697101597678, 1697101598589, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600101, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
386,386,499,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614853,1697101616319.0,120,,,"[133, 1309]","[1697101614986, 1697101616295]"
387,387,361,6,[],200,llama-13b,128,1,2239.0,1.0,1,H100,1697101610271,1697101612510.0,120,67.0,7.0,"[201, 574, 46, 1094, 84, 82, 81, 77]","[1697101610472, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510]"
388,388,857,16,[],200,llama-13b,128,1,1920.0,1.0,1,H100,1697101616336,1697101618256.0,120,18.0,1.0,"[554, 1366]","[1697101616890, 1697101618256]"
389,389,287,17,[],200,llama-13b,128,1,831.0,1.0,1,H100,1697101618257,1697101619088.0,120,10.0,1.0,"[25, 806]","[1697101618282, 1697101619088]"
390,390,49,3,[],200,llama-13b,128,1,2211.0,1.0,1,H100,1697101601197,1697101603408.0,120,109.0,3.0,"[135, 779, 48, 1249]","[1697101601332, 1697101602111, 1697101602159, 1697101603408]"
391,391,638,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619090,1697101623658.0,120,,,"[6, 1282, 196, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 60, 614, 87, 87, 84, 83]","[1697101619096, 1697101620378, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
392,392,921,0,[],200,llama-13b,128,1,2019.0,1.0,1,H100,1697101590984,1697101593003.0,120,31.0,1.0,"[408, 1611]","[1697101591392, 1697101593003]"
393,393,257,10,[],200,llama-13b,128,1,1694.0,1.0,1,H100,1697101618684,1697101620378.0,120,14.0,1.0,"[12, 1682]","[1697101618696, 1697101620378]"
394,394,405,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603409,1697101604272.0,120,,,"[81, 704]","[1697101603490, 1697101604194]"
395,395,645,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601194,1697101604272.0,120,,,"[200, 1898, 116, 101, 74]","[1697101601394, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
396,396,737,5,[],200,llama-13b,128,1,2166.0,1.0,1,H100,1697101604280,1697101606446.0,120,216.0,2.0,"[502, 1425, 238]","[1697101604782, 1697101606207, 1697101606445]"
397,397,68,19,[],200,llama-13b,128,1,2029.0,1.0,1,H100,1697101623665,1697101625694.0,120,12.0,1.0,"[212, 1817]","[1697101623877, 1697101625694]"
398,398,396,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625696,1697101626656.0,120,,,"[31, 839]","[1697101625727, 1697101626566]"
399,399,346,1,[],200,llama-13b,128,1,4647.0,1.0,1,H100,1697101593005,1697101597652.0,120,85.0,20.0,"[99, 854, 289, 98, 92, 89, 89, 86, 642, 102, 98, 96, 95, 72, 93, 89, 528, 81, 75, 74, 906]","[1697101593104, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
400,400,761,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628904.0,120,,,"[226, 1802]","[1697101626890, 1697101628692]"
401,401,191,22,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628909,1697101633688.0,120,85.0,20.0,"[266, 1455, 104, 81, 68, 66, 865, 92, 91, 89, 89, 88, 84, 82, 743, 94, 72, 94, 93, 92, 71]","[1697101629175, 1697101630630, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
402,402,167,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606446,1697101610267.0,120,,,"[7, 817, 178, 90, 86, 84, 65, 83, 80, 491, 95, 73, 73, 94, 89, 87, 511, 78, 78]","[1697101606453, 1697101607270, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609605]"
403,403,116,0,[],200,llama-13b,128,1,993.0,1.0,1,H100,1697101590980,1697101591973.0,120,23.0,1.0,"[142, 851]","[1697101591122, 1697101591973]"
404,404,372,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601191.0,120,,,"[19, 896, 219, 93, 89, 89, 88, 85, 729, 96, 91, 86, 83, 819, 100, 100, 101, 91, 89]","[1697101596537, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598921, 1697101599012, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
405,405,704,2,[],200,llama-13b,128,1,2096.0,1.0,1,H100,1697101601197,1697101603293.0,120,14.0,1.0,"[285, 1811]","[1697101601482, 1697101603293]"
406,406,289,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606678,1697101610268.0,120,,,"[12, 1643, 94, 96, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101606690, 1697101608333, 1697101608427, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
407,407,897,4,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101601196,1697101602111.0,120,9.0,1.0,"[124, 791]","[1697101601320, 1697101602111]"
408,408,702,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597653,1697101601193.0,120,,,"[48, 888, 237, 96, 90, 86, 84, 818, 100, 100, 101, 90, 90]","[1697101597701, 1697101598589, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600391, 1697101600481]"
409,409,549,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,[97],[1697101633787]
410,410,478,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101591975,1697101601193.0,120,,,"[36, 992, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 91, 90, 529, 81, 74, 74, 905, 94, 89, 89, 87, 85, 730, 96, 90, 87, 83, 819, 100, 100, 101, 91, 88]","[1697101592011, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595898, 1697101595988, 1697101596517, 1697101596598, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
411,411,903,24,[],200,llama-13b,128,1,2852.0,1.0,1,H100,1697101634999,1697101637851.0,120,244.0,7.0,"[279, 1530, 89, 87, 56, 628, 91, 92]","[1697101635278, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851]"
412,412,102,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604271.0,120,,,"[173, 741, 47, 1250, 101, 74]","[1697101601370, 1697101602111, 1697101602158, 1697101603408, 1697101603509, 1697101603583]"
413,413,331,5,[],200,llama-13b,128,1,1181.0,1.0,1,H100,1697101602112,1697101603293.0,120,26.0,1.0,"[18, 1163]","[1697101602130, 1697101603293]"
414,414,389,0,[],200,llama-13b,128,1,996.0,1.0,1,H100,1697101590977,1697101591973.0,120,8.0,1.0,"[126, 870]","[1697101591103, 1697101591973]"
415,415,689,6,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101603295,1697101604194.0,120,15.0,1.0,"[131, 768]","[1697101603426, 1697101604194]"
416,416,463,4,[],200,llama-13b,128,1,1932.0,1.0,1,H100,1697101604275,1697101606207.0,120,39.0,1.0,"[305, 1627]","[1697101604580, 1697101606207]"
417,417,650,11,[],200,llama-13b,128,1,1791.0,1.0,1,H100,1697101610278,1697101612069.0,120,13.0,1.0,"[434, 1357]","[1697101610712, 1697101612069]"
418,418,822,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606208,1697101610267.0,120,,,"[7, 1054, 179, 90, 86, 84, 64, 84, 80, 491, 96, 72, 72, 94, 90, 86, 512, 78, 78]","[1697101606215, 1697101607269, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608937, 1697101609449, 1697101609527, 1697101609605]"
419,419,837,1,[],200,llama-13b,128,1,4543.0,1.0,1,H100,1697101591974,1697101596517.0,120,85.0,20.0,"[19, 1010, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 92, 90, 528]","[1697101591993, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596517]"
420,420,114,7,[],200,llama-13b,128,1,4742.0,1.0,1,H100,1697101604195,1697101608937.0,120,88.0,20.0,"[13, 304, 736, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73, 94, 89, 87]","[1697101604208, 1697101604512, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667, 1697101608761, 1697101608850, 1697101608937]"
421,421,849,3,[],200,llama-13b,128,1,919.0,1.0,1,H100,1697101604275,1697101605194.0,120,10.0,1.0,"[44, 875]","[1697101604319, 1697101605194]"
422,422,273,4,[],200,llama-13b,128,1,1011.0,1.0,1,H100,1697101605196,1697101606207.0,120,19.0,1.0,"[47, 964]","[1697101605243, 1697101606207]"
423,423,629,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606210,1697101610267.0,120,,,"[88, 971, 179, 90, 86, 84, 65, 83, 80, 491, 95, 73, 72, 94, 90, 87, 511, 78, 78]","[1697101606298, 1697101607269, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609605]"
424,424,510,3,[],200,llama-13b,128,1,972.0,1.0,1,H100,1697101595545,1697101596517.0,120,79.0,2.0,"[6, 859, 107]","[1697101595551, 1697101596410, 1697101596517]"
425,425,79,12,[],200,llama-13b,128,1,944.0,1.0,1,H100,1697101612070,1697101613014.0,120,12.0,1.0,"[54, 890]","[1697101612124, 1697101613014]"
426,426,438,13,[],200,llama-13b,128,1,1122.0,1.0,1,H100,1697101613015,1697101614137.0,120,9.0,1.0,"[42, 1080]","[1697101613057, 1697101614137]"
427,427,788,14,[],200,llama-13b,128,1,1262.0,1.0,1,H100,1697101614138,1697101615400.0,120,31.0,1.0,"[30, 1232]","[1697101614168, 1697101615400]"
428,428,831,1,[],200,llama-13b,128,1,2097.0,1.0,1,H100,1697101601196,1697101603293.0,120,11.0,1.0,"[228, 1869]","[1697101601424, 1697101603293]"
429,429,188,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101615401,1697101616319.0,120,,,"[6, 888]","[1697101615407, 1697101616295]"
430,430,870,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601192.0,120,,,"[36, 879, 219, 93, 89, 89, 88, 85, 729, 96, 90, 87, 84, 818, 100, 100, 101, 91, 89]","[1697101596554, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
431,431,233,1,[],200,llama-13b,128,1,1029.0,1.0,1,H100,1697101591974,1697101593003.0,120,6.0,1.0,"[13, 1016]","[1697101591987, 1697101593003]"
432,432,232,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603294,1697101604272.0,120,,,[31],[1697101603325]
433,433,586,3,[],200,llama-13b,128,1,5169.0,1.0,1,H100,1697101604280,1697101609449.0,120,85.0,20.0,"[420, 1507, 238, 81, 76, 74, 772, 90, 86, 84, 65, 84, 79, 491, 95, 73, 73, 93, 90, 87, 511]","[1697101604700, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607857, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609449]"
434,434,12,2,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101597653,1697101598589.0,120,11.0,1.0,"[7, 929]","[1697101597660, 1697101598589]"
435,435,367,3,[],200,llama-13b,128,1,1802.0,1.0,1,H100,1697101598590,1697101600392.0,120,92.0,6.0,"[13, 1162, 235, 100, 100, 101, 91]","[1697101598603, 1697101599765, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392]"
436,436,593,2,[],200,llama-13b,128,1,2440.0,1.0,1,H100,1697101593005,1697101595445.0,120,335.0,9.0,"[12, 941, 289, 98, 92, 89, 89, 85, 643, 102]","[1697101593017, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445]"
437,437,25,3,[],200,llama-13b,128,1,964.0,1.0,1,H100,1697101595446,1697101596410.0,120,12.0,1.0,"[7, 957]","[1697101595453, 1697101596410]"
438,438,724,4,[],200,llama-13b,128,1,700.0,1.0,1,H100,1697101600393,1697101601093.0,120,11.0,1.0,"[7, 693]","[1697101600400, 1697101601093]"
439,439,547,16,[],200,llama-13b,128,1,1046.0,1.0,1,H100,1697101616330,1697101617376.0,120,12.0,1.0,"[50, 996]","[1697101616380, 1697101617376]"
440,440,912,17,[],200,llama-13b,128,1,4679.0,1.0,1,H100,1697101617378,1697101622057.0,120,92.0,20.0,"[96, 1614, 263, 95, 88, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60]","[1697101617474, 1697101619088, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
441,441,15,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610268.0,120,,,"[36, 617]","[1697101609487, 1697101610104]"
442,442,342,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622059,1697101623660.0,120,,,"[25, 1488]","[1697101622084, 1697101623572]"
443,443,840,2,[],200,llama-13b,128,1,2095.0,1.0,1,H100,1697101601197,1697101603292.0,120,17.0,1.0,"[478, 1617]","[1697101601675, 1697101603292]"
444,444,239,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603294,1697101604272.0,120,,,"[49, 851]","[1697101603343, 1697101604194]"
445,445,386,4,[],200,llama-13b,128,1,1600.0,1.0,1,H100,1697101596411,1697101598011.0,120,140.0,6.0,"[13, 1009, 219, 93, 89, 89, 88]","[1697101596424, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011]"
446,446,378,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610274,1697101616320.0,120,,,"[329, 1583, 84, 82, 81, 78, 610, 90, 86, 83, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87, 634, 79, 78, 76]","[1697101610603, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
447,447,671,19,[],200,llama-13b,128,1,2018.0,1.0,1,H100,1697101623677,1697101625695.0,120,12.0,1.0,"[517, 1501]","[1697101624194, 1697101625695]"
448,448,595,4,[],200,llama-13b,128,1,1931.0,1.0,1,H100,1697101604276,1697101606207.0,120,8.0,1.0,"[418, 1513]","[1697101604694, 1697101606207]"
449,449,96,20,[],200,llama-13b,128,1,869.0,1.0,1,H100,1697101625697,1697101626566.0,120,31.0,1.0,"[84, 785]","[1697101625781, 1697101626566]"
450,450,269,5,[],200,llama-13b,128,1,2099.0,1.0,1,H100,1697101601194,1697101603293.0,120,11.0,1.0,"[403, 1696]","[1697101601597, 1697101603293]"
451,451,20,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606209,1697101610267.0,120,,,"[72, 989, 178, 90, 86, 84, 65, 83, 80, 491, 96, 72, 72, 94, 90, 87, 511, 78, 78]","[1697101606281, 1697101607270, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609605]"
452,452,452,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626567,1697101628903.0,120,,,"[30, 277, 725]","[1697101626597, 1697101626874, 1697101627599]"
453,453,105,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610279,1697101616320.0,120,,,"[552, 1238, 117, 84, 82, 81, 77, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 70, 86, 634, 79, 78, 76]","[1697101610831, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
454,454,717,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598012,1697101601193.0,120,,,"[6, 571, 237, 96, 90, 86, 84, 818, 100, 100, 101, 91, 89]","[1697101598018, 1697101598589, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
455,455,365,8,[],200,llama-13b,128,1,1797.0,1.0,1,H100,1697101610272,1697101612069.0,120,23.0,1.0,"[307, 1490]","[1697101610579, 1697101612069]"
456,456,722,9,[],200,llama-13b,128,1,944.0,1.0,1,H100,1697101612070,1697101613014.0,120,39.0,1.0,"[30, 914]","[1697101612100, 1697101613014]"
457,457,626,6,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101603295,1697101604194.0,120,10.0,1.0,"[119, 780]","[1697101603414, 1697101604194]"
458,458,57,7,[],200,llama-13b,128,1,317.0,1.0,1,H100,1697101604196,1697101604513.0,120,13.0,1.0,"[53, 263]","[1697101604249, 1697101604512]"
459,459,127,10,[],200,llama-13b,128,1,1588.0,1.0,1,H100,1697101613015,1697101614603.0,120,100.0,5.0,"[6, 1116, 186, 95, 93, 92]","[1697101613021, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603]"
460,460,482,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614604,1697101616321.0,120,,,"[12, 784, 84, 79, 78, 76]","[1697101614616, 1697101615400, 1697101615484, 1697101615563, 1697101615641, 1697101615717]"
461,461,414,8,[],200,llama-13b,128,1,4935.0,1.0,1,H100,1697101604513,1697101609448.0,120,87.0,20.0,"[384, 1310, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 73, 93, 89, 88, 510]","[1697101604897, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608761, 1697101608850, 1697101608938, 1697101609448]"
462,462,840,12,[],200,llama-13b,128,1,1919.0,1.0,1,H100,1697101616337,1697101618256.0,120,17.0,1.0,"[551, 1368]","[1697101616888, 1697101618256]"
463,463,265,13,[],200,llama-13b,128,1,3800.0,1.0,1,H100,1697101618257,1697101622057.0,120,86.0,20.0,"[43, 788, 264, 94, 88, 68, 85, 85, 83, 719, 96, 94, 93, 91, 92, 75, 643, 81, 79, 79, 60]","[1697101618300, 1697101619088, 1697101619352, 1697101619446, 1697101619534, 1697101619602, 1697101619687, 1697101619772, 1697101619855, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621115, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
464,464,375,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601194,1697101604270.0,120,,,"[21, 940, 1253, 101, 74]","[1697101601215, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
465,465,705,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604275,1697101610269.0,120,,,"[205, 714, 54, 1198, 80, 76, 74, 772, 89, 87, 84, 64, 84, 80, 491, 95, 73, 72, 94, 89, 88, 510, 79, 77]","[1697101604480, 1697101605194, 1697101605248, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608938, 1697101609448, 1697101609527, 1697101609604]"
466,466,312,0,[],200,llama-13b,128,1,193.0,1.0,1,H100,1697101590979,1697101591172.0,120,23.0,1.0,"[24, 169]","[1697101591003, 1697101591172]"
467,467,668,1,[],200,llama-13b,128,1,3074.0,1.0,1,H100,1697101591173,1697101594247.0,120,109.0,6.0,"[442, 1388, 272, 83, 60, 80, 749]","[1697101591615, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247]"
468,468,342,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597653,1697101601192.0,120,,,"[37, 899, 237, 96, 90, 86, 84, 818, 100, 100, 101, 90, 90]","[1697101597690, 1697101598589, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600391, 1697101600481]"
469,469,768,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610268.0,120,,,"[42, 611]","[1697101609493, 1697101610104]"
470,470,173,10,[],200,llama-13b,128,1,4572.0,1.0,1,H100,1697101610277,1697101614849.0,120,96.0,20.0,"[413, 1379, 117, 84, 82, 80, 78, 611, 90, 86, 82, 82, 63, 79, 720, 95, 93, 92, 90, 69, 87]","[1697101610690, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613121, 1697101613211, 1697101613297, 1697101613379, 1697101613461, 1697101613524, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
471,471,699,3,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101601196,1697101602111.0,120,39.0,1.0,"[89, 826]","[1697101601285, 1697101602111]"
472,472,348,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610268.0,120,,,"[53, 600]","[1697101609504, 1697101610104]"
473,473,708,6,[],200,llama-13b,128,1,1791.0,1.0,1,H100,1697101610278,1697101612069.0,120,140.0,1.0,"[535, 1256]","[1697101610813, 1697101612069]"
474,474,136,7,[],200,llama-13b,128,1,944.0,1.0,1,H100,1697101612070,1697101613014.0,120,31.0,1.0,"[42, 902]","[1697101612112, 1697101613014]"
475,475,101,4,[],200,llama-13b,128,1,1181.0,1.0,1,H100,1697101602112,1697101603293.0,120,13.0,1.0,"[24, 1157]","[1697101602136, 1697101603293]"
476,476,208,0,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101590979,1697101595899.0,120,96.0,20.0,"[25, 168, 22, 822, 1258, 83, 61, 79, 750, 98, 91, 90, 89, 85, 643, 102, 98, 96, 95, 72, 92]","[1697101591004, 1697101591172, 1697101591194, 1697101592016, 1697101593274, 1697101593357, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594436, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595898]"
477,477,119,0,[],200,llama-13b,128,1,2020.0,1.0,1,H100,1697101590983,1697101593003.0,120,31.0,1.0,"[397, 1623]","[1697101591380, 1697101593003]"
478,478,837,0,[],200,llama-13b,128,1,5540.0,1.0,1,H100,1697101590977,1697101596517.0,120,85.0,20.0,"[507, 1518, 272, 84, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 95, 95, 72, 93, 90, 527]","[1697101591484, 1697101593002, 1697101593274, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516]"
479,479,196,8,[],200,llama-13b,128,1,1443.0,1.0,1,H100,1697101614852,1697101616295.0,120,13.0,1.0,"[62, 1381]","[1697101614914, 1697101616295]"
480,480,592,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101591975,1697101601193.0,120,,,"[30, 998, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 91, 90, 529, 80, 75, 74, 905, 94, 89, 89, 87, 85, 730, 97, 89, 87, 83, 819, 100, 100, 101, 90, 89]","[1697101592005, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595898, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600391, 1697101600480]"
481,481,864,1,[],200,llama-13b,128,1,4376.0,1.0,1,H100,1697101593276,1697101597652.0,120,83.0,20.0,"[6, 676, 289, 98, 92, 89, 89, 86, 643, 101, 98, 96, 95, 72, 93, 89, 528, 82, 74, 74, 906]","[1697101593282, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595344, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596598, 1697101596672, 1697101596746, 1697101597652]"
482,482,773,10,[],200,llama-13b,128,1,4709.0,1.0,1,H100,1697101616332,1697101621041.0,120,90.0,20.0,"[194, 850, 49, 939, 81, 81, 78, 78, 669, 94, 89, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93]","[1697101616526, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041]"
483,483,555,9,[],200,llama-13b,128,1,307.0,1.0,1,H100,1697101616296,1697101616603.0,120,11.0,1.0,"[6, 301]","[1697101616302, 1697101616603]"
484,484,844,4,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101601196,1697101602111.0,120,10.0,1.0,"[100, 815]","[1697101601296, 1697101602111]"
485,485,17,10,[],200,llama-13b,128,1,2029.0,1.0,1,H100,1697101623665,1697101625694.0,120,23.0,1.0,"[230, 1799]","[1697101623895, 1697101625694]"
486,486,694,0,[],200,llama-13b,128,1,4464.0,1.0,1,H100,1697101590981,1697101595445.0,120,161.0,13.0,"[240, 2054, 83, 60, 80, 749, 98, 92, 89, 89, 85, 643, 102]","[1697101591221, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445]"
487,487,248,6,[],200,llama-13b,128,1,4472.0,1.0,1,H100,1697101604195,1697101608667.0,120,182.0,17.0,"[36, 281, 736, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73]","[1697101604231, 1697101604512, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667]"
488,488,74,4,[],200,llama-13b,128,1,5173.0,1.0,1,H100,1697101604276,1697101609449.0,120,88.0,20.0,"[436, 1495, 238, 81, 76, 74, 772, 90, 86, 84, 65, 84, 79, 491, 95, 73, 73, 93, 90, 87, 511]","[1697101604712, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607857, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609449]"
489,489,129,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603296,1697101604272.0,120,,,"[174, 724]","[1697101603470, 1697101604194]"
490,490,723,0,[],200,llama-13b,128,1,1984.0,1.0,1,H100,1697101591019,1697101593003.0,120,14.0,1.0,"[566, 1418]","[1697101591585, 1697101593003]"
491,491,208,1,[],200,llama-13b,128,1,5635.0,1.0,1,H100,1697101592017,1697101597652.0,120,96.0,20.0,"[7, 1934, 289, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 92, 89, 529, 80, 75, 74, 906]","[1697101592024, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
492,492,148,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,16.0,1.0,"[86, 867]","[1697101593091, 1697101593958]"
493,493,716,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604270.0,120,,,"[395, 1702, 115, 101, 74]","[1697101601591, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
494,494,507,2,[],200,llama-13b,128,1,4866.0,1.0,1,H100,1697101593959,1697101598825.0,120,83.0,20.0,"[12, 1248, 125, 101, 99, 95, 95, 72, 93, 89, 528, 81, 75, 74, 906, 93, 89, 89, 88, 84, 730]","[1697101593971, 1697101595219, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825]"
495,495,262,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,39.0,1.0,"[128, 825]","[1697101593133, 1697101593958]"
496,496,620,2,[],200,llama-13b,128,1,1940.0,1.0,1,H100,1697101593959,1697101595899.0,120,100.0,8.0,"[24, 1237, 124, 101, 99, 95, 95, 72, 93]","[1697101593983, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899]"
497,497,150,7,[],200,llama-13b,128,1,974.0,1.0,1,H100,1697101604274,1697101605248.0,120,216.0,2.0,"[27, 947]","[1697101604301, 1697101605248]"
498,498,503,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101605249,1697101610266.0,120,,,"[20, 2000, 179, 89, 87, 84, 64, 84, 80, 491, 96, 72, 72, 94, 90, 86, 512, 79, 77]","[1697101605269, 1697101607269, 1697101607448, 1697101607537, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608937, 1697101609449, 1697101609528, 1697101609605]"
499,499,670,2,[],200,llama-13b,128,1,4879.0,1.0,1,H100,1697101595221,1697101600100.0,120,67.0,18.0,"[18, 1171, 107, 80, 75, 74, 905, 94, 89, 89, 88, 85, 730, 96, 89, 87, 84, 818, 100]","[1697101595239, 1697101596410, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100]"
500,500,55,5,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101604275,1697101605195.0,120,12.0,1.0,"[236, 684]","[1697101604511, 1697101605195]"
501,501,416,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101605196,1697101610266.0,120,,,"[41, 970, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 72, 94, 89, 87, 512, 79, 77]","[1697101605237, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608937, 1697101609449, 1697101609528, 1697101609605]"
502,502,673,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601191.0,120,,,"[7, 1127, 93, 89, 89, 88, 84, 730, 96, 91, 86, 84, 818, 100, 99, 102, 91, 89]","[1697101596525, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598921, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600392, 1697101600481]"
503,503,458,5,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101603295,1697101604194.0,120,11.0,1.0,"[60, 839]","[1697101603355, 1697101604194]"
504,504,244,4,[],200,llama-13b,128,1,2097.0,1.0,1,H100,1697101601196,1697101603293.0,120,9.0,1.0,"[210, 1886]","[1697101601406, 1697101603292]"
505,505,598,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[77, 822]","[1697101603372, 1697101604194]"
506,506,134,4,[],200,llama-13b,128,1,4574.0,1.0,1,H100,1697101610275,1697101614849.0,120,86.0,20.0,"[334, 1460, 117, 84, 82, 81, 77, 611, 90, 86, 83, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87]","[1697101610609, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
507,507,812,6,[],200,llama-13b,128,1,317.0,1.0,1,H100,1697101604196,1697101604513.0,120,16.0,1.0,"[53, 264]","[1697101604249, 1697101604513]"
508,508,735,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600305,1697101601192.0,120,,,"[12, 776]","[1697101600317, 1697101601093]"
509,509,400,3,[],200,llama-13b,128,1,2400.0,1.0,1,H100,1697101595345,1697101597745.0,120,123.0,7.0,"[6, 1059, 107, 80, 75, 74, 905, 94]","[1697101595351, 1697101596410, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745]"
510,510,164,4,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101601196,1697101602111.0,120,15.0,1.0,"[172, 743]","[1697101601368, 1697101602111]"
511,511,246,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604514,1697101610266.0,120,,,"[377, 1316, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 73, 93, 89, 88, 510, 80, 77]","[1697101604891, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608761, 1697101608850, 1697101608938, 1697101609448, 1697101609528, 1697101609605]"
512,512,734,4,[],200,llama-13b,128,1,1436.0,1.0,1,H100,1697101597746,1697101599182.0,120,100.0,6.0,"[6, 837, 237, 96, 90, 86, 84]","[1697101597752, 1697101598589, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182]"
513,513,430,1,[],200,llama-13b,128,1,510.0,1.0,1,H100,1697101595900,1697101596410.0,120,15.0,1.0,"[12, 498]","[1697101595912, 1697101596410]"
514,514,789,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596412,1697101601191.0,120,,,"[18, 1003, 219, 93, 89, 89, 88, 84, 730, 97, 90, 86, 84, 818, 100, 99, 102, 91, 89]","[1697101596430, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600392, 1697101600481]"
515,515,871,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600303,1697101601192.0,120,,,"[9, 781]","[1697101600312, 1697101601093]"
516,516,701,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598827,1697101601193.0,120,,,"[18, 921, 234, 100, 100, 101, 91, 89]","[1697101598845, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
517,517,199,8,[],200,llama-13b,128,1,375.0,1.0,1,H100,1697101610105,1697101610480.0,120,13.0,1.0,"[30, 345]","[1697101610135, 1697101610480]"
518,518,564,2,[],200,llama-13b,128,1,4663.0,1.0,1,H100,1697101604275,1697101608938.0,120,84.0,20.0,"[199, 720, 54, 1198, 80, 76, 74, 772, 89, 87, 84, 64, 84, 80, 491, 95, 73, 72, 94, 89, 88]","[1697101604474, 1697101605194, 1697101605248, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608938]"
519,519,547,3,[],200,llama-13b,128,1,753.0,1.0,1,H100,1697101599013,1697101599766.0,120,12.0,1.0,"[6, 747]","[1697101599019, 1697101599766]"
520,520,911,6,[],200,llama-13b,128,1,3109.0,1.0,1,H100,1697101610271,1697101613380.0,120,335.0,11.0,"[28, 747, 46, 1094, 84, 82, 80, 78, 610, 90, 86, 83]","[1697101610299, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379]"
521,521,616,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101620381,1697101623659.0,120,,,"[28, 1264, 85, 81, 79, 79, 60, 614, 88, 86, 85, 82]","[1697101620409, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622759, 1697101622845, 1697101622930, 1697101623012]"
522,522,175,0,[],200,llama-13b,128,1,3366.0,1.0,1,H100,1697101590979,1697101594345.0,120,140.0,8.0,"[94, 943, 1258, 84, 60, 79, 750, 98]","[1697101591073, 1697101592016, 1697101593274, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345]"
523,523,469,8,[],200,llama-13b,128,1,1165.0,1.0,1,H100,1697101608939,1697101610104.0,120,17.0,1.0,"[37, 1128]","[1697101608976, 1697101610104]"
524,524,52,3,[],200,llama-13b,128,1,1752.0,1.0,1,H100,1697101595900,1697101597652.0,120,58.0,6.0,"[18, 492, 107, 80, 75, 74, 905]","[1697101595918, 1697101596410, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651]"
525,525,233,1,[],200,llama-13b,128,1,1029.0,1.0,1,H100,1697101591974,1697101593003.0,120,6.0,1.0,"[7, 1022]","[1697101591981, 1697101593003]"
526,526,330,0,[],200,llama-13b,128,1,4466.0,1.0,1,H100,1697101590979,1697101595445.0,120,345.0,14.0,"[113, 881, 43, 1259, 83, 60, 79, 750, 98, 92, 89, 89, 85, 643, 102]","[1697101591092, 1697101591973, 1697101592016, 1697101593275, 1697101593358, 1697101593418, 1697101593497, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445]"
527,527,28,6,[],200,llama-13b,128,1,5172.0,1.0,1,H100,1697101604276,1697101609448.0,120,86.0,20.0,"[394, 1537, 238, 81, 76, 74, 772, 90, 86, 84, 65, 83, 80, 491, 95, 73, 73, 93, 90, 87, 510]","[1697101604670, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609448]"
528,528,792,15,[],200,llama-13b,128,1,2029.0,1.0,1,H100,1697101623665,1697101625694.0,120,11.0,1.0,"[224, 1805]","[1697101623889, 1697101625694]"
529,529,225,16,[],200,llama-13b,128,1,870.0,1.0,1,H100,1697101625696,1697101626566.0,120,23.0,1.0,"[25, 845]","[1697101625721, 1697101626566]"
530,530,583,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626567,1697101628903.0,120,,,"[24, 283, 725]","[1697101626591, 1697101626874, 1697101627599]"
531,531,8,18,[],200,llama-13b,128,1,1907.0,1.0,1,H100,1697101628908,1697101630815.0,120,39.0,3.0,"[65, 1656, 105, 81]","[1697101628973, 1697101630629, 1697101630734, 1697101630815]"
532,532,283,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601191.0,120,,,"[30, 885, 219, 93, 89, 89, 88, 85, 729, 97, 90, 86, 83, 819, 100, 100, 101, 91, 89]","[1697101596548, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598922, 1697101599012, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
533,533,367,19,[],200,llama-13b,128,1,1359.0,1.0,1,H100,1697101630816,1697101632175.0,120,92.0,6.0,"[6, 862, 131, 92, 90, 89, 89]","[1697101630822, 1697101631684, 1697101631815, 1697101631907, 1697101631997, 1697101632086, 1697101632175]"
534,534,692,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101632176,1697101634995.0,120,,,"[12, 779, 205, 95, 71, 94, 93, 92, 70, 614, 75, 71]","[1697101632188, 1697101632967, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
535,535,132,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604272.0,120,,,"[280, 1816, 116, 101, 74]","[1697101601476, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
536,536,125,21,[],200,llama-13b,128,1,1809.0,1.0,1,H100,1697101635000,1697101636809.0,120,13.0,1.0,"[526, 1283]","[1697101635526, 1697101636809]"
537,537,484,22,[],200,llama-13b,128,1,7109.0,1.0,1,H100,1697101636810,1697101643919.0,120,86.0,36.0,"[76, 593, 189, 91, 93, 87, 85, 83, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618, 81, 93, 735, 85, 78, 78, 761, 100, 99, 96, 89, 82, 63, 800, 99, 96, 93, 95, 86]","[1697101636886, 1697101637479, 1697101637668, 1697101637759, 1697101637852, 1697101637939, 1697101638024, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919]"
538,538,556,9,[],200,llama-13b,128,1,1588.0,1.0,1,H100,1697101610481,1697101612069.0,120,9.0,1.0,"[417, 1171]","[1697101610898, 1697101612069]"
539,539,810,1,[],200,llama-13b,128,1,4956.0,1.0,1,H100,1697101595345,1697101600301.0,120,91.0,20.0,"[12, 1053, 107, 80, 75, 74, 905, 94, 89, 89, 88, 85, 730, 96, 89, 87, 84, 818, 100, 100, 101]","[1697101595357, 1697101596410, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301]"
540,540,540,2,[],200,llama-13b,128,1,1445.0,1.0,1,H100,1697101597653,1697101599098.0,120,140.0,5.0,"[13, 923, 237, 96, 89, 87]","[1697101597666, 1697101598589, 1697101598826, 1697101598922, 1697101599011, 1697101599098]"
541,541,388,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601194,1697101604270.0,120,,,"[397, 1702, 115, 101, 74]","[1697101601591, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
542,542,878,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601191.0,120,,,"[18, 1308]","[1697101599785, 1697101601093]"
543,543,714,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612512,1697101616322.0,120,,,"[24, 1601, 186, 95, 93, 92, 90, 69, 87, 634, 80, 77, 77]","[1697101612536, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615563, 1697101615640, 1697101615717]"
544,544,258,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101590981,1697101601193.0,120,,,"[515, 1507, 271, 84, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 95, 95, 72, 93, 90, 527, 81, 75, 74, 905, 94, 89, 89, 87, 85, 730, 96, 90, 87, 83, 819, 100, 99, 102, 90, 89]","[1697101591496, 1697101593003, 1697101593274, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600391, 1697101600480]"
545,545,853,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608939,1697101610266.0,120,,,"[26, 1139]","[1697101608965, 1697101610104]"
546,546,528,7,[],200,llama-13b,128,1,4488.0,1.0,1,H100,1697101610274,1697101614762.0,120,52.0,20.0,"[200, 572, 46, 1094, 84, 82, 81, 77, 610, 91, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69]","[1697101610474, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613120, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762]"
547,547,118,1,[],200,llama-13b,128,1,4376.0,1.0,1,H100,1697101593276,1697101597652.0,120,85.0,20.0,"[6, 676, 289, 98, 92, 89, 89, 86, 643, 101, 98, 96, 95, 72, 93, 89, 528, 81, 75, 74, 906]","[1697101593282, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595344, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
548,548,606,7,[],200,llama-13b,128,1,689.0,1.0,1,H100,1697101608669,1697101609358.0,120,9.0,1.0,"[6, 683]","[1697101608675, 1697101609358]"
549,549,300,25,[],200,llama-13b,128,1,887.0,1.0,1,H100,1697101637852,1697101638739.0,120,9.0,1.0,"[7, 880]","[1697101637859, 1697101638739]"
550,550,241,5,[],200,llama-13b,128,1,278.0,1.0,1,H100,1697101601094,1697101601372.0,120,19.0,1.0,"[30, 248]","[1697101601124, 1697101601372]"
551,551,36,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609359,1697101610267.0,120,,,"[12, 733]","[1697101609371, 1697101610104]"
552,552,913,9,[],200,llama-13b,128,1,4657.0,1.0,1,H100,1697101610105,1697101614762.0,120,88.0,20.0,"[6, 369, 611, 1095, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 92, 93, 90, 69]","[1697101610111, 1697101610480, 1697101611091, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614510, 1697101614603, 1697101614693, 1697101614762]"
553,553,807,1,[],200,llama-13b,128,1,5326.0,1.0,1,H100,1697101593499,1697101598825.0,120,90.0,20.0,"[12, 1708, 125, 101, 98, 96, 95, 72, 93, 89, 528, 81, 75, 74, 906, 93, 89, 89, 88, 84, 730]","[1697101593511, 1697101595219, 1697101595344, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825]"
554,554,393,9,[],200,llama-13b,128,1,5289.0,1.0,1,H100,1697101610273,1697101615562.0,120,182.0,22.0,"[243, 1552, 118, 84, 82, 81, 77, 611, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87, 634, 79]","[1697101610516, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562]"
555,555,477,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597653,1697101601193.0,120,,,"[66, 870, 237, 96, 90, 86, 84, 818, 100, 100, 101, 91, 89]","[1697101597719, 1697101598589, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
556,556,236,2,[],200,llama-13b,128,1,939.0,1.0,1,H100,1697101598827,1697101599766.0,120,8.0,1.0,"[12, 927]","[1697101598839, 1697101599766]"
557,557,566,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601191.0,120,,,"[36, 1290]","[1697101599803, 1697101601093]"
558,558,927,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604271.0,120,,,"[508, 1587, 116, 101, 74]","[1697101601705, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
559,559,753,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101615563,1697101616319.0,120,,,"[7, 725]","[1697101615570, 1697101616295]"
560,560,153,11,[],200,llama-13b,128,1,2189.0,1.0,1,H100,1697101616336,1697101618525.0,120,335.0,4.0,"[448, 1472, 108, 81, 80]","[1697101616784, 1697101618256, 1697101618364, 1697101618445, 1697101618525]"
561,561,835,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[118, 797, 45, 1252, 101, 74]","[1697101601314, 1697101602111, 1697101602156, 1697101603408, 1697101603509, 1697101603583]"
562,562,271,4,[],200,llama-13b,128,1,4663.0,1.0,1,H100,1697101604275,1697101608938.0,120,87.0,20.0,"[132, 787, 54, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 73, 72, 94, 89, 88]","[1697101604407, 1697101605194, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608938]"
563,563,739,6,[],200,llama-13b,128,1,1046.0,1.0,1,H100,1697101616331,1697101617377.0,120,216.0,1.0,"[66, 979]","[1697101616397, 1697101617376]"
564,564,257,7,[],200,llama-13b,128,1,1711.0,1.0,1,H100,1697101617378,1697101619089.0,120,14.0,1.0,"[101, 1609]","[1697101617479, 1697101619088]"
565,565,610,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619090,1697101623658.0,120,,,"[18, 1270, 196, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 60, 614, 87, 87, 84, 83]","[1697101619108, 1697101620378, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
566,566,408,4,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101597653,1697101598589.0,120,16.0,1.0,"[24, 912]","[1697101597677, 1697101598589]"
567,567,737,5,[],200,llama-13b,128,1,1410.0,1.0,1,H100,1697101598590,1697101600000.0,120,216.0,2.0,"[13, 1162, 235]","[1697101598603, 1697101599765, 1697101600000]"
568,568,861,9,[],200,llama-13b,128,1,1796.0,1.0,1,H100,1697101610273,1697101612069.0,120,10.0,1.0,"[333, 1463]","[1697101610606, 1697101612069]"
569,569,422,2,[],200,llama-13b,128,1,890.0,1.0,1,H100,1697101600482,1697101601372.0,120,26.0,1.0,"[6, 884]","[1697101600488, 1697101601372]"
570,570,779,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601373,1697101604271.0,120,,,"[415, 1505, 115, 101, 75]","[1697101601788, 1697101603293, 1697101603408, 1697101603509, 1697101603584]"
571,571,849,5,[],200,llama-13b,128,1,1060.0,1.0,1,H100,1697101606209,1697101607269.0,120,10.0,1.0,"[73, 987]","[1697101606282, 1697101607269]"
572,572,274,6,[],200,llama-13b,128,1,2335.0,1.0,1,H100,1697101607270,1697101609605.0,120,364.0,11.0,"[7, 1056, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607277, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
573,573,21,1,[],200,llama-13b,128,1,2094.0,1.0,1,H100,1697101601198,1697101603292.0,120,15.0,1.0,"[469, 1625]","[1697101601667, 1697101603292]"
574,574,57,6,[],200,llama-13b,128,1,1791.0,1.0,1,H100,1697101610278,1697101612069.0,120,13.0,1.0,"[496, 1295]","[1697101610774, 1697101612069]"
575,575,375,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603294,1697101604272.0,120,,,[19],[1697101603313]
576,576,626,14,[],200,llama-13b,128,1,1513.0,1.0,1,H100,1697101622059,1697101623572.0,120,10.0,1.0,"[108, 1405]","[1697101622167, 1697101623572]"
577,577,886,0,[],200,llama-13b,128,1,2021.0,1.0,1,H100,1697101590982,1697101593003.0,120,17.0,1.0,"[291, 1730]","[1697101591273, 1697101593003]"
578,578,27,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623574,1697101626656.0,120,,,"[35, 265, 733, 1201, 98, 68]","[1697101623609, 1697101623874, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
579,579,316,1,[],200,llama-13b,128,1,4647.0,1.0,1,H100,1697101593005,1697101597652.0,120,86.0,20.0,"[80, 873, 289, 98, 92, 89, 89, 86, 642, 102, 98, 96, 95, 72, 93, 89, 529, 80, 75, 74, 906]","[1697101593085, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
580,580,558,2,[],200,llama-13b,128,1,4647.0,1.0,1,H100,1697101593005,1697101597652.0,120,58.0,20.0,"[74, 879, 289, 98, 92, 89, 89, 86, 642, 102, 98, 96, 95, 72, 93, 89, 529, 80, 75, 74, 906]","[1697101593079, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
581,581,376,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597653,1697101601193.0,120,,,"[7, 929, 237, 96, 89, 87, 84, 818, 101, 99, 101, 90, 89]","[1697101597660, 1697101598589, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600101, 1697101600200, 1697101600301, 1697101600391, 1697101600480]"
582,582,808,22,[],200,llama-13b,128,1,1825.0,1.0,1,H100,1697101628909,1697101630734.0,120,286.0,2.0,"[179, 1541, 104]","[1697101629088, 1697101630629, 1697101630733]"
583,583,240,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630735,1697101634994.0,120,,,"[6, 943, 131, 92, 90, 89, 89, 88, 84, 83, 742, 94, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101630741, 1697101631684, 1697101631815, 1697101631907, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
584,584,575,24,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634998,1697101640209.0,120,86.0,20.0,"[301, 1509, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 768, 97, 93, 93, 92, 91, 91, 79, 617]","[1697101635299, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209]"
585,585,385,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[314, 1716]","[1697101626977, 1697101628693]"
586,586,743,17,[],200,llama-13b,128,1,2905.0,1.0,1,H100,1697101628909,1697101631814.0,120,123.0,6.0,"[208, 1513, 104, 81, 68, 66, 864]","[1697101629117, 1697101630630, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813]"
587,587,730,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601198,1697101604271.0,120,,,"[584, 1511, 115, 101, 74]","[1697101601782, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
588,588,515,5,[],200,llama-13b,128,1,1181.0,1.0,1,H100,1697101602112,1697101603293.0,120,11.0,1.0,"[36, 1145]","[1697101602148, 1697101603293]"
589,589,874,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[183, 716]","[1697101603478, 1697101604194]"
590,590,3,25,[],200,llama-13b,128,1,4208.0,1.0,1,H100,1697101640212,1697101644420.0,120,89.0,20.0,"[99, 691, 118, 84, 79, 77, 761, 100, 99, 96, 89, 82, 63, 801, 98, 97, 92, 96, 86, 85, 414]","[1697101640311, 1697101641002, 1697101641120, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643549, 1697101643646, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644419]"
591,591,160,4,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101604275,1697101605195.0,120,13.0,1.0,"[229, 691]","[1697101604504, 1697101605195]"
592,592,273,7,[],200,llama-13b,128,1,1927.0,1.0,1,H100,1697101604280,1697101606207.0,120,19.0,1.0,"[502, 1425]","[1697101604782, 1697101606207]"
593,593,492,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616318.0,120,,,"[50, 1393]","[1697101614902, 1697101616295]"
594,594,514,5,[],200,llama-13b,128,1,4253.0,1.0,1,H100,1697101605196,1697101609449.0,120,85.0,20.0,"[24, 987, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 72, 94, 89, 87, 512]","[1697101605220, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608937, 1697101609449]"
595,595,851,6,[],200,llama-13b,128,1,1931.0,1.0,1,H100,1697101616325,1697101618256.0,120,23.0,1.0,"[493, 1438]","[1697101616818, 1697101618256]"
596,596,280,7,[],200,llama-13b,128,1,3800.0,1.0,1,H100,1697101618257,1697101622057.0,120,91.0,20.0,"[55, 777, 263, 94, 88, 68, 85, 85, 83, 719, 95, 95, 93, 91, 92, 75, 643, 81, 79, 79, 60]","[1697101618312, 1697101619089, 1697101619352, 1697101619446, 1697101619534, 1697101619602, 1697101619687, 1697101619772, 1697101619855, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621115, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
597,597,279,8,[],200,llama-13b,128,1,4333.0,1.0,1,H100,1697101610270,1697101614603.0,120,67.0,18.0,"[208, 568, 46, 1094, 84, 82, 81, 77, 611, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92]","[1697101610478, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603]"
598,598,643,2,[],200,llama-13b,128,1,2097.0,1.0,1,H100,1697101601196,1697101603293.0,120,18.0,1.0,"[310, 1787]","[1697101601506, 1697101603293]"
599,599,75,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[107, 792]","[1697101603402, 1697101604194]"
600,600,695,1,[],200,llama-13b,128,1,4855.0,1.0,1,H100,1697101595446,1697101600301.0,120,92.0,20.0,"[13, 951, 107, 80, 75, 74, 905, 94, 89, 89, 87, 86, 730, 96, 89, 87, 84, 818, 100, 100, 101]","[1697101595459, 1697101596410, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598096, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301]"
601,601,604,8,[],200,llama-13b,128,1,1999.0,1.0,1,H100,1697101610271,1697101612270.0,120,161.0,4.0,"[51, 724, 46, 1094, 84]","[1697101610322, 1697101611046, 1697101611092, 1697101612186, 1697101612270]"
602,602,123,9,[],200,llama-13b,128,1,743.0,1.0,1,H100,1697101612271,1697101613014.0,120,14.0,1.0,"[7, 736]","[1697101612278, 1697101613014]"
603,603,478,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613015,1697101616319.0,120,,,"[12, 1110, 186, 95, 93, 92, 90, 69, 88, 633, 80, 77, 77]","[1697101613027, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615563, 1697101615640, 1697101615717]"
604,604,919,10,[],200,llama-13b,128,1,944.0,1.0,1,H100,1697101612070,1697101613014.0,120,14.0,1.0,"[59, 885]","[1697101612129, 1697101613014]"
605,605,344,11,[],200,llama-13b,128,1,1122.0,1.0,1,H100,1697101613015,1697101614137.0,120,13.0,1.0,"[36, 1086]","[1697101613051, 1697101614137]"
606,606,788,12,[],200,llama-13b,128,1,1262.0,1.0,1,H100,1697101614138,1697101615400.0,120,31.0,1.0,"[30, 1232]","[1697101614168, 1697101615400]"
607,607,215,13,[],200,llama-13b,128,1,894.0,1.0,1,H100,1697101615401,1697101616295.0,120,12.0,1.0,"[12, 882]","[1697101615413, 1697101616295]"
608,608,836,11,[],200,llama-13b,128,1,1924.0,1.0,1,H100,1697101616332,1697101618256.0,120,11.0,1.0,"[439, 1485]","[1697101616771, 1697101618256]"
609,609,576,14,[],200,llama-13b,128,1,1080.0,1.0,1,H100,1697101616296,1697101617376.0,120,14.0,1.0,"[36, 1044]","[1697101616332, 1697101617376]"
610,610,174,1,[],200,llama-13b,128,1,4233.0,1.0,1,H100,1697101593419,1697101597652.0,120,87.0,20.0,"[6, 533, 289, 98, 92, 89, 89, 86, 643, 101, 98, 96, 95, 72, 93, 89, 528, 82, 74, 74, 906]","[1697101593425, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594701, 1697101595344, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596516, 1697101596598, 1697101596672, 1697101596746, 1697101597652]"
611,611,266,12,[],200,llama-13b,128,1,831.0,1.0,1,H100,1697101618257,1697101619088.0,120,9.0,1.0,"[31, 800]","[1697101618288, 1697101619088]"
612,612,100,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599013,1697101601191.0,120,,,"[6, 747, 234, 100, 100, 101, 91, 89]","[1697101599019, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
613,613,419,6,[],200,llama-13b,128,1,5173.0,1.0,1,H100,1697101604275,1697101609448.0,120,88.0,20.0,"[299, 1633, 239, 80, 76, 74, 772, 90, 86, 84, 64, 84, 80, 491, 95, 73, 72, 94, 90, 87, 510]","[1697101604574, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609448]"
614,614,620,13,[],200,llama-13b,128,1,2024.0,1.0,1,H100,1697101619090,1697101621114.0,120,100.0,8.0,"[18, 1270, 196, 96, 94, 93, 91, 92, 74]","[1697101619108, 1697101620378, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114]"
615,615,531,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597653,1697101601193.0,120,,,"[54, 882, 237, 96, 90, 86, 84, 818, 100, 100, 101, 90, 90]","[1697101597707, 1697101598589, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600391, 1697101600481]"
616,616,463,3,[],200,llama-13b,128,1,2097.0,1.0,1,H100,1697101601196,1697101603293.0,120,39.0,1.0,"[407, 1690]","[1697101601603, 1697101603293]"
617,617,821,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[177, 722]","[1697101603472, 1697101604194]"
618,618,246,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604280,1697101610266.0,120,,,"[490, 1437, 238, 81, 76, 74, 772, 90, 86, 84, 65, 84, 79, 492, 94, 72, 74, 93, 90, 87, 510, 79, 78]","[1697101604770, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607857, 1697101607936, 1697101608428, 1697101608522, 1697101608594, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609448, 1697101609527, 1697101609605]"
619,619,286,1,[],200,llama-13b,128,1,2056.0,1.0,1,H100,1697101594616,1697101596672.0,120,161.0,12.0,"[13, 715, 102, 98, 96, 94, 73, 92, 90, 527, 81, 75]","[1697101594629, 1697101595344, 1697101595446, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672]"
620,620,124,1,[],200,llama-13b,128,1,1071.0,1.0,1,H100,1697101595446,1697101596517.0,120,83.0,2.0,"[7, 1064]","[1697101595453, 1697101596517]"
621,621,287,8,[],200,llama-13b,128,1,1797.0,1.0,1,H100,1697101610272,1697101612069.0,120,10.0,1.0,"[316, 1481]","[1697101610588, 1697101612069]"
622,622,837,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601192.0,120,,,"[60, 855, 219, 93, 89, 89, 88, 85, 729, 97, 89, 87, 84, 818, 100, 100, 101, 91, 89]","[1697101596578, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
623,623,616,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612071,1697101616321.0,120,,,"[111, 832, 107, 90, 86, 83, 81, 62, 81, 719, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612182, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
624,624,699,3,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101597653,1697101598589.0,120,39.0,1.0,"[60, 876]","[1697101597713, 1697101598589]"
625,625,369,1,[],200,llama-13b,128,1,3291.0,1.0,1,H100,1697101595807,1697101599098.0,120,216.0,15.0,"[7, 596, 107, 80, 75, 74, 905, 94, 89, 89, 87, 85, 731, 96, 89, 87]","[1697101595814, 1697101596410, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598826, 1697101598922, 1697101599011, 1697101599098]"
626,626,370,1,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101596518,1697101597433.0,120,31.0,1.0,"[61, 854]","[1697101596579, 1697101597433]"
627,627,433,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610268.0,120,,,[36],[1697101609487]
628,628,727,2,[],200,llama-13b,128,1,1664.0,1.0,1,H100,1697101597434,1697101599098.0,120,58.0,5.0,"[6, 1149, 236, 97, 89, 87]","[1697101597440, 1697101598589, 1697101598825, 1697101598922, 1697101599011, 1697101599098]"
629,629,881,6,[],200,llama-13b,128,1,2232.0,1.0,1,H100,1697101610278,1697101612510.0,120,58.0,6.0,"[440, 1351, 117, 84, 82, 80, 78]","[1697101610718, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510]"
630,630,156,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599099,1697101601191.0,120,,,"[7, 660, 234, 100, 100, 101, 91, 89]","[1697101599106, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
631,631,314,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612512,1697101616322.0,120,,,"[18, 1607, 186, 95, 93, 92, 90, 69, 88, 633, 80, 77, 77]","[1697101612530, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615563, 1697101615640, 1697101615717]"
632,632,723,2,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101597653,1697101598589.0,120,14.0,1.0,"[54, 882]","[1697101597707, 1697101598589]"
633,633,607,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601373,1697101604271.0,120,,,"[409, 1511, 115, 101, 75]","[1697101601782, 1697101603293, 1697101603408, 1697101603509, 1697101603584]"
634,634,658,26,[],200,llama-13b,128,1,1345.0,1.0,1,H100,1697101638741,1697101640086.0,120,11.0,1.0,"[36, 1309]","[1697101638777, 1697101640086]"
635,635,90,27,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101640087,1697101641002.0,120,19.0,1.0,"[12, 903]","[1697101640099, 1697101641002]"
636,636,35,7,[],200,llama-13b,128,1,4663.0,1.0,1,H100,1697101604275,1697101608938.0,120,87.0,20.0,"[114, 806, 53, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 73, 72, 94, 89, 88]","[1697101604389, 1697101605195, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608938]"
637,637,515,4,[],200,llama-13b,128,1,2098.0,1.0,1,H100,1697101601194,1697101603292.0,120,11.0,1.0,"[192, 1906]","[1697101601386, 1697101603292]"
638,638,870,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603294,1697101604272.0,120,,,"[7, 893]","[1697101603301, 1697101604194]"
639,639,15,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623665,1697101626657.0,120,,,"[114, 789, 40, 1200, 97, 69]","[1697101623779, 1697101624568, 1697101624608, 1697101625808, 1697101625905, 1697101625974]"
640,640,151,3,[],200,llama-13b,128,1,1176.0,1.0,1,H100,1697101598590,1697101599766.0,120,39.0,1.0,"[19, 1157]","[1697101598609, 1697101599766]"
641,641,716,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608940,1697101610267.0,120,,,"[43, 1121]","[1697101608983, 1697101610104]"
642,642,514,12,[],200,llama-13b,128,1,3531.0,1.0,1,H100,1697101618527,1697101622058.0,120,85.0,20.0,"[6, 556, 263, 94, 88, 68, 85, 85, 83, 719, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 61]","[1697101618533, 1697101619089, 1697101619352, 1697101619446, 1697101619534, 1697101619602, 1697101619687, 1697101619772, 1697101619855, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058]"
643,643,513,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601191.0,120,,,"[6, 1320]","[1697101599773, 1697101601093]"
644,644,872,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604270.0,120,,,"[406, 1690, 115, 101, 74]","[1697101601603, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
645,645,275,6,[],200,llama-13b,128,1,2327.0,1.0,1,H100,1697101604275,1697101606602.0,120,161.0,4.0,"[247, 1685, 239, 80, 76]","[1697101604522, 1697101606207, 1697101606446, 1697101606526, 1697101606602]"
646,646,635,7,[],200,llama-13b,128,1,667.0,1.0,1,H100,1697101606603,1697101607270.0,120,23.0,1.0,"[12, 655]","[1697101606615, 1697101607270]"
647,647,59,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607271,1697101610268.0,120,,,"[18, 1044, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607289, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
648,648,841,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604272.0,120,,,"[292, 1920, 101, 74]","[1697101601488, 1697101603408, 1697101603509, 1697101603583]"
649,649,895,4,[],200,llama-13b,128,1,2099.0,1.0,1,H100,1697101601194,1697101603293.0,120,15.0,1.0,"[415, 1684]","[1697101601609, 1697101603293]"
650,650,396,1,[],200,llama-13b,128,1,4647.0,1.0,1,H100,1697101593005,1697101597652.0,120,89.0,20.0,"[48, 905, 289, 98, 92, 89, 89, 85, 643, 102, 98, 96, 95, 72, 93, 89, 529, 80, 75, 74, 906]","[1697101593053, 1697101593958, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595343, 1697101595445, 1697101595543, 1697101595639, 1697101595734, 1697101595806, 1697101595899, 1697101595988, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597652]"
651,651,93,2,[],200,llama-13b,128,1,4578.0,1.0,1,H100,1697101594248,1697101598826.0,120,88.0,20.0,"[13, 958, 125, 101, 99, 96, 94, 73, 92, 90, 527, 81, 75, 74, 906, 93, 89, 89, 88, 84, 731]","[1697101594261, 1697101595219, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598826]"
652,652,558,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597653,1697101601193.0,120,,,"[13, 923, 237, 96, 89, 87, 84, 818, 101, 99, 101, 91, 89]","[1697101597666, 1697101598589, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600101, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
653,653,920,1,[],200,llama-13b,128,1,772.0,1.0,1,H100,1697101595900,1697101596672.0,120,96.0,4.0,"[6, 504, 107, 80, 75]","[1697101595906, 1697101596410, 1697101596517, 1697101596597, 1697101596672]"
654,654,207,1,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101601196,1697101602111.0,120,10.0,1.0,"[107, 808]","[1697101601303, 1697101602111]"
655,655,565,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101602112,1697101604271.0,120,,,"[6, 1175, 115, 101, 75]","[1697101602118, 1697101603293, 1697101603408, 1697101603509, 1697101603584]"
656,656,328,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[125, 774]","[1697101603420, 1697101604194]"
657,657,351,2,[],200,llama-13b,128,1,1338.0,1.0,1,H100,1697101596673,1697101598011.0,120,216.0,6.0,"[7, 753, 219, 93, 90, 88, 88]","[1697101596680, 1697101597433, 1697101597652, 1697101597745, 1697101597835, 1697101597923, 1697101598011]"
658,658,312,2,[],200,llama-13b,128,1,2095.0,1.0,1,H100,1697101601198,1697101603293.0,120,23.0,1.0,"[536, 1559]","[1697101601734, 1697101603293]"
659,659,675,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603294,1697101604272.0,120,,,"[13, 887]","[1697101603307, 1697101604194]"
660,660,102,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600101,1697101601192.0,120,,,"[7, 985]","[1697101600108, 1697101601093]"
661,661,103,4,[],200,llama-13b,128,1,1931.0,1.0,1,H100,1697101604276,1697101606207.0,120,15.0,1.0,"[424, 1507]","[1697101604700, 1697101606207]"
662,662,545,4,[],200,llama-13b,128,1,2389.0,1.0,1,H100,1697101601194,1697101603583.0,120,216.0,5.0,"[9, 908, 44, 1253, 101, 74]","[1697101601203, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
663,663,895,3,[],200,llama-13b,128,1,1931.0,1.0,1,H100,1697101604276,1697101606207.0,120,15.0,1.0,"[340, 1591]","[1697101604616, 1697101606207]"
664,664,721,7,[],200,llama-13b,128,1,2746.0,1.0,1,H100,1697101609606,1697101612352.0,120,286.0,5.0,"[6, 868, 611, 1095, 83, 83]","[1697101609612, 1697101610480, 1697101611091, 1697101612186, 1697101612269, 1697101612352]"
665,665,587,8,[],200,llama-13b,128,1,1122.0,1.0,1,H100,1697101613015,1697101614137.0,120,13.0,1.0,"[30, 1092]","[1697101613045, 1697101614137]"
666,666,18,9,[],200,llama-13b,128,1,1262.0,1.0,1,H100,1697101614138,1697101615400.0,120,15.0,1.0,"[24, 1238]","[1697101614162, 1697101615400]"
667,667,375,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101615401,1697101616319.0,120,,,[6],[1697101615407]
668,668,324,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606208,1697101610267.0,120,,,"[31, 1030, 179, 90, 86, 84, 65, 83, 80, 491, 96, 72, 72, 94, 90, 87, 511, 78, 78]","[1697101606239, 1697101607269, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609605]"
669,669,729,11,[],200,llama-13b,128,1,2028.0,1.0,1,H100,1697101616336,1697101618364.0,120,874.0,2.0,"[454, 1574]","[1697101616790, 1697101618364]"
670,670,708,3,[],200,llama-13b,128,1,577.0,1.0,1,H100,1697101598012,1697101598589.0,120,140.0,1.0,"[12, 565]","[1697101598024, 1697101598589]"
671,671,156,12,[],200,llama-13b,128,1,3693.0,1.0,1,H100,1697101618365,1697101622058.0,120,86.0,20.0,"[6, 718, 263, 94, 88, 68, 85, 85, 83, 719, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 60]","[1697101618371, 1697101619089, 1697101619352, 1697101619446, 1697101619534, 1697101619602, 1697101619687, 1697101619772, 1697101619855, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057]"
672,672,112,4,[],200,llama-13b,128,1,1410.0,1.0,1,H100,1697101598590,1697101600000.0,120,16.0,2.0,"[19, 1157, 234]","[1697101598609, 1697101599766, 1697101600000]"
673,673,903,5,[],200,llama-13b,128,1,3864.0,1.0,1,H100,1697101603584,1697101607448.0,120,244.0,7.0,"[7, 921, 736, 1197, 81, 76, 74, 772]","[1697101603591, 1697101604512, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448]"
674,674,141,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604271.0,120,,,"[508, 1587, 116, 101, 74]","[1697101601705, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
675,675,705,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604276,1697101610269.0,120,,,"[49, 869, 54, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101604325, 1697101605194, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667, 1697101608761, 1697101608850, 1697101608937, 1697101609448, 1697101609527, 1697101609604]"
676,676,130,2,[],200,llama-13b,128,1,2095.0,1.0,1,H100,1697101601198,1697101603293.0,120,14.0,1.0,"[513, 1582]","[1697101601711, 1697101603293]"
677,677,467,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600001,1697101601192.0,120,,,[13],[1697101600014]
678,678,825,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604270.0,120,,,"[490, 1605, 116, 101, 74]","[1697101601687, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
679,679,480,3,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101603295,1697101604194.0,120,26.0,1.0,"[113, 786]","[1697101603408, 1697101604194]"
680,680,173,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101631815,1697101634994.0,120,,,"[14, 1137, 206, 95, 71, 94, 93, 92, 70, 614, 75, 71]","[1697101631829, 1697101632966, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
681,681,531,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616318.0,120,,,"[44, 1399]","[1697101614896, 1697101616295]"
682,682,237,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598827,1697101601193.0,120,,,"[6, 933, 234, 100, 100, 101, 91, 89]","[1697101598833, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
683,683,474,0,[],200,llama-13b,128,1,8079.0,1.0,1,H100,1697101591019,1697101599098.0,120,109.0,33.0,"[573, 1683, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 101, 99, 96, 94, 73, 92, 90, 528, 80, 75, 74, 905, 94, 89, 89, 87, 85, 730, 96, 90, 87]","[1697101591592, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598921, 1697101599011, 1697101599098]"
684,684,531,19,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634997,1697101640208.0,120,52.0,20.0,"[197, 1614, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 80, 615]","[1697101635194, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640208]"
685,685,130,4,[],200,llama-13b,128,1,1794.0,1.0,1,H100,1697101610275,1697101612069.0,120,14.0,1.0,"[403, 1391]","[1697101610678, 1697101612069]"
686,686,491,5,[],200,llama-13b,128,1,944.0,1.0,1,H100,1697101612070,1697101613014.0,120,14.0,1.0,"[60, 884]","[1697101612130, 1697101613014]"
687,687,406,4,[],200,llama-13b,128,1,2252.0,1.0,1,H100,1697101604274,1697101606526.0,120,244.0,4.0,"[21, 899, 54, 1197, 81]","[1697101604295, 1697101605194, 1697101605248, 1697101606445, 1697101606526]"
688,688,859,20,[],200,llama-13b,128,1,790.0,1.0,1,H100,1697101640212,1697101641002.0,120,23.0,1.0,"[58, 732]","[1697101640270, 1697101641002]"
689,689,382,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610267.0,120,,,"[18, 635]","[1697101609469, 1697101610104]"
690,690,869,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599099,1697101601191.0,120,,,"[13, 654, 234, 100, 100, 101, 91, 89]","[1697101599112, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
691,691,767,5,[],200,llama-13b,128,1,743.0,1.0,1,H100,1697101606527,1697101607270.0,120,11.0,1.0,"[7, 736]","[1697101606534, 1697101607270]"
692,692,195,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607271,1697101610268.0,120,,,"[35, 1027, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607306, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
693,693,551,7,[],200,llama-13b,128,1,4571.0,1.0,1,H100,1697101610278,1697101614849.0,120,90.0,20.0,"[490, 1301, 117, 84, 82, 80, 78, 611, 89, 87, 82, 82, 63, 79, 720, 95, 93, 92, 90, 69, 87]","[1697101610768, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613121, 1697101613210, 1697101613297, 1697101613379, 1697101613461, 1697101613524, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
694,694,218,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[130, 833, 1249, 101, 74]","[1697101601326, 1697101602159, 1697101603408, 1697101603509, 1697101603583]"
695,695,743,6,[],200,llama-13b,128,1,2402.0,1.0,1,H100,1697101604274,1697101606676.0,120,123.0,6.0,"[16, 904, 54, 1197, 81, 76, 74]","[1697101604290, 1697101605194, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676]"
696,696,545,4,[],200,llama-13b,128,1,2401.0,1.0,1,H100,1697101604275,1697101606676.0,120,216.0,5.0,"[335, 1597, 238, 81, 76, 74]","[1697101604610, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676]"
697,697,24,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621116,1697101623664.0,120,,,"[155, 1401, 87, 86, 84, 84]","[1697101621271, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
698,698,299,4,[],200,llama-13b,128,1,914.0,1.0,1,H100,1697101601197,1697101602111.0,120,14.0,1.0,"[129, 785]","[1697101601326, 1697101602111]"
699,699,166,7,[],200,llama-13b,128,1,1655.0,1.0,1,H100,1697101606678,1697101608333.0,120,14.0,1.0,"[6, 1649]","[1697101606684, 1697101608333]"
700,700,656,5,[],200,llama-13b,128,1,1181.0,1.0,1,H100,1697101602112,1697101603293.0,120,26.0,1.0,"[30, 1151]","[1697101602142, 1697101603293]"
701,701,916,10,[],200,llama-13b,128,1,1652.0,1.0,1,H100,1697101616604,1697101618256.0,120,8.0,1.0,"[310, 1342]","[1697101616914, 1697101618256]"
702,702,85,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[83, 816]","[1697101603378, 1697101604194]"
703,703,526,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608334,1697101610269.0,120,,,"[6, 1018, 91, 79, 77]","[1697101608340, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
704,704,444,7,[],200,llama-13b,128,1,3168.0,1.0,1,H100,1697101604280,1697101607448.0,120,457.0,6.0,"[525, 1402, 238, 81, 76, 74, 772]","[1697101604805, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448]"
705,705,899,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606678,1697101610268.0,120,,,"[18, 1637, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101606696, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
706,706,375,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625696,1697101626656.0,120,,,[19],[1697101625715]
707,707,734,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628904.0,120,,,"[220, 1808]","[1697101626884, 1697101628692]"
708,708,162,13,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628909,1697101633687.0,120,90.0,20.0,"[178, 1542, 104, 82, 68, 66, 864, 93, 91, 89, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70]","[1697101629087, 1697101630629, 1697101630733, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687]"
709,709,776,8,[],200,llama-13b,128,1,979.0,1.0,1,H100,1697101607449,1697101608428.0,120,67.0,2.0,"[12, 872, 95]","[1697101607461, 1697101608333, 1697101608428]"
710,710,905,8,[],200,llama-13b,128,1,1443.0,1.0,1,H100,1697101614852,1697101616295.0,120,11.0,1.0,"[122, 1321]","[1697101614974, 1697101616295]"
711,711,304,9,[],200,llama-13b,128,1,4744.0,1.0,1,H100,1697101616296,1697101621040.0,120,86.0,20.0,"[12, 295, 821, 939, 82, 80, 78, 79, 668, 95, 89, 67, 86, 84, 83, 720, 95, 94, 93, 92, 92]","[1697101616308, 1697101616603, 1697101617424, 1697101618363, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619350, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619771, 1697101619854, 1697101620574, 1697101620669, 1697101620763, 1697101620856, 1697101620948, 1697101621040]"
712,712,102,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601191.0,120,,,"[13, 902, 219, 93, 89, 89, 88, 84, 730, 96, 91, 86, 83, 819, 100, 100, 101, 91, 89]","[1697101596531, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598921, 1697101599012, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
713,713,640,2,[],200,llama-13b,128,1,760.0,1.0,1,H100,1697101596673,1697101597433.0,120,15.0,1.0,"[7, 753]","[1697101596680, 1697101597433]"
714,714,268,5,[],200,llama-13b,128,1,1181.0,1.0,1,H100,1697101602112,1697101603293.0,120,19.0,1.0,"[12, 1169]","[1697101602124, 1697101603293]"
715,715,481,2,[],200,llama-13b,128,1,914.0,1.0,1,H100,1697101596519,1697101597433.0,120,10.0,1.0,"[65, 849]","[1697101596584, 1697101597433]"
716,716,460,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601192.0,120,,,"[24, 1302]","[1697101599791, 1697101601093]"
717,717,602,6,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101603295,1697101604194.0,120,15.0,1.0,"[137, 762]","[1697101603432, 1697101604194]"
718,718,821,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604272.0,120,,,"[315, 1781, 115, 101, 74]","[1697101601512, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
719,719,251,5,[],200,llama-13b,128,1,1927.0,1.0,1,H100,1697101604280,1697101606207.0,120,31.0,1.0,"[520, 1407]","[1697101604800, 1697101606207]"
720,720,576,6,[],200,llama-13b,128,1,1062.0,1.0,1,H100,1697101606208,1697101607270.0,120,14.0,1.0,"[13, 1049]","[1697101606221, 1697101607270]"
721,721,30,7,[],200,llama-13b,128,1,4742.0,1.0,1,H100,1697101604195,1697101608937.0,120,93.0,20.0,"[42, 1011, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73, 94, 89, 87]","[1697101604237, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667, 1697101608761, 1697101608850, 1697101608937]"
722,722,119,0,[],200,llama-13b,128,1,993.0,1.0,1,H100,1697101590980,1697101591973.0,120,31.0,1.0,"[193, 800]","[1697101591173, 1697101591973]"
723,723,476,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101591974,1697101601193.0,120,,,"[25, 1004, 272, 83, 60, 80, 749, 98, 92, 89, 89, 85, 644, 102, 98, 96, 94, 73, 91, 91, 528, 80, 75, 74, 905, 94, 89, 89, 87, 85, 730, 97, 89, 87, 83, 819, 100, 100, 101, 91, 88]","[1697101591999, 1697101593003, 1697101593275, 1697101593358, 1697101593418, 1697101593498, 1697101594247, 1697101594345, 1697101594437, 1697101594526, 1697101594615, 1697101594700, 1697101595344, 1697101595446, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595898, 1697101595989, 1697101596517, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
724,724,453,28,[],200,llama-13b,128,1,814.0,1.0,1,H100,1697101641003,1697101641817.0,120,26.0,1.0,"[37, 777]","[1697101641040, 1697101641817]"
725,725,897,29,[],200,llama-13b,128,1,1313.0,1.0,1,H100,1697101641818,1697101643131.0,120,9.0,1.0,"[13, 1300]","[1697101641831, 1697101643131]"
726,726,373,13,[],200,llama-13b,128,1,893.0,1.0,1,H100,1697101626664,1697101627557.0,120,15.0,1.0,"[160, 733]","[1697101626824, 1697101627557]"
727,727,732,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101627558,1697101628906.0,120,,,"[30, 1105]","[1697101627588, 1697101628693]"
728,728,160,15,[],200,llama-13b,128,1,1721.0,1.0,1,H100,1697101628908,1697101630629.0,120,13.0,1.0,"[33, 1688]","[1697101628941, 1697101630629]"
729,729,234,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598827,1697101601193.0,120,,,"[24, 915, 234, 100, 100, 101, 91, 89]","[1697101598851, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
730,730,520,16,[],200,llama-13b,128,1,1054.0,1.0,1,H100,1697101630631,1697101631685.0,120,11.0,1.0,"[54, 1000]","[1697101630685, 1697101631685]"
731,731,645,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598923,1697101601191.0,120,,,"[6, 837, 234, 100, 100, 101, 91, 89]","[1697101598929, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
732,732,865,3,[],200,llama-13b,128,1,939.0,1.0,1,H100,1697101598827,1697101599766.0,120,9.0,1.0,"[24, 915]","[1697101598851, 1697101599766]"
733,733,288,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612070,1697101616320.0,120,,,"[54, 997, 90, 86, 83, 81, 62, 81, 719, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612124, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
734,734,244,4,[],200,llama-13b,128,1,1931.0,1.0,1,H100,1697101604276,1697101606207.0,120,9.0,1.0,"[400, 1531]","[1697101604676, 1697101606207]"
735,735,274,6,[],200,llama-13b,128,1,3582.0,1.0,1,H100,1697101604275,1697101607857.0,120,364.0,11.0,"[305, 1627, 238, 81, 76, 74, 772, 90, 86, 84, 64, 84]","[1697101604580, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772, 1697101607856]"
736,736,685,6,[],200,llama-13b,128,1,2169.0,1.0,1,H100,1697101604276,1697101606445.0,120,364.0,2.0,"[430, 1501, 238]","[1697101604706, 1697101606207, 1697101606445]"
737,737,601,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606208,1697101610267.0,120,,,"[37, 1025, 178, 90, 86, 84, 65, 83, 80, 491, 96, 72, 73, 93, 90, 87, 511, 78, 77]","[1697101606245, 1697101607270, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609604]"
738,738,436,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606208,1697101610267.0,120,,,"[25, 1036, 179, 90, 86, 84, 64, 84, 80, 491, 96, 72, 73, 93, 90, 86, 512, 78, 78]","[1697101606233, 1697101607269, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608668, 1697101608761, 1697101608851, 1697101608937, 1697101609449, 1697101609527, 1697101609605]"
739,739,210,4,[],200,llama-13b,128,1,2170.0,1.0,1,H100,1697101604275,1697101606445.0,120,140.0,2.0,"[311, 1621, 238]","[1697101604586, 1697101606207, 1697101606445]"
740,740,481,1,[],200,llama-13b,128,1,953.0,1.0,1,H100,1697101593005,1697101593958.0,120,10.0,1.0,"[116, 837]","[1697101593121, 1697101593958]"
741,741,567,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606446,1697101610267.0,120,,,"[7, 816, 179, 90, 86, 84, 65, 83, 80, 491, 95, 73, 72, 94, 90, 87, 511, 78, 78]","[1697101606453, 1697101607269, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609605]"
742,742,116,7,[],200,llama-13b,128,1,824.0,1.0,1,H100,1697101606446,1697101607270.0,120,23.0,1.0,"[13, 811]","[1697101606459, 1697101607270]"
743,743,444,8,[],200,llama-13b,128,1,1491.0,1.0,1,H100,1697101607271,1697101608762.0,120,457.0,6.0,"[12, 1050, 95, 95, 72, 73, 94]","[1697101607283, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762]"
744,744,839,2,[],200,llama-13b,128,1,1681.0,1.0,1,H100,1697101593959,1697101595640.0,120,58.0,5.0,"[30, 1231, 124, 101, 99, 96]","[1697101593989, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595640]"
745,745,83,6,[],200,llama-13b,128,1,4144.0,1.0,1,H100,1697101610274,1697101614418.0,120,123.0,15.0,"[239, 1555, 118, 84, 82, 81, 77, 611, 90, 85, 84, 81, 62, 80, 720, 95]","[1697101610513, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418]"
746,746,620,11,[],200,llama-13b,128,1,3114.0,1.0,1,H100,1697101616332,1697101619446.0,120,100.0,8.0,"[360, 1564, 108, 81, 81, 77, 79, 669, 95]","[1697101616692, 1697101618256, 1697101618364, 1697101618445, 1697101618526, 1697101618603, 1697101618682, 1697101619351, 1697101619446]"
747,747,485,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622059,1697101623660.0,120,,,"[37, 1476]","[1697101622096, 1697101623572]"
748,748,328,6,[],200,llama-13b,128,1,1313.0,1.0,1,H100,1697101607449,1697101608762.0,120,109.0,6.0,"[6, 878, 95, 95, 72, 73, 94]","[1697101607455, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762]"
749,749,802,9,[],200,llama-13b,128,1,595.0,1.0,1,H100,1697101608763,1697101609358.0,120,9.0,1.0,"[12, 583]","[1697101608775, 1697101609358]"
750,750,846,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623666,1697101626657.0,120,,,"[143, 759, 39, 1202, 97, 69]","[1697101623809, 1697101624568, 1697101624607, 1697101625809, 1697101625906, 1697101625975]"
751,751,687,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608763,1697101610266.0,120,,,"[6, 589, 91, 79, 77]","[1697101608769, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
752,752,232,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609359,1697101610267.0,120,,,[12],[1697101609371]
753,753,277,15,[],200,llama-13b,128,1,2026.0,1.0,1,H100,1697101626667,1697101628693.0,120,18.0,1.0,"[530, 1496]","[1697101627197, 1697101628693]"
754,754,586,11,[],200,llama-13b,128,1,4577.0,1.0,1,H100,1697101610272,1697101614849.0,120,85.0,20.0,"[247, 1549, 118, 84, 82, 81, 77, 611, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87]","[1697101610519, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
755,755,840,4,[],200,llama-13b,128,1,318.0,1.0,1,H100,1697101604195,1697101604513.0,120,17.0,1.0,"[48, 270]","[1697101604243, 1697101604513]"
756,756,255,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604275,1697101610278.0,120,,,"[253, 1679, 239, 80, 76, 74, 772, 90, 86, 84, 64, 84, 80, 491, 95, 73, 73, 93, 89, 88, 510, 79, 78]","[1697101604528, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608668, 1697101608761, 1697101608850, 1697101608938, 1697101609448, 1697101609527, 1697101609605]"
757,757,586,11,[],200,llama-13b,128,1,4708.0,1.0,1,H100,1697101616332,1697101621040.0,120,85.0,20.0,"[161, 883, 49, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92]","[1697101616493, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040]"
758,758,269,5,[],200,llama-13b,128,1,1693.0,1.0,1,H100,1697101604514,1697101606207.0,120,11.0,1.0,"[389, 1304]","[1697101604903, 1697101606207]"
759,759,892,12,[],200,llama-13b,128,1,4717.0,1.0,1,H100,1697101616324,1697101621041.0,120,87.0,20.0,"[270, 783, 48, 939, 81, 81, 78, 78, 668, 96, 88, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93]","[1697101616594, 1697101617377, 1697101617425, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619350, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041]"
760,760,603,6,[],200,llama-13b,128,1,1061.0,1.0,1,H100,1697101606209,1697101607270.0,120,9.0,1.0,"[66, 995]","[1697101606275, 1697101607270]"
761,761,357,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101644421,1697101651605.0,120,,,"[13, 903, 103, 79, 77, 77, 75, 741, 98, 97, 84, 78, 1236, 96, 101, 100, 97, 88, 485, 96, 97, 82, 941, 107, 103, 102, 90, 86]","[1697101644434, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
762,762,34,7,[],200,llama-13b,128,1,1062.0,1.0,1,H100,1697101607271,1697101608333.0,120,12.0,1.0,"[42, 1020]","[1697101607313, 1697101608333]"
763,763,916,10,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616332,1697101617377.0,120,8.0,1.0,"[268, 777]","[1697101616600, 1697101617377]"
764,764,755,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600302,1697101601192.0,120,,,"[10, 781]","[1697101600312, 1697101601093]"
765,765,184,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[112, 803, 44, 1253, 101, 74]","[1697101601308, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
766,766,547,5,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101604275,1697101605195.0,120,12.0,1.0,"[96, 823]","[1697101604371, 1697101605194]"
767,767,901,6,[],200,llama-13b,128,1,1011.0,1.0,1,H100,1697101605196,1697101606207.0,120,17.0,1.0,"[48, 963]","[1697101605244, 1697101606207]"
768,768,301,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606209,1697101610267.0,120,,,"[85, 975, 179, 90, 86, 84, 65, 83, 80, 491, 95, 73, 73, 93, 90, 87, 511, 78, 78]","[1697101606294, 1697101607269, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609605]"
769,769,401,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604270.0,120,,,"[389, 1708, 115, 101, 74]","[1697101601585, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
770,770,748,7,[],200,llama-13b,128,1,3332.0,1.0,1,H100,1697101610271,1697101613603.0,120,182.0,14.0,"[22, 798, 1095, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80]","[1697101610293, 1697101611091, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603]"
771,771,317,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101617378,1697101623658.0,120,,,"[90, 1620, 263, 95, 88, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60, 614, 87, 87, 84, 83]","[1697101617468, 1697101619088, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
772,772,493,4,[],200,llama-13b,128,1,4663.0,1.0,1,H100,1697101604275,1697101608938.0,120,83.0,20.0,"[134, 785, 54, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 73, 72, 94, 89, 88]","[1697101604409, 1697101605194, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608938]"
773,773,867,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101595990,1697101601191.0,120,,,"[19, 1424, 219, 93, 89, 89, 87, 85, 731, 96, 90, 86, 84, 818, 100, 100, 101, 91, 88]","[1697101596009, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
774,774,875,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610267.0,120,,,"[6, 647]","[1697101609457, 1697101610104]"
775,775,213,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600305,1697101601192.0,120,,,"[18, 770]","[1697101600323, 1697101601093]"
776,776,743,8,[],200,llama-13b,128,1,2232.0,1.0,1,H100,1697101610278,1697101612510.0,120,123.0,6.0,"[436, 1355, 117, 84, 82, 80, 78]","[1697101610714, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510]"
777,777,277,7,[],200,llama-13b,128,1,1790.0,1.0,1,H100,1697101610278,1697101612068.0,120,18.0,1.0,"[512, 1278]","[1697101610790, 1697101612068]"
778,778,634,8,[],200,llama-13b,128,1,944.0,1.0,1,H100,1697101612070,1697101613014.0,120,13.0,1.0,"[48, 896]","[1697101612118, 1697101613014]"
779,779,145,9,[],200,llama-13b,128,1,2971.0,1.0,1,H100,1697101612512,1697101615483.0,120,161.0,9.0,"[12, 1613, 186, 95, 93, 92, 90, 69, 88, 633]","[1697101612524, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483]"
780,780,68,9,[],200,llama-13b,128,1,1122.0,1.0,1,H100,1697101613015,1697101614137.0,120,12.0,1.0,"[24, 1098]","[1697101613039, 1697101614137]"
781,781,422,10,[],200,llama-13b,128,1,1261.0,1.0,1,H100,1697101614139,1697101615400.0,120,26.0,1.0,"[35, 1226]","[1697101614174, 1697101615400]"
782,782,383,15,[],200,llama-13b,128,1,2023.0,1.0,1,H100,1697101623671,1697101625694.0,120,15.0,1.0,"[397, 1626]","[1697101624068, 1697101625694]"
783,783,779,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101615401,1697101616319.0,120,,,"[30, 864]","[1697101615431, 1697101616295]"
784,784,748,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610268.0,120,,,[48],[1697101609499]
785,785,173,8,[],200,llama-13b,128,1,4571.0,1.0,1,H100,1697101610279,1697101614850.0,120,96.0,20.0,"[540, 1250, 117, 84, 82, 81, 77, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 70, 86]","[1697101610819, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614849]"
786,786,202,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608429,1697101610266.0,120,,,"[6, 923, 91, 79, 77]","[1697101608435, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
787,787,71,1,[],200,llama-13b,128,1,2580.0,1.0,1,H100,1697101596518,1697101599098.0,120,364.0,11.0,"[24, 891, 219, 93, 89, 89, 88, 85, 729, 96, 91, 86]","[1697101596542, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598921, 1697101599012, 1697101599098]"
788,788,744,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625696,1697101626656.0,120,,,"[7, 863]","[1697101625703, 1697101626566]"
789,789,321,3,[],200,llama-13b,128,1,2315.0,1.0,1,H100,1697101601194,1697101603509.0,120,182.0,4.0,"[27, 890, 44, 1253, 101]","[1697101601221, 1697101602111, 1697101602155, 1697101603408, 1697101603509]"
790,790,169,17,[],200,llama-13b,128,1,2029.0,1.0,1,H100,1697101626664,1697101628693.0,120,10.0,1.0,"[343, 1686]","[1697101627007, 1697101628693]"
791,791,499,18,[],200,llama-13b,128,1,4992.0,1.0,1,H100,1697101628695,1697101633687.0,120,88.0,20.0,"[36, 686, 1317, 81, 68, 66, 864, 92, 91, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 92, 71]","[1697101628731, 1697101629417, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631905, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633687]"
792,792,541,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604272.0,120,,,"[304, 1793, 115, 101, 74]","[1697101601500, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
793,793,489,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604278,1697101610266.0,120,,,"[410, 1519, 238, 81, 76, 74, 772, 90, 86, 84, 65, 83, 80, 491, 95, 73, 72, 94, 90, 87, 510, 79, 78]","[1697101604688, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609448, 1697101609527, 1697101609605]"
794,794,1,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607271,1697101610268.0,120,,,"[18, 1044, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607289, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
795,795,900,4,[],200,llama-13b,128,1,2401.0,1.0,1,H100,1697101604275,1697101606676.0,120,67.0,6.0,"[217, 702, 54, 1198, 80, 76, 74]","[1697101604492, 1697101605194, 1697101605248, 1697101606446, 1697101606526, 1697101606602, 1697101606676]"
796,796,332,5,[],200,llama-13b,128,1,1655.0,1.0,1,H100,1697101606678,1697101608333.0,120,39.0,1.0,"[24, 1631]","[1697101606702, 1697101608333]"
797,797,462,2,[],200,llama-13b,128,1,2097.0,1.0,1,H100,1697101601196,1697101603293.0,120,52.0,1.0,"[292, 1805]","[1697101601488, 1697101603293]"
798,798,686,6,[],200,llama-13b,128,1,1024.0,1.0,1,H100,1697101608334,1697101609358.0,120,31.0,1.0,"[24, 1000]","[1697101608358, 1697101609358]"
799,799,811,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[42, 857]","[1697101603337, 1697101604194]"
800,800,360,8,[],200,llama-13b,128,1,1790.0,1.0,1,H100,1697101610278,1697101612068.0,120,16.0,1.0,"[514, 1276]","[1697101610792, 1697101612068]"
801,801,718,9,[],200,llama-13b,128,1,944.0,1.0,1,H100,1697101612070,1697101613014.0,120,13.0,1.0,"[18, 926]","[1697101612088, 1697101613014]"
802,802,154,10,[],200,llama-13b,128,1,1122.0,1.0,1,H100,1697101613015,1697101614137.0,120,13.0,1.0,"[24, 1098]","[1697101613039, 1697101614137]"
803,803,484,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614138,1697101616320.0,120,,,"[24, 1238, 84, 78, 79, 76]","[1697101614162, 1697101615400, 1697101615484, 1697101615562, 1697101615641, 1697101615717]"
804,804,328,30,[],200,llama-13b,128,1,1625.0,1.0,1,H100,1697101643132,1697101644757.0,120,109.0,6.0,"[13, 1175, 100, 86, 84, 84, 82]","[1697101643145, 1697101644320, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756]"
805,805,239,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604279,1697101610266.0,120,,,"[397, 1531, 238, 81, 76, 74, 772, 90, 86, 84, 65, 83, 80, 491, 95, 73, 72, 94, 90, 87, 510, 79, 77]","[1697101604676, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609448, 1697101609527, 1697101609604]"
806,806,597,4,[],200,llama-13b,128,1,2095.0,1.0,1,H100,1697101601198,1697101603293.0,120,39.0,1.0,"[578, 1517]","[1697101601776, 1697101603293]"
807,807,113,5,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101603295,1697101604194.0,120,13.0,1.0,"[125, 774]","[1697101603420, 1697101604194]"
808,808,612,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604271.0,120,,,"[213, 1998, 101, 74]","[1697101601410, 1697101603408, 1697101603509, 1697101603583]"
809,809,323,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[43, 872, 44, 1253, 101, 74]","[1697101601239, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
810,810,36,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101631685,1697101634995.0,120,,,"[31, 1250, 206, 95, 71, 94, 93, 92, 70, 614, 75, 71]","[1697101631716, 1697101632966, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
811,811,348,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598827,1697101601193.0,120,,,"[30, 909, 234, 100, 100, 101, 91, 89]","[1697101598857, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
812,812,346,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614763,1697101616321.0,120,,,"[8, 629, 84, 79, 78, 76]","[1697101614771, 1697101615400, 1697101615484, 1697101615563, 1697101615641, 1697101615717]"
813,813,42,2,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101604275,1697101605195.0,120,10.0,1.0,"[128, 792]","[1697101604403, 1697101605195]"
814,814,404,3,[],200,llama-13b,128,1,4253.0,1.0,1,H100,1697101605196,1697101609449.0,120,87.0,20.0,"[30, 981, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 72, 94, 89, 87, 512]","[1697101605226, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608937, 1697101609449]"
815,815,734,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610268.0,120,,,"[30, 623]","[1697101609481, 1697101610104]"
816,816,165,5,[],200,llama-13b,128,1,4571.0,1.0,1,H100,1697101610278,1697101614849.0,120,83.0,20.0,"[500, 1290, 118, 84, 82, 80, 78, 610, 90, 86, 83, 82, 63, 79, 720, 95, 93, 92, 90, 69, 87]","[1697101610778, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613524, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
817,817,705,11,[],200,llama-13b,128,1,6425.0,1.0,1,H100,1697101616333,1697101622758.0,120,79.0,27.0,"[287, 1635, 109, 81, 81, 78, 78, 669, 95, 88, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93, 72, 644, 81, 80, 78, 61, 614, 87]","[1697101616620, 1697101618255, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041, 1697101621113, 1697101621757, 1697101621838, 1697101621918, 1697101621996, 1697101622057, 1697101622671, 1697101622758]"
818,818,918,3,[],200,llama-13b,128,1,2097.0,1.0,1,H100,1697101601196,1697101603293.0,120,23.0,1.0,"[310, 1787]","[1697101601506, 1697101603293]"
819,819,707,4,[],200,llama-13b,128,1,2095.0,1.0,1,H100,1697101601198,1697101603293.0,120,8.0,1.0,"[578, 1517]","[1697101601776, 1697101603293]"
820,820,345,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[30, 869]","[1697101603325, 1697101604194]"
821,821,264,3,[],200,llama-13b,128,1,4660.0,1.0,1,H100,1697101595641,1697101600301.0,120,86.0,20.0,"[6, 763, 107, 81, 74, 74, 905, 94, 89, 89, 87, 85, 731, 96, 89, 87, 84, 818, 100, 100, 101]","[1697101595647, 1697101596410, 1697101596517, 1697101596598, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301]"
822,822,710,5,[],200,llama-13b,128,1,1927.0,1.0,1,H100,1697101604280,1697101606207.0,120,14.0,1.0,"[599, 1328]","[1697101604879, 1697101606207]"
823,823,112,6,[],200,llama-13b,128,1,1240.0,1.0,1,H100,1697101606208,1697101607448.0,120,16.0,2.0,"[49, 1013, 178]","[1697101606257, 1697101607270, 1697101607448]"
824,824,919,3,[],200,llama-13b,128,1,939.0,1.0,1,H100,1697101598827,1697101599766.0,120,14.0,1.0,"[48, 891]","[1697101598875, 1697101599766]"
825,825,398,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[204, 1892, 116, 101, 74]","[1697101601400, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
826,826,756,2,[],200,llama-13b,128,1,1927.0,1.0,1,H100,1697101604280,1697101606207.0,120,19.0,1.0,"[593, 1334]","[1697101604873, 1697101606207]"
827,827,186,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606208,1697101610267.0,120,,,"[42, 1020, 178, 90, 86, 84, 65, 83, 80, 491, 96, 72, 72, 94, 90, 87, 511, 78, 78]","[1697101606250, 1697101607270, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609605]"
828,828,380,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604272.0,120,,,"[274, 1938, 101, 74]","[1697101601470, 1697101603408, 1697101603509, 1697101603583]"
829,829,22,12,[],200,llama-13b,128,1,626.0,1.0,1,H100,1697101621047,1697101621673.0,120,16.0,1.0,"[42, 584]","[1697101621089, 1697101621673]"
830,830,392,8,[],200,llama-13b,128,1,1024.0,1.0,1,H100,1697101608334,1697101609358.0,120,20.0,1.0,"[48, 976]","[1697101608382, 1697101609358]"
831,831,634,8,[],200,llama-13b,128,1,1062.0,1.0,1,H100,1697101606208,1697101607270.0,120,13.0,1.0,"[43, 1019]","[1697101606251, 1697101607270]"
832,832,468,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621674,1697101623664.0,120,,,"[7, 900, 91, 87, 86, 84, 84]","[1697101621681, 1697101622581, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
833,833,844,4,[],200,llama-13b,128,1,788.0,1.0,1,H100,1697101600305,1697101601093.0,120,10.0,1.0,"[24, 764]","[1697101600329, 1697101601093]"
834,834,243,5,[],200,llama-13b,128,1,2415.0,1.0,1,H100,1697101601094,1697101603509.0,120,67.0,4.0,"[6, 272, 783, 1253, 101]","[1697101601100, 1697101601372, 1697101602155, 1697101603408, 1697101603509]"
835,835,606,6,[],200,llama-13b,128,1,684.0,1.0,1,H100,1697101603510,1697101604194.0,120,9.0,1.0,"[6, 678]","[1697101603516, 1697101604194]"
836,836,68,9,[],200,llama-13b,128,1,1062.0,1.0,1,H100,1697101607271,1697101608333.0,120,12.0,1.0,"[24, 1038]","[1697101607295, 1697101608333]"
837,837,31,7,[],200,llama-13b,128,1,4742.0,1.0,1,H100,1697101604195,1697101608937.0,120,84.0,20.0,"[24, 293, 736, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73, 94, 89, 87]","[1697101604219, 1697101604512, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667, 1697101608761, 1697101608850, 1697101608937]"
838,838,475,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616322.0,120,,,"[13, 1429]","[1697101614865, 1697101616294]"
839,839,426,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608334,1697101610266.0,120,,,"[36, 988, 91, 79, 77]","[1697101608370, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
840,840,751,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609359,1697101610267.0,120,,,[30],[1697101609389]
841,841,172,10,[],200,llama-13b,128,1,1791.0,1.0,1,H100,1697101610278,1697101612069.0,120,19.0,1.0,"[428, 1363]","[1697101610706, 1697101612069]"
842,842,826,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623671,1697101626657.0,120,,,"[325, 1698, 115, 97, 69]","[1697101623996, 1697101625694, 1697101625809, 1697101625906, 1697101625975]"
843,843,317,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601198,1697101604271.0,120,,,"[501, 1593, 116, 101, 74]","[1697101601699, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
844,844,806,10,[],200,llama-13b,128,1,4782.0,1.0,1,H100,1697101616332,1697101621114.0,120,89.0,20.0,"[445, 1479, 108, 81, 80, 78, 79, 669, 95, 88, 67, 86, 85, 82, 720, 96, 93, 94, 91, 93, 73]","[1697101616777, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620763, 1697101620857, 1697101620948, 1697101621041, 1697101621114]"
845,845,299,4,[],200,llama-13b,128,1,476.0,1.0,1,H100,1697101607857,1697101608333.0,120,14.0,1.0,"[13, 463]","[1697101607870, 1697101608333]"
846,846,660,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610273,1697101616319.0,120,,,"[318, 1478, 117, 84, 82, 81, 77, 611, 90, 86, 83, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87, 634, 79, 78, 76]","[1697101610591, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
847,847,247,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626668,1697101628904.0,120,,,"[434, 1590]","[1697101627102, 1697101628692]"
848,848,660,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608334,1697101610266.0,120,,,"[24, 1000, 91, 79, 77]","[1697101608358, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
849,849,780,11,[],200,llama-13b,128,1,4578.0,1.0,1,H100,1697101610271,1697101614849.0,120,85.0,20.0,"[218, 1579, 118, 84, 82, 81, 77, 611, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87]","[1697101610489, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
850,850,606,16,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,9.0,1.0,"[388, 1332]","[1697101629297, 1697101630629]"
851,851,34,17,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101630634,1697101631685.0,120,12.0,1.0,"[64, 987]","[1697101630698, 1697101631685]"
852,852,85,6,[],200,llama-13b,128,1,4579.0,1.0,1,H100,1697101610270,1697101614849.0,120,88.0,20.0,"[222, 1576, 118, 84, 82, 81, 77, 611, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87]","[1697101610492, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
853,853,367,18,[],200,llama-13b,128,1,1839.0,1.0,1,H100,1697101631686,1697101633525.0,120,92.0,6.0,"[29, 1251, 206, 94, 72, 94, 93]","[1697101631715, 1697101632966, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525]"
854,854,639,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614604,1697101616321.0,120,,,"[6, 790, 84, 79, 78, 76]","[1697101614610, 1697101615400, 1697101615484, 1697101615563, 1697101615641, 1697101615717]"
855,855,730,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633526,1697101634995.0,120,,,"[6, 674, 96, 74, 71]","[1697101633532, 1697101634206, 1697101634302, 1697101634376, 1697101634447]"
856,856,158,20,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101635000,1697101640209.0,120,85.0,20.0,"[399, 1409, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 80, 616]","[1697101635399, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209]"
857,857,64,10,[],200,llama-13b,128,1,4780.0,1.0,1,H100,1697101616334,1697101621114.0,120,89.0,20.0,"[560, 1362, 108, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 98, 68]","[1697101616894, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621046, 1697101621114]"
858,858,231,11,[],200,llama-13b,128,1,1466.0,1.0,1,H100,1697101621116,1697101622582.0,120,13.0,1.0,"[81, 1385]","[1697101621197, 1697101622582]"
859,859,592,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622583,1697101623664.0,120,,,"[41, 948]","[1697101622624, 1697101623572]"
860,860,23,13,[],200,llama-13b,128,1,2023.0,1.0,1,H100,1697101623672,1697101625695.0,120,26.0,1.0,"[402, 1620]","[1697101624074, 1697101625694]"
861,861,467,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626656.0,120,,,[36],[1697101625733]
862,862,825,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628905.0,120,,,"[332, 1698]","[1697101626995, 1697101628693]"
863,863,887,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610275,1697101616320.0,120,,,"[319, 1475, 117, 84, 82, 81, 77, 611, 90, 86, 83, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87, 634, 79, 78, 76]","[1697101610594, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
864,864,88,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604270.0,120,,,"[418, 1678, 115, 101, 74]","[1697101601615, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
865,865,518,21,[],200,llama-13b,128,1,790.0,1.0,1,H100,1697101640212,1697101641002.0,120,23.0,1.0,"[112, 678]","[1697101640324, 1697101641002]"
866,866,679,4,[],200,llama-13b,128,1,684.0,1.0,1,H100,1697101603510,1697101604194.0,120,15.0,1.0,"[12, 672]","[1697101603522, 1697101604194]"
867,867,292,2,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101601196,1697101602111.0,120,286.0,1.0,"[19, 896]","[1697101601215, 1697101602111]"
868,868,561,10,[],200,llama-13b,128,1,4491.0,1.0,1,H100,1697101610271,1697101614762.0,120,87.0,20.0,"[131, 644, 46, 1094, 84, 82, 80, 78, 610, 90, 86, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69]","[1697101610402, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762]"
869,869,872,22,[],200,llama-13b,128,1,3754.0,1.0,1,H100,1697101641003,1697101644757.0,120,91.0,20.0,"[30, 784, 305, 100, 98, 97, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 86, 84, 84, 82]","[1697101641033, 1697101641817, 1697101642122, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756]"
870,870,80,5,[],200,llama-13b,128,1,317.0,1.0,1,H100,1697101604196,1697101604513.0,120,13.0,1.0,"[71, 246]","[1697101604267, 1697101604513]"
871,871,172,11,[],200,llama-13b,128,1,626.0,1.0,1,H100,1697101621048,1697101621674.0,120,19.0,1.0,"[65, 561]","[1697101621113, 1697101621674]"
872,872,124,4,[],200,llama-13b,128,1,1410.0,1.0,1,H100,1697101598590,1697101600000.0,120,83.0,2.0,"[7, 1403]","[1697101598597, 1697101600000]"
873,873,835,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597434,1697101601193.0,120,,,"[24, 1131, 237, 96, 89, 87, 84, 818, 100, 100, 101, 90, 89]","[1697101597458, 1697101598589, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600391, 1697101600480]"
874,874,432,2,[],200,llama-13b,128,1,667.0,1.0,1,H100,1697101599099,1697101599766.0,120,13.0,1.0,"[13, 654]","[1697101599112, 1697101599766]"
875,875,333,7,[],200,llama-13b,128,1,2261.0,1.0,1,H100,1697101613380,1697101615641.0,120,563.0,11.0,"[7, 750, 187, 94, 93, 92, 90, 69, 88, 633, 79, 79]","[1697101613387, 1697101614137, 1697101614324, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641]"
876,876,792,3,[],200,llama-13b,128,1,1326.0,1.0,1,H100,1697101599767,1697101601093.0,120,11.0,1.0,"[48, 1278]","[1697101599815, 1697101601093]"
877,877,570,5,[],200,llama-13b,128,1,1092.0,1.0,1,H100,1697101600001,1697101601093.0,120,18.0,1.0,"[12, 1080]","[1697101600013, 1697101601093]"
878,878,223,4,[],200,llama-13b,128,1,278.0,1.0,1,H100,1697101601094,1697101601372.0,120,16.0,1.0,"[30, 248]","[1697101601124, 1697101601372]"
879,879,928,6,[],200,llama-13b,128,1,278.0,1.0,1,H100,1697101601094,1697101601372.0,120,20.0,1.0,"[36, 242]","[1697101601130, 1697101601372]"
880,880,577,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601373,1697101604271.0,120,,,"[427, 1608, 101, 75]","[1697101601800, 1697101603408, 1697101603509, 1697101603584]"
881,881,499,1,[],200,llama-13b,128,1,4210.0,1.0,1,H100,1697101594616,1697101598826.0,120,88.0,20.0,"[7, 597, 124, 102, 98, 96, 94, 73, 92, 90, 527, 81, 75, 74, 905, 94, 89, 89, 88, 85, 730]","[1697101594623, 1697101595220, 1697101595344, 1697101595446, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598826]"
882,882,438,6,[],200,llama-13b,128,1,1693.0,1.0,1,H100,1697101604514,1697101606207.0,120,9.0,1.0,"[395, 1298]","[1697101604909, 1697101606207]"
883,883,682,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101644759,1697101651605.0,120,,,"[37, 1503, 190, 98, 98, 83, 78, 1226, 106, 101, 100, 97, 87, 487, 96, 96, 82, 942, 106, 103, 102, 91, 85]","[1697101644796, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648563, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
884,884,704,2,[],200,llama-13b,128,1,667.0,1.0,1,H100,1697101599099,1697101599766.0,120,14.0,1.0,"[19, 648]","[1697101599118, 1697101599766]"
885,885,135,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601192.0,120,,,"[30, 1296]","[1697101599797, 1697101601093]"
886,886,264,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601191.0,120,,,"[24, 1302]","[1697101599791, 1697101601093]"
887,887,493,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604271.0,120,,,"[502, 1593, 116, 101, 74]","[1697101601699, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
888,888,842,12,[],200,llama-13b,128,1,4337.0,1.0,1,H100,1697101616332,1697101620669.0,120,161.0,16.0,"[83, 962, 48, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95]","[1697101616415, 1697101617377, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669]"
889,889,847,5,[],200,llama-13b,128,1,919.0,1.0,1,H100,1697101604275,1697101605194.0,120,10.0,1.0,"[38, 881]","[1697101604313, 1697101605194]"
890,890,623,5,[],200,llama-13b,128,1,2212.0,1.0,1,H100,1697101601196,1697101603408.0,120,140.0,3.0,"[77, 882, 1253]","[1697101601273, 1697101602155, 1697101603408]"
891,891,860,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598827,1697101601191.0,120,,,"[54, 885, 234, 100, 100, 101, 91, 89]","[1697101598881, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
892,892,146,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612353,1697101616321.0,120,,,"[6, 655, 107, 90, 86, 83, 81, 63, 80, 719, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612359, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613524, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
893,893,419,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612071,1697101616321.0,120,,,"[110, 833, 107, 90, 86, 83, 81, 62, 81, 719, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612181, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
894,894,609,8,[],200,llama-13b,128,1,4567.0,1.0,1,H100,1697101610283,1697101614850.0,120,88.0,20.0,"[591, 1195, 117, 84, 82, 81, 78, 609, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 70, 86]","[1697101610874, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614849]"
895,895,679,5,[],200,llama-13b,128,1,773.0,1.0,1,H100,1697101610273,1697101611046.0,120,15.0,1.0,"[135, 638]","[1697101610408, 1697101611046]"
896,896,103,2,[],200,llama-13b,128,1,2095.0,1.0,1,H100,1697101601197,1697101603292.0,120,15.0,1.0,"[191, 1904]","[1697101601388, 1697101603292]"
897,897,851,6,[],200,llama-13b,128,1,1122.0,1.0,1,H100,1697101613015,1697101614137.0,120,23.0,1.0,"[42, 1080]","[1697101613057, 1697101614137]"
898,898,283,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614138,1697101616320.0,120,,,"[18, 1244, 84, 78, 79, 76]","[1697101614156, 1697101615400, 1697101615484, 1697101615562, 1697101615641, 1697101615717]"
899,899,569,4,[],200,llama-13b,128,1,2095.0,1.0,1,H100,1697101601197,1697101603292.0,120,16.0,1.0,"[179, 1916]","[1697101601376, 1697101603292]"
900,900,0,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[36, 863]","[1697101603331, 1697101604194]"
901,901,361,6,[],200,llama-13b,128,1,3263.0,1.0,1,H100,1697101604275,1697101607538.0,120,67.0,7.0,"[299, 1633, 239, 80, 76, 74, 772, 90]","[1697101604574, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538]"
902,902,718,7,[],200,llama-13b,128,1,794.0,1.0,1,H100,1697101607539,1697101608333.0,120,13.0,1.0,"[6, 788]","[1697101607545, 1697101608333]"
903,903,238,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600303,1697101601192.0,120,,,"[14, 776]","[1697101600317, 1697101601093]"
904,904,116,8,[],200,llama-13b,128,1,1024.0,1.0,1,H100,1697101608334,1697101609358.0,120,23.0,1.0,"[30, 994]","[1697101608364, 1697101609358]"
905,905,469,9,[],200,llama-13b,128,1,745.0,1.0,1,H100,1697101609359,1697101610104.0,120,17.0,1.0,"[24, 721]","[1697101609383, 1697101610104]"
906,906,827,10,[],200,llama-13b,128,1,4657.0,1.0,1,H100,1697101610105,1697101614762.0,120,96.0,20.0,"[18, 357, 611, 1095, 84, 82, 80, 78, 610, 90, 86, 83, 81, 63, 80, 720, 95, 92, 93, 90, 69]","[1697101610123, 1697101610480, 1697101611091, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613460, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614510, 1697101614603, 1697101614693, 1697101614762]"
907,907,611,8,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616332,1697101617377.0,120,14.0,1.0,"[60, 984]","[1697101616392, 1697101617376]"
908,908,289,21,[],200,llama-13b,128,1,3753.0,1.0,1,H100,1697101641003,1697101644756.0,120,89.0,20.0,"[7, 807, 304, 101, 98, 97, 88, 82, 63, 801, 99, 95, 93, 96, 86, 85, 415, 86, 84, 84, 82]","[1697101641010, 1697101641817, 1697101642121, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643550, 1697101643645, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756]"
909,909,38,9,[],200,llama-13b,128,1,4679.0,1.0,1,H100,1697101617378,1697101622057.0,120,88.0,20.0,"[54, 1656, 263, 95, 88, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60]","[1697101617432, 1697101619088, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
910,910,255,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628910,1697101634994.0,120,,,"[416, 1303, 105, 81, 68, 66, 865, 92, 90, 90, 89, 87, 84, 84, 741, 95, 72, 93, 93, 92, 71, 614, 75, 71]","[1697101629326, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
911,911,261,11,[],200,llama-13b,128,1,721.0,1.0,1,H100,1697101614763,1697101615484.0,120,874.0,2.0,"[25, 696]","[1697101614788, 1697101615484]"
912,912,593,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[202, 1894, 116, 101, 74]","[1697101601398, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
913,913,622,12,[],200,llama-13b,128,1,810.0,1.0,1,H100,1697101615485,1697101616295.0,120,20.0,1.0,"[12, 798]","[1697101615497, 1697101616295]"
914,914,110,4,[],200,llama-13b,128,1,2322.0,1.0,1,H100,1697101604280,1697101606602.0,120,96.0,4.0,"[336, 1591, 238, 81, 76]","[1697101604616, 1697101606207, 1697101606445, 1697101606526, 1697101606602]"
915,915,464,5,[],200,llama-13b,128,1,667.0,1.0,1,H100,1697101606603,1697101607270.0,120,12.0,1.0,"[7, 660]","[1697101606610, 1697101607270]"
916,916,825,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607271,1697101610268.0,120,,,"[24, 1038, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607295, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
917,917,445,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616322.0,120,,,"[38, 1405]","[1697101614890, 1697101616295]"
918,918,24,13,[],200,llama-13b,128,1,3149.0,1.0,1,H100,1697101616296,1697101619445.0,120,79.0,9.0,"[18, 1111, 938, 82, 80, 78, 79, 668, 95]","[1697101616314, 1697101617425, 1697101618363, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619350, 1697101619445]"
919,919,261,11,[],200,llama-13b,128,1,1202.0,1.0,1,H100,1697101613122,1697101614324.0,120,874.0,2.0,"[6, 1195]","[1697101613128, 1697101614323]"
920,920,619,12,[],200,llama-13b,128,1,1075.0,1.0,1,H100,1697101614325,1697101615400.0,120,10.0,1.0,"[6, 1069]","[1697101614331, 1697101615400]"
921,921,21,13,[],200,llama-13b,128,1,894.0,1.0,1,H100,1697101615401,1697101616295.0,120,15.0,1.0,"[18, 876]","[1697101615419, 1697101616295]"
922,922,382,14,[],200,llama-13b,128,1,4744.0,1.0,1,H100,1697101616296,1697101621040.0,120,47.0,20.0,"[36, 1044, 49, 939, 81, 80, 78, 79, 668, 95, 89, 67, 86, 85, 82, 720, 95, 94, 94, 91, 92]","[1697101616332, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619350, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620763, 1697101620857, 1697101620948, 1697101621040]"
923,923,71,3,[],200,llama-13b,128,1,2958.0,1.0,1,H100,1697101597434,1697101600392.0,120,364.0,11.0,"[25, 1130, 236, 97, 89, 87, 84, 818, 101, 99, 101, 90]","[1697101597459, 1697101598589, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600101, 1697101600200, 1697101600301, 1697101600391]"
924,924,71,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[101, 814, 44, 1253, 101, 74]","[1697101601297, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
925,925,885,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601198,1697101604271.0,120,,,"[518, 1577, 115, 101, 74]","[1697101601716, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
926,926,421,2,[],200,llama-13b,128,1,4663.0,1.0,1,H100,1697101604275,1697101608938.0,120,85.0,20.0,"[116, 804, 53, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 73, 72, 94, 89, 88]","[1697101604391, 1697101605195, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608938]"
927,927,254,6,[],200,llama-13b,128,1,774.0,1.0,1,H100,1697101610272,1697101611046.0,120,58.0,1.0,"[32, 742]","[1697101610304, 1697101611046]"
928,928,98,1,[],200,llama-13b,128,1,1443.0,1.0,1,H100,1697101595990,1697101597433.0,120,14.0,1.0,"[37, 1406]","[1697101596027, 1697101597433]"
929,929,406,4,[],200,llama-13b,128,1,2327.0,1.0,1,H100,1697101604275,1697101606602.0,120,244.0,4.0,"[311, 1621, 238, 81, 76]","[1697101604586, 1697101606207, 1697101606445, 1697101606526, 1697101606602]"
930,930,456,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597434,1697101601192.0,120,,,"[6, 1149, 236, 97, 89, 87, 84, 818, 101, 99, 101, 91, 88]","[1697101597440, 1697101598589, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600101, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
931,931,608,7,[],200,llama-13b,128,1,3803.0,1.0,1,H100,1697101611047,1697101614850.0,120,96.0,20.0,"[12, 1010, 117, 84, 82, 81, 78, 609, 90, 86, 84, 81, 62, 80, 721, 94, 93, 92, 90, 70, 87]","[1697101611059, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613120, 1697101613210, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614324, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614850]"
932,932,690,8,[],200,llama-13b,128,1,653.0,1.0,1,H100,1697101615642,1697101616295.0,120,39.0,1.0,"[6, 647]","[1697101615648, 1697101616295]"
933,933,267,23,[],200,llama-13b,128,1,5818.0,1.0,1,H100,1697101644759,1697101650577.0,120,83.0,20.0,"[110, 1430, 190, 98, 98, 83, 78, 1226, 106, 101, 100, 97, 88, 486, 96, 96, 82, 942, 106, 104, 101]","[1697101644869, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577]"
934,934,120,9,[],200,llama-13b,128,1,1079.0,1.0,1,H100,1697101616297,1697101617376.0,120,17.0,1.0,"[41, 1038]","[1697101616338, 1697101617376]"
935,935,456,10,[],200,llama-13b,128,1,3736.0,1.0,1,H100,1697101617378,1697101621114.0,120,90.0,20.0,"[36, 842, 108, 82, 80, 78, 79, 668, 94, 89, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74]","[1697101617414, 1697101618256, 1697101618364, 1697101618446, 1697101618526, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114]"
936,936,673,8,[],200,llama-13b,128,1,4709.0,1.0,1,H100,1697101616332,1697101621041.0,120,93.0,20.0,"[203, 890, 939, 81, 81, 78, 78, 669, 94, 89, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93]","[1697101616535, 1697101617425, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041]"
937,937,474,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604195,1697101610270.0,120,,,"[18, 1035, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101604213, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667, 1697101608761, 1697101608850, 1697101608937, 1697101609448, 1697101609527, 1697101609604]"
938,938,262,2,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101601196,1697101602111.0,120,39.0,1.0,"[89, 826]","[1697101601285, 1697101602111]"
939,939,626,24,[],200,llama-13b,128,1,1297.0,1.0,1,H100,1697101650579,1697101651876.0,120,10.0,1.0,"[48, 1249]","[1697101650627, 1697101651876]"
940,940,59,25,[],200,llama-13b,128,1,4735.0,1.0,1,H100,1697101651877,1697101656612.0,120,91.0,20.0,"[294, 1301, 138, 86, 82, 82, 81, 896, 107, 106, 100, 75, 99, 97, 92, 614, 105, 101, 98, 97, 84]","[1697101652171, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656612]"
941,941,625,3,[],200,llama-13b,128,1,1297.0,1.0,1,H100,1697101602112,1697101603409.0,120,364.0,2.0,"[6, 1175, 115]","[1697101602118, 1697101603293, 1697101603408]"
942,942,679,5,[],200,llama-13b,128,1,1932.0,1.0,1,H100,1697101604275,1697101606207.0,120,15.0,1.0,"[329, 1603]","[1697101604604, 1697101606207]"
943,943,108,6,[],200,llama-13b,128,1,1240.0,1.0,1,H100,1697101606208,1697101607448.0,120,182.0,2.0,"[25, 1036, 179]","[1697101606233, 1697101607269, 1697101607448]"
944,944,395,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608940,1697101610267.0,120,,,"[47, 1117]","[1697101608987, 1697101610104]"
945,945,642,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597924,1697101601193.0,120,,,"[6, 659, 237, 96, 90, 86, 84, 818, 100, 100, 101, 91, 89]","[1697101597930, 1697101598589, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
946,946,440,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607449,1697101610268.0,120,,,"[12, 872, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607461, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
947,947,799,8,[],200,llama-13b,128,1,4572.0,1.0,1,H100,1697101610278,1697101614850.0,120,84.0,20.0,"[524, 1266, 118, 84, 82, 81, 77, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 70, 86]","[1697101610802, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614849]"
948,948,748,9,[],200,llama-13b,128,1,3330.0,1.0,1,H100,1697101610273,1697101613603.0,120,182.0,14.0,"[99, 720, 1094, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80]","[1697101610372, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603]"
949,949,144,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613605,1697101616320.0,120,,,"[12, 1783, 83, 79, 79, 75]","[1697101613617, 1697101615400, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
950,950,542,6,[],200,llama-13b,128,1,1165.0,1.0,1,H100,1697101608939,1697101610104.0,120,11.0,1.0,"[7, 1158]","[1697101608946, 1697101610104]"
951,951,631,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607858,1697101610278.0,120,,,"[17, 553, 95, 73, 72, 94, 89, 87, 511, 79, 77]","[1697101607875, 1697101608428, 1697101608523, 1697101608596, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
952,952,73,2,[],200,llama-13b,128,1,2094.0,1.0,1,H100,1697101601198,1697101603292.0,120,9.0,1.0,"[477, 1617]","[1697101601675, 1697101603292]"
953,953,401,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603294,1697101604272.0,120,,,"[37, 863]","[1697101603331, 1697101604194]"
954,954,759,4,[],200,llama-13b,128,1,4662.0,1.0,1,H100,1697101604275,1697101608937.0,120,92.0,20.0,"[9, 910, 54, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 74, 93, 89, 87]","[1697101604284, 1697101605194, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608668, 1697101608761, 1697101608850, 1697101608937]"
955,955,349,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601191.0,120,,,"[6, 1320]","[1697101599773, 1697101601093]"
956,956,377,3,[],200,llama-13b,128,1,2096.0,1.0,1,H100,1697101601196,1697101603292.0,120,13.0,1.0,"[208, 1888]","[1697101601404, 1697101603292]"
957,957,138,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622759,1697101623664.0,120,,,"[7, 806]","[1697101622766, 1697101623572]"
958,958,492,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623671,1697101626657.0,120,,,"[404, 1619, 114, 98, 69]","[1697101624075, 1697101625694, 1697101625808, 1697101625906, 1697101625975]"
959,959,232,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616318.0,120,,,[68],[1697101614920]
960,960,637,16,[],200,llama-13b,128,1,4992.0,1.0,1,H100,1697101628695,1697101633687.0,120,96.0,20.0,"[66, 656, 1317, 81, 68, 66, 864, 93, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 92, 71]","[1697101628761, 1697101629417, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633687]"
961,961,738,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,[48],[1697101603343]
962,962,163,5,[],200,llama-13b,128,1,3172.0,1.0,1,H100,1697101604276,1697101607448.0,120,67.0,6.0,"[328, 1603, 238, 81, 76, 74, 772]","[1697101604604, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448]"
963,963,822,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628905.0,120,,,"[325, 1704]","[1697101626989, 1697101628693]"
964,964,591,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600306,1697101601192.0,120,,,"[29, 758]","[1697101600335, 1697101601093]"
965,965,495,7,[],200,llama-13b,128,1,919.0,1.0,1,H100,1697101604275,1697101605194.0,120,13.0,1.0,"[138, 781]","[1697101604413, 1697101605194]"
966,966,522,6,[],200,llama-13b,128,1,884.0,1.0,1,H100,1697101607449,1697101608333.0,120,20.0,1.0,"[24, 860]","[1697101607473, 1697101608333]"
967,967,19,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601198,1697101604271.0,120,,,"[489, 1605, 116, 101, 74]","[1697101601687, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
968,968,251,15,[],200,llama-13b,128,1,1719.0,1.0,1,H100,1697101628910,1697101630629.0,120,31.0,1.0,"[357, 1362]","[1697101629267, 1697101630629]"
969,969,605,16,[],200,llama-13b,128,1,1053.0,1.0,1,H100,1697101630631,1697101631684.0,120,8.0,1.0,"[36, 1017]","[1697101630667, 1697101631684]"
970,970,381,6,[],200,llama-13b,128,1,2171.0,1.0,1,H100,1697101604275,1697101606446.0,120,140.0,2.0,"[241, 1691, 239]","[1697101604516, 1697101606207, 1697101606446]"
971,971,854,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101605195,1697101610266.0,120,,,"[13, 999, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 72, 94, 89, 88, 510, 80, 77]","[1697101605208, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608938, 1697101609448, 1697101609528, 1697101609605]"
972,972,38,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101631685,1697101634994.0,120,,,"[24, 1257, 206, 94, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101631709, 1697101632966, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
973,973,740,7,[],200,llama-13b,128,1,2404.0,1.0,1,H100,1697101606447,1697101608851.0,120,563.0,14.0,"[18, 805, 178, 90, 86, 84, 65, 83, 80, 491, 96, 72, 73, 94, 89]","[1697101606465, 1697101607270, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851]"
974,974,32,6,[],200,llama-13b,128,1,2232.0,1.0,1,H100,1697101610278,1697101612510.0,120,140.0,6.0,"[508, 1282, 118, 84, 82, 80, 78]","[1697101610786, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510]"
975,975,119,8,[],200,llama-13b,128,1,775.0,1.0,1,H100,1697101610271,1697101611046.0,120,31.0,1.0,"[101, 674]","[1697101610372, 1697101611046]"
976,976,382,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612512,1697101616321.0,120,,,"[18, 1607, 186, 95, 93, 92, 90, 69, 88, 633, 80, 77, 76]","[1697101612530, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615563, 1697101615640, 1697101615716]"
977,977,449,9,[],200,llama-13b,128,1,3803.0,1.0,1,H100,1697101611047,1697101614850.0,120,86.0,20.0,"[30, 992, 117, 84, 82, 81, 78, 609, 90, 86, 84, 81, 62, 80, 720, 95, 93, 92, 90, 70, 87]","[1697101611077, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613120, 1697101613210, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614850]"
978,978,617,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612070,1697101616321.0,120,,,"[66, 878, 107, 90, 86, 83, 81, 62, 81, 719, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612136, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
979,979,732,3,[],200,llama-13b,128,1,3582.0,1.0,1,H100,1697101604274,1697101607856.0,120,345.0,12.0,"[33, 887, 54, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84]","[1697101604307, 1697101605194, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856]"
980,980,551,4,[],200,llama-13b,128,1,4489.0,1.0,1,H100,1697101610273,1697101614762.0,120,90.0,20.0,"[117, 656, 46, 1094, 83, 83, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 69]","[1697101610390, 1697101611046, 1697101611092, 1697101612186, 1697101612269, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762]"
981,981,675,2,[],200,llama-13b,128,1,2401.0,1.0,1,H100,1697101604275,1697101606676.0,120,563.0,5.0,"[247, 1685, 239, 80, 76, 74]","[1697101604522, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676]"
982,982,287,9,[],200,llama-13b,128,1,1796.0,1.0,1,H100,1697101610272,1697101612068.0,120,10.0,1.0,"[238, 1558]","[1697101610510, 1697101612068]"
983,983,717,27,[],200,llama-13b,128,1,5001.0,1.0,1,H100,1697101651610,1697101656611.0,120,89.0,20.0,"[306, 1556, 138, 85, 84, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 82]","[1697101651916, 1697101653472, 1697101653610, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656611]"
984,984,179,8,[],200,llama-13b,128,1,2036.0,1.0,1,H100,1697101613605,1697101615641.0,120,161.0,4.0,"[6, 1788, 85, 78, 79]","[1697101613611, 1697101615399, 1697101615484, 1697101615562, 1697101615641]"
985,985,617,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612070,1697101616320.0,120,,,"[24, 920, 107, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612094, 1697101613014, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
986,986,76,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606678,1697101610268.0,120,,,"[12, 1643, 94, 96, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101606690, 1697101608333, 1697101608427, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
987,987,319,11,[],200,llama-13b,128,1,831.0,1.0,1,H100,1697101618257,1697101619088.0,120,31.0,1.0,"[30, 801]","[1697101618287, 1697101619088]"
988,988,678,12,[],200,llama-13b,128,1,3922.0,1.0,1,H100,1697101619090,1697101623012.0,120,244.0,18.0,"[48, 1241, 196, 95, 94, 93, 91, 92, 74, 643, 81, 81, 78, 60, 615, 86, 87, 84, 83]","[1697101619138, 1697101620379, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621757, 1697101621838, 1697101621919, 1697101621997, 1697101622057, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
989,989,421,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621116,1697101623664.0,120,,,"[99, 1366, 91, 87, 86, 84, 84]","[1697101621215, 1697101622581, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
990,990,783,12,[],200,llama-13b,128,1,2026.0,1.0,1,H100,1697101623669,1697101625695.0,120,286.0,1.0,"[249, 1777]","[1697101623918, 1697101625695]"
991,991,612,17,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634997,1697101640208.0,120,93.0,20.0,"[131, 1769, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 79, 616]","[1697101635128, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208]"
992,992,502,10,[],200,llama-13b,128,1,810.0,1.0,1,H100,1697101615485,1697101616295.0,120,19.0,1.0,"[6, 804]","[1697101615491, 1697101616295]"
993,993,103,13,[],200,llama-13b,128,1,860.0,1.0,1,H100,1697101623014,1697101623874.0,120,15.0,1.0,"[6, 853]","[1697101623020, 1697101623873]"
994,994,187,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626656.0,120,,,"[12, 857]","[1697101625709, 1697101626566]"
995,995,457,14,[],200,llama-13b,128,1,1935.0,1.0,1,H100,1697101623874,1697101625809.0,120,874.0,2.0,"[398, 1537]","[1697101624272, 1697101625809]"
996,996,817,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625810,1697101626657.0,120,,,"[6, 750]","[1697101625816, 1697101626566]"
997,997,646,3,[],200,llama-13b,128,1,1181.0,1.0,1,H100,1697101602112,1697101603293.0,120,14.0,1.0,"[24, 1157]","[1697101602136, 1697101603293]"
998,998,432,4,[],200,llama-13b,128,1,700.0,1.0,1,H100,1697101600393,1697101601093.0,120,13.0,1.0,"[6, 694]","[1697101600399, 1697101601093]"
999,999,859,11,[],200,llama-13b,128,1,1079.0,1.0,1,H100,1697101616297,1697101617376.0,120,23.0,1.0,"[70, 1009]","[1697101616367, 1697101617376]"
1000,1000,289,12,[],200,llama-13b,128,1,3736.0,1.0,1,H100,1697101617378,1697101621114.0,120,89.0,20.0,"[37, 841, 108, 82, 80, 78, 79, 668, 95, 88, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74]","[1697101617415, 1697101618256, 1697101618364, 1697101618446, 1697101618526, 1697101618604, 1697101618683, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114]"
1001,1001,880,5,[],200,llama-13b,128,1,1061.0,1.0,1,H100,1697101601094,1697101602155.0,120,84.0,2.0,"[12, 266, 783]","[1697101601106, 1697101601372, 1697101602155]"
1002,1002,311,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101602160,1697101604271.0,120,,,[12],[1697101602172]
1003,1003,669,7,[],200,llama-13b,128,1,5169.0,1.0,1,H100,1697101604279,1697101609448.0,120,83.0,20.0,"[588, 1340, 238, 81, 76, 74, 772, 90, 86, 84, 65, 84, 79, 492, 94, 73, 73, 93, 90, 87, 510]","[1697101604867, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607857, 1697101607936, 1697101608428, 1697101608522, 1697101608595, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609448]"
1004,1004,894,3,[],200,llama-13b,128,1,1164.0,1.0,1,H100,1697101608940,1697101610104.0,120,14.0,1.0,"[53, 1111]","[1697101608993, 1697101610104]"
1005,1005,77,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[131, 768]","[1697101603426, 1697101604194]"
1006,1006,532,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621674,1697101623664.0,120,,,"[18, 890, 90, 87, 86, 84, 84]","[1697101621692, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
1007,1007,94,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610267.0,120,,,"[24, 629]","[1697101609475, 1697101610104]"
1008,1008,763,5,[],200,llama-13b,128,1,667.0,1.0,1,H100,1697101606603,1697101607270.0,120,20.0,1.0,"[6, 661]","[1697101606609, 1697101607270]"
1009,1009,448,9,[],200,llama-13b,128,1,3250.0,1.0,1,H100,1697101610274,1697101613524.0,120,335.0,12.0,"[408, 1387, 117, 84, 82, 80, 78, 611, 90, 86, 82, 82, 63]","[1697101610682, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613121, 1697101613211, 1697101613297, 1697101613379, 1697101613461, 1697101613524]"
1010,1010,38,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601191.0,120,,,"[13, 902, 219, 93, 89, 89, 88, 84, 730, 96, 91, 86, 83, 819, 100, 99, 102, 91, 89]","[1697101596531, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598921, 1697101599012, 1697101599098, 1697101599181, 1697101600000, 1697101600100, 1697101600199, 1697101600301, 1697101600392, 1697101600481]"
1011,1011,897,13,[],200,llama-13b,128,1,2022.0,1.0,1,H100,1697101623673,1697101625695.0,120,9.0,1.0,"[504, 1518]","[1697101624177, 1697101625695]"
1012,1012,325,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626657.0,120,,,"[89, 780]","[1697101625786, 1697101626566]"
1013,1013,771,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626665,1697101628905.0,120,,,"[335, 1693]","[1697101627000, 1697101628693]"
1014,1014,192,16,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628910,1697101633688.0,120,93.0,20.0,"[363, 1461, 81, 68, 66, 865, 92, 90, 90, 89, 87, 84, 83, 743, 95, 71, 94, 93, 92, 71]","[1697101629273, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
1015,1015,196,6,[],200,llama-13b,128,1,1062.0,1.0,1,H100,1697101607271,1697101608333.0,120,13.0,1.0,"[30, 1032]","[1697101607301, 1697101608333]"
1016,1016,551,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608334,1697101610266.0,120,,,"[30, 994, 91, 79, 77]","[1697101608364, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
1017,1017,550,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[94, 1185]","[1697101633784, 1697101634969]"
1018,1018,909,18,[],200,llama-13b,128,1,5210.0,1.0,1,H100,1697101635000,1697101640210.0,120,86.0,20.0,"[576, 1233, 88, 87, 56, 628, 91, 92, 87, 87, 82, 81, 767, 99, 93, 91, 93, 91, 91, 79, 618]","[1697101635576, 1697101636809, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638955, 1697101639054, 1697101639147, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210]"
1019,1019,647,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621116,1697101623673.0,120,,,"[75, 1391, 90, 87, 86, 84, 83]","[1697101621191, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
1020,1020,880,8,[],200,llama-13b,128,1,821.0,1.0,1,H100,1697101610271,1697101611092.0,120,84.0,2.0,"[143, 632, 46]","[1697101610414, 1697101611046, 1697101611092]"
1021,1021,163,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623677,1697101626657.0,120,,,"[523, 1495, 113, 99, 68]","[1697101624200, 1697101625695, 1697101625808, 1697101625907, 1697101625975]"
1022,1022,269,4,[],200,llama-13b,128,1,2095.0,1.0,1,H100,1697101601197,1697101603292.0,120,11.0,1.0,"[484, 1611]","[1697101601681, 1697101603292]"
1023,1023,362,7,[],200,llama-13b,128,1,1920.0,1.0,1,H100,1697101601373,1697101603293.0,120,14.0,1.0,"[433, 1487]","[1697101601806, 1697101603293]"
1024,1024,723,8,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101603295,1697101604194.0,120,14.0,1.0,"[113, 786]","[1697101603408, 1697101604194]"
1025,1025,116,7,[],200,llama-13b,128,1,745.0,1.0,1,H100,1697101609359,1697101610104.0,120,23.0,1.0,"[18, 727]","[1697101609377, 1697101610104]"
1026,1026,441,8,[],200,llama-13b,128,1,375.0,1.0,1,H100,1697101610105,1697101610480.0,120,6.0,1.0,"[24, 351]","[1697101610129, 1697101610480]"
1027,1027,123,9,[],200,llama-13b,128,1,318.0,1.0,1,H100,1697101604195,1697101604513.0,120,14.0,1.0,"[42, 275]","[1697101604237, 1697101604512]"
1028,1028,273,6,[],200,llama-13b,128,1,1011.0,1.0,1,H100,1697101605196,1697101606207.0,120,19.0,1.0,"[42, 969]","[1697101605238, 1697101606207]"
1029,1029,846,5,[],200,llama-13b,128,1,2163.0,1.0,1,H100,1697101610270,1697101612433.0,120,140.0,6.0,"[138, 638, 46, 1094, 84, 82, 81]","[1697101610408, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612433]"
1030,1030,54,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603409,1697101604272.0,120,,,"[73, 712]","[1697101603482, 1697101604194]"
1031,1031,420,26,[],200,llama-13b,128,1,5765.0,1.0,1,H100,1697101656613,1697101662378.0,120,52.0,20.0,"[19, 1528, 129, 96, 82, 82, 80, 1077, 100, 98, 99, 92, 85, 805, 102, 75, 104, 87, 87, 831, 107]","[1697101656632, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659706, 1697101659806, 1697101659904, 1697101660003, 1697101660095, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378]"
1032,1032,415,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604280,1697101610266.0,120,,,"[432, 1733, 81, 76, 74, 772, 90, 86, 84, 65, 84, 79, 491, 95, 72, 74, 93, 90, 87, 511, 78, 78]","[1697101604712, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607857, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609605]"
1033,1033,107,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603294,1697101604272.0,120,,,[7],[1697101603301]
1034,1034,900,7,[],200,llama-13b,128,1,2327.0,1.0,1,H100,1697101610105,1697101612432.0,120,67.0,6.0,"[6, 369, 611, 1095, 83, 83, 80]","[1697101610111, 1697101610480, 1697101611091, 1697101612186, 1697101612269, 1697101612352, 1697101612432]"
1035,1035,457,6,[],200,llama-13b,128,1,973.0,1.0,1,H100,1697101604275,1697101605248.0,120,874.0,2.0,"[104, 869]","[1697101604379, 1697101605248]"
1036,1036,817,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101605249,1697101610266.0,120,,,"[6, 2014, 179, 89, 86, 85, 64, 84, 80, 491, 96, 72, 72, 94, 90, 86, 512, 79, 77]","[1697101605255, 1697101607269, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608937, 1697101609449, 1697101609528, 1697101609605]"
1037,1037,601,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606209,1697101610267.0,120,,,"[91, 969, 179, 90, 86, 84, 65, 83, 80, 491, 95, 73, 73, 93, 90, 87, 511, 78, 78]","[1697101606300, 1697101607269, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609605]"
1038,1038,422,3,[],200,llama-13b,128,1,939.0,1.0,1,H100,1697101598827,1697101599766.0,120,26.0,1.0,"[36, 903]","[1697101598863, 1697101599766]"
1039,1039,781,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601192.0,120,,,"[36, 1290]","[1697101599803, 1697101601093]"
1040,1040,670,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597653,1697101601192.0,120,,,"[42, 894, 237, 96, 90, 86, 84, 818, 100, 100, 101, 90, 90]","[1697101597695, 1697101598589, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600391, 1697101600481]"
1041,1041,216,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604272.0,120,,,"[273, 1823, 115, 101, 74]","[1697101601470, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
1042,1042,795,6,[],200,llama-13b,128,1,1795.0,1.0,1,H100,1697101610273,1697101612068.0,120,12.0,1.0,"[231, 1564]","[1697101610504, 1697101612068]"
1043,1043,220,7,[],200,llama-13b,128,1,1052.0,1.0,1,H100,1697101612069,1697101613121.0,120,67.0,2.0,"[7, 938, 107]","[1697101612076, 1697101613014, 1697101613121]"
1044,1044,504,11,[],200,llama-13b,128,1,4708.0,1.0,1,H100,1697101616332,1697101621040.0,120,58.0,20.0,"[90, 955, 48, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92]","[1697101616422, 1697101617377, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040]"
1045,1045,574,8,[],200,llama-13b,128,1,1202.0,1.0,1,H100,1697101613122,1697101614324.0,120,364.0,2.0,"[6, 1009, 186]","[1697101613128, 1697101614137, 1697101614323]"
1046,1046,4,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614324,1697101616321.0,120,,,"[7, 1069, 84, 78, 79, 76]","[1697101614331, 1697101615400, 1697101615484, 1697101615562, 1697101615641, 1697101615717]"
1047,1047,575,6,[],200,llama-13b,128,1,5172.0,1.0,1,H100,1697101604276,1697101609448.0,120,86.0,20.0,"[322, 1609, 238, 81, 76, 74, 772, 90, 86, 84, 65, 83, 80, 491, 95, 73, 72, 94, 90, 87, 510]","[1697101604598, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609448]"
1048,1048,703,5,[],200,llama-13b,128,1,914.0,1.0,1,H100,1697101601197,1697101602111.0,120,12.0,1.0,"[123, 791]","[1697101601320, 1697101602111]"
1049,1049,135,6,[],200,llama-13b,128,1,1297.0,1.0,1,H100,1697101602112,1697101603409.0,120,52.0,2.0,"[12, 1169, 116]","[1697101602124, 1697101603293, 1697101603409]"
1050,1050,581,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603409,1697101604273.0,120,,,"[87, 698]","[1697101603496, 1697101604194]"
1051,1051,452,10,[],200,llama-13b,128,1,2193.0,1.0,1,H100,1697101616332,1697101618525.0,120,216.0,4.0,"[379, 1545, 108, 81, 80]","[1697101616711, 1697101618256, 1697101618364, 1697101618445, 1697101618525]"
1052,1052,13,8,[],200,llama-13b,128,1,5168.0,1.0,1,H100,1697101604280,1697101609448.0,120,90.0,20.0,"[605, 1322, 239, 80, 76, 74, 772, 89, 86, 85, 65, 83, 80, 492, 95, 72, 73, 93, 89, 88, 510]","[1697101604885, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608761, 1697101608850, 1697101608938, 1697101609448]"
1053,1053,812,11,[],200,llama-13b,128,1,562.0,1.0,1,H100,1697101618527,1697101619089.0,120,16.0,1.0,"[6, 556]","[1697101618533, 1697101619089]"
1054,1054,244,12,[],200,llama-13b,128,1,1289.0,1.0,1,H100,1697101619090,1697101620379.0,120,9.0,1.0,"[42, 1247]","[1697101619132, 1697101620379]"
1055,1055,91,7,[],200,llama-13b,128,1,653.0,1.0,1,H100,1697101609451,1697101610104.0,120,23.0,1.0,"[60, 593]","[1697101609511, 1697101610104]"
1056,1056,601,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101620380,1697101623659.0,120,,,"[18, 1275, 85, 81, 79, 79, 60, 614, 88, 86, 84, 84]","[1697101620398, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
1057,1057,101,3,[],200,llama-13b,128,1,2097.0,1.0,1,H100,1697101601196,1697101603293.0,120,13.0,1.0,"[389, 1708]","[1697101601585, 1697101603293]"
1058,1058,426,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[89, 810]","[1697101603384, 1697101604194]"
1059,1059,787,5,[],200,llama-13b,128,1,3172.0,1.0,1,H100,1697101604276,1697101607448.0,120,123.0,6.0,"[334, 1597, 238, 81, 76, 74, 772]","[1697101604610, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448]"
1060,1060,189,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608939,1697101610267.0,120,,,"[38, 1127]","[1697101608977, 1697101610104]"
1061,1061,546,6,[],200,llama-13b,128,1,4577.0,1.0,1,H100,1697101610272,1697101614849.0,120,93.0,20.0,"[298, 1616, 84, 82, 81, 77, 611, 90, 86, 83, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87]","[1697101610570, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
1062,1062,877,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616318.0,120,,,"[50, 1393]","[1697101614902, 1697101616295]"
1063,1063,305,8,[],200,llama-13b,128,1,4790.0,1.0,1,H100,1697101616324,1697101621114.0,120,86.0,20.0,"[302, 1629, 109, 81, 81, 78, 78, 669, 95, 88, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93, 72]","[1697101616626, 1697101618255, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041, 1697101621113]"
1064,1064,218,6,[],200,llama-13b,128,1,1402.0,1.0,1,H100,1697101607449,1697101608851.0,120,109.0,7.0,"[6, 973, 95, 72, 73, 94, 89]","[1697101607455, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851]"
1065,1065,578,7,[],200,llama-13b,128,1,506.0,1.0,1,H100,1697101608852,1697101609358.0,120,31.0,1.0,"[7, 499]","[1697101608859, 1697101609358]"
1066,1066,734,5,[],200,llama-13b,128,1,2401.0,1.0,1,H100,1697101604275,1697101606676.0,120,100.0,6.0,"[212, 707, 54, 1198, 80, 76, 74]","[1697101604487, 1697101605194, 1697101605248, 1697101606446, 1697101606526, 1697101606602, 1697101606676]"
1067,1067,164,6,[],200,llama-13b,128,1,1655.0,1.0,1,H100,1697101606678,1697101608333.0,120,15.0,1.0,"[30, 1625]","[1697101606708, 1697101608333]"
1068,1068,3,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609359,1697101610267.0,120,,,"[24, 721]","[1697101609383, 1697101610104]"
1069,1069,525,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608334,1697101610266.0,120,,,"[42, 982, 91, 79, 77]","[1697101608376, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
1070,1070,38,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634995.0,120,,,"[7, 1272]","[1697101633697, 1697101634969]"
1071,1071,111,6,[],200,llama-13b,128,1,1386.0,1.0,1,H100,1697101611047,1697101612433.0,120,79.0,5.0,"[6, 1016, 117, 84, 82, 81]","[1697101611053, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433]"
1072,1072,884,8,[],200,llama-13b,128,1,4491.0,1.0,1,H100,1697101610271,1697101614762.0,120,90.0,20.0,"[119, 656, 46, 1094, 84, 82, 80, 78, 610, 90, 86, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69]","[1697101610390, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762]"
1073,1073,392,18,[],200,llama-13b,128,1,1809.0,1.0,1,H100,1697101635000,1697101636809.0,120,20.0,1.0,"[522, 1287]","[1697101635522, 1697101636809]"
1074,1074,746,19,[],200,llama-13b,128,1,3481.0,1.0,1,H100,1697101636810,1697101640291.0,120,345.0,18.0,"[24, 645, 189, 91, 92, 87, 87, 82, 81, 768, 98, 92, 93, 92, 91, 91, 70, 627, 81]","[1697101636834, 1697101637479, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639583, 1697101640210, 1697101640291]"
1075,1075,740,8,[],200,llama-13b,128,1,3522.0,1.0,1,H100,1697101616332,1697101619854.0,120,563.0,14.0,"[250, 795, 48, 939, 81, 81, 78, 78, 668, 96, 88, 67, 86, 85, 82]","[1697101616582, 1697101617377, 1697101617425, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619350, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854]"
1076,1076,407,4,[],200,llama-13b,128,1,1926.0,1.0,1,H100,1697101604281,1697101606207.0,120,16.0,1.0,"[519, 1407]","[1697101604800, 1697101606207]"
1077,1077,89,9,[],200,llama-13b,128,1,4709.0,1.0,1,H100,1697101616331,1697101621040.0,120,52.0,20.0,"[56, 989, 49, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95, 94, 94, 91, 92]","[1697101616387, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620763, 1697101620857, 1697101620948, 1697101621040]"
1078,1078,773,8,[],200,llama-13b,128,1,4707.0,1.0,1,H100,1697101616334,1697101621041.0,120,90.0,20.0,"[250, 793, 48, 939, 81, 81, 78, 78, 668, 96, 88, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93]","[1697101616584, 1697101617377, 1697101617425, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619350, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041]"
1079,1079,647,22,[],200,llama-13b,128,1,5819.0,1.0,1,H100,1697101644758,1697101650577.0,120,83.0,20.0,"[37, 1504, 190, 98, 98, 83, 78, 1226, 106, 101, 100, 97, 88, 486, 96, 96, 82, 942, 106, 103, 102]","[1697101644795, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577]"
1080,1080,295,2,[],200,llama-13b,128,1,1173.0,1.0,1,H100,1697101597653,1697101598826.0,120,52.0,2.0,"[49, 887, 237]","[1697101597702, 1697101598589, 1697101598826]"
1081,1081,445,10,[],200,llama-13b,128,1,712.0,1.0,1,H100,1697101621046,1697101621758.0,120,457.0,2.0,"[15, 613, 84]","[1697101621061, 1697101621674, 1697101621758]"
1082,1082,649,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598827,1697101601193.0,120,,,"[18, 921, 234, 100, 100, 101, 91, 89]","[1697101598845, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
1083,1083,426,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610277,1697101616320.0,120,,,"[431, 1361, 117, 84, 82, 80, 78, 611, 90, 86, 82, 82, 63, 79, 720, 95, 93, 92, 90, 69, 87, 634, 79, 78, 76]","[1697101610708, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613121, 1697101613211, 1697101613297, 1697101613379, 1697101613461, 1697101613524, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1084,1084,74,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604270.0,120,,,"[383, 1714, 115, 101, 74]","[1697101601579, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
1085,1085,892,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621759,1697101623660.0,120,,,"[6, 817, 90, 87, 86, 85, 83]","[1697101621765, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622930, 1697101623013]"
1086,1086,200,9,[],200,llama-13b,128,1,1798.0,1.0,1,H100,1697101621047,1697101622845.0,120,6.0,9.0,"[47, 579, 85, 81, 80, 78, 61, 614, 86, 87]","[1697101621094, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622672, 1697101622758, 1697101622845]"
1087,1087,538,2,[],200,llama-13b,128,1,4663.0,1.0,1,H100,1697101604275,1697101608938.0,120,89.0,20.0,"[230, 690, 53, 1198, 80, 76, 74, 772, 90, 86, 84, 64, 84, 80, 491, 95, 73, 72, 94, 89, 88]","[1697101604505, 1697101605195, 1697101605248, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608938]"
1088,1088,177,12,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616332,1697101617377.0,120,14.0,1.0,"[143, 902]","[1697101616475, 1697101617377]"
1089,1089,538,13,[],200,llama-13b,128,1,4679.0,1.0,1,H100,1697101617378,1697101622057.0,120,89.0,20.0,"[118, 1593, 262, 95, 88, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60]","[1697101617496, 1697101619089, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
1090,1090,320,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623663,1697101626656.0,120,,,"[8, 936, 1201, 98, 68]","[1697101623671, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
1091,1091,680,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628903.0,120,,,"[151, 743, 42]","[1697101626814, 1697101627557, 1697101627599]"
1092,1092,736,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621049,1697101623664.0,120,,,"[75, 1457, 91, 86, 87, 84, 83]","[1697101621124, 1697101622581, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1093,1093,109,14,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628909,1697101633687.0,120,90.0,20.0,"[76, 1644, 105, 81, 68, 66, 865, 92, 90, 90, 89, 88, 84, 82, 742, 95, 72, 94, 92, 93, 70]","[1697101628985, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633524, 1697101633617, 1697101633687]"
1094,1094,352,5,[],200,llama-13b,128,1,2171.0,1.0,1,H100,1697101604275,1697101606446.0,120,11.0,3.0,"[143, 830, 1198]","[1697101604418, 1697101605248, 1697101606446]"
1095,1095,712,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606447,1697101610268.0,120,,,"[18, 805, 178, 90, 86, 84, 65, 83, 80, 491, 96, 72, 73, 94, 89, 87, 511, 78, 78]","[1697101606465, 1697101607270, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609605]"
1096,1096,165,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623671,1697101626657.0,120,,,"[319, 1704, 115, 97, 69]","[1697101623990, 1697101625694, 1697101625809, 1697101625906, 1697101625975]"
1097,1097,9,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614853,1697101616319.0,120,,,"[125, 1317]","[1697101614978, 1697101616295]"
1098,1098,597,5,[],200,llama-13b,128,1,900.0,1.0,1,H100,1697101603294,1697101604194.0,120,39.0,1.0,"[13, 887]","[1697101603307, 1697101604194]"
1099,1099,522,15,[],200,llama-13b,128,1,2024.0,1.0,1,H100,1697101626669,1697101628693.0,120,20.0,1.0,"[510, 1514]","[1697101627179, 1697101628693]"
1100,1100,797,7,[],200,llama-13b,128,1,1060.0,1.0,1,H100,1697101606209,1697101607269.0,120,26.0,1.0,"[78, 982]","[1697101606287, 1697101607269]"
1101,1101,880,16,[],200,llama-13b,128,1,2039.0,1.0,1,H100,1697101628695,1697101630734.0,120,84.0,2.0,"[30, 692, 1316]","[1697101628725, 1697101629417, 1697101630733]"
1102,1102,226,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607271,1697101610268.0,120,,,"[6, 1056, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607277, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
1103,1103,310,17,[],200,llama-13b,128,1,949.0,1.0,1,H100,1697101630735,1697101631684.0,120,26.0,1.0,"[6, 943]","[1697101630741, 1697101631684]"
1104,1104,668,18,[],200,llama-13b,128,1,1840.0,1.0,1,H100,1697101631685,1697101633525.0,120,109.0,6.0,"[7, 1274, 206, 94, 72, 94, 93]","[1697101631692, 1697101632966, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525]"
1105,1105,67,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633527,1697101634995.0,120,,,"[7, 672, 96, 74, 71]","[1697101633534, 1697101634206, 1697101634302, 1697101634376, 1697101634447]"
1106,1106,802,9,[],200,llama-13b,128,1,1588.0,1.0,1,H100,1697101610481,1697101612069.0,120,9.0,1.0,"[418, 1170]","[1697101610899, 1697101612069]"
1107,1107,426,20,[],200,llama-13b,128,1,8645.0,1.0,1,H100,1697101635000,1697101643645.0,120,79.0,36.0,"[570, 1239, 88, 87, 56, 628, 91, 92, 87, 87, 81, 82, 767, 98, 94, 91, 93, 92, 90, 80, 616, 82, 93, 734, 86, 78, 78, 760, 101, 99, 96, 89, 82, 63, 800, 99, 96]","[1697101635570, 1697101636809, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639238, 1697101639331, 1697101639423, 1697101639513, 1697101639593, 1697101640209, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645]"
1108,1108,233,10,[],200,llama-13b,128,1,944.0,1.0,1,H100,1697101612070,1697101613014.0,120,6.0,1.0,"[99, 845]","[1697101612169, 1697101613014]"
1109,1109,590,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613015,1697101616319.0,120,,,"[18, 1104, 186, 95, 93, 92, 90, 69, 88, 633, 80, 77, 77]","[1697101613033, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615563, 1697101615640, 1697101615717]"
1110,1110,145,6,[],200,llama-13b,128,1,2939.0,1.0,1,H100,1697101610272,1697101613211.0,120,161.0,9.0,"[148, 626, 46, 1094, 84, 82, 81, 77, 610, 90]","[1697101610420, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613120, 1697101613210]"
1111,1111,369,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610267.0,120,,,"[12, 641]","[1697101609463, 1697101610104]"
1112,1112,442,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614419,1697101616321.0,120,,,"[7, 974, 84, 78, 79, 76]","[1697101614426, 1697101615400, 1697101615484, 1697101615562, 1697101615641, 1697101615717]"
1113,1113,519,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616318.0,120,,,"[62, 1380]","[1697101614914, 1697101616294]"
1114,1114,727,10,[],200,llama-13b,128,1,2161.0,1.0,1,H100,1697101610272,1697101612433.0,120,58.0,5.0,"[325, 1472, 117, 84, 82, 81]","[1697101610597, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433]"
1115,1115,441,8,[],200,llama-13b,128,1,375.0,1.0,1,H100,1697101610105,1697101610480.0,120,6.0,1.0,"[36, 339]","[1697101610141, 1697101610480]"
1116,1116,919,3,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101597653,1697101598589.0,120,14.0,1.0,"[42, 894]","[1697101597695, 1697101598589]"
1117,1117,879,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101616324,1697101623658.0,120,,,"[482, 1450, 108, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 98, 68, 644, 81, 79, 78, 61, 614, 87, 87, 84, 83]","[1697101616806, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621046, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621996, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1118,1118,880,7,[],200,llama-13b,128,1,1115.0,1.0,1,H100,1697101608334,1697101609449.0,120,84.0,2.0,"[12, 1012, 91]","[1697101608346, 1697101609358, 1697101609449]"
1119,1119,928,14,[],200,llama-13b,128,1,902.0,1.0,1,H100,1697101623666,1697101624568.0,120,20.0,1.0,"[141, 761]","[1697101623807, 1697101624568]"
1120,1120,353,15,[],200,llama-13b,128,1,1406.0,1.0,1,H100,1697101624569,1697101625975.0,120,52.0,4.0,"[24, 1102, 114, 97, 69]","[1697101624593, 1697101625695, 1697101625809, 1697101625906, 1697101625975]"
1121,1121,280,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609450,1697101610267.0,120,,,"[7, 647]","[1697101609457, 1697101610104]"
1122,1122,801,9,[],200,llama-13b,128,1,4369.0,1.0,1,H100,1697101610481,1697101614850.0,120,47.0,20.0,"[411, 1177, 117, 84, 82, 81, 78, 609, 90, 86, 84, 81, 62, 80, 721, 94, 93, 92, 90, 70, 87]","[1697101610892, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613120, 1697101613210, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614324, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614850]"
1123,1123,711,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625976,1697101628903.0,120,,,"[6, 892, 725]","[1697101625982, 1697101626874, 1697101627599]"
1124,1124,796,8,[],200,llama-13b,128,1,4782.0,1.0,1,H100,1697101616332,1697101621114.0,120,86.0,20.0,"[373, 1551, 108, 81, 81, 77, 79, 669, 95, 88, 67, 86, 85, 82, 720, 96, 93, 94, 91, 93, 73]","[1697101616705, 1697101618256, 1697101618364, 1697101618445, 1697101618526, 1697101618603, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620763, 1697101620857, 1697101620948, 1697101621041, 1697101621114]"
1125,1125,350,4,[],200,llama-13b,128,1,1176.0,1.0,1,H100,1697101598590,1697101599766.0,120,216.0,1.0,"[7, 1169]","[1697101598597, 1697101599766]"
1126,1126,166,5,[],200,llama-13b,128,1,1910.0,1.0,1,H100,1697101599183,1697101601093.0,120,14.0,1.0,"[7, 1903]","[1697101599190, 1697101601093]"
1127,1127,318,13,[],200,llama-13b,128,1,1011.0,1.0,1,H100,1697101621047,1697101622058.0,120,6.0,6.0,"[53, 573, 85, 81, 80, 78, 61]","[1697101621100, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058]"
1128,1128,177,12,[],200,llama-13b,128,1,1443.0,1.0,1,H100,1697101614852,1697101616295.0,120,14.0,1.0,"[74, 1369]","[1697101614926, 1697101616295]"
1129,1129,535,13,[],200,llama-13b,128,1,4744.0,1.0,1,H100,1697101616296,1697101621040.0,120,84.0,20.0,"[18, 289, 822, 939, 81, 80, 78, 79, 668, 95, 89, 67, 86, 84, 83, 720, 95, 94, 93, 92, 92]","[1697101616314, 1697101616603, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619350, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619771, 1697101619854, 1697101620574, 1697101620669, 1697101620763, 1697101620856, 1697101620948, 1697101621040]"
1130,1130,663,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621116,1697101623673.0,120,,,"[85, 1381, 90, 87, 86, 84, 84]","[1697101621201, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
1131,1131,94,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623677,1697101626658.0,120,,,"[535, 1483, 113, 98, 69]","[1697101624212, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
1132,1132,455,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626672,1697101628905.0,120,,,"[551, 1470]","[1697101627223, 1697101628693]"
1133,1133,894,14,[],200,llama-13b,128,1,627.0,1.0,1,H100,1697101621046,1697101621673.0,120,14.0,1.0,"[25, 602]","[1697101621071, 1697101621673]"
1134,1134,779,12,[],200,llama-13b,128,1,3265.0,1.0,1,H100,1697101628910,1697101632175.0,120,563.0,10.0,"[411, 1308, 105, 81, 68, 66, 865, 92, 90, 90, 89]","[1697101629321, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175]"
1135,1135,209,13,[],200,llama-13b,128,1,791.0,1.0,1,H100,1697101632176,1697101632967.0,120,20.0,1.0,"[6, 785]","[1697101632182, 1697101632967]"
1136,1136,327,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621674,1697101623660.0,120,,,"[31, 877, 90, 87, 86, 85, 83]","[1697101621705, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622930, 1697101623013]"
1137,1137,563,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101632968,1697101634995.0,120,,,"[30, 1208, 96, 74, 71]","[1697101632998, 1697101634206, 1697101634302, 1697101634376, 1697101634447]"
1138,1138,659,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623678,1697101626655.0,120,,,"[552, 1465, 113, 98, 69]","[1697101624230, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
1139,1139,288,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614763,1697101616321.0,120,,,"[31, 690, 79, 78, 76]","[1697101614794, 1697101615484, 1697101615563, 1697101615641, 1697101615717]"
1140,1140,646,10,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616332,1697101617377.0,120,14.0,1.0,"[149, 896]","[1697101616481, 1697101617377]"
1141,1141,610,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622059,1697101623671.0,120,,,"[49, 1464]","[1697101622108, 1697101623572]"
1142,1142,38,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623677,1697101626655.0,120,,,"[541, 1477, 113, 98, 69]","[1697101624218, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
1143,1143,831,1,[],200,llama-13b,128,1,667.0,1.0,1,H100,1697101599099,1697101599766.0,120,11.0,1.0,"[7, 660]","[1697101599106, 1697101599766]"
1144,1144,72,11,[],200,llama-13b,128,1,4679.0,1.0,1,H100,1697101617378,1697101622057.0,120,84.0,20.0,"[102, 1608, 263, 95, 88, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60]","[1697101617480, 1697101619088, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
1145,1145,233,2,[],200,llama-13b,128,1,1326.0,1.0,1,H100,1697101599767,1697101601093.0,120,6.0,1.0,"[42, 1284]","[1697101599809, 1697101601093]"
1146,1146,591,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601094,1697101604270.0,120,,,"[18, 260, 783, 1253, 101, 74]","[1697101601112, 1697101601372, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
1147,1147,926,15,[],200,llama-13b,128,1,7416.0,1.0,1,H100,1697101635000,1697101642416.0,120,563.0,30.0,"[402, 1406, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 94, 92, 92, 91, 91, 80, 616, 82, 93, 734, 86, 78, 78, 760, 101, 99, 96]","[1697101635402, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642221, 1697101642320, 1697101642416]"
1148,1148,395,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[54, 840, 42]","[1697101626717, 1697101627557, 1697101627599]"
1149,1149,755,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628909,1697101634994.0,120,,,"[202, 1519, 104, 81, 68, 66, 864, 93, 91, 89, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 71, 613, 75, 71]","[1697101629111, 1697101630630, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688, 1697101634301, 1697101634376, 1697101634447]"
1150,1150,21,4,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101604274,1697101605194.0,120,15.0,1.0,"[27, 893]","[1697101604301, 1697101605194]"
1151,1151,379,5,[],200,llama-13b,128,1,4254.0,1.0,1,H100,1697101605195,1697101609449.0,120,182.0,20.0,"[7, 1244, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 73, 93, 89, 88, 510]","[1697101605202, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608761, 1697101608850, 1697101608938, 1697101609448]"
1152,1152,88,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[24, 869, 43]","[1697101626687, 1697101627556, 1697101627599]"
1153,1153,302,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604270.0,120,,,"[472, 1623, 116, 101, 74]","[1697101601669, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
1154,1154,661,6,[],200,llama-13b,128,1,3499.0,1.0,1,H100,1697101604274,1697101607773.0,120,161.0,10.0,"[242, 1691, 239, 80, 76, 74, 772, 90, 86, 84, 64]","[1697101604516, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772]"
1155,1155,675,12,[],200,llama-13b,128,1,2310.0,1.0,1,H100,1697101623664,1697101625974.0,120,563.0,5.0,"[25, 879, 39, 1201, 98, 68]","[1697101623689, 1697101624568, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
1156,1156,183,12,[],200,llama-13b,128,1,10751.0,1.0,1,H100,1697101634997,1697101645748.0,120,17.0,50.0,"[191, 1620, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 79, 616, 83, 93, 734, 86, 78, 78, 760, 101, 99, 96, 89, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 683, 79, 78, 77, 75]","[1697101635188, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748]"
1157,1157,736,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609452,1697101610268.0,120,,,"[65, 587]","[1697101609517, 1697101610104]"
1158,1158,381,14,[],200,llama-13b,128,1,1128.0,1.0,1,H100,1697101619447,1697101620575.0,120,140.0,2.0,"[6, 925, 197]","[1697101619453, 1697101620378, 1697101620575]"
1159,1159,732,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101620576,1697101623659.0,120,,,"[6, 1091, 85, 81, 79, 79, 60, 614, 87, 87, 85, 82]","[1697101620582, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622930, 1697101623012]"
1160,1160,493,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[83, 1196]","[1697101633773, 1697101634969]"
1161,1161,137,7,[],200,llama-13b,128,1,4574.0,1.0,1,H100,1697101610275,1697101614849.0,120,86.0,20.0,"[401, 1393, 117, 84, 82, 80, 78, 611, 90, 86, 82, 82, 63, 79, 720, 95, 93, 92, 90, 69, 87]","[1697101610676, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613121, 1697101613211, 1697101613297, 1697101613379, 1697101613461, 1697101613524, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
1162,1162,851,15,[],200,llama-13b,128,1,1808.0,1.0,1,H100,1697101635000,1697101636808.0,120,23.0,1.0,"[390, 1418]","[1697101635390, 1697101636808]"
1163,1163,279,16,[],200,llama-13b,128,1,3481.0,1.0,1,H100,1697101636810,1697101640291.0,120,67.0,18.0,"[12, 657, 189, 91, 92, 87, 87, 82, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618, 81]","[1697101636822, 1697101637479, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291]"
1164,1164,291,10,[],200,llama-13b,128,1,4709.0,1.0,1,H100,1697101616332,1697101621041.0,120,79.0,20.0,"[196, 897, 939, 81, 81, 78, 78, 669, 94, 89, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93]","[1697101616528, 1697101617425, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041]"
1165,1165,216,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626666,1697101628904.0,120,,,"[448, 1578]","[1697101627114, 1697101628692]"
1166,1166,579,17,[],200,llama-13b,128,1,1719.0,1.0,1,H100,1697101628910,1697101630629.0,120,19.0,1.0,"[307, 1412]","[1697101629217, 1697101630629]"
1167,1167,11,18,[],200,llama-13b,128,1,3668.0,1.0,1,H100,1697101630634,1697101634302.0,120,732.0,17.0,"[39, 1011, 130, 92, 91, 89, 89, 88, 84, 83, 741, 95, 72, 94, 93, 92, 70, 615]","[1697101630673, 1697101631684, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634302]"
1168,1168,561,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622846,1697101623663.0,120,,,"[12, 714]","[1697101622858, 1697101623572]"
1169,1169,141,8,[],200,llama-13b,128,1,4708.0,1.0,1,H100,1697101616332,1697101621040.0,120,89.0,20.0,"[155, 889, 49, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92]","[1697101616487, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040]"
1170,1170,919,11,[],200,llama-13b,128,1,2028.0,1.0,1,H100,1697101623666,1697101625694.0,120,14.0,1.0,"[228, 1800]","[1697101623894, 1697101625694]"
1171,1171,323,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625696,1697101626656.0,120,,,"[31, 839]","[1697101625727, 1697101626566]"
1172,1172,664,10,[],200,llama-13b,128,1,1798.0,1.0,1,H100,1697101621047,1697101622845.0,120,364.0,9.0,"[41, 585, 85, 81, 80, 78, 61, 614, 86, 87]","[1697101621088, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622672, 1697101622758, 1697101622845]"
1173,1173,677,13,[],200,llama-13b,128,1,894.0,1.0,1,H100,1697101626663,1697101627557.0,120,9.0,1.0,"[49, 845]","[1697101626712, 1697101627557]"
1174,1174,180,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601198,1697101604272.0,120,,,"[381, 1714, 115, 101, 74]","[1697101601579, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
1175,1175,531,3,[],200,llama-13b,128,1,5173.0,1.0,1,H100,1697101604276,1697101609449.0,120,52.0,20.0,"[406, 1525, 238, 81, 76, 74, 772, 90, 86, 84, 65, 83, 80, 491, 95, 73, 72, 94, 90, 87, 510]","[1697101604682, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609448]"
1176,1176,106,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101627558,1697101628906.0,120,,,"[24, 1111]","[1697101627582, 1697101628693]"
1177,1177,378,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610273,1697101616319.0,120,,,"[14, 804, 1095, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 69, 87, 634, 79, 78, 76]","[1697101610287, 1697101611091, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1178,1178,777,10,[],200,llama-13b,128,1,612.0,1.0,1,H100,1697101613525,1697101614137.0,120,9.0,1.0,"[6, 606]","[1697101613531, 1697101614137]"
1179,1179,211,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614140,1697101616320.0,120,,,"[34, 1226, 84, 78, 79, 76]","[1697101614174, 1697101615400, 1697101615484, 1697101615562, 1697101615641, 1697101615717]"
1180,1180,465,15,[],200,llama-13b,128,1,1905.0,1.0,1,H100,1697101628910,1697101630815.0,120,364.0,3.0,"[318, 1401, 105, 81]","[1697101629228, 1697101630629, 1697101630734, 1697101630815]"
1181,1181,368,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101634303,1697101634996.0,120,,,"[6, 660]","[1697101634309, 1697101634969]"
1182,1182,811,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101635000,1697101651603.0,120,,,"[479, 1329, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 94, 91, 93, 92, 90, 79, 617, 82, 93, 734, 86, 78, 78, 760, 101, 99, 96, 89, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 683, 79, 78, 77, 75, 741, 98, 97, 84, 78, 1226, 106, 101, 100, 97, 87, 486, 96, 96, 83, 941, 107, 103, 102, 90, 86]","[1697101635479, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639238, 1697101639331, 1697101639423, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648563, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
1183,1183,392,8,[],200,llama-13b,128,1,1164.0,1.0,1,H100,1697101608940,1697101610104.0,120,20.0,1.0,"[7, 1157]","[1697101608947, 1697101610104]"
1184,1184,822,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630816,1697101634994.0,120,,,"[12, 856, 131, 92, 90, 89, 89, 88, 84, 83, 742, 94, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101630828, 1697101631684, 1697101631815, 1697101631907, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
1185,1185,751,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610105,1697101616319.0,120,,,"[12, 974, 1095, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 92, 93, 90, 69, 87, 634, 79, 78, 76]","[1697101610117, 1697101611091, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614510, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1186,1186,310,9,[],200,llama-13b,128,1,1921.0,1.0,1,H100,1697101611093,1697101613014.0,120,26.0,1.0,"[6, 1915]","[1697101611099, 1697101613014]"
1187,1187,664,10,[],200,llama-13b,128,1,2468.0,1.0,1,H100,1697101613015,1697101615483.0,120,364.0,9.0,"[30, 1092, 186, 95, 93, 92, 90, 69, 88, 633]","[1697101613045, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483]"
1188,1188,737,7,[],200,llama-13b,128,1,1094.0,1.0,1,H100,1697101616331,1697101617425.0,120,216.0,2.0,"[168, 877, 49]","[1697101616499, 1697101617376, 1697101617425]"
1189,1189,363,9,[],200,llama-13b,128,1,5503.0,1.0,1,H100,1697101616336,1697101621839.0,120,286.0,22.0,"[466, 1454, 108, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 98, 68, 644, 81]","[1697101616802, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621046, 1697101621114, 1697101621758, 1697101621839]"
1190,1190,853,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[180, 1100]","[1697101633870, 1697101634970]"
1191,1191,144,8,[],200,llama-13b,128,1,4631.0,1.0,1,H100,1697101617426,1697101622057.0,120,96.0,20.0,"[66, 1597, 262, 95, 88, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60]","[1697101617492, 1697101619089, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
1192,1192,915,11,[],200,llama-13b,128,1,637.0,1.0,1,H100,1697101614763,1697101615400.0,120,182.0,1.0,"[8, 629]","[1697101614771, 1697101615400]"
1193,1193,283,20,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101635000,1697101640209.0,120,85.0,20.0,"[520, 1289, 88, 87, 56, 628, 91, 92, 87, 87, 81, 82, 767, 98, 94, 91, 93, 92, 90, 80, 616]","[1697101635520, 1697101636809, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639238, 1697101639331, 1697101639423, 1697101639513, 1697101639593, 1697101640209]"
1194,1194,162,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600001,1697101601192.0,120,,,"[7, 1085]","[1697101600008, 1697101601093]"
1195,1195,345,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101615401,1697101616318.0,120,,,"[24, 870]","[1697101615425, 1697101616295]"
1196,1196,587,9,[],200,llama-13b,128,1,1790.0,1.0,1,H100,1697101610279,1697101612069.0,120,13.0,1.0,"[543, 1247]","[1697101610822, 1697101612069]"
1197,1197,793,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101616336,1697101623658.0,120,,,"[464, 1564, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 93, 73, 644, 81, 79, 78, 61, 614, 87, 87, 84, 83]","[1697101616800, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621996, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1198,1198,524,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604271.0,120,,,"[77, 838, 44, 1253, 101, 74]","[1697101601273, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
1199,1199,882,8,[],200,llama-13b,128,1,3582.0,1.0,1,H100,1697101604275,1697101607857.0,120,345.0,11.0,"[293, 1639, 239, 80, 76, 74, 772, 90, 86, 84, 64, 84]","[1697101604568, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607772, 1697101607856]"
1200,1200,913,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612070,1697101616320.0,120,,,"[18, 926, 107, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612088, 1697101613014, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
1201,1201,147,10,[],200,llama-13b,128,1,1917.0,1.0,1,H100,1697101616339,1697101618256.0,120,182.0,1.0,"[457, 1460]","[1697101616796, 1697101618256]"
1202,1202,784,21,[],200,llama-13b,128,1,4733.0,1.0,1,H100,1697101643647,1697101648380.0,120,89.0,20.0,"[12, 661, 100, 87, 84, 83, 83, 682, 79, 78, 77, 75, 742, 97, 98, 83, 78, 1227, 105, 101, 101]","[1697101643659, 1697101644320, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648073, 1697101648178, 1697101648279, 1697101648380]"
1203,1203,70,2,[],200,llama-13b,128,1,2097.0,1.0,1,H100,1697101601196,1697101603293.0,120,39.0,1.0,"[216, 1880]","[1697101601412, 1697101603292]"
1204,1204,430,3,[],200,llama-13b,128,1,900.0,1.0,1,H100,1697101603294,1697101604194.0,120,15.0,1.0,"[19, 881]","[1697101603313, 1697101604194]"
1205,1205,874,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604195,1697101610270.0,120,,,"[24, 293, 736, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101604219, 1697101604512, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667, 1697101608761, 1697101608850, 1697101608937, 1697101609448, 1697101609527, 1697101609604]"
1206,1206,722,2,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101597653,1697101598589.0,120,39.0,1.0,"[30, 906]","[1697101597683, 1697101598589]"
1207,1207,870,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622060,1697101623663.0,120,,,"[54, 1458]","[1697101622114, 1697101623572]"
1208,1208,155,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598590,1697101601193.0,120,,,"[25, 1151, 234, 100, 100, 101, 91, 89]","[1697101598615, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
1209,1209,246,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610272,1697101616319.0,120,,,"[214, 1582, 118, 84, 82, 81, 77, 611, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87, 634, 79, 78, 76]","[1697101610486, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1210,1210,268,13,[],200,llama-13b,128,1,1002.0,1.0,1,H100,1697101620671,1697101621673.0,120,19.0,1.0,"[6, 996]","[1697101620677, 1697101621673]"
1211,1211,750,27,[],200,llama-13b,128,1,4757.0,1.0,1,H100,1697101662379,1697101667136.0,120,88.0,20.0,"[13, 945, 127, 101, 93, 88, 87, 845, 108, 103, 97, 93, 101, 687, 106, 100, 72, 72, 93, 92, 734]","[1697101662392, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664986, 1697101665079, 1697101665180, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136]"
1212,1212,622,14,[],200,llama-13b,128,1,907.0,1.0,1,H100,1697101621675,1697101622582.0,120,20.0,1.0,"[41, 866]","[1697101621716, 1697101622582]"
1213,1213,297,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623672,1697101626658.0,120,,,"[432, 1591, 113, 98, 69]","[1697101624104, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
1214,1214,508,9,[],200,llama-13b,128,1,4777.0,1.0,1,H100,1697101616337,1697101621114.0,120,86.0,20.0,"[571, 1348, 108, 81, 80, 79, 79, 668, 94, 89, 67, 86, 85, 82, 721, 95, 94, 92, 92, 98, 68]","[1697101616908, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620856, 1697101620948, 1697101621046, 1697101621114]"
1215,1215,19,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616319.0,120,,,"[128, 1315]","[1697101614980, 1697101616295]"
1216,1216,259,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608852,1697101610266.0,120,,,"[6, 500, 91, 79, 77]","[1697101608858, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
1217,1217,749,8,[],200,llama-13b,128,1,4708.0,1.0,1,H100,1697101616332,1697101621040.0,120,47.0,20.0,"[171, 873, 49, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92]","[1697101616503, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040]"
1218,1218,613,9,[],200,llama-13b,128,1,4491.0,1.0,1,H100,1697101610271,1697101614762.0,120,90.0,20.0,"[107, 668, 45, 1095, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 69]","[1697101610378, 1697101611046, 1697101611091, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762]"
1219,1219,43,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614763,1697101616321.0,120,,,"[17, 620, 84, 79, 78, 76]","[1697101614780, 1697101615400, 1697101615484, 1697101615563, 1697101615641, 1697101615717]"
1220,1220,349,13,[],200,llama-13b,128,1,4776.0,1.0,1,H100,1697101616338,1697101621114.0,120,88.0,20.0,"[568, 1350, 108, 81, 80, 79, 79, 668, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 98, 68]","[1697101616906, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621046, 1697101621114]"
1221,1221,397,11,[],200,llama-13b,128,1,2031.0,1.0,1,H100,1697101616333,1697101618364.0,120,67.0,2.0,"[291, 1631, 109]","[1697101616624, 1697101618255, 1697101618364]"
1222,1222,758,12,[],200,llama-13b,128,1,3693.0,1.0,1,H100,1697101618365,1697101622058.0,120,84.0,20.0,"[12, 712, 263, 94, 88, 68, 85, 85, 83, 719, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 61]","[1697101618377, 1697101619089, 1697101619352, 1697101619446, 1697101619534, 1697101619602, 1697101619687, 1697101619772, 1697101619855, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058]"
1223,1223,449,9,[],200,llama-13b,128,1,4490.0,1.0,1,H100,1697101610272,1697101614762.0,120,86.0,20.0,"[124, 650, 46, 1094, 84, 82, 80, 78, 610, 90, 86, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69]","[1697101610396, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762]"
1224,1224,810,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616321.0,120,,,"[7, 1435]","[1697101614859, 1697101616294]"
1225,1225,235,11,[],200,llama-13b,128,1,3440.0,1.0,1,H100,1697101616332,1697101619772.0,120,161.0,12.0,"[282, 1750, 81, 81, 78, 78, 669, 95, 88, 67, 86, 85]","[1697101616614, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772]"
1226,1226,161,4,[],200,llama-13b,128,1,994.0,1.0,1,H100,1697101607857,1697101608851.0,120,109.0,7.0,"[7, 564, 95, 74, 71, 94, 89]","[1697101607864, 1697101608428, 1697101608523, 1697101608597, 1697101608668, 1697101608762, 1697101608851]"
1227,1227,517,5,[],200,llama-13b,128,1,505.0,1.0,1,H100,1697101608853,1697101609358.0,120,15.0,1.0,"[11, 494]","[1697101608864, 1697101609358]"
1228,1228,439,7,[],200,llama-13b,128,1,863.0,1.0,1,H100,1697101612434,1697101613297.0,120,13.0,4.0,"[12, 568, 107, 90, 86]","[1697101612446, 1697101613014, 1697101613121, 1697101613211, 1697101613297]"
1229,1229,871,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609359,1697101610267.0,120,,,"[18, 727]","[1697101609377, 1697101610104]"
1230,1230,562,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601198,1697101604271.0,120,,,"[519, 1576, 115, 101, 74]","[1697101601717, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
1231,1231,301,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610273,1697101616319.0,120,,,"[43, 730, 46, 1094, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 69, 87, 634, 79, 78, 76]","[1697101610316, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1232,1232,144,17,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628908,1697101633687.0,120,96.0,20.0,"[39, 1682, 105, 81, 68, 66, 864, 93, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 93, 70]","[1697101628947, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687]"
1233,1233,595,12,[],200,llama-13b,128,1,606.0,1.0,1,H100,1697101619773,1697101620379.0,120,8.0,1.0,"[13, 593]","[1697101619786, 1697101620379]"
1234,1234,460,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[95, 804]","[1697101603390, 1697101604194]"
1235,1235,818,4,[],200,llama-13b,128,1,1927.0,1.0,1,H100,1697101604280,1697101606207.0,120,13.0,1.0,"[426, 1501]","[1697101604706, 1697101606207]"
1236,1236,248,5,[],200,llama-13b,128,1,3318.0,1.0,1,H100,1697101606209,1697101609527.0,120,182.0,17.0,"[67, 994, 178, 90, 86, 84, 65, 83, 80, 491, 96, 72, 72, 94, 90, 87, 511, 78]","[1697101606276, 1697101607270, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609449, 1697101609527]"
1237,1237,141,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619855,1697101623659.0,120,,,"[7, 1811, 85, 81, 79, 79, 60, 614, 88, 86, 84, 83]","[1697101619862, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
1238,1238,767,5,[],200,llama-13b,128,1,1061.0,1.0,1,H100,1697101606209,1697101607270.0,120,11.0,1.0,"[60, 1001]","[1697101606269, 1697101607270]"
1239,1239,576,6,[],200,llama-13b,128,1,575.0,1.0,1,H100,1697101609529,1697101610104.0,120,14.0,1.0,"[6, 569]","[1697101609535, 1697101610104]"
1240,1240,7,7,[],200,llama-13b,128,1,3274.0,1.0,1,H100,1697101610105,1697101613379.0,120,345.0,11.0,"[24, 351, 611, 1095, 84, 82, 80, 78, 610, 90, 86, 83]","[1697101610129, 1697101610480, 1697101611091, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379]"
1241,1241,506,18,[],200,llama-13b,128,1,1279.0,1.0,1,H100,1697101633690,1697101634969.0,120,16.0,1.0,"[26, 1253]","[1697101633716, 1697101634969]"
1242,1242,502,10,[],200,llama-13b,128,1,902.0,1.0,1,H100,1697101623666,1697101624568.0,120,19.0,1.0,"[47, 855]","[1697101623713, 1697101624568]"
1243,1243,865,11,[],200,llama-13b,128,1,1996.0,1.0,1,H100,1697101624569,1697101626565.0,120,9.0,1.0,"[42, 1954]","[1697101624611, 1697101626565]"
1244,1244,784,5,[],200,llama-13b,128,1,4707.0,1.0,1,H100,1697101616333,1697101621040.0,120,89.0,20.0,"[181, 862, 49, 939, 81, 80, 79, 78, 669, 94, 89, 67, 86, 85, 82, 720, 96, 94, 93, 91, 92]","[1697101616514, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618604, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040]"
1245,1245,267,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601191.0,120,,,"[31, 884, 219, 93, 89, 89, 88, 85, 729, 97, 89, 87, 84, 818, 100, 100, 101, 91, 89]","[1697101596549, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
1246,1246,294,12,[],200,llama-13b,128,1,1033.0,1.0,1,H100,1697101626566,1697101627599.0,120,9.0,2.0,"[13, 1020]","[1697101626579, 1697101627599]"
1247,1247,77,23,[],200,llama-13b,128,1,5950.0,1.0,1,H100,1697101650578,1697101656528.0,120,92.0,20.0,"[31, 1267, 842, 891, 86, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 91, 614, 106, 100, 99, 97]","[1697101650609, 1697101651876, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655512, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
1248,1248,327,4,[],200,llama-13b,128,1,3191.0,1.0,1,H100,1697101610105,1697101613296.0,120,563.0,10.0,"[18, 357, 611, 1095, 84, 82, 80, 78, 610, 90, 86]","[1697101610123, 1697101610480, 1697101611091, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296]"
1249,1249,49,11,[],200,llama-13b,128,1,2109.0,1.0,1,H100,1697101616336,1697101618445.0,120,109.0,3.0,"[546, 1373, 109, 81]","[1697101616882, 1697101618255, 1697101618364, 1697101618445]"
1250,1250,403,12,[],200,llama-13b,128,1,906.0,1.0,1,H100,1697101618446,1697101619352.0,120,874.0,2.0,"[7, 636, 263]","[1697101618453, 1697101619089, 1697101619352]"
1251,1251,647,13,[],200,llama-13b,128,1,6087.0,1.0,1,H100,1697101627600,1697101633687.0,120,83.0,20.0,"[7, 1809, 1317, 81, 69, 66, 864, 92, 91, 90, 88, 88, 84, 83, 742, 95, 72, 93, 93, 92, 70]","[1697101627607, 1697101629416, 1697101630733, 1697101630814, 1697101630883, 1697101630949, 1697101631813, 1697101631905, 1697101631996, 1697101632086, 1697101632174, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633686]"
1252,1252,684,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613297,1697101616319.0,120,,,"[7, 833, 187, 94, 93, 92, 90, 69, 88, 633, 79, 78, 76]","[1697101613304, 1697101614137, 1697101614324, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1253,1253,763,13,[],200,llama-13b,128,1,1025.0,1.0,1,H100,1697101619353,1697101620378.0,120,20.0,1.0,"[12, 1013]","[1697101619365, 1697101620378]"
1254,1254,187,14,[],200,llama-13b,128,1,1678.0,1.0,1,H100,1697101620379,1697101622057.0,120,161.0,6.0,"[7, 1287, 85, 81, 79, 79, 60]","[1697101620386, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
1255,1255,519,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622059,1697101623660.0,120,,,"[19, 1494]","[1697101622078, 1697101623572]"
1256,1256,601,6,[],200,llama-13b,128,1,4491.0,1.0,1,H100,1697101610271,1697101614762.0,120,83.0,20.0,"[113, 662, 46, 1094, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 69]","[1697101610384, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762]"
1257,1257,100,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622846,1697101623663.0,120,,,"[6, 720]","[1697101622852, 1697101623572]"
1258,1258,435,24,[],200,llama-13b,128,1,6138.0,1.0,1,H100,1697101656530,1697101662668.0,120,563.0,27.0,"[6, 611, 87, 81, 79, 60, 835, 95, 83, 81, 81, 1076, 102, 98, 97, 93, 85, 805, 101, 76, 98, 93, 87, 831, 107, 101, 95, 94]","[1697101656536, 1697101657147, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668]"
1259,1259,459,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623668,1697101626657.0,120,,,"[239, 1787, 115, 97, 69]","[1697101623907, 1697101625694, 1697101625809, 1697101625906, 1697101625975]"
1260,1260,788,13,[],200,llama-13b,128,1,2030.0,1.0,1,H100,1697101626663,1697101628693.0,120,31.0,1.0,"[254, 1775]","[1697101626917, 1697101628692]"
1261,1261,266,2,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101596518,1697101597433.0,120,9.0,1.0,"[67, 848]","[1697101596585, 1697101597433]"
1262,1262,29,6,[],200,llama-13b,128,1,2481.0,1.0,1,H100,1697101604195,1697101606676.0,120,161.0,6.0,"[6, 311, 736, 1197, 81, 76, 74]","[1697101604201, 1697101604512, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676]"
1263,1263,470,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634995.0,120,,,"[28, 1251]","[1697101633718, 1697101634969]"
1264,1264,813,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621116,1697101623672.0,120,,,"[68, 1398, 90, 87, 86, 84, 83]","[1697101621184, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
1265,1265,243,12,[],200,llama-13b,128,1,2298.0,1.0,1,H100,1697101623677,1697101625975.0,120,67.0,4.0,"[541, 1477, 113, 98, 69]","[1697101624218, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
1266,1266,287,1,[],200,llama-13b,128,1,665.0,1.0,1,H100,1697101597924,1697101598589.0,120,10.0,1.0,"[6, 659]","[1697101597930, 1697101598589]"
1267,1267,646,2,[],200,llama-13b,128,1,1175.0,1.0,1,H100,1697101598591,1697101599766.0,120,14.0,1.0,"[36, 1139]","[1697101598627, 1697101599766]"
1268,1268,75,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101599767,1697101601192.0,120,,,"[42, 1284]","[1697101599809, 1697101601093]"
1269,1269,597,13,[],200,llama-13b,128,1,898.0,1.0,1,H100,1697101625976,1697101626874.0,120,39.0,1.0,"[18, 880]","[1697101625994, 1697101626874]"
1270,1270,24,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626875,1697101628905.0,120,,,[360],[1697101627235]
1271,1271,385,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606678,1697101610268.0,120,,,"[6, 1649, 94, 96, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101606684, 1697101608333, 1697101608427, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
1272,1272,722,10,[],200,llama-13b,128,1,742.0,1.0,1,H100,1697101621840,1697101622582.0,120,39.0,1.0,"[6, 736]","[1697101621846, 1697101622582]"
1273,1273,353,15,[],200,llama-13b,128,1,1973.0,1.0,1,H100,1697101628910,1697101630883.0,120,52.0,4.0,"[380, 1339, 105, 81, 68]","[1697101629290, 1697101630629, 1697101630734, 1697101630815, 1697101630883]"
1274,1274,714,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630884,1697101634994.0,120,,,"[13, 787, 131, 92, 90, 90, 88, 88, 84, 83, 742, 94, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101630897, 1697101631684, 1697101631815, 1697101631907, 1697101631997, 1697101632087, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
1275,1275,155,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622583,1697101623666.0,120,,,"[30, 959]","[1697101622613, 1697101623572]"
1276,1276,834,7,[],200,llama-13b,128,1,4574.0,1.0,1,H100,1697101610275,1697101614849.0,120,85.0,20.0,"[396, 1398, 117, 84, 82, 81, 77, 611, 90, 86, 82, 82, 63, 79, 720, 95, 93, 92, 90, 69, 87]","[1697101610671, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613297, 1697101613379, 1697101613461, 1697101613524, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
1277,1277,107,32,[],200,llama-13b,128,1,2000.0,1.0,1,H100,1697101651610,1697101653610.0,120,216.0,2.0,"[557, 1443]","[1697101652167, 1697101653610]"
1278,1278,570,5,[],200,llama-13b,128,1,776.0,1.0,1,H100,1697101610270,1697101611046.0,120,18.0,1.0,"[198, 578]","[1697101610468, 1697101611046]"
1279,1279,502,9,[],200,llama-13b,128,1,1513.0,1.0,1,H100,1697101622059,1697101623572.0,120,19.0,1.0,"[43, 1470]","[1697101622102, 1697101623572]"
1280,1280,2,6,[],200,llama-13b,128,1,1464.0,1.0,1,H100,1697101611047,1697101612511.0,120,58.0,6.0,"[24, 998, 117, 84, 82, 81, 78]","[1697101611071, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511]"
1281,1281,860,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623573,1697101626656.0,120,,,"[7, 294, 733, 1201, 97, 69]","[1697101623580, 1697101623874, 1697101624607, 1697101625808, 1697101625905, 1697101625974]"
1282,1282,273,6,[],200,llama-13b,128,1,580.0,1.0,1,H100,1697101612434,1697101613014.0,120,19.0,1.0,"[18, 562]","[1697101612452, 1697101613014]"
1283,1283,605,7,[],200,llama-13b,128,1,1122.0,1.0,1,H100,1697101613015,1697101614137.0,120,8.0,1.0,"[36, 1086]","[1697101613051, 1697101614137]"
1284,1284,305,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612434,1697101616321.0,120,,,"[6, 574, 107, 90, 86, 83, 81, 63, 80, 719, 95, 93, 92, 90, 69, 88, 633, 80, 78, 75]","[1697101612440, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613524, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615563, 1697101615641, 1697101615716]"
1285,1285,364,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612513,1697101616322.0,120,,,"[23, 1601, 186, 95, 93, 92, 90, 69, 87, 634, 80, 77, 77]","[1697101612536, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615563, 1697101615640, 1697101615717]"
1286,1286,745,6,[],200,llama-13b,128,1,775.0,1.0,1,H100,1697101610271,1697101611046.0,120,17.0,1.0,"[45, 730]","[1697101610316, 1697101611046]"
1287,1287,33,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614138,1697101616320.0,120,,,"[18, 1244, 84, 78, 79, 76]","[1697101614156, 1697101615400, 1697101615484, 1697101615562, 1697101615641, 1697101615717]"
1288,1288,170,7,[],200,llama-13b,128,1,3371.0,1.0,1,H100,1697101611047,1697101614418.0,120,335.0,15.0,"[12, 1010, 117, 84, 82, 81, 78, 609, 90, 86, 84, 81, 62, 80, 721, 94]","[1697101611059, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613120, 1697101613210, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614324, 1697101614418]"
1289,1289,591,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101616336,1697101623658.0,120,,,"[350, 1570, 108, 81, 81, 77, 79, 669, 95, 88, 67, 86, 85, 82, 720, 96, 93, 94, 91, 93, 73, 644, 80, 80, 78, 61, 614, 87, 87, 84, 83]","[1697101616686, 1697101618256, 1697101618364, 1697101618445, 1697101618526, 1697101618603, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620763, 1697101620857, 1697101620948, 1697101621041, 1697101621114, 1697101621758, 1697101621838, 1697101621918, 1697101621996, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1290,1290,666,9,[],200,llama-13b,128,1,4782.0,1.0,1,H100,1697101616331,1697101621113.0,120,84.0,20.0,"[275, 1649, 109, 81, 81, 78, 78, 669, 95, 88, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93, 72]","[1697101616606, 1697101618255, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041, 1697101621113]"
1291,1291,50,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622583,1697101623673.0,120,,,"[18, 971]","[1697101622601, 1697101623572]"
1292,1292,723,8,[],200,llama-13b,128,1,1044.0,1.0,1,H100,1697101616332,1697101617376.0,120,14.0,1.0,"[173, 871]","[1697101616505, 1697101617376]"
1293,1293,497,16,[],200,llama-13b,128,1,2132.0,1.0,1,H100,1697101623677,1697101625809.0,120,67.0,2.0,"[547, 1471, 114]","[1697101624224, 1697101625695, 1697101625809]"
1294,1294,148,9,[],200,llama-13b,128,1,1711.0,1.0,1,H100,1697101617378,1697101619089.0,120,16.0,1.0,"[106, 1605]","[1697101617484, 1697101619089]"
1295,1295,51,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619447,1697101623658.0,120,,,"[18, 913, 197, 95, 94, 93, 92, 91, 74, 644, 81, 79, 78, 61, 615, 87, 86, 84, 83]","[1697101619465, 1697101620378, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620949, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621996, 1697101622057, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
1296,1296,478,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619090,1697101623659.0,120,,,"[54, 1235, 196, 95, 94, 93, 91, 92, 74, 644, 80, 80, 79, 60, 615, 86, 87, 84, 83]","[1697101619144, 1697101620379, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621838, 1697101621918, 1697101621997, 1697101622057, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1297,1297,864,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621046,1697101623660.0,120,,,"[19, 608, 85, 81, 80, 78, 61, 614, 86, 87, 84, 83]","[1697101621065, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1298,1298,97,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621116,1697101623664.0,120,,,"[91, 1375, 90, 87, 86, 84, 84]","[1697101621207, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
1299,1299,72,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101596518,1697101601192.0,120,,,"[48, 867, 219, 93, 89, 89, 88, 85, 729, 97, 89, 87, 84, 818, 100, 100, 101, 91, 89]","[1697101596566, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598096, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
1300,1300,651,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626673,1697101628905.0,120,,,"[556, 1464]","[1697101627229, 1697101628693]"
1301,1301,408,13,[],200,llama-13b,128,1,903.0,1.0,1,H100,1697101623665,1697101624568.0,120,16.0,1.0,"[205, 698]","[1697101623870, 1697101624568]"
1302,1302,766,14,[],200,llama-13b,128,1,1996.0,1.0,1,H100,1697101624570,1697101626566.0,120,11.0,1.0,"[47, 1949]","[1697101624617, 1697101626566]"
1303,1303,196,15,[],200,llama-13b,128,1,307.0,1.0,1,H100,1697101626567,1697101626874.0,120,13.0,1.0,"[18, 289]","[1697101626585, 1697101626874]"
1304,1304,431,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604272.0,120,,,"[219, 1876, 116, 101, 74]","[1697101601416, 1697101603292, 1697101603408, 1697101603509, 1697101603583]"
1305,1305,524,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626875,1697101628905.0,120,,,"[392, 1426]","[1697101627267, 1697101628693]"
1306,1306,304,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623664,1697101626656.0,120,,,"[8, 896, 39, 1201, 98, 68]","[1697101623672, 1697101624568, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
1307,1307,882,17,[],200,llama-13b,128,1,3353.0,1.0,1,H100,1697101628910,1697101632263.0,120,345.0,11.0,"[467, 1253, 104, 81, 69, 65, 865, 92, 90, 90, 89, 88]","[1697101629377, 1697101630630, 1697101630734, 1697101630815, 1697101630884, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263]"
1308,1308,298,13,[],200,llama-13b,128,1,902.0,1.0,1,H100,1697101623666,1697101624568.0,120,17.0,1.0,"[205, 697]","[1697101623871, 1697101624568]"
1309,1309,639,9,[],200,llama-13b,128,1,2238.0,1.0,1,H100,1697101610273,1697101612511.0,120,100.0,6.0,"[309, 1486, 118, 84, 82, 81, 77]","[1697101610582, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510]"
1310,1310,627,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101624569,1697101626655.0,120,,,"[24, 1216, 97, 69]","[1697101624593, 1697101625809, 1697101625906, 1697101625975]"
1311,1311,51,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628903.0,120,,,"[149, 745, 42]","[1697101626812, 1697101627557, 1697101627599]"
1312,1312,788,3,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101604275,1697101605195.0,120,31.0,1.0,"[218, 701]","[1697101604493, 1697101605194]"
1313,1313,411,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628909,1697101634994.0,120,,,"[82, 1743, 81, 68, 66, 865, 92, 90, 90, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101628991, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
1314,1314,219,4,[],200,llama-13b,128,1,4253.0,1.0,1,H100,1697101605196,1697101609449.0,120,90.0,20.0,"[18, 993, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 72, 94, 89, 87, 512]","[1697101605214, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608937, 1697101609449]"
1315,1315,765,17,[],200,llama-13b,128,1,1900.0,1.0,1,H100,1697101634997,1697101636897.0,120,84.0,2.0,"[125, 1686, 89]","[1697101635122, 1697101636808, 1697101636897]"
1316,1316,68,10,[],200,llama-13b,128,1,1625.0,1.0,1,H100,1697101612512,1697101614137.0,120,12.0,1.0,"[6, 1619]","[1697101612518, 1697101614137]"
1317,1317,175,28,[],200,llama-13b,128,1,2431.0,1.0,1,H100,1697101667137,1697101669568.0,120,140.0,8.0,"[43, 1216, 101, 95, 71, 91, 90, 724]","[1697101667180, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568]"
1318,1318,198,18,[],200,llama-13b,128,1,4221.0,1.0,1,H100,1697101636898,1697101641119.0,120,96.0,20.0,"[26, 555, 190, 90, 93, 87, 86, 81, 82, 768, 98, 92, 93, 92, 91, 91, 79, 617, 82, 93, 734]","[1697101636924, 1697101637479, 1697101637669, 1697101637759, 1697101637852, 1697101637939, 1697101638025, 1697101638106, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640384, 1697101641118]"
1319,1319,535,29,[],200,llama-13b,128,1,4318.0,1.0,1,H100,1697101669570,1697101673888.0,120,84.0,20.0,"[12, 903, 226, 99, 71, 92, 91, 90, 511, 98, 95, 94, 93, 90, 521, 114, 91, 68, 68, 88, 803]","[1697101669582, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888]"
1320,1320,430,11,[],200,llama-13b,128,1,1262.0,1.0,1,H100,1697101614138,1697101615400.0,120,15.0,1.0,"[6, 1256]","[1697101614144, 1697101615400]"
1321,1321,784,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101615401,1697101616318.0,120,,,"[24, 870]","[1697101615425, 1697101616295]"
1322,1322,183,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101616324,1697101623658.0,120,,,"[560, 1371, 109, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 96, 70, 644, 81, 79, 79, 60, 614, 87, 87, 84, 83]","[1697101616884, 1697101618255, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621044, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1323,1323,710,14,[],200,llama-13b,128,1,1465.0,1.0,1,H100,1697101621117,1697101622582.0,120,14.0,1.0,"[96, 1369]","[1697101621213, 1697101622582]"
1324,1324,136,15,[],200,llama-13b,128,1,989.0,1.0,1,H100,1697101622583,1697101623572.0,120,31.0,1.0,"[36, 953]","[1697101622619, 1697101623572]"
1325,1325,541,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607857,1697101610274.0,120,,,"[7, 469, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607864, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
1326,1326,889,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673893,1697101675812.0,120,,,"[40, 915, 133, 102, 99, 95, 70, 95]","[1697101673933, 1697101674848, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675442]"
1327,1327,763,14,[],200,llama-13b,128,1,1512.0,1.0,1,H100,1697101622060,1697101623572.0,120,20.0,1.0,"[30, 1482]","[1697101622090, 1697101623572]"
1328,1328,193,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623573,1697101626656.0,120,,,"[13, 1021, 1201, 97, 69]","[1697101623586, 1697101624607, 1697101625808, 1697101625905, 1697101625974]"
1329,1329,880,5,[],200,llama-13b,128,1,721.0,1.0,1,H100,1697101614763,1697101615484.0,120,84.0,2.0,"[17, 620, 84]","[1697101614780, 1697101615400, 1697101615484]"
1330,1330,310,6,[],200,llama-13b,128,1,810.0,1.0,1,H100,1697101615485,1697101616295.0,120,26.0,1.0,"[18, 792]","[1697101615503, 1697101616295]"
1331,1331,900,6,[],200,llama-13b,128,1,2233.0,1.0,1,H100,1697101610278,1697101612511.0,120,67.0,6.0,"[518, 1272, 118, 84, 82, 81, 77]","[1697101610796, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510]"
1332,1332,320,31,[],200,llama-13b,128,1,8034.0,1.0,1,H100,1697101675815,1697101683849.0,120,109.0,36.0,"[396, 1266, 64, 50, 764, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 91, 460, 103, 57, 73, 803, 85, 83, 79, 78, 771, 97, 93, 91, 69, 91, 90, 614]","[1697101676211, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681607, 1697101681692, 1697101681775, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848]"
1333,1333,664,7,[],200,llama-13b,128,1,3148.0,1.0,1,H100,1697101616297,1697101619445.0,120,364.0,9.0,"[71, 1008, 49, 939, 81, 80, 78, 79, 668, 95]","[1697101616368, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619350, 1697101619445]"
1334,1334,89,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619447,1697101623658.0,120,,,"[6, 925, 197, 95, 94, 93, 92, 91, 74, 644, 81, 79, 78, 61, 615, 87, 86, 84, 83]","[1697101619453, 1697101620378, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620949, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621996, 1697101622057, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
1335,1335,420,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612512,1697101616321.0,120,,,"[12, 1613, 186, 95, 93, 92, 90, 69, 88, 633, 80, 78, 75]","[1697101612524, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615563, 1697101615641, 1697101615716]"
1336,1336,547,16,[],200,llama-13b,128,1,894.0,1.0,1,H100,1697101626663,1697101627557.0,120,12.0,1.0,"[143, 751]","[1697101626806, 1697101627557]"
1337,1337,909,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101627558,1697101628905.0,120,,,"[24, 1111]","[1697101627582, 1697101628693]"
1338,1338,339,18,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628910,1697101633688.0,120,87.0,20.0,"[357, 1362, 105, 81, 68, 66, 865, 92, 91, 89, 89, 88, 84, 82, 743, 94, 72, 94, 93, 92, 71]","[1697101629267, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
1339,1339,646,1,[],200,llama-13b,128,1,1443.0,1.0,1,H100,1697101595990,1697101597433.0,120,14.0,1.0,"[42, 1401]","[1697101596032, 1697101597433]"
1340,1340,571,3,[],200,llama-13b,128,1,961.0,1.0,1,H100,1697101601194,1697101602155.0,120,67.0,2.0,"[9, 908, 44]","[1697101601203, 1697101602111, 1697101602155]"
1341,1341,924,4,[],200,llama-13b,128,1,2034.0,1.0,1,H100,1697101602160,1697101604194.0,120,9.0,1.0,"[7, 2027]","[1697101602167, 1697101604194]"
1342,1342,191,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607271,1697101610268.0,120,,,"[12, 1050, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607283, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
1343,1343,7,15,[],200,llama-13b,128,1,2309.0,1.0,1,H100,1697101617378,1697101619687.0,120,345.0,11.0,"[30, 848, 108, 82, 80, 78, 79, 668, 94, 89, 67, 86]","[1697101617408, 1697101618256, 1697101618364, 1697101618446, 1697101618526, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687]"
1344,1344,255,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610275,1697101616319.0,120,,,"[325, 1469, 117, 84, 82, 81, 78, 610, 90, 86, 83, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87, 634, 79, 78, 76]","[1697101610600, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1345,1345,547,7,[],200,llama-13b,128,1,1790.0,1.0,1,H100,1697101610279,1697101612069.0,120,12.0,1.0,"[589, 1201]","[1697101610868, 1697101612069]"
1346,1346,532,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616321.0,120,,,"[7, 1435]","[1697101614859, 1697101616294]"
1347,1347,110,6,[],200,llama-13b,128,1,2113.0,1.0,1,H100,1697101616332,1697101618445.0,120,96.0,4.0,"[77, 968, 48, 939, 81]","[1697101616409, 1697101617377, 1697101617425, 1697101618364, 1697101618445]"
1348,1348,162,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623666,1697101626657.0,120,,,"[154, 748, 40, 1201, 97, 69]","[1697101623820, 1697101624568, 1697101624608, 1697101625809, 1697101625906, 1697101625975]"
1349,1349,469,9,[],200,llama-13b,128,1,626.0,1.0,1,H100,1697101621047,1697101621673.0,120,17.0,1.0,"[36, 590]","[1697101621083, 1697101621673]"
1350,1350,464,7,[],200,llama-13b,128,1,643.0,1.0,1,H100,1697101618446,1697101619089.0,120,12.0,1.0,"[7, 636]","[1697101618453, 1697101619089]"
1351,1351,519,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628903.0,120,,,"[163, 731, 42]","[1697101626826, 1697101627557, 1697101627599]"
1352,1352,911,8,[],200,llama-13b,128,1,2829.0,1.0,1,H100,1697101619090,1697101621919.0,120,335.0,11.0,"[12, 1276, 196, 96, 94, 93, 91, 92, 74, 644, 81, 80]","[1697101619102, 1697101620378, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919]"
1353,1353,830,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621674,1697101623664.0,120,,,"[13, 895, 90, 87, 86, 84, 84]","[1697101621687, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
1354,1354,572,12,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616332,1697101617377.0,120,16.0,1.0,"[259, 786]","[1697101616591, 1697101617377]"
1355,1355,264,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623672,1697101626657.0,120,,,"[421, 1602, 113, 98, 69]","[1697101624093, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
1356,1356,42,18,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,10.0,1.0,"[89, 1631]","[1697101628998, 1697101630629]"
1357,1357,340,17,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634997,1697101640208.0,120,85.0,20.0,"[105, 1705, 90, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 92, 93, 91, 91, 79, 616]","[1697101635102, 1697101636807, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208]"
1358,1358,622,12,[],200,llama-13b,128,1,2021.0,1.0,1,H100,1697101626672,1697101628693.0,120,20.0,1.0,"[543, 1478]","[1697101627215, 1697101628693]"
1359,1359,887,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101595990,1697101601191.0,120,,,"[25, 1418, 219, 93, 89, 89, 88, 84, 730, 97, 90, 86, 84, 818, 100, 100, 101, 91, 88]","[1697101596015, 1697101597433, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598825, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
1360,1360,47,13,[],200,llama-13b,128,1,4992.0,1.0,1,H100,1697101628695,1697101633687.0,120,90.0,20.0,"[54, 668, 1317, 81, 68, 66, 864, 92, 91, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 92, 71]","[1697101628749, 1697101629417, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631905, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633687]"
1361,1361,401,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630631,1697101634994.0,120,,,"[43, 1010, 130, 92, 91, 89, 89, 88, 84, 83, 741, 95, 72, 94, 93, 92, 70, 615, 74, 71]","[1697101630674, 1697101631684, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
1362,1362,759,20,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634997,1697101640208.0,120,92.0,20.0,"[173, 1638, 89, 87, 55, 629, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 80, 615]","[1697101635170, 1697101636808, 1697101636897, 1697101636984, 1697101637039, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640208]"
1363,1363,342,19,[],200,llama-13b,128,1,3338.0,1.0,1,H100,1697101640211,1697101643549.0,120,364.0,14.0,"[37, 754, 117, 85, 78, 78, 761, 100, 99, 96, 89, 82, 63, 800, 99]","[1697101640248, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549]"
1364,1364,629,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597436,1697101601193.0,120,,,"[34, 1119, 237, 96, 89, 87, 84, 818, 101, 99, 101, 90, 89]","[1697101597470, 1697101598589, 1697101598826, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600101, 1697101600200, 1697101600301, 1697101600391, 1697101600480]"
1365,1365,882,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614763,1697101616321.0,120,,,"[19, 618, 84, 79, 78, 76]","[1697101614782, 1697101615400, 1697101615484, 1697101615563, 1697101615641, 1697101615717]"
1366,1366,519,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626666,1697101628905.0,120,,,"[340, 1687]","[1697101627006, 1697101628693]"
1367,1367,797,16,[],200,llama-13b,128,1,1809.0,1.0,1,H100,1697101635000,1697101636809.0,120,26.0,1.0,"[497, 1311]","[1697101635497, 1697101636808]"
1368,1368,853,18,[],200,llama-13b,128,1,5466.0,1.0,1,H100,1697101628910,1697101634376.0,120,364.0,22.0,"[362, 1357, 105, 81, 68, 66, 865, 92, 90, 90, 89, 87, 84, 83, 743, 94, 72, 94, 93, 92, 71, 613, 75]","[1697101629272, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688, 1697101634301, 1697101634376]"
1369,1369,225,17,[],200,llama-13b,128,1,668.0,1.0,1,H100,1697101636811,1697101637479.0,120,23.0,1.0,"[77, 591]","[1697101636888, 1697101637479]"
1370,1370,856,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598827,1697101601191.0,120,,,"[48, 891, 234, 100, 100, 101, 91, 89]","[1697101598875, 1697101599766, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
1371,1371,257,3,[],200,llama-13b,128,1,2096.0,1.0,1,H100,1697101601197,1697101603293.0,120,14.0,1.0,"[302, 1794]","[1697101601499, 1697101603293]"
1372,1372,184,21,[],200,llama-13b,128,1,4207.0,1.0,1,H100,1697101640212,1697101644419.0,120,87.0,20.0,"[76, 714, 118, 84, 79, 77, 761, 100, 99, 96, 89, 82, 63, 801, 98, 97, 92, 96, 86, 85, 414]","[1697101640288, 1697101641002, 1697101641120, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643549, 1697101643646, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644419]"
1373,1373,501,11,[],200,llama-13b,128,1,832.0,1.0,1,H100,1697101618257,1697101619089.0,120,19.0,1.0,"[60, 772]","[1697101618317, 1697101619089]"
1374,1374,341,11,[],200,llama-13b,128,1,4780.0,1.0,1,H100,1697101616334,1697101621114.0,120,87.0,20.0,"[542, 1380, 108, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 98, 68]","[1697101616876, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621046, 1697101621114]"
1375,1375,879,25,[],200,llama-13b,128,1,10415.0,1.0,1,H100,1697101662669,1697101673084.0,120,39.0,55.0,"[6, 662, 127, 101, 93, 89, 86, 845, 108, 103, 96, 94, 101, 688, 105, 101, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88]","[1697101662675, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084]"
1376,1376,861,12,[],200,llama-13b,128,1,1289.0,1.0,1,H100,1697101619090,1697101620379.0,120,10.0,1.0,"[42, 1247]","[1697101619132, 1697101620379]"
1377,1377,676,20,[],200,llama-13b,128,1,770.0,1.0,1,H100,1697101643551,1697101644321.0,120,19.0,1.0,"[6, 764]","[1697101643557, 1697101644321]"
1378,1378,104,21,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101644322,1697101649242.0,120,93.0,20.0,"[30, 1088, 79, 77, 77, 75, 741, 98, 98, 83, 78, 1237, 95, 101, 101, 96, 88, 485, 96, 97]","[1697101644352, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242]"
1379,1379,292,13,[],200,llama-13b,128,1,1293.0,1.0,1,H100,1697101620380,1697101621673.0,120,286.0,1.0,"[18, 1275]","[1697101620398, 1697101621673]"
1380,1380,651,14,[],200,llama-13b,128,1,997.0,1.0,1,H100,1697101621675,1697101622672.0,120,457.0,2.0,"[42, 865, 90]","[1697101621717, 1697101622582, 1697101622672]"
1381,1381,623,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,[101],[1697101603396]
1382,1382,458,22,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101649243,1697101649912.0,120,11.0,1.0,"[48, 620]","[1697101649291, 1697101649911]"
1383,1383,171,15,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101622673,1697101623572.0,120,6.0,1.0,"[7, 892]","[1697101622680, 1697101623572]"
1384,1384,528,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623573,1697101626656.0,120,,,"[24, 277, 733, 1201, 98, 68]","[1697101623597, 1697101623874, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
1385,1385,281,11,[],200,llama-13b,128,1,893.0,1.0,1,H100,1697101626663,1697101627556.0,120,23.0,1.0,"[120, 773]","[1697101626783, 1697101627556]"
1386,1386,53,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604280,1697101610266.0,120,,,"[490, 1437, 238, 81, 76, 74, 772, 90, 86, 84, 65, 84, 79, 492, 94, 72, 74, 93, 90, 87, 510, 79, 78]","[1697101604770, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607857, 1697101607936, 1697101608428, 1697101608522, 1697101608594, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609448, 1697101609527, 1697101609605]"
1387,1387,413,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610278,1697101616320.0,120,,,"[532, 1258, 118, 84, 82, 81, 77, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 70, 86, 634, 79, 78, 76]","[1697101610810, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1388,1388,641,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101627558,1697101628905.0,120,,,[12],[1697101627570]
1389,1389,818,23,[],200,llama-13b,128,1,1395.0,1.0,1,H100,1697101649913,1697101651308.0,120,13.0,1.0,"[48, 1347]","[1697101649961, 1697101651308]"
1390,1390,242,24,[],200,llama-13b,128,1,3635.0,1.0,1,H100,1697101651309,1697101654944.0,120,345.0,9.0,"[13, 555, 841, 891, 86, 83, 82, 81, 896, 107]","[1697101651322, 1697101651877, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944]"
1391,1391,303,5,[],200,llama-13b,128,1,4574.0,1.0,1,H100,1697101610275,1697101614849.0,120,88.0,20.0,"[343, 1451, 117, 84, 82, 81, 77, 611, 90, 86, 82, 82, 63, 79, 720, 95, 93, 92, 90, 69, 87]","[1697101610618, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613297, 1697101613379, 1697101613461, 1697101613524, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
1392,1392,884,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628904.0,120,,,"[241, 1787]","[1697101626905, 1697101628692]"
1393,1393,309,18,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628909,1697101633687.0,120,52.0,20.0,"[376, 1344, 105, 81, 68, 66, 865, 92, 90, 90, 89, 87, 84, 83, 743, 95, 71, 93, 94, 91, 71]","[1697101629285, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633172, 1697101633267, 1697101633338, 1697101633431, 1697101633525, 1697101633616, 1697101633687]"
1394,1394,574,25,[],200,llama-13b,128,1,1181.0,1.0,1,H100,1697101654946,1697101656127.0,120,364.0,2.0,"[6, 1046, 129]","[1697101654952, 1697101655998, 1697101656127]"
1395,1395,35,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623666,1697101626657.0,120,,,"[253, 1776, 114, 97, 68]","[1697101623919, 1697101625695, 1697101625809, 1697101625906, 1697101625974]"
1396,1396,411,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610271,1697101616319.0,120,,,"[39, 782, 1094, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 69, 87, 634, 79, 78, 76]","[1697101610310, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1397,1397,394,10,[],200,llama-13b,128,1,2026.0,1.0,1,H100,1697101626667,1697101628693.0,120,11.0,1.0,"[536, 1490]","[1697101627203, 1697101628693]"
1398,1398,561,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604271.0,120,,,"[111, 803, 44, 1253, 101, 74]","[1697101601308, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
1399,1399,4,26,[],200,llama-13b,128,1,5132.0,1.0,1,H100,1697101656128,1697101661260.0,120,89.0,20.0,"[6, 1012, 88, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97, 93, 85, 805, 101, 76, 98]","[1697101656134, 1697101657146, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260]"
1400,1400,80,2,[],200,llama-13b,128,1,1928.0,1.0,1,H100,1697101604280,1697101606208.0,120,13.0,1.0,"[496, 1431]","[1697101604776, 1697101606207]"
1401,1401,914,9,[],200,llama-13b,128,1,4577.0,1.0,1,H100,1697101610272,1697101614849.0,120,84.0,20.0,"[211, 1585, 118, 84, 82, 81, 77, 611, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87]","[1697101610483, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
1402,1402,434,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606209,1697101610267.0,120,,,"[84, 976, 179, 90, 86, 84, 65, 83, 80, 491, 96, 72, 72, 94, 90, 87, 511, 78, 78]","[1697101606293, 1697101607269, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608938, 1697101609449, 1697101609527, 1697101609605]"
1403,1403,667,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[121, 1159]","[1697101633811, 1697101634970]"
1404,1404,234,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601199,1697101604271.0,120,,,"[530, 1564, 115, 101, 74]","[1697101601729, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
1405,1405,56,16,[],200,llama-13b,128,1,4777.0,1.0,1,H100,1697101628910,1697101633687.0,120,86.0,20.0,"[172, 1547, 104, 82, 68, 66, 864, 93, 91, 89, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70]","[1697101629082, 1697101630629, 1697101630733, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687]"
1406,1406,181,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621048,1697101623660.0,120,,,"[70, 1463, 91, 86, 87, 84, 83]","[1697101621118, 1697101622581, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1407,1407,68,20,[],200,llama-13b,128,1,1808.0,1.0,1,H100,1697101635000,1697101636808.0,120,12.0,1.0,"[411, 1397]","[1697101635411, 1697101636808]"
1408,1408,576,5,[],200,llama-13b,128,1,653.0,1.0,1,H100,1697101609451,1697101610104.0,120,14.0,1.0,"[59, 594]","[1697101609510, 1697101610104]"
1409,1409,908,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610105,1697101616319.0,120,,,"[30, 345, 611, 1095, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 92, 93, 90, 69, 87, 634, 79, 78, 76]","[1697101610135, 1697101610480, 1697101611091, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614510, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1410,1410,710,5,[],200,llama-13b,128,1,1326.0,1.0,1,H100,1697101599767,1697101601093.0,120,14.0,1.0,"[54, 1272]","[1697101599821, 1697101601093]"
1411,1411,112,6,[],200,llama-13b,128,1,1061.0,1.0,1,H100,1697101601094,1697101602155.0,120,16.0,2.0,"[18, 260, 783]","[1697101601112, 1697101601372, 1697101602155]"
1412,1412,465,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101602159,1697101604271.0,120,,,"[8, 2027]","[1697101602167, 1697101604194]"
1413,1413,432,21,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101636810,1697101637479.0,120,13.0,1.0,"[36, 633]","[1697101636846, 1697101637479]"
1414,1414,230,9,[],200,llama-13b,128,1,1813.0,1.0,1,H100,1697101621116,1697101622929.0,120,86.0,5.0,"[87, 1379, 90, 87, 86, 84]","[1697101621203, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622929]"
1415,1415,792,22,[],200,llama-13b,128,1,1260.0,1.0,1,H100,1697101637480,1697101638740.0,120,11.0,1.0,"[36, 1224]","[1697101637516, 1697101638740]"
1416,1416,220,23,[],200,llama-13b,128,1,1469.0,1.0,1,H100,1697101638740,1697101640209.0,120,67.0,2.0,"[19, 1326, 124]","[1697101638759, 1697101640085, 1697101640209]"
1417,1417,588,10,[],200,llama-13b,128,1,642.0,1.0,1,H100,1697101622930,1697101623572.0,120,11.0,1.0,"[7, 635]","[1697101622937, 1697101623572]"
1418,1418,919,11,[],200,llama-13b,128,1,300.0,1.0,1,H100,1697101623574,1697101623874.0,120,14.0,1.0,"[41, 259]","[1697101623615, 1697101623874]"
1419,1419,571,24,[],200,llama-13b,128,1,908.0,1.0,1,H100,1697101640212,1697101641120.0,120,67.0,2.0,"[88, 702, 118]","[1697101640300, 1697101641002, 1697101641120]"
1420,1420,346,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623875,1697101626655.0,120,,,"[399, 1422, 113, 97, 69]","[1697101624274, 1697101625696, 1697101625809, 1697101625906, 1697101625975]"
1421,1421,20,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101620380,1697101623659.0,120,,,"[30, 1263, 85, 81, 79, 79, 60, 614, 87, 87, 85, 82]","[1697101620410, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622930, 1697101623012]"
1422,1422,807,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614763,1697101616321.0,120,,,"[31, 606, 84, 79, 78, 76]","[1697101614794, 1697101615400, 1697101615484, 1697101615563, 1697101615641, 1697101615717]"
1423,1423,890,2,[],200,llama-13b,128,1,4663.0,1.0,1,H100,1697101604275,1697101608938.0,120,93.0,20.0,"[98, 875, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 73, 72, 94, 89, 87]","[1697101604373, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608937]"
1424,1424,536,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101615642,1697101616319.0,120,,,"[6, 647]","[1697101615648, 1697101616295]"
1425,1425,886,10,[],200,llama-13b,128,1,1044.0,1.0,1,H100,1697101616332,1697101617376.0,120,17.0,1.0,"[165, 879]","[1697101616497, 1697101617376]"
1426,1426,104,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625976,1697101628903.0,120,,,"[12, 1611]","[1697101625988, 1697101627599]"
1427,1427,546,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628903.0,120,,,"[120, 816]","[1697101626783, 1697101627599]"
1428,1428,391,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622059,1697101623660.0,120,,,[7],[1697101622066]
1429,1429,364,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613382,1697101616320.0,120,,,"[6, 749, 187, 94, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101613388, 1697101614137, 1697101614324, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
1430,1430,900,15,[],200,llama-13b,128,1,2906.0,1.0,1,H100,1697101628908,1697101631814.0,120,67.0,6.0,"[59, 1662, 105, 81, 68, 66, 865]","[1697101628967, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814]"
1431,1431,751,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623676,1697101626657.0,120,,,"[507, 1625, 99, 68]","[1697101624183, 1697101625808, 1697101625907, 1697101625975]"
1432,1432,1,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101617378,1697101623658.0,120,,,"[25, 853, 108, 82, 80, 78, 79, 668, 94, 89, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60, 614, 87, 87, 84, 83]","[1697101617403, 1697101618256, 1697101618364, 1697101618446, 1697101618526, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1433,1433,183,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626666,1697101628905.0,120,,,"[424, 1603]","[1697101627090, 1697101628693]"
1434,1434,316,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601196,1697101604270.0,120,,,"[413, 1684, 115, 101, 74]","[1697101601609, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
1435,1435,631,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628910,1697101634994.0,120,,,"[369, 1455, 81, 68, 66, 865, 92, 90, 90, 89, 87, 84, 83, 743, 95, 71, 94, 93, 92, 71, 613, 75, 71]","[1697101629279, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688, 1697101634301, 1697101634376, 1697101634447]"
1436,1436,45,14,[],200,llama-13b,128,1,1280.0,1.0,1,H100,1697101633690,1697101634970.0,120,19.0,1.0,"[109, 1170]","[1697101633799, 1697101634969]"
1437,1437,679,3,[],200,llama-13b,128,1,1932.0,1.0,1,H100,1697101604275,1697101606207.0,120,15.0,1.0,"[317, 1615]","[1697101604592, 1697101606207]"
1438,1438,104,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101606208,1697101610266.0,120,,,"[7, 1233, 89, 87, 84, 64, 84, 80, 491, 96, 72, 72, 94, 90, 86, 512, 79, 77]","[1697101606215, 1697101607448, 1697101607537, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608851, 1697101608937, 1697101609449, 1697101609528, 1697101609605]"
1439,1439,1,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614763,1697101616321.0,120,,,"[19, 618, 84, 79, 78, 76]","[1697101614782, 1697101615400, 1697101615484, 1697101615563, 1697101615641, 1697101615717]"
1440,1440,403,15,[],200,llama-13b,128,1,614.0,1.0,1,H100,1697101634970,1697101635584.0,120,874.0,2.0,"[32, 516, 65]","[1697101635002, 1697101635518, 1697101635583]"
1441,1441,764,16,[],200,llama-13b,128,1,1894.0,1.0,1,H100,1697101635585,1697101637479.0,120,39.0,1.0,"[33, 1861]","[1697101635618, 1697101637479]"
1442,1442,52,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603409,1697101604272.0,120,,,"[67, 718]","[1697101603476, 1697101604194]"
1443,1443,409,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604280,1697101610265.0,120,,,"[414, 1513, 238, 81, 76, 74, 772, 90, 86, 84, 65, 84, 79, 491, 95, 73, 73, 93, 90, 87, 510, 79, 78]","[1697101604694, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607857, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609448, 1697101609527, 1697101609605]"
1444,1444,360,8,[],200,llama-13b,128,1,1923.0,1.0,1,H100,1697101616333,1697101618256.0,120,16.0,1.0,"[639, 1284]","[1697101616972, 1697101618256]"
1445,1445,721,9,[],200,llama-13b,128,1,1345.0,1.0,1,H100,1697101618257,1697101619602.0,120,286.0,5.0,"[36, 795, 264, 94, 88, 68]","[1697101618293, 1697101619088, 1697101619352, 1697101619446, 1697101619534, 1697101619602]"
1446,1446,769,8,[],200,llama-13b,128,1,4575.0,1.0,1,H100,1697101610274,1697101614849.0,120,47.0,20.0,"[311, 1483, 118, 84, 82, 81, 77, 611, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87]","[1697101610585, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
1447,1447,466,33,[],200,llama-13b,128,1,4678.0,1.0,1,H100,1697101653611,1697101658289.0,120,457.0,20.0,"[12, 1008, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 83, 622, 81, 79, 60, 835]","[1697101653623, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
1448,1448,501,7,[],200,llama-13b,128,1,925.0,1.0,1,H100,1697101613212,1697101614137.0,120,19.0,1.0,"[6, 919]","[1697101613218, 1697101614137]"
1449,1449,855,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614138,1697101616320.0,120,,,"[12, 1250, 84, 78, 79, 76]","[1697101614150, 1697101615400, 1697101615484, 1697101615562, 1697101615641, 1697101615717]"
1450,1450,284,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101616332,1697101623658.0,120,,,"[343, 1689, 81, 81, 77, 79, 669, 95, 88, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93, 73, 643, 81, 80, 78, 61, 614, 87, 87, 84, 83]","[1697101616675, 1697101618364, 1697101618445, 1697101618526, 1697101618603, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041, 1697101621114, 1697101621757, 1697101621838, 1697101621918, 1697101621996, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1451,1451,259,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616322.0,120,,,"[18, 1425]","[1697101614870, 1697101616295]"
1452,1452,225,14,[],200,llama-13b,128,1,904.0,1.0,1,H100,1697101623664,1697101624568.0,120,23.0,1.0,"[113, 791]","[1697101623777, 1697101624568]"
1453,1453,589,9,[],200,llama-13b,128,1,4708.0,1.0,1,H100,1697101616332,1697101621040.0,120,92.0,20.0,"[66, 978, 49, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95, 94, 94, 91, 92]","[1697101616398, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620763, 1697101620857, 1697101620948, 1697101621040]"
1454,1454,153,10,[],200,llama-13b,128,1,1162.0,1.0,1,H100,1697101619602,1697101620764.0,120,335.0,4.0,"[7, 770, 196, 95, 94]","[1697101619609, 1697101620379, 1697101620575, 1697101620670, 1697101620764]"
1455,1455,862,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625810,1697101626655.0,120,,,[26],[1697101625836]
1456,1456,290,18,[],200,llama-13b,128,1,894.0,1.0,1,H100,1697101626663,1697101627557.0,120,14.0,1.0,"[132, 762]","[1697101626795, 1697101627557]"
1457,1457,650,19,[],200,llama-13b,128,1,1859.0,1.0,1,H100,1697101627558,1697101629417.0,120,13.0,1.0,"[42, 1816]","[1697101627600, 1697101629416]"
1458,1458,600,11,[],200,llama-13b,128,1,909.0,1.0,1,H100,1697101620765,1697101621674.0,120,23.0,1.0,"[7, 901]","[1697101620772, 1697101621673]"
1459,1459,25,12,[],200,llama-13b,128,1,907.0,1.0,1,H100,1697101621675,1697101622582.0,120,12.0,1.0,"[35, 872]","[1697101621710, 1697101622582]"
1460,1460,366,27,[],200,llama-13b,128,1,1401.0,1.0,1,H100,1697101661267,1697101662668.0,120,85.0,6.0,"[13, 636, 356, 106, 101, 95, 94]","[1697101661280, 1697101661916, 1697101662272, 1697101662378, 1697101662479, 1697101662574, 1697101662668]"
1461,1461,454,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623667,1697101626656.0,120,,,"[116, 785, 40, 1201, 97, 68]","[1697101623783, 1697101624568, 1697101624608, 1697101625809, 1697101625906, 1697101625974]"
1462,1462,75,20,[],200,llama-13b,128,1,4958.0,1.0,1,H100,1697101629418,1697101634376.0,120,345.0,18.0,"[6, 2260, 130, 92, 90, 90, 89, 88, 84, 83, 741, 95, 72, 93, 93, 93, 70, 615, 74]","[1697101629424, 1697101631684, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634302, 1697101634376]"
1463,1463,605,9,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616332,1697101617377.0,120,8.0,1.0,"[264, 781]","[1697101616596, 1697101617377]"
1464,1464,524,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614419,1697101616321.0,120,,,"[7, 974, 84, 78, 79, 76]","[1697101614426, 1697101615400, 1697101615484, 1697101615562, 1697101615641, 1697101615717]"
1465,1465,752,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628903.0,120,,,"[213, 1816]","[1697101626876, 1697101628692]"
1466,1466,128,10,[],200,llama-13b,128,1,878.0,1.0,1,H100,1697101617378,1697101618256.0,120,9.0,1.0,"[42, 836]","[1697101617420, 1697101618256]"
1467,1467,485,11,[],200,llama-13b,128,1,1189.0,1.0,1,H100,1697101618257,1697101619446.0,120,67.0,3.0,"[13, 819, 262, 95]","[1697101618270, 1697101619089, 1697101619351, 1697101619446]"
1468,1468,844,12,[],200,llama-13b,128,1,932.0,1.0,1,H100,1697101619447,1697101620379.0,120,10.0,1.0,"[12, 920]","[1697101619459, 1697101620379]"
1469,1469,265,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101620379,1697101623659.0,120,,,"[13, 1281, 85, 81, 79, 79, 60, 614, 88, 86, 84, 84]","[1697101620392, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
1470,1470,720,28,[],200,llama-13b,128,1,1164.0,1.0,1,H100,1697101662669,1697101663833.0,120,286.0,6.0,"[12, 656, 127, 101, 93, 89, 86]","[1697101662681, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833]"
1471,1471,524,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601094,1697101604270.0,120,,,"[24, 254, 783, 1253, 101, 74]","[1697101601118, 1697101601372, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
1472,1472,866,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621116,1697101623664.0,120,,,"[57, 1499, 86, 87, 84, 83]","[1697101621173, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1473,1473,160,13,[],200,llama-13b,128,1,1513.0,1.0,1,H100,1697101622059,1697101623572.0,120,13.0,1.0,"[13, 1500]","[1697101622072, 1697101623572]"
1474,1474,517,14,[],200,llama-13b,128,1,301.0,1.0,1,H100,1697101623573,1697101623874.0,120,15.0,1.0,"[30, 271]","[1697101623603, 1697101623874]"
1475,1475,880,15,[],200,llama-13b,128,1,1934.0,1.0,1,H100,1697101623875,1697101625809.0,120,84.0,2.0,"[405, 1416, 113]","[1697101624280, 1697101625696, 1697101625809]"
1476,1476,494,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623573,1697101626656.0,120,,,"[24, 277, 733, 1201, 98, 68]","[1697101623597, 1697101623874, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
1477,1477,305,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625810,1697101626657.0,120,,,"[12, 744]","[1697101625822, 1697101626566]"
1478,1478,704,13,[],200,llama-13b,128,1,894.0,1.0,1,H100,1697101626663,1697101627557.0,120,14.0,1.0,"[131, 763]","[1697101626794, 1697101627557]"
1479,1479,663,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[42, 852, 42]","[1697101626705, 1697101627557, 1697101627599]"
1480,1480,671,19,[],200,llama-13b,128,1,1278.0,1.0,1,H100,1697101633691,1697101634969.0,120,12.0,1.0,"[90, 1188]","[1697101633781, 1697101634969]"
1481,1481,852,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608940,1697101610266.0,120,,,"[12, 1151]","[1697101608952, 1697101610103]"
1482,1482,100,20,[],200,llama-13b,128,1,4083.0,1.0,1,H100,1697101634970,1697101639053.0,120,732.0,14.0,"[34, 514, 65, 1314, 87, 55, 629, 91, 92, 87, 86, 82, 82, 767, 98]","[1697101635004, 1697101635518, 1697101635583, 1697101636897, 1697101636984, 1697101637039, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053]"
1483,1483,779,8,[],200,llama-13b,128,1,3267.0,1.0,1,H100,1697101616334,1697101619601.0,120,563.0,10.0,"[353, 1569, 108, 81, 81, 77, 79, 669, 95, 88, 67]","[1697101616687, 1697101618256, 1697101618364, 1697101618445, 1697101618526, 1697101618603, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601]"
1484,1484,248,6,[],200,llama-13b,128,1,4240.0,1.0,1,H100,1697101610271,1697101614511.0,120,182.0,17.0,"[11, 764, 45, 1095, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93]","[1697101610282, 1697101611046, 1697101611091, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511]"
1485,1485,129,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101627560,1697101634994.0,120,,,"[39, 1817, 1317, 81, 69, 66, 864, 92, 91, 90, 88, 88, 84, 83, 742, 95, 72, 93, 93, 92, 70, 615, 75, 71]","[1697101627599, 1697101629416, 1697101630733, 1697101630814, 1697101630883, 1697101630949, 1697101631813, 1697101631905, 1697101631996, 1697101632086, 1697101632174, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633686, 1697101634301, 1697101634376, 1697101634447]"
1486,1486,351,5,[],200,llama-13b,128,1,2481.0,1.0,1,H100,1697101604195,1697101606676.0,120,216.0,6.0,"[13, 304, 736, 1197, 81, 76, 74]","[1697101604208, 1697101604512, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676]"
1487,1487,60,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628909,1697101634994.0,120,,,"[197, 1628, 81, 68, 66, 864, 93, 91, 89, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101629106, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
1488,1488,204,9,[],200,llama-13b,128,1,1347.0,1.0,1,H100,1697101619602,1697101620949.0,120,67.0,6.0,"[7, 770, 196, 95, 94, 93, 92]","[1697101619609, 1697101620379, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620949]"
1489,1489,601,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614512,1697101616321.0,120,,,"[6, 882, 84, 79, 78, 76]","[1697101614518, 1697101615400, 1697101615484, 1697101615563, 1697101615641, 1697101615717]"
1490,1490,558,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101620950,1697101623659.0,120,,,"[6, 718, 84, 81, 80, 78, 60, 614, 87, 87, 84, 83]","[1697101620956, 1697101621674, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1491,1491,32,8,[],200,llama-13b,128,1,2347.0,1.0,1,H100,1697101616336,1697101618683.0,120,140.0,6.0,"[542, 1377, 109, 81, 80, 78, 79]","[1697101616878, 1697101618255, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682]"
1492,1492,328,6,[],200,llama-13b,128,1,2233.0,1.0,1,H100,1697101610277,1697101612510.0,120,109.0,6.0,"[411, 1381, 117, 84, 82, 80, 78]","[1697101610688, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510]"
1493,1493,687,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612512,1697101616321.0,120,,,"[6, 1619, 186, 95, 93, 92, 90, 69, 87, 634, 80, 78, 75]","[1697101612518, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615563, 1697101615641, 1697101615716]"
1494,1494,348,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623670,1697101626657.0,120,,,"[309, 1716, 114, 97, 69]","[1697101623979, 1697101625695, 1697101625809, 1697101625906, 1697101625975]"
1495,1495,393,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101618684,1697101623658.0,120,,,"[6, 1688, 196, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 60, 614, 87, 87, 84, 83]","[1697101618690, 1697101620378, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1496,1496,711,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626665,1697101628905.0,120,,,"[414, 1614]","[1697101627079, 1697101628693]"
1497,1497,143,16,[],200,llama-13b,128,1,3437.0,1.0,1,H100,1697101628910,1697101632347.0,120,6.0,12.0,"[417, 1302, 105, 81, 68, 66, 865, 92, 90, 90, 89, 87, 85]","[1697101629327, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632347]"
1498,1498,502,17,[],200,llama-13b,128,1,619.0,1.0,1,H100,1697101632348,1697101632967.0,120,19.0,1.0,"[6, 613]","[1697101632354, 1697101632967]"
1499,1499,639,17,[],200,llama-13b,128,1,1830.0,1.0,1,H100,1697101640292,1697101642122.0,120,100.0,6.0,"[31, 679, 118, 84, 79, 78, 760]","[1697101640323, 1697101641002, 1697101641120, 1697101641204, 1697101641283, 1697101641361, 1697101642121]"
1500,1500,123,8,[],200,llama-13b,128,1,1046.0,1.0,1,H100,1697101616331,1697101617377.0,120,14.0,1.0,"[96, 950]","[1697101616427, 1697101617377]"
1501,1501,451,9,[],200,llama-13b,128,1,1710.0,1.0,1,H100,1697101617378,1697101619088.0,120,286.0,1.0,"[94, 1616]","[1697101617472, 1697101619088]"
1502,1502,810,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619090,1697101623659.0,120,,,"[30, 1258, 196, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 60, 614, 87, 87, 84, 83]","[1697101619120, 1697101620378, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1503,1503,66,18,[],200,llama-13b,128,1,4465.0,1.0,1,H100,1697101642122,1697101646587.0,120,84.0,20.0,"[19, 990, 320, 99, 96, 92, 95, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 78, 74, 741, 98]","[1697101642141, 1697101643131, 1697101643451, 1697101643550, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587]"
1504,1504,754,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623665,1697101626656.0,120,,,"[108, 835, 1200, 98, 68]","[1697101623773, 1697101624608, 1697101625808, 1697101625906, 1697101625974]"
1505,1505,494,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616322.0,120,,,"[30, 1413]","[1697101614882, 1697101616295]"
1506,1506,39,18,[],200,llama-13b,128,1,791.0,1.0,1,H100,1697101640211,1697101641002.0,120,8.0,1.0,"[95, 696]","[1697101640306, 1697101641002]"
1507,1507,317,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608940,1697101610266.0,120,,,"[18, 1146]","[1697101608958, 1697101610104]"
1508,1508,678,4,[],200,llama-13b,128,1,4332.0,1.0,1,H100,1697101610271,1697101614603.0,120,244.0,18.0,"[21, 754, 45, 1095, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92]","[1697101610292, 1697101611046, 1697101611091, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603]"
1509,1509,41,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101616335,1697101623658.0,120,,,"[567, 1354, 108, 81, 80, 79, 79, 668, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 93, 73, 644, 81, 79, 79, 60, 614, 87, 87, 84, 83]","[1697101616902, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1510,1510,395,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646588,1697101651603.0,120,,,"[37, 1117, 341, 95, 101, 101, 96, 88, 485, 97, 95, 83, 942, 106, 104, 101, 91, 85]","[1697101646625, 1697101647742, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
1511,1511,534,1,[],200,llama-13b,128,1,4480.0,1.0,1,H100,1697101594346,1697101598826.0,120,96.0,20.0,"[6, 868, 124, 101, 99, 96, 94, 73, 92, 90, 527, 81, 75, 74, 906, 93, 89, 89, 88, 84, 731]","[1697101594352, 1697101595220, 1697101595344, 1697101595445, 1697101595544, 1697101595640, 1697101595734, 1697101595807, 1697101595899, 1697101595989, 1697101596516, 1697101596597, 1697101596672, 1697101596746, 1697101597652, 1697101597745, 1697101597834, 1697101597923, 1697101598011, 1697101598095, 1697101598826]"
1512,1512,208,14,[],200,llama-13b,128,1,4992.0,1.0,1,H100,1697101628695,1697101633687.0,120,96.0,20.0,"[48, 674, 1317, 81, 68, 66, 864, 92, 91, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 92, 71]","[1697101628743, 1697101629417, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631905, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633687]"
1513,1513,106,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614604,1697101616321.0,120,,,"[6, 790, 84, 79, 78, 76]","[1697101614610, 1697101615400, 1697101615484, 1697101615563, 1697101615641, 1697101615717]"
1514,1514,434,5,[],200,llama-13b,128,1,4577.0,1.0,1,H100,1697101610272,1697101614849.0,120,85.0,20.0,"[229, 1567, 118, 84, 82, 81, 77, 611, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87]","[1697101610501, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
1515,1515,483,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623677,1697101626658.0,120,,,"[511, 1507, 113, 99, 68]","[1697101624188, 1697101625695, 1697101625808, 1697101625907, 1697101625975]"
1516,1516,478,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604514,1697101610266.0,120,,,"[371, 1322, 239, 80, 76, 74, 772, 89, 86, 85, 65, 83, 80, 492, 95, 72, 73, 93, 89, 88, 510, 80, 77]","[1697101604885, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608761, 1697101608850, 1697101608938, 1697101609448, 1697101609528, 1697101609605]"
1517,1517,846,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626668,1697101628904.0,120,,,"[440, 1584]","[1697101627108, 1697101628692]"
1518,1518,556,6,[],200,llama-13b,128,1,1918.0,1.0,1,H100,1697101616338,1697101618256.0,120,9.0,1.0,"[470, 1448]","[1697101616808, 1697101618256]"
1519,1519,579,18,[],200,llama-13b,128,1,1259.0,1.0,1,H100,1697101637480,1697101638739.0,120,19.0,1.0,"[54, 1205]","[1697101637534, 1697101638739]"
1520,1520,910,7,[],200,llama-13b,128,1,831.0,1.0,1,H100,1697101618257,1697101619088.0,120,8.0,1.0,"[48, 783]","[1697101618305, 1697101619088]"
1521,1521,12,19,[],200,llama-13b,128,1,1346.0,1.0,1,H100,1697101638740,1697101640086.0,120,11.0,1.0,"[19, 1327]","[1697101638759, 1697101640086]"
1522,1522,370,20,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101640087,1697101641002.0,120,31.0,1.0,"[6, 909]","[1697101640093, 1697101641002]"
1523,1523,702,21,[],200,llama-13b,128,1,3753.0,1.0,1,H100,1697101641003,1697101644756.0,120,89.0,20.0,"[25, 789, 305, 100, 98, 97, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 86, 84, 84, 82]","[1697101641028, 1697101641817, 1697101642122, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756]"
1524,1524,339,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619090,1697101623659.0,120,,,"[36, 1252, 196, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 60, 615, 86, 86, 85, 83]","[1697101619126, 1697101620378, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057, 1697101622672, 1697101622758, 1697101622844, 1697101622929, 1697101623012]"
1525,1525,540,22,[],200,llama-13b,128,1,1253.0,1.0,1,H100,1697101644421,1697101645674.0,120,140.0,5.0,"[19, 897, 103, 79, 77, 78]","[1697101644440, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645674]"
1526,1526,271,14,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628909,1697101633688.0,120,87.0,20.0,"[260, 1461, 104, 81, 68, 66, 865, 92, 91, 89, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 71]","[1697101629169, 1697101630630, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
1527,1527,156,13,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628910,1697101633688.0,120,86.0,20.0,"[207, 1513, 104, 81, 68, 66, 864, 93, 91, 89, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 71]","[1697101629117, 1697101630630, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
1528,1528,469,7,[],200,llama-13b,128,1,884.0,1.0,1,H100,1697101607449,1697101608333.0,120,17.0,1.0,"[18, 866]","[1697101607467, 1697101608333]"
1529,1529,824,8,[],200,llama-13b,128,1,1271.0,1.0,1,H100,1697101608334,1697101609605.0,120,58.0,4.0,"[18, 1006, 91, 79, 77]","[1697101608352, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
1530,1530,518,14,[],200,llama-13b,128,1,1278.0,1.0,1,H100,1697101633691,1697101634969.0,120,23.0,1.0,"[185, 1093]","[1697101633876, 1697101634969]"
1531,1531,250,9,[],200,llama-13b,128,1,873.0,1.0,1,H100,1697101609607,1697101610480.0,120,31.0,1.0,"[6, 867]","[1697101609613, 1697101610480]"
1532,1532,628,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[112, 1167]","[1697101633802, 1697101634969]"
1533,1533,55,16,[],200,llama-13b,128,1,1809.0,1.0,1,H100,1697101635000,1697101636809.0,120,12.0,1.0,"[491, 1317]","[1697101635491, 1697101636808]"
1534,1534,133,22,[],200,llama-13b,128,1,1540.0,1.0,1,H100,1697101644759,1697101646299.0,120,15.0,1.0,"[18, 1522]","[1697101644777, 1697101646299]"
1535,1535,310,26,[],200,llama-13b,128,1,1762.0,1.0,1,H100,1697101673086,1697101674848.0,120,26.0,1.0,"[18, 1744]","[1697101673104, 1697101674848]"
1536,1536,487,23,[],200,llama-13b,128,1,4453.0,1.0,1,H100,1697101646300,1697101650753.0,120,123.0,17.0,"[6, 1776, 96, 101, 101, 96, 88, 485, 96, 96, 83, 942, 106, 104, 102, 90, 85]","[1697101646306, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650578, 1697101650668, 1697101650753]"
1537,1537,671,27,[],200,llama-13b,128,1,940.0,1.0,1,H100,1697101674849,1697101675789.0,120,12.0,1.0,"[18, 922]","[1697101674867, 1697101675789]"
1538,1538,95,28,[],200,llama-13b,128,1,589.0,1.0,1,H100,1697101675791,1697101676380.0,120,12.0,1.0,"[36, 553]","[1697101675827, 1697101676380]"
1539,1539,608,10,[],200,llama-13b,128,1,4369.0,1.0,1,H100,1697101610481,1697101614850.0,120,96.0,20.0,"[405, 1183, 117, 84, 82, 81, 78, 609, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 70, 87]","[1697101610886, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614850]"
1540,1540,426,29,[],200,llama-13b,128,1,7767.0,1.0,1,H100,1697101676381,1697101684148.0,120,79.0,36.0,"[98, 1768, 107, 90, 86, 64, 84, 81, 646, 98, 98, 95, 71, 92, 91, 70, 90, 469, 95, 57, 73, 804, 84, 83, 79, 79, 770, 97, 93, 91, 69, 91, 90, 615, 101, 101, 97]","[1697101676479, 1697101678247, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679950, 1697101680020, 1697101680110, 1697101680579, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148]"
1541,1541,512,4,[],200,llama-13b,128,1,2095.0,1.0,1,H100,1697101601198,1697101603293.0,120,11.0,1.0,"[495, 1599]","[1697101601693, 1697101603292]"
1542,1542,872,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603294,1697101604272.0,120,,,"[25, 875]","[1697101603319, 1697101604194]"
1543,1543,584,15,[],200,llama-13b,128,1,1126.0,1.0,1,H100,1697101624569,1697101625695.0,120,10.0,1.0,"[30, 1096]","[1697101624599, 1697101625695]"
1544,1544,9,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626656.0,120,,,"[90, 779]","[1697101625787, 1697101626566]"
1545,1545,300,6,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101604275,1697101605195.0,120,9.0,1.0,"[235, 685]","[1697101604510, 1697101605195]"
1546,1546,536,1,[],200,llama-13b,128,1,4401.0,1.0,1,H100,1697101595900,1697101600301.0,120,83.0,20.0,"[12, 498, 107, 81, 74, 74, 905, 94, 89, 89, 87, 85, 731, 96, 90, 86, 84, 818, 100, 100, 101]","[1697101595912, 1697101596410, 1697101596517, 1697101596598, 1697101596672, 1697101596746, 1697101597651, 1697101597745, 1697101597834, 1697101597923, 1697101598010, 1697101598095, 1697101598826, 1697101598922, 1697101599012, 1697101599098, 1697101599182, 1697101600000, 1697101600100, 1697101600200, 1697101600301]"
1547,1547,745,7,[],200,llama-13b,128,1,1011.0,1.0,1,H100,1697101605196,1697101606207.0,120,17.0,1.0,"[36, 975]","[1697101605232, 1697101606207]"
1548,1548,175,8,[],200,llama-13b,128,1,1727.0,1.0,1,H100,1697101606209,1697101607936.0,120,140.0,8.0,"[47, 1192, 90, 86, 84, 65, 83, 80]","[1697101606256, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936]"
1549,1549,804,12,[],200,llama-13b,128,1,893.0,1.0,1,H100,1697101626663,1697101627556.0,120,20.0,1.0,"[126, 767]","[1697101626789, 1697101627556]"
1550,1550,363,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[48, 846, 42]","[1697101626711, 1697101627557, 1697101627599]"
1551,1551,321,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101627557,1697101628905.0,120,,,"[7, 1129]","[1697101627564, 1697101628693]"
1552,1552,14,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621046,1697101623660.0,120,,,"[15, 612, 85, 81, 80, 78, 61, 613, 87, 87, 84, 83]","[1697101621061, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1553,1553,883,7,[],200,llama-13b,128,1,921.0,1.0,1,H100,1697101604274,1697101605195.0,120,563.0,1.0,"[224, 696]","[1697101604498, 1697101605194]"
1554,1554,150,29,[],200,llama-13b,128,1,2034.0,1.0,1,H100,1697101663834,1697101665868.0,120,216.0,2.0,"[7, 2027]","[1697101663841, 1697101665868]"
1555,1555,693,18,[],200,llama-13b,128,1,1825.0,1.0,1,H100,1697101628909,1697101630734.0,120,67.0,2.0,"[284, 1436, 105]","[1697101629193, 1697101630629, 1697101630734]"
1556,1556,479,30,[],200,llama-13b,128,1,7127.0,1.0,1,H100,1697101665869,1697101672996.0,120,140.0,36.0,"[7, 894, 366, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 513, 97, 95, 94, 93, 90, 521, 113, 92, 68, 67]","[1697101665876, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672929, 1697101672996]"
1557,1557,298,11,[],200,llama-13b,128,1,2023.0,1.0,1,H100,1697101623671,1697101625694.0,120,17.0,1.0,"[343, 1680]","[1697101624014, 1697101625694]"
1558,1558,628,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626656.0,120,,,"[12, 857]","[1697101625709, 1697101626566]"
1559,1559,304,8,[],200,llama-13b,128,1,4253.0,1.0,1,H100,1697101605196,1697101609449.0,120,86.0,20.0,"[30, 981, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 72, 94, 89, 87, 512]","[1697101605226, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608937, 1697101609449]"
1560,1560,122,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630735,1697101634994.0,120,,,"[18, 931, 131, 92, 90, 89, 89, 88, 84, 83, 742, 94, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101630753, 1697101631684, 1697101631815, 1697101631907, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
1561,1561,623,14,[],200,llama-13b,128,1,2143.0,1.0,1,H100,1697101623665,1697101625808.0,120,140.0,3.0,"[36, 906, 1201]","[1697101623701, 1697101624607, 1697101625808]"
1562,1562,823,8,[],200,llama-13b,128,1,4662.0,1.0,1,H100,1697101604275,1697101608937.0,120,90.0,20.0,"[9, 910, 54, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73, 94, 89, 87]","[1697101604284, 1697101605194, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667, 1697101608761, 1697101608850, 1697101608937]"
1563,1563,24,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625810,1697101626657.0,120,,,[12],[1697101625822]
1564,1564,386,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626666,1697101628904.0,120,,,"[501, 1525]","[1697101627167, 1697101628692]"
1565,1565,749,17,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628909,1697101633688.0,120,47.0,20.0,"[272, 1448, 105, 81, 68, 66, 865, 92, 91, 89, 89, 88, 84, 82, 743, 94, 72, 94, 93, 92, 71]","[1697101629181, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
1566,1566,426,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622059,1697101623660.0,120,,,"[19, 1494]","[1697101622078, 1697101623572]"
1567,1567,746,8,[],200,llama-13b,128,1,4616.0,1.0,1,H100,1697101616332,1697101620948.0,120,345.0,18.0,"[337, 1586, 109, 81, 81, 77, 80, 668, 95, 88, 67, 86, 85, 82, 720, 96, 94, 93, 91]","[1697101616669, 1697101618255, 1697101618364, 1697101618445, 1697101618526, 1697101618603, 1697101618683, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948]"
1568,1568,783,13,[],200,llama-13b,128,1,2023.0,1.0,1,H100,1697101623671,1697101625694.0,120,286.0,1.0,"[337, 1686]","[1697101624008, 1697101625694]"
1569,1569,188,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625696,1697101626656.0,120,,,"[25, 845]","[1697101625721, 1697101626566]"
1570,1570,549,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628904.0,120,,,[250],[1697101626914]
1571,1571,182,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101620949,1697101623659.0,120,,,"[7, 718, 84, 81, 80, 78, 60, 614, 87, 87, 84, 83]","[1697101620956, 1697101621674, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1572,1572,909,16,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628909,1697101633688.0,120,86.0,20.0,"[290, 1430, 105, 81, 68, 66, 865, 92, 91, 89, 89, 88, 84, 83, 742, 94, 72, 94, 93, 92, 71]","[1697101629199, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
1573,1573,487,20,[],200,llama-13b,128,1,4424.0,1.0,1,H100,1697101634998,1697101639422.0,120,123.0,17.0,"[307, 1592, 87, 56, 628, 91, 92, 87, 86, 82, 82, 768, 97, 93, 93, 92, 91]","[1697101635305, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422]"
1574,1574,74,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101597434,1697101601192.0,120,,,"[18, 1137, 236, 97, 89, 87, 84, 818, 101, 99, 101, 91, 88]","[1697101597452, 1697101598589, 1697101598825, 1697101598922, 1697101599011, 1697101599098, 1697101599182, 1697101600000, 1697101600101, 1697101600200, 1697101600301, 1697101600392, 1697101600480]"
1575,1575,541,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623673,1697101626657.0,120,,,"[407, 1614, 114, 98, 69]","[1697101624080, 1697101625694, 1697101625808, 1697101625906, 1697101625975]"
1576,1576,316,11,[],200,llama-13b,128,1,3736.0,1.0,1,H100,1697101617378,1697101621114.0,120,86.0,20.0,"[42, 836, 108, 82, 80, 78, 79, 668, 95, 88, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74]","[1697101617420, 1697101618256, 1697101618364, 1697101618446, 1697101618526, 1697101618604, 1697101618683, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114]"
1577,1577,898,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626668,1697101628905.0,120,,,"[526, 1499]","[1697101627194, 1697101628693]"
1578,1578,320,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628910,1697101634994.0,120,,,"[374, 1450, 81, 68, 66, 865, 92, 90, 90, 89, 87, 84, 83, 743, 95, 71, 94, 93, 91, 72, 613, 75, 71]","[1697101629284, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633616, 1697101633688, 1697101634301, 1697101634376, 1697101634447]"
1579,1579,404,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604271.0,120,,,"[106, 808, 44, 1253, 101, 74]","[1697101601303, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
1580,1580,868,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608940,1697101610267.0,120,,,"[49, 1115]","[1697101608989, 1697101610104]"
1581,1581,905,6,[],200,llama-13b,128,1,1931.0,1.0,1,H100,1697101604276,1697101606207.0,120,11.0,1.0,"[394, 1537]","[1697101604670, 1697101606207]"
1582,1582,332,7,[],200,llama-13b,128,1,1061.0,1.0,1,H100,1697101606209,1697101607270.0,120,39.0,1.0,"[79, 981]","[1697101606288, 1697101607269]"
1583,1583,650,13,[],200,llama-13b,128,1,1811.0,1.0,1,H100,1697101634997,1697101636808.0,120,13.0,1.0,"[181, 1630]","[1697101635178, 1697101636808]"
1584,1584,78,14,[],200,llama-13b,128,1,4309.0,1.0,1,H100,1697101636810,1697101641119.0,120,84.0,20.0,"[18, 651, 189, 91, 92, 87, 87, 82, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618, 81, 93, 735]","[1697101636828, 1697101637479, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641119]"
1585,1585,896,14,[],200,llama-13b,128,1,1513.0,1.0,1,H100,1697101622059,1697101623572.0,120,15.0,1.0,"[31, 1482]","[1697101622090, 1697101623572]"
1586,1586,693,8,[],200,llama-13b,128,1,1157.0,1.0,1,H100,1697101607271,1697101608428.0,120,67.0,2.0,"[29, 1033, 95]","[1697101607300, 1697101608333, 1697101608428]"
1587,1587,811,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604271.0,120,,,"[117, 797, 48, 1249, 101, 74]","[1697101601314, 1697101602111, 1697101602159, 1697101603408, 1697101603509, 1697101603583]"
1588,1588,124,9,[],200,llama-13b,128,1,1021.0,1.0,1,H100,1697101608429,1697101609450.0,120,83.0,2.0,"[6, 1014]","[1697101608435, 1697101609449]"
1589,1589,343,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621920,1697101623660.0,120,,,"[6, 656, 90, 87, 87, 84, 83]","[1697101621926, 1697101622582, 1697101622672, 1697101622759, 1697101622846, 1697101622930, 1697101623013]"
1590,1590,704,10,[],200,llama-13b,128,1,2018.0,1.0,1,H100,1697101623677,1697101625695.0,120,14.0,1.0,"[505, 1513]","[1697101624182, 1697101625695]"
1591,1591,452,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610268.0,120,,,"[65, 588]","[1697101609516, 1697101610104]"
1592,1592,326,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623573,1697101626655.0,120,,,"[6, 295, 733, 1201, 97, 69]","[1697101623579, 1697101623874, 1697101624607, 1697101625808, 1697101625905, 1697101625974]"
1593,1593,809,11,[],200,llama-13b,128,1,1791.0,1.0,1,H100,1697101610278,1697101612069.0,120,16.0,1.0,"[424, 1367]","[1697101610702, 1697101612069]"
1594,1594,240,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612070,1697101616320.0,120,,,"[42, 902, 107, 90, 86, 83, 81, 62, 81, 719, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612112, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
1595,1595,684,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628903.0,120,,,"[144, 750, 42]","[1697101626807, 1697101627557, 1697101627599]"
1596,1596,135,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626656.0,120,,,"[42, 827]","[1697101625739, 1697101626566]"
1597,1597,463,12,[],200,llama-13b,128,1,2029.0,1.0,1,H100,1697101626664,1697101628693.0,120,39.0,1.0,"[349, 1680]","[1697101627013, 1697101628693]"
1598,1598,814,13,[],200,llama-13b,128,1,4992.0,1.0,1,H100,1697101628695,1697101633687.0,120,89.0,20.0,"[71, 651, 1317, 81, 68, 66, 864, 93, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 93, 70]","[1697101628766, 1697101629417, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687]"
1599,1599,201,17,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628908,1697101633687.0,120,67.0,20.0,"[15, 1705, 106, 81, 68, 66, 864, 93, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 93, 70]","[1697101628923, 1697101630628, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687]"
1600,1600,440,15,[],200,llama-13b,128,1,3636.0,1.0,1,H100,1697101641121,1697101644757.0,120,84.0,20.0,"[18, 678, 304, 101, 99, 96, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 87, 84, 83, 83]","[1697101641139, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757]"
1601,1601,852,9,[],200,llama-13b,128,1,4710.0,1.0,1,H100,1697101616330,1697101621040.0,120,100.0,20.0,"[80, 966, 49, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95, 94, 94, 91, 92]","[1697101616410, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620763, 1697101620857, 1697101620948, 1697101621040]"
1602,1602,597,13,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616332,1697101617377.0,120,39.0,1.0,"[191, 853]","[1697101616523, 1697101617376]"
1603,1603,890,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610267.0,120,,,[12],[1697101609463]
1604,1604,25,14,[],200,llama-13b,128,1,1710.0,1.0,1,H100,1697101617378,1697101619088.0,120,12.0,1.0,"[61, 1649]","[1697101617439, 1697101619088]"
1605,1605,356,15,[],200,llama-13b,128,1,1485.0,1.0,1,H100,1697101619090,1697101620575.0,120,874.0,2.0,"[54, 1235, 196]","[1697101619144, 1697101620379, 1697101620575]"
1606,1606,312,9,[],200,llama-13b,128,1,1924.0,1.0,1,H100,1697101616332,1697101618256.0,120,23.0,1.0,"[446, 1478]","[1697101616778, 1697101618256]"
1607,1607,641,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101618257,1697101623658.0,120,,,"[19, 1076, 94, 88, 67, 86, 85, 83, 719, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60, 614, 87, 87, 84, 83]","[1697101618276, 1697101619352, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619855, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1608,1608,282,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621046,1697101623660.0,120,,,"[25, 602, 85, 81, 80, 78, 61, 614, 86, 87, 84, 83]","[1697101621071, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1609,1609,881,15,[],200,llama-13b,128,1,2698.0,1.0,1,H100,1697101634970,1697101637668.0,120,58.0,6.0,"[7, 123, 483, 1314, 87, 55, 629]","[1697101634977, 1697101635100, 1697101635583, 1697101636897, 1697101636984, 1697101637039, 1697101637668]"
1610,1610,846,24,[],200,llama-13b,128,1,3106.0,1.0,1,H100,1697101650754,1697101653860.0,120,140.0,6.0,"[7, 1116, 841, 892, 85, 83, 82]","[1697101650761, 1697101651877, 1697101652718, 1697101653610, 1697101653695, 1697101653778, 1697101653860]"
1611,1611,63,8,[],200,llama-13b,128,1,1786.0,1.0,1,H100,1697101610283,1697101612069.0,120,39.0,1.0,"[597, 1189]","[1697101610880, 1697101612069]"
1612,1612,503,17,[],200,llama-13b,128,1,4308.0,1.0,1,H100,1697101636811,1697101641119.0,120,109.0,20.0,"[81, 587, 189, 91, 93, 87, 85, 83, 81, 768, 98, 92, 93, 92, 91, 91, 79, 616, 83, 93, 735]","[1697101636892, 1697101637479, 1697101637668, 1697101637759, 1697101637852, 1697101637939, 1697101638024, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208, 1697101640291, 1697101640384, 1697101641119]"
1613,1613,419,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612070,1697101616320.0,120,,,"[30, 914, 107, 90, 86, 83, 81, 62, 80, 720, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612100, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
1614,1614,832,11,[],200,llama-13b,128,1,1797.0,1.0,1,H100,1697101610271,1697101612068.0,120,15.0,1.0,"[224, 1573]","[1697101610495, 1697101612068]"
1615,1615,260,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612070,1697101616320.0,120,,,"[12, 932, 107, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612082, 1697101613014, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
1616,1616,271,25,[],200,llama-13b,128,1,4429.0,1.0,1,H100,1697101653861,1697101658290.0,120,87.0,20.0,"[6, 764, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 83, 622, 81, 79, 59, 836]","[1697101653867, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289]"
1617,1617,602,26,[],200,llama-13b,128,1,1082.0,1.0,1,H100,1697101658291,1697101659373.0,120,15.0,1.0,"[54, 1028]","[1697101658345, 1697101659373]"
1618,1618,33,27,[],200,llama-13b,128,1,2066.0,1.0,1,H100,1697101659374,1697101661440.0,120,140.0,7.0,"[7, 1265, 339, 101, 76, 98, 93, 87]","[1697101659381, 1697101660646, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440]"
1619,1619,285,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616319.0,120,,,"[116, 1327]","[1697101614968, 1697101616295]"
1620,1620,646,10,[],200,llama-13b,128,1,1916.0,1.0,1,H100,1697101616340,1697101618256.0,120,14.0,1.0,"[532, 1384]","[1697101616872, 1697101618256]"
1621,1621,73,11,[],200,llama-13b,128,1,831.0,1.0,1,H100,1697101618258,1697101619089.0,120,9.0,1.0,"[48, 782]","[1697101618306, 1697101619088]"
1622,1622,430,12,[],200,llama-13b,128,1,1288.0,1.0,1,H100,1697101619090,1697101620378.0,120,15.0,1.0,"[12, 1276]","[1697101619102, 1697101620378]"
1623,1623,760,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101620380,1697101623659.0,120,,,"[24, 1269, 85, 81, 79, 79, 60, 614, 88, 86, 84, 84]","[1697101620404, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
1624,1624,863,18,[],200,llama-13b,128,1,697.0,1.0,1,H100,1697101641121,1697101641818.0,120,10.0,1.0,"[66, 630]","[1697101641187, 1697101641817]"
1625,1625,291,19,[],200,llama-13b,128,1,4769.0,1.0,1,H100,1697101641818,1697101646587.0,120,79.0,20.0,"[24, 1608, 100, 96, 92, 95, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 77, 75, 741, 98]","[1697101641842, 1697101643450, 1697101643550, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587]"
1626,1626,614,10,[],200,llama-13b,128,1,904.0,1.0,1,H100,1697101623664,1697101624568.0,120,15.0,1.0,"[103, 801]","[1697101623767, 1697101624568]"
1627,1627,50,11,[],200,llama-13b,128,1,1406.0,1.0,1,H100,1697101624569,1697101625975.0,120,90.0,4.0,"[18, 1108, 114, 97, 69]","[1697101624587, 1697101625695, 1697101625809, 1697101625906, 1697101625975]"
1628,1628,649,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646588,1697101651603.0,120,,,"[18, 1136, 339, 97, 101, 101, 96, 88, 485, 96, 96, 83, 942, 106, 104, 101, 91, 85]","[1697101646606, 1697101647742, 1697101648081, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
1629,1629,33,8,[],200,llama-13b,128,1,2846.0,1.0,1,H100,1697101610275,1697101613121.0,120,140.0,7.0,"[425, 1369, 117, 84, 82, 80, 78, 611]","[1697101610700, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613121]"
1630,1630,748,10,[],200,llama-13b,128,1,3523.0,1.0,1,H100,1697101616331,1697101619854.0,120,182.0,14.0,"[142, 952, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82]","[1697101616473, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854]"
1631,1631,394,28,[],200,llama-13b,128,1,1896.0,1.0,1,H100,1697101661441,1697101663337.0,120,11.0,1.0,"[13, 1883]","[1697101661454, 1697101663337]"
1632,1632,752,29,[],200,llama-13b,128,1,1449.0,1.0,1,H100,1697101663338,1697101664787.0,120,39.0,3.0,"[18, 1089, 233, 108]","[1697101663356, 1697101664445, 1697101664678, 1697101664786]"
1633,1633,266,30,[],200,llama-13b,128,1,951.0,1.0,1,H100,1697101664788,1697101665739.0,120,9.0,1.0,"[12, 939]","[1697101664800, 1697101665739]"
1634,1634,620,31,[],200,llama-13b,128,1,2656.0,1.0,1,H100,1697101665740,1697101668396.0,120,100.0,8.0,"[24, 1006, 366, 103, 99, 72, 99, 94, 793]","[1697101665764, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396]"
1635,1635,79,21,[],200,llama-13b,128,1,1053.0,1.0,1,H100,1697101651608,1697101652661.0,120,12.0,1.0,"[8, 1045]","[1697101651616, 1697101652661]"
1636,1636,408,22,[],200,llama-13b,128,1,810.0,1.0,1,H100,1697101652662,1697101653472.0,120,16.0,1.0,"[12, 798]","[1697101652674, 1697101653472]"
1637,1637,883,9,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616332,1697101617377.0,120,563.0,1.0,"[238, 807]","[1697101616570, 1697101617377]"
1638,1638,315,10,[],200,llama-13b,128,1,3662.0,1.0,1,H100,1697101617378,1697101621040.0,120,335.0,14.0,"[54, 1656, 263, 95, 88, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92]","[1697101617432, 1697101619088, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040]"
1639,1639,50,32,[],200,llama-13b,128,1,1365.0,1.0,1,H100,1697101668397,1697101669762.0,120,90.0,4.0,"[6, 1041, 125, 98, 95]","[1697101668403, 1697101669444, 1697101669569, 1697101669667, 1697101669762]"
1640,1640,16,11,[],200,llama-13b,128,1,903.0,1.0,1,H100,1697101623665,1697101624568.0,120,9.0,1.0,"[120, 783]","[1697101623785, 1697101624568]"
1641,1641,407,33,[],200,llama-13b,128,1,722.0,1.0,1,H100,1697101669763,1697101670485.0,120,16.0,1.0,"[6, 716]","[1697101669769, 1697101670485]"
1642,1642,535,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623671,1697101626657.0,120,,,"[320, 1703, 115, 97, 69]","[1697101623991, 1697101625694, 1697101625809, 1697101625906, 1697101625975]"
1643,1643,679,14,[],200,llama-13b,128,1,1719.0,1.0,1,H100,1697101628910,1697101630629.0,120,15.0,1.0,"[386, 1333]","[1697101629296, 1697101630629]"
1644,1644,769,23,[],200,llama-13b,128,1,4810.0,1.0,1,H100,1697101653479,1697101658289.0,120,47.0,20.0,"[47, 1105, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 98, 82, 623, 81, 79, 60, 835]","[1697101653526, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656529, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
1645,1645,115,15,[],200,llama-13b,128,1,1052.0,1.0,1,H100,1697101630632,1697101631684.0,120,13.0,1.0,"[18, 1034]","[1697101630650, 1697101631684]"
1646,1646,901,25,[],200,llama-13b,128,1,697.0,1.0,1,H100,1697101641121,1697101641818.0,120,17.0,1.0,"[72, 625]","[1697101641193, 1697101641818]"
1647,1647,390,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608939,1697101610267.0,120,,,"[32, 1133]","[1697101608971, 1697101610104]"
1648,1648,834,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672997,1697101675812.0,120,,,"[7, 755, 129, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101673004, 1697101673759, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
1649,1649,433,5,[],200,llama-13b,128,1,974.0,1.0,1,H100,1697101604274,1697101605248.0,120,109.0,2.0,"[225, 749]","[1697101604499, 1697101605248]"
1650,1650,835,19,[],200,llama-13b,128,1,4623.0,1.0,1,H100,1697101634970,1697101639593.0,120,87.0,20.0,"[13, 117, 483, 1314, 87, 55, 629, 90, 93, 87, 86, 82, 82, 767, 98, 93, 92, 92, 92, 91, 79]","[1697101634983, 1697101635100, 1697101635583, 1697101636897, 1697101636984, 1697101637039, 1697101637668, 1697101637758, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639330, 1697101639422, 1697101639513, 1697101639592]"
1651,1651,543,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623670,1697101626657.0,120,,,"[302, 1723, 114, 97, 69]","[1697101623972, 1697101625695, 1697101625809, 1697101625906, 1697101625975]"
1652,1652,748,9,[],200,llama-13b,128,1,3332.0,1.0,1,H100,1697101610271,1697101613603.0,120,182.0,14.0,"[131, 690, 1094, 84, 82, 81, 77, 610, 90, 86, 84, 81, 62, 80]","[1697101610402, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603]"
1653,1653,762,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101605249,1697101610266.0,120,,,"[26, 1994, 179, 89, 87, 84, 64, 84, 80, 491, 96, 72, 73, 93, 90, 86, 512, 79, 77]","[1697101605275, 1697101607269, 1697101607448, 1697101607537, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608523, 1697101608595, 1697101608668, 1697101608761, 1697101608851, 1697101608937, 1697101609449, 1697101609528, 1697101609605]"
1654,1654,266,32,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,9.0,1.0,"[511, 1053]","[1697101676327, 1697101677380]"
1655,1655,455,21,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101639055,1697101643833.0,120,91.0,20.0,"[6, 1025, 123, 83, 92, 735, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 800, 100, 95, 93, 95]","[1697101639061, 1697101640086, 1697101640209, 1697101640292, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833]"
1656,1656,623,33,[],200,llama-13b,128,1,1064.0,1.0,1,H100,1697101677381,1697101678445.0,120,140.0,3.0,"[60, 914, 90]","[1697101677441, 1697101678355, 1697101678445]"
1657,1657,463,14,[],200,llama-13b,128,1,1721.0,1.0,1,H100,1697101628908,1697101630629.0,120,39.0,1.0,"[9, 1712]","[1697101628917, 1697101630629]"
1658,1658,177,18,[],200,llama-13b,128,1,1278.0,1.0,1,H100,1697101633691,1697101634969.0,120,14.0,1.0,"[36, 1242]","[1697101633727, 1697101634969]"
1659,1659,530,19,[],200,llama-13b,128,1,548.0,1.0,1,H100,1697101634970,1697101635518.0,120,26.0,1.0,"[45, 503]","[1697101635015, 1697101635518]"
1660,1660,791,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630631,1697101634995.0,120,,,"[12, 1171, 92, 91, 89, 89, 88, 84, 83, 741, 95, 72, 94, 92, 93, 70, 615, 74, 71]","[1697101630643, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633524, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
1661,1661,145,34,[],200,llama-13b,128,1,1574.0,1.0,1,H100,1697101678446,1697101680020.0,120,161.0,9.0,"[18, 824, 117, 99, 98, 94, 72, 92, 90, 70]","[1697101678464, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020]"
1662,1662,409,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604280,1697101610266.0,120,,,"[496, 1431, 238, 81, 76, 74, 772, 90, 86, 84, 65, 84, 79, 492, 95, 71, 74, 93, 90, 87, 510, 79, 78]","[1697101604776, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607857, 1697101607936, 1697101608428, 1697101608523, 1697101608594, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609448, 1697101609527, 1697101609605]"
1663,1663,857,20,[],200,llama-13b,128,1,1960.0,1.0,1,H100,1697101635519,1697101637479.0,120,18.0,1.0,"[99, 1861]","[1697101635618, 1697101637479]"
1664,1664,827,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101632968,1697101634995.0,120,,,"[36, 1202, 96, 74, 71]","[1697101633004, 1697101634206, 1697101634302, 1697101634376, 1697101634447]"
1665,1665,252,19,[],200,llama-13b,128,1,5385.0,1.0,1,H100,1697101634999,1697101640384.0,120,182.0,22.0,"[321, 1488, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 80, 616, 82, 93]","[1697101635320, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209, 1697101640291, 1697101640384]"
1666,1666,488,15,[],200,llama-13b,128,1,1811.0,1.0,1,H100,1697101634997,1697101636808.0,120,6.0,1.0,"[129, 1682]","[1697101635126, 1697101636808]"
1667,1667,286,21,[],200,llama-13b,128,1,2905.0,1.0,1,H100,1697101637480,1697101640385.0,120,161.0,12.0,"[42, 1434, 97, 93, 93, 92, 91, 91, 80, 616, 82, 94]","[1697101637522, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209, 1697101640291, 1697101640385]"
1668,1668,820,16,[],200,llama-13b,128,1,2146.0,1.0,1,H100,1697101636810,1697101638956.0,120,161.0,9.0,"[30, 639, 189, 91, 92, 87, 86, 83, 81, 768]","[1697101636840, 1697101637479, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638107, 1697101638188, 1697101638956]"
1669,1669,253,17,[],200,llama-13b,128,1,4876.0,1.0,1,H100,1697101638957,1697101643833.0,120,67.0,20.0,"[6, 1123, 123, 83, 92, 735, 85, 79, 77, 761, 101, 99, 95, 89, 82, 63, 800, 100, 95, 93, 95]","[1697101638963, 1697101640086, 1697101640209, 1697101640292, 1697101640384, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833]"
1670,1670,647,22,[],200,llama-13b,128,1,4370.0,1.0,1,H100,1697101640386,1697101644756.0,120,83.0,20.0,"[6, 1425, 304, 101, 98, 97, 88, 82, 63, 801, 99, 95, 93, 96, 86, 85, 415, 86, 84, 84, 82]","[1697101640392, 1697101641817, 1697101642121, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643550, 1697101643645, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756]"
1671,1671,498,35,[],200,llama-13b,128,1,459.0,1.0,1,H100,1697101680021,1697101680480.0,120,9.0,1.0,"[7, 452]","[1697101680028, 1697101680480]"
1672,1672,177,20,[],200,llama-13b,128,1,710.0,1.0,1,H100,1697101640292,1697101641002.0,120,14.0,1.0,"[78, 632]","[1697101640370, 1697101641002]"
1673,1673,612,18,[],200,llama-13b,128,1,4546.0,1.0,1,H100,1697101643834,1697101648380.0,120,93.0,20.0,"[21, 566, 86, 84, 83, 83, 683, 78, 78, 77, 75, 742, 97, 98, 83, 78, 1227, 105, 101, 101]","[1697101643855, 1697101644421, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648073, 1697101648178, 1697101648279, 1697101648380]"
1674,1674,538,21,[],200,llama-13b,128,1,3754.0,1.0,1,H100,1697101641003,1697101644757.0,120,89.0,20.0,"[37, 777, 305, 100, 98, 97, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 87, 83, 84, 83]","[1697101641040, 1697101641817, 1697101642122, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644590, 1697101644674, 1697101644757]"
1675,1675,612,20,[],200,llama-13b,128,1,4370.0,1.0,1,H100,1697101640386,1697101644756.0,120,93.0,20.0,"[6, 1729, 101, 98, 97, 88, 82, 63, 801, 99, 95, 93, 96, 86, 85, 415, 86, 84, 84, 82]","[1697101640392, 1697101642121, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643550, 1697101643645, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756]"
1676,1676,217,16,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634997,1697101640208.0,120,85.0,20.0,"[199, 1612, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 79, 616]","[1697101635196, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208]"
1677,1677,866,22,[],200,llama-13b,128,1,5818.0,1.0,1,H100,1697101644759,1697101650577.0,120,93.0,20.0,"[6, 1724, 98, 97, 84, 78, 1237, 95, 101, 100, 97, 88, 485, 97, 96, 82, 942, 106, 103, 102]","[1697101644765, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577]"
1678,1678,156,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628904.0,120,,,"[518, 1511]","[1697101627182, 1697101628693]"
1679,1679,371,19,[],200,llama-13b,128,1,813.0,1.0,1,H100,1697101641004,1697101641817.0,120,13.0,1.0,"[72, 741]","[1697101641076, 1697101641817]"
1680,1680,493,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[192, 1087]","[1697101633882, 1697101634969]"
1681,1681,54,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601198,1697101604271.0,120,,,"[590, 1505, 115, 101, 75]","[1697101601788, 1697101603293, 1697101603408, 1697101603509, 1697101603584]"
1682,1682,97,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101615485,1697101616319.0,120,,,"[6, 804]","[1697101615491, 1697101616295]"
1683,1683,412,5,[],200,llama-13b,128,1,3349.0,1.0,1,H100,1697101604275,1697101607624.0,120,244.0,9.0,"[110, 810, 53, 1197, 81, 76, 74, 772, 89, 86]","[1697101604385, 1697101605195, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623]"
1684,1684,456,12,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101616336,1697101621114.0,120,90.0,20.0,"[459, 1461, 108, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 721, 95, 93, 94, 91, 93, 73]","[1697101616795, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620763, 1697101620857, 1697101620948, 1697101621041, 1697101621114]"
1685,1685,847,15,[],200,llama-13b,128,1,1808.0,1.0,1,H100,1697101635000,1697101636808.0,120,10.0,1.0,"[414, 1394]","[1697101635414, 1697101636808]"
1686,1686,836,2,[],200,llama-13b,128,1,2095.0,1.0,1,H100,1697101601197,1697101603292.0,120,11.0,1.0,"[177, 1918]","[1697101601374, 1697101603292]"
1687,1687,280,16,[],200,llama-13b,128,1,4309.0,1.0,1,H100,1697101636810,1697101641119.0,120,91.0,20.0,"[48, 621, 189, 91, 93, 87, 85, 83, 81, 768, 98, 92, 93, 92, 91, 91, 80, 617, 81, 93, 735]","[1697101636858, 1697101637479, 1697101637668, 1697101637759, 1697101637852, 1697101637939, 1697101638024, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640210, 1697101640291, 1697101640384, 1697101641119]"
1688,1688,264,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101603295,1697101604272.0,120,,,"[54, 845]","[1697101603349, 1697101604194]"
1689,1689,709,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604281,1697101610266.0,120,,,"[524, 1402, 238, 81, 76, 74, 772, 90, 86, 84, 65, 84, 79, 492, 94, 72, 74, 93, 90, 87, 510, 79, 78]","[1697101604805, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607538, 1697101607624, 1697101607708, 1697101607773, 1697101607857, 1697101607936, 1697101608428, 1697101608522, 1697101608594, 1697101608668, 1697101608761, 1697101608851, 1697101608938, 1697101609448, 1697101609527, 1697101609605]"
1690,1690,437,6,[],200,llama-13b,128,1,6516.0,1.0,1,H100,1697101616329,1697101622845.0,120,91.0,29.0,"[74, 973, 49, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95, 94, 94, 91, 92, 73, 644, 81, 80, 78, 61, 614, 87, 87]","[1697101616403, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620763, 1697101620857, 1697101620948, 1697101621040, 1697101621113, 1697101621757, 1697101621838, 1697101621918, 1697101621996, 1697101622057, 1697101622671, 1697101622758, 1697101622845]"
1691,1691,396,9,[],200,llama-13b,128,1,4709.0,1.0,1,H100,1697101616332,1697101621041.0,120,89.0,20.0,"[244, 801, 48, 939, 81, 81, 78, 78, 668, 96, 88, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93]","[1697101616576, 1697101617377, 1697101617425, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619350, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041]"
1692,1692,752,11,[],200,llama-13b,128,1,2120.0,1.0,1,H100,1697101628695,1697101630815.0,120,39.0,3.0,"[36, 686, 1317, 81]","[1697101628731, 1697101629417, 1697101630734, 1697101630815]"
1693,1693,638,17,[],200,llama-13b,128,1,3636.0,1.0,1,H100,1697101641121,1697101644757.0,120,88.0,20.0,"[30, 666, 304, 101, 99, 96, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 87, 84, 83, 83]","[1697101641151, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757]"
1694,1694,152,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612434,1697101616321.0,120,,,"[18, 562, 107, 90, 86, 83, 81, 63, 80, 719, 95, 93, 92, 90, 69, 87, 634, 80, 78, 75]","[1697101612452, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613524, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615563, 1697101615641, 1697101615716]"
1695,1695,924,12,[],200,llama-13b,128,1,1046.0,1.0,1,H100,1697101616331,1697101617377.0,120,9.0,1.0,"[138, 908]","[1697101616469, 1697101617377]"
1696,1696,798,34,[],200,llama-13b,128,1,1804.0,1.0,1,H100,1697101658291,1697101660095.0,120,79.0,6.0,"[24, 1390, 102, 98, 97, 93]","[1697101658315, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095]"
1697,1697,340,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101624569,1697101626655.0,120,,,"[6, 1120, 114, 97, 69]","[1697101624575, 1697101625695, 1697101625809, 1697101625906, 1697101625975]"
1698,1698,532,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607937,1697101610269.0,120,,,"[7, 1414, 91, 79, 77]","[1697101607944, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
1699,1699,12,9,[],200,llama-13b,128,1,1443.0,1.0,1,H100,1697101614852,1697101616295.0,120,11.0,1.0,"[25, 1418]","[1697101614877, 1697101616295]"
1700,1700,677,2,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101604275,1697101605195.0,120,9.0,1.0,"[102, 818]","[1697101604377, 1697101605195]"
1701,1701,102,3,[],200,llama-13b,128,1,4253.0,1.0,1,H100,1697101605196,1697101609449.0,120,84.0,20.0,"[18, 993, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 72, 94, 89, 87, 512]","[1697101605214, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608937, 1697101609449]"
1702,1702,396,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613122,1697101616319.0,120,,,"[18, 997, 186, 95, 93, 92, 90, 69, 88, 633, 79, 78, 76]","[1697101613140, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1703,1703,53,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628905.0,120,,,"[331, 1699]","[1697101626994, 1697101628693]"
1704,1704,407,14,[],200,llama-13b,128,1,1719.0,1.0,1,H100,1697101628910,1697101630629.0,120,16.0,1.0,"[393, 1326]","[1697101629303, 1697101630629]"
1705,1705,767,15,[],200,llama-13b,128,1,1050.0,1.0,1,H100,1697101630634,1697101631684.0,120,11.0,1.0,"[75, 975]","[1697101630709, 1697101631684]"
1706,1706,195,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101631685,1697101634995.0,120,,,"[25, 1256, 206, 95, 71, 94, 93, 92, 70, 614, 75, 71]","[1697101631710, 1697101632966, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
1707,1707,374,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623673,1697101626658.0,120,,,"[437, 1585, 113, 98, 69]","[1697101624110, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
1708,1708,173,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619855,1697101623659.0,120,,,"[13, 1805, 85, 81, 79, 79, 60, 614, 88, 86, 84, 84]","[1697101619868, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
1709,1709,330,26,[],200,llama-13b,128,1,3622.0,1.0,1,H100,1697101641818,1697101645440.0,120,345.0,14.0,"[30, 1283, 320, 99, 96, 92, 95, 87, 85, 415, 86, 84, 84, 82, 684]","[1697101641848, 1697101643131, 1697101643451, 1697101643550, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440]"
1710,1710,734,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628904.0,120,,,"[236, 1792]","[1697101626900, 1697101628692]"
1711,1711,236,11,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616332,1697101617377.0,120,8.0,1.0,"[89, 956]","[1697101616421, 1697101617377]"
1712,1712,708,6,[],200,llama-13b,128,1,1655.0,1.0,1,H100,1697101606678,1697101608333.0,120,140.0,1.0,"[18, 1637]","[1697101606696, 1697101608333]"
1713,1713,595,12,[],200,llama-13b,128,1,1710.0,1.0,1,H100,1697101617378,1697101619088.0,120,8.0,1.0,"[60, 1650]","[1697101617438, 1697101619088]"
1714,1714,166,13,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,14.0,1.0,"[302, 1418]","[1697101629211, 1697101630629]"
1715,1715,111,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608334,1697101610269.0,120,,,"[6, 1018, 91, 79, 77]","[1697101608340, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
1716,1716,846,21,[],200,llama-13b,128,1,1781.0,1.0,1,H100,1697101639423,1697101641204.0,120,140.0,6.0,"[7, 656, 123, 83, 93, 734, 85]","[1697101639430, 1697101640086, 1697101640209, 1697101640292, 1697101640385, 1697101641119, 1697101641204]"
1717,1717,448,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623664,1697101626656.0,120,,,"[37, 867, 39, 1201, 98, 68]","[1697101623701, 1697101624568, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
1718,1718,335,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101616331,1697101623658.0,120,,,"[86, 960, 48, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 73, 644, 81, 80, 78, 61, 614, 87, 87, 84, 83]","[1697101616417, 1697101617377, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621113, 1697101621757, 1697101621838, 1697101621918, 1697101621996, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1719,1719,25,2,[],200,llama-13b,128,1,2094.0,1.0,1,H100,1697101601198,1697101603292.0,120,12.0,1.0,"[483, 1611]","[1697101601681, 1697101603292]"
1720,1720,494,14,[],200,llama-13b,128,1,2540.0,1.0,1,H100,1697101630631,1697101633171.0,120,6.0,10.0,"[31, 1022, 130, 92, 91, 89, 89, 88, 84, 83, 741]","[1697101630662, 1697101631684, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171]"
1721,1721,245,22,[],200,llama-13b,128,1,3551.0,1.0,1,H100,1697101641205,1697101644756.0,120,100.0,20.0,"[6, 607, 303, 101, 99, 96, 88, 83, 63, 799, 100, 95, 93, 95, 87, 85, 415, 86, 85, 83, 82]","[1697101641211, 1697101641818, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642588, 1697101642651, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644591, 1697101644674, 1697101644756]"
1722,1722,780,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628905.0,120,,,"[348, 1681]","[1697101627012, 1697101628693]"
1723,1723,601,23,[],200,llama-13b,128,1,5818.0,1.0,1,H100,1697101644759,1697101650577.0,120,83.0,20.0,"[113, 1427, 190, 98, 98, 83, 78, 1226, 106, 101, 101, 96, 88, 486, 96, 96, 82, 942, 106, 104, 101]","[1697101644872, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577]"
1724,1724,26,24,[],200,llama-13b,128,1,1298.0,1.0,1,H100,1697101650579,1697101651877.0,120,18.0,1.0,"[54, 1244]","[1697101650633, 1697101651877]"
1725,1725,214,11,[],200,llama-13b,128,1,4777.0,1.0,1,H100,1697101628910,1697101633687.0,120,52.0,20.0,"[398, 1321, 105, 81, 68, 66, 865, 92, 90, 90, 89, 87, 84, 83, 743, 94, 72, 93, 94, 91, 71]","[1697101629308, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633172, 1697101633266, 1697101633338, 1697101633431, 1697101633525, 1697101633616, 1697101633687]"
1726,1726,383,3,[],200,llama-13b,128,1,900.0,1.0,1,H100,1697101603294,1697101604194.0,120,15.0,1.0,"[25, 875]","[1697101603319, 1697101604194]"
1727,1727,708,4,[],200,llama-13b,128,1,317.0,1.0,1,H100,1697101604195,1697101604512.0,120,140.0,1.0,"[30, 287]","[1697101604225, 1697101604512]"
1728,1728,139,5,[],200,llama-13b,128,1,5015.0,1.0,1,H100,1697101604513,1697101609528.0,120,39.0,21.0,"[366, 1328, 238, 81, 76, 74, 772, 89, 87, 84, 65, 83, 80, 492, 95, 72, 73, 93, 89, 88, 510, 80]","[1697101604879, 1697101606207, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607624, 1697101607708, 1697101607773, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608761, 1697101608850, 1697101608938, 1697101609448, 1697101609528]"
1729,1729,492,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609529,1697101610268.0,120,,,"[6, 569]","[1697101609535, 1697101610104]"
1730,1730,384,25,[],200,llama-13b,128,1,4733.0,1.0,1,H100,1697101651878,1697101656611.0,120,92.0,20.0,"[307, 1287, 138, 86, 82, 82, 81, 896, 107, 106, 100, 75, 99, 97, 92, 614, 105, 101, 98, 97, 83]","[1697101652185, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611]"
1731,1731,917,11,[],200,llama-13b,128,1,943.0,1.0,1,H100,1697101623665,1697101624608.0,120,123.0,2.0,"[150, 753, 40]","[1697101623815, 1697101624568, 1697101624608]"
1732,1732,718,9,[],200,llama-13b,128,1,1923.0,1.0,1,H100,1697101616333,1697101618256.0,120,13.0,1.0,"[360, 1563]","[1697101616693, 1697101618256]"
1733,1733,625,2,[],200,llama-13b,128,1,2212.0,1.0,1,H100,1697101601196,1697101603408.0,120,364.0,2.0,"[298, 1799, 115]","[1697101601494, 1697101603293, 1697101603408]"
1734,1734,55,3,[],200,llama-13b,128,1,785.0,1.0,1,H100,1697101603409,1697101604194.0,120,12.0,1.0,"[75, 710]","[1697101603484, 1697101604194]"
1735,1735,300,23,[],200,llama-13b,128,1,730.0,1.0,1,H100,1697101650578,1697101651308.0,120,9.0,1.0,"[25, 705]","[1697101650603, 1697101651308]"
1736,1736,320,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101624609,1697101626656.0,120,,,[20],[1697101624629]
1737,1737,905,8,[],200,llama-13b,128,1,944.0,1.0,1,H100,1697101612070,1697101613014.0,120,11.0,1.0,"[65, 879]","[1697101612135, 1697101613014]"
1738,1738,40,21,[],200,llama-13b,128,1,5818.0,1.0,1,H100,1697101644759,1697101650577.0,120,86.0,20.0,"[122, 1418, 190, 98, 98, 83, 78, 1226, 106, 101, 101, 96, 88, 486, 96, 96, 82, 942, 106, 104, 101]","[1697101644881, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577]"
1739,1739,308,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613015,1697101616319.0,120,,,"[12, 1110, 186, 95, 93, 92, 90, 69, 87, 634, 80, 77, 77]","[1697101613027, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615563, 1697101615640, 1697101615717]"
1740,1740,650,11,[],200,llama-13b,128,1,1534.0,1.0,1,H100,1697101621048,1697101622582.0,120,13.0,1.0,"[71, 1462]","[1697101621119, 1697101622581]"
1741,1741,654,24,[],200,llama-13b,128,1,2386.0,1.0,1,H100,1697101651309,1697101653695.0,120,47.0,4.0,"[13, 555, 841, 891, 86]","[1697101651322, 1697101651877, 1697101652718, 1697101653609, 1697101653695]"
1742,1742,75,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622583,1697101623673.0,120,,,"[6, 983]","[1697101622589, 1697101623572]"
1743,1743,759,4,[],200,llama-13b,128,1,4663.0,1.0,1,H100,1697101604275,1697101608938.0,120,92.0,20.0,"[200, 719, 54, 1198, 80, 76, 74, 772, 89, 87, 84, 64, 84, 80, 491, 95, 73, 72, 94, 89, 88]","[1697101604475, 1697101605194, 1697101605248, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607624, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608938]"
1744,1744,667,10,[],200,llama-13b,128,1,4524.0,1.0,1,H100,1697101616333,1697101620857.0,120,364.0,17.0,"[275, 1647, 109, 81, 81, 78, 78, 669, 95, 88, 67, 86, 85, 82, 720, 96, 94, 93]","[1697101616608, 1697101618255, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857]"
1745,1745,84,25,[],200,llama-13b,128,1,934.0,1.0,1,H100,1697101653697,1697101654631.0,120,26.0,1.0,"[12, 922]","[1697101653709, 1697101654631]"
1746,1746,231,11,[],200,llama-13b,128,1,2029.0,1.0,1,H100,1697101623665,1697101625694.0,120,13.0,1.0,"[217, 1812]","[1697101623882, 1697101625694]"
1747,1747,591,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625696,1697101626656.0,120,,,"[49, 821]","[1697101625745, 1697101626566]"
1748,1748,19,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626665,1697101628904.0,120,,,"[505, 1522]","[1697101627170, 1697101628692]"
1749,1749,351,14,[],200,llama-13b,128,1,2905.0,1.0,1,H100,1697101628909,1697101631814.0,120,216.0,6.0,"[296, 1424, 105, 81, 68, 66, 865]","[1697101629205, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814]"
1750,1750,99,11,[],200,llama-13b,128,1,816.0,1.0,1,H100,1697101620858,1697101621674.0,120,10.0,1.0,"[7, 809]","[1697101620865, 1697101621674]"
1751,1751,454,12,[],200,llama-13b,128,1,1338.0,1.0,1,H100,1697101621675,1697101623013.0,120,182.0,6.0,"[36, 871, 90, 87, 86, 85, 83]","[1697101621711, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622930, 1697101623013]"
1752,1752,813,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623014,1697101626655.0,120,,,"[18, 842, 733, 1201, 97, 69]","[1697101623032, 1697101623874, 1697101624607, 1697101625808, 1697101625905, 1697101625974]"
1753,1753,324,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[108, 786, 42]","[1697101626771, 1697101627557, 1697101627599]"
1754,1754,184,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608940,1697101610266.0,120,,,"[13, 1151]","[1697101608953, 1697101610104]"
1755,1755,685,15,[],200,llama-13b,128,1,1825.0,1.0,1,H100,1697101628909,1697101630734.0,120,364.0,2.0,"[190, 1530, 105]","[1697101629099, 1697101630629, 1697101630734]"
1756,1756,400,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623664,1697101626656.0,120,,,"[32, 872, 39, 1201, 98, 68]","[1697101623696, 1697101624568, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
1757,1757,116,16,[],200,llama-13b,128,1,949.0,1.0,1,H100,1697101630735,1697101631684.0,120,23.0,1.0,"[12, 937]","[1697101630747, 1697101631684]"
1758,1758,542,6,[],200,llama-13b,128,1,774.0,1.0,1,H100,1697101610272,1697101611046.0,120,11.0,1.0,"[142, 632]","[1697101610414, 1697101611046]"
1759,1759,715,15,[],200,llama-13b,128,1,1152.0,1.0,1,H100,1697101631815,1697101632967.0,120,20.0,1.0,"[8, 1144]","[1697101631823, 1697101632967]"
1760,1760,473,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101631685,1697101634994.0,120,,,"[18, 1263, 206, 94, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101631703, 1697101632966, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
1761,1761,143,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101632968,1697101634995.0,120,,,"[18, 1220, 95, 75, 71]","[1697101632986, 1697101634206, 1697101634301, 1697101634376, 1697101634447]"
1762,1762,406,5,[],200,llama-13b,128,1,1997.0,1.0,1,H100,1697101610273,1697101612270.0,120,244.0,4.0,"[123, 650, 46, 1094, 84]","[1697101610396, 1697101611046, 1697101611092, 1697101612186, 1697101612270]"
1763,1763,753,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[31, 862, 43]","[1697101626694, 1697101627556, 1697101627599]"
1764,1764,765,6,[],200,llama-13b,128,1,850.0,1.0,1,H100,1697101612271,1697101613121.0,120,84.0,2.0,"[6, 737, 107]","[1697101612277, 1697101613014, 1697101613121]"
1765,1765,496,17,[],200,llama-13b,128,1,3188.0,1.0,1,H100,1697101635000,1697101638188.0,120,335.0,11.0,"[486, 1322, 89, 87, 56, 628, 91, 92, 87, 87, 81, 82]","[1697101635486, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638106, 1697101638188]"
1766,1766,186,13,[],200,llama-13b,128,1,5467.0,1.0,1,H100,1697101628909,1697101634376.0,120,123.0,22.0,"[172, 1548, 105, 81, 68, 66, 864, 93, 91, 89, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70, 614, 75]","[1697101629081, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376]"
1767,1767,852,18,[],200,llama-13b,128,1,5644.0,1.0,1,H100,1697101638189,1697101643833.0,120,100.0,20.0,"[7, 1889, 124, 82, 93, 735, 84, 80, 77, 761, 101, 98, 96, 89, 82, 63, 800, 99, 96, 93, 95]","[1697101638196, 1697101640085, 1697101640209, 1697101640291, 1697101640384, 1697101641119, 1697101641203, 1697101641283, 1697101641360, 1697101642121, 1697101642222, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833]"
1768,1768,200,7,[],200,llama-13b,128,1,2361.0,1.0,1,H100,1697101613122,1697101615483.0,120,6.0,9.0,"[12, 1003, 186, 95, 93, 92, 90, 69, 88, 633]","[1697101613134, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483]"
1769,1769,136,7,[],200,llama-13b,128,1,1791.0,1.0,1,H100,1697101610278,1697101612069.0,120,31.0,1.0,"[494, 1297]","[1697101610772, 1697101612069]"
1770,1770,557,8,[],200,llama-13b,128,1,810.0,1.0,1,H100,1697101615485,1697101616295.0,120,31.0,1.0,"[12, 798]","[1697101615497, 1697101616295]"
1771,1771,792,6,[],200,llama-13b,128,1,1443.0,1.0,1,H100,1697101614852,1697101616295.0,120,11.0,1.0,"[38, 1405]","[1697101614890, 1697101616295]"
1772,1772,695,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621116,1697101623664.0,120,,,"[161, 1304, 91, 87, 86, 84, 84]","[1697101621277, 1697101622581, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
1773,1773,70,11,[],200,llama-13b,128,1,2030.0,1.0,1,H100,1697101623665,1697101625695.0,120,39.0,1.0,"[307, 1723]","[1697101623972, 1697101625695]"
1774,1774,429,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626656.0,120,,,"[48, 820]","[1697101625745, 1697101626565]"
1775,1775,568,15,[],200,llama-13b,128,1,1279.0,1.0,1,H100,1697101633690,1697101634969.0,120,11.0,1.0,"[199, 1080]","[1697101633889, 1697101634969]"
1776,1776,925,16,[],200,llama-13b,128,1,4623.0,1.0,1,H100,1697101634970,1697101639593.0,120,87.0,20.0,"[25, 523, 65, 1314, 87, 55, 629, 91, 92, 87, 86, 82, 82, 767, 98, 93, 92, 93, 91, 91, 69]","[1697101634995, 1697101635518, 1697101635583, 1697101636897, 1697101636984, 1697101637039, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639582]"
1777,1777,129,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623677,1697101626655.0,120,,,"[535, 1483, 113, 98, 69]","[1697101624212, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
1778,1778,787,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628904.0,120,,,"[256, 1772]","[1697101626920, 1697101628692]"
1779,1779,215,14,[],200,llama-13b,128,1,1719.0,1.0,1,H100,1697101628910,1697101630629.0,120,12.0,1.0,"[313, 1406]","[1697101629223, 1697101630629]"
1780,1780,487,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626660,1697101628903.0,120,,,"[11, 928]","[1697101626671, 1697101627599]"
1781,1781,194,17,[],200,llama-13b,128,1,3880.0,1.0,1,H100,1697101637480,1697101641360.0,120,335.0,16.0,"[30, 1229, 217, 97, 93, 93, 92, 91, 91, 80, 616, 82, 93, 735, 85, 78, 78]","[1697101637510, 1697101638739, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360]"
1782,1782,643,18,[],200,llama-13b,128,1,1769.0,1.0,1,H100,1697101641362,1697101643131.0,120,18.0,1.0,"[6, 1763]","[1697101641368, 1697101643131]"
1783,1783,819,15,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,13.0,1.0,"[26, 1694]","[1697101628935, 1697101630629]"
1784,1784,383,13,[],200,llama-13b,128,1,989.0,1.0,1,H100,1697101622583,1697101623572.0,120,15.0,1.0,"[36, 953]","[1697101622619, 1697101623572]"
1785,1785,737,14,[],200,llama-13b,128,1,1034.0,1.0,1,H100,1697101623573,1697101624607.0,120,216.0,2.0,"[13, 288, 733]","[1697101623586, 1697101623874, 1697101624607]"
1786,1786,170,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101624608,1697101626655.0,120,,,"[9, 1948]","[1697101624617, 1697101626565]"
1787,1787,698,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623667,1697101626657.0,120,,,"[245, 1782, 115, 97, 69]","[1697101623912, 1697101625694, 1697101625809, 1697101625906, 1697101625975]"
1788,1788,67,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101643137,1697101651604.0,120,,,"[32, 1152, 99, 87, 83, 84, 83, 683, 79, 77, 78, 74, 741, 98, 97, 84, 78, 1236, 96, 101, 100, 97, 88, 485, 96, 96, 83, 941, 107, 103, 102, 90, 86]","[1697101643169, 1697101644321, 1697101644420, 1697101644507, 1697101644590, 1697101644674, 1697101644757, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
1789,1789,250,16,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101630633,1697101631684.0,120,31.0,1.0,"[35, 1016]","[1697101630668, 1697101631684]"
1790,1790,500,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[225, 1804]","[1697101626888, 1697101628692]"
1791,1791,861,17,[],200,llama-13b,128,1,1721.0,1.0,1,H100,1697101628909,1697101630630.0,120,10.0,1.0,"[184, 1536]","[1697101629093, 1697101630629]"
1792,1792,286,18,[],200,llama-13b,128,1,2704.0,1.0,1,H100,1697101630634,1697101633338.0,120,161.0,12.0,"[70, 1111, 92, 90, 89, 89, 88, 84, 83, 742, 94, 72]","[1697101630704, 1697101631815, 1697101631907, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338]"
1793,1793,604,17,[],200,llama-13b,128,1,1653.0,1.0,1,H100,1697101631685,1697101633338.0,120,161.0,4.0,"[19, 1262, 206, 94, 72]","[1697101631704, 1697101632966, 1697101633172, 1697101633266, 1697101633338]"
1794,1794,407,12,[],200,llama-13b,128,1,898.0,1.0,1,H100,1697101625976,1697101626874.0,120,16.0,1.0,"[18, 880]","[1697101625994, 1697101626874]"
1795,1795,159,12,[],200,llama-13b,128,1,869.0,1.0,1,H100,1697101630816,1697101631685.0,120,31.0,1.0,"[6, 862]","[1697101630822, 1697101631684]"
1796,1796,766,13,[],200,llama-13b,128,1,1818.0,1.0,1,H100,1697101626875,1697101628693.0,120,11.0,1.0,"[404, 1414]","[1697101627279, 1697101628693]"
1797,1797,518,13,[],200,llama-13b,128,1,1281.0,1.0,1,H100,1697101631686,1697101632967.0,120,23.0,1.0,"[35, 1245]","[1697101631721, 1697101632966]"
1798,1798,798,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622846,1697101623664.0,120,,,[6],[1697101622852]
1799,1799,646,19,[],200,llama-13b,128,1,866.0,1.0,1,H100,1697101633340,1697101634206.0,120,14.0,1.0,"[6, 860]","[1697101633346, 1697101634206]"
1800,1800,312,18,[],200,llama-13b,128,1,703.0,1.0,1,H100,1697101632264,1697101632967.0,120,23.0,1.0,"[6, 697]","[1697101632270, 1697101632967]"
1801,1801,229,8,[],200,llama-13b,128,1,2024.0,1.0,1,H100,1697101623671,1697101625695.0,120,15.0,1.0,"[313, 1711]","[1697101623984, 1697101625695]"
1802,1802,229,10,[],200,llama-13b,128,1,1442.0,1.0,1,H100,1697101614853,1697101616295.0,120,15.0,1.0,"[139, 1303]","[1697101614992, 1697101616295]"
1803,1803,74,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101634207,1697101634996.0,120,,,"[19, 743]","[1697101634226, 1697101634969]"
1804,1804,366,18,[],200,llama-13b,128,1,2761.0,1.0,1,H100,1697101634998,1697101637759.0,120,85.0,6.0,"[316, 1494, 89, 87, 56, 628, 91]","[1697101635314, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759]"
1805,1805,876,14,[],200,llama-13b,128,1,1238.0,1.0,1,H100,1697101632968,1697101634206.0,120,11.0,1.0,"[24, 1214]","[1697101632992, 1697101634206]"
1806,1806,301,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101634207,1697101634996.0,120,,,"[7, 755]","[1697101634214, 1697101634969]"
1807,1807,482,12,[],200,llama-13b,128,1,4780.0,1.0,1,H100,1697101616334,1697101621114.0,120,91.0,20.0,"[566, 1356, 108, 81, 80, 79, 78, 669, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 98, 68]","[1697101616900, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618604, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621046, 1697101621114]"
1808,1808,586,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626656.0,120,,,"[54, 814]","[1697101625751, 1697101626565]"
1809,1809,656,16,[],200,llama-13b,128,1,1808.0,1.0,1,H100,1697101635000,1697101636808.0,120,26.0,1.0,"[408, 1400]","[1697101635408, 1697101636808]"
1810,1810,891,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101600305,1697101601192.0,120,,,"[30, 758]","[1697101600335, 1697101601093]"
1811,1811,56,17,[],200,llama-13b,128,1,4309.0,1.0,1,H100,1697101636810,1697101641119.0,120,86.0,20.0,"[58, 611, 189, 91, 93, 87, 85, 83, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618, 81, 93, 735]","[1697101636868, 1697101637479, 1697101637668, 1697101637759, 1697101637852, 1697101637939, 1697101638024, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641119]"
1812,1812,403,21,[],200,llama-13b,128,1,1897.0,1.0,1,H100,1697101635000,1697101636897.0,120,874.0,2.0,"[308, 1500, 89]","[1697101635308, 1697101636808, 1697101636897]"
1813,1813,700,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626662,1697101628904.0,120,,,"[31, 906]","[1697101626693, 1697101627599]"
1814,1814,316,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601194,1697101604270.0,120,,,"[33, 884, 44, 1253, 101, 74]","[1697101601227, 1697101602111, 1697101602155, 1697101603408, 1697101603509, 1697101603583]"
1815,1815,132,14,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628909,1697101633687.0,120,100.0,20.0,"[160, 1560, 105, 81, 68, 66, 865, 92, 90, 90, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70]","[1697101629069, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687]"
1816,1816,895,11,[],200,llama-13b,128,1,2025.0,1.0,1,H100,1697101626668,1697101628693.0,120,15.0,1.0,"[532, 1493]","[1697101627200, 1697101628693]"
1817,1817,461,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610268.0,120,,,[30],[1697101609481]
1818,1818,319,12,[],200,llama-13b,128,1,722.0,1.0,1,H100,1697101628695,1697101629417.0,120,31.0,1.0,"[6, 715]","[1697101628701, 1697101629416]"
1819,1819,528,19,[],200,llama-13b,128,1,3636.0,1.0,1,H100,1697101641121,1697101644757.0,120,52.0,20.0,"[48, 648, 304, 101, 99, 96, 88, 82, 64, 799, 100, 95, 93, 95, 87, 85, 415, 87, 84, 83, 83]","[1697101641169, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642651, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757]"
1820,1820,819,5,[],200,llama-13b,128,1,1792.0,1.0,1,H100,1697101610277,1697101612069.0,120,13.0,1.0,"[419, 1373]","[1697101610696, 1697101612069]"
1821,1821,255,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612071,1697101616321.0,120,,,"[105, 838, 107, 90, 86, 83, 81, 62, 81, 719, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612176, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
1822,1822,674,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101604275,1697101610274.0,120,,,"[20, 899, 54, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101604295, 1697101605194, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667, 1697101608761, 1697101608850, 1697101608937, 1697101609448, 1697101609527, 1697101609604]"
1823,1823,195,24,[],200,llama-13b,128,1,13567.0,1.0,1,H100,1697101658291,1697101671858.0,120,286.0,64.0,"[36, 1046, 332, 102, 98, 97, 93, 85, 805, 101, 76, 98, 93, 87, 831, 107, 102, 94, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 98, 71, 93, 90, 90, 512, 98, 95]","[1697101658327, 1697101659373, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662480, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858]"
1824,1824,691,27,[],200,llama-13b,128,1,858.0,1.0,1,H100,1697101645441,1697101646299.0,120,47.0,1.0,"[12, 846]","[1697101645453, 1697101646299]"
1825,1825,125,28,[],200,llama-13b,128,1,1442.0,1.0,1,H100,1697101646300,1697101647742.0,120,13.0,1.0,"[36, 1406]","[1697101646336, 1697101647742]"
1826,1826,848,17,[],200,llama-13b,128,1,894.0,1.0,1,H100,1697101626663,1697101627557.0,120,47.0,1.0,"[115, 779]","[1697101626778, 1697101627557]"
1827,1827,754,10,[],200,llama-13b,128,1,3019.0,1.0,1,H100,1697101616332,1697101619351.0,120,88.0,7.0,"[378, 1654, 81, 81, 77, 79, 669]","[1697101616710, 1697101618364, 1697101618445, 1697101618526, 1697101618603, 1697101618682, 1697101619351]"
1828,1828,365,18,[],200,llama-13b,128,1,1136.0,1.0,1,H100,1697101627558,1697101628694.0,120,23.0,1.0,"[36, 1099]","[1697101627594, 1697101628693]"
1829,1829,453,29,[],200,llama-13b,128,1,1204.0,1.0,1,H100,1697101647743,1697101648947.0,120,26.0,1.0,"[18, 1186]","[1697101647761, 1697101648947]"
1830,1830,474,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610275,1697101616320.0,120,,,"[337, 1574, 84, 82, 81, 77, 611, 90, 86, 83, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87, 634, 79, 78, 76]","[1697101610612, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1831,1831,850,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633172,1697101634995.0,120,,,"[7, 1027, 96, 74, 71]","[1697101633179, 1697101634206, 1697101634302, 1697101634376, 1697101634447]"
1832,1832,275,16,[],200,llama-13b,128,1,2040.0,1.0,1,H100,1697101635000,1697101637040.0,120,161.0,4.0,"[502, 1307, 88, 87, 56]","[1697101635502, 1697101636809, 1697101636897, 1697101636984, 1697101637040]"
1833,1833,850,7,[],200,llama-13b,128,1,4574.0,1.0,1,H100,1697101610275,1697101614849.0,120,109.0,20.0,"[397, 1397, 117, 84, 82, 81, 77, 611, 90, 86, 82, 82, 63, 79, 720, 95, 93, 92, 90, 69, 87]","[1697101610672, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613297, 1697101613379, 1697101613461, 1697101613524, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
1834,1834,807,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651604.0,120,,,"[25, 938, 355, 106, 104, 101, 91, 85]","[1697101648973, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
1835,1835,832,9,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616332,1697101617377.0,120,15.0,1.0,"[256, 789]","[1697101616588, 1697101617377]"
1836,1836,77,23,[],200,llama-13b,128,1,5819.0,1.0,1,H100,1697101644758,1697101650577.0,120,92.0,20.0,"[32, 1509, 190, 98, 98, 83, 78, 1226, 106, 101, 100, 97, 88, 486, 96, 96, 82, 942, 106, 103, 102]","[1697101644790, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577]"
1837,1837,234,31,[],200,llama-13b,128,1,6679.0,1.0,1,H100,1697101651610,1697101658289.0,120,457.0,25.0,"[387, 1475, 138, 85, 83, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 614, 105, 101, 98, 97, 83, 623, 81, 79, 59, 836]","[1697101651997, 1697101653472, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289]"
1838,1838,878,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623666,1697101626657.0,120,,,"[123, 779, 40, 1201, 97, 68]","[1697101623789, 1697101624568, 1697101624608, 1697101625809, 1697101625906, 1697101625974]"
1839,1839,405,4,[],200,llama-13b,128,1,4742.0,1.0,1,H100,1697101604195,1697101608937.0,120,87.0,20.0,"[30, 287, 736, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73, 94, 89, 87]","[1697101604225, 1697101604512, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667, 1697101608761, 1697101608850, 1697101608937]"
1840,1840,36,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648381,1697101651604.0,120,,,"[30, 536, 103, 96, 96, 83, 941, 106, 103, 102, 91, 85]","[1697101648411, 1697101648947, 1697101649050, 1697101649146, 1697101649242, 1697101649325, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
1841,1841,213,6,[],200,llama-13b,128,1,1011.0,1.0,1,H100,1697101621047,1697101622058.0,120,123.0,6.0,"[36, 590, 85, 81, 80, 78, 61]","[1697101621083, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058]"
1842,1842,243,4,[],200,llama-13b,128,1,2251.0,1.0,1,H100,1697101604275,1697101606526.0,120,67.0,4.0,"[32, 887, 54, 1197, 81]","[1697101604307, 1697101605194, 1697101605248, 1697101606445, 1697101606526]"
1843,1843,62,14,[],200,llama-13b,128,1,5212.0,1.0,1,H100,1697101634997,1697101640209.0,120,91.0,20.0,"[187, 1624, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 79, 616]","[1697101635184, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208]"
1844,1844,397,18,[],200,llama-13b,128,1,1900.0,1.0,1,H100,1697101634997,1697101636897.0,120,67.0,2.0,"[114, 1696, 90]","[1697101635111, 1697101636807, 1697101636897]"
1845,1845,753,19,[],200,llama-13b,128,1,4220.0,1.0,1,H100,1697101636898,1697101641118.0,120,83.0,20.0,"[20, 561, 190, 90, 93, 87, 86, 81, 82, 768, 98, 92, 93, 92, 91, 91, 79, 617, 82, 93, 734]","[1697101636918, 1697101637479, 1697101637669, 1697101637759, 1697101637852, 1697101637939, 1697101638025, 1697101638106, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640384, 1697101641118]"
1846,1846,181,20,[],200,llama-13b,128,1,8120.0,1.0,1,H100,1697101641121,1697101649241.0,120,91.0,39.0,"[6, 690, 304, 101, 99, 96, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 87, 83, 84, 83, 683, 78, 78, 77, 75, 741, 98, 97, 84, 78, 1226, 106, 101, 100, 97, 88, 485, 96, 96]","[1697101641127, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644590, 1697101644674, 1697101644757, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241]"
1847,1847,282,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101634377,1697101634996.0,120,,,"[6, 586]","[1697101634383, 1697101634969]"
1848,1848,750,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621048,1697101623660.0,120,,,"[64, 561, 85, 81, 80, 78, 61, 614, 86, 87, 84, 83]","[1697101621112, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1849,1849,236,21,[],200,llama-13b,128,1,1053.0,1.0,1,H100,1697101651608,1697101652661.0,120,8.0,1.0,"[182, 871]","[1697101651790, 1697101652661]"
1850,1850,594,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101652662,1697101675812.0,120,,,"[60, 1909, 206, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83, 623, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 844, 109, 103, 96, 94, 91, 697, 106, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 94, 793, 102, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 98, 71, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101652722, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664677, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665170, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667602, 1697101668395, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
1851,1851,807,18,[],200,llama-13b,128,1,4596.0,1.0,1,H100,1697101634997,1697101639593.0,120,90.0,20.0,"[78, 443, 66, 1313, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 92, 93, 91, 91, 79]","[1697101635075, 1697101635518, 1697101635584, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639592]"
1852,1852,643,20,[],200,llama-13b,128,1,1809.0,1.0,1,H100,1697101635000,1697101636809.0,120,18.0,1.0,"[498, 1311]","[1697101635498, 1697101636809]"
1853,1853,312,10,[],200,llama-13b,128,1,1443.0,1.0,1,H100,1697101614852,1697101616295.0,120,23.0,1.0,"[56, 1386]","[1697101614908, 1697101616294]"
1854,1854,233,19,[],200,llama-13b,128,1,1407.0,1.0,1,H100,1697101639595,1697101641002.0,120,6.0,1.0,"[28, 1379]","[1697101639623, 1697101641002]"
1855,1855,620,13,[],200,llama-13b,128,1,3019.0,1.0,1,H100,1697101616332,1697101619351.0,120,100.0,8.0,"[73, 971, 49, 939, 81, 80, 78, 79, 669]","[1697101616405, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351]"
1856,1856,33,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633339,1697101634995.0,120,,,"[7, 860, 96, 74, 71]","[1697101633346, 1697101634206, 1697101634302, 1697101634376, 1697101634447]"
1857,1857,590,20,[],200,llama-13b,128,1,3754.0,1.0,1,H100,1697101641003,1697101644757.0,120,88.0,20.0,"[31, 783, 305, 100, 98, 97, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 87, 83, 84, 82]","[1697101641034, 1697101641817, 1697101642122, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644590, 1697101644674, 1697101644756]"
1858,1858,729,19,[],200,llama-13b,128,1,1197.0,1.0,1,H100,1697101637760,1697101638957.0,120,874.0,2.0,"[7, 1189]","[1697101637767, 1697101638956]"
1859,1859,762,22,[],200,llama-13b,128,1,4220.0,1.0,1,H100,1697101636898,1697101641118.0,120,92.0,20.0,"[14, 567, 190, 90, 93, 87, 85, 82, 82, 768, 98, 92, 93, 92, 91, 91, 79, 617, 82, 93, 734]","[1697101636912, 1697101637479, 1697101637669, 1697101637759, 1697101637852, 1697101637939, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640384, 1697101641118]"
1860,1860,154,20,[],200,llama-13b,128,1,1128.0,1.0,1,H100,1697101638958,1697101640086.0,120,13.0,1.0,"[17, 1111]","[1697101638975, 1697101640086]"
1861,1861,512,21,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101640087,1697101641002.0,120,11.0,1.0,"[18, 897]","[1697101640105, 1697101641002]"
1862,1862,871,22,[],200,llama-13b,128,1,1502.0,1.0,1,H100,1697101641003,1697101642505.0,120,123.0,6.0,"[19, 795, 305, 100, 98, 97, 88]","[1697101641022, 1697101641817, 1697101642122, 1697101642222, 1697101642320, 1697101642417, 1697101642505]"
1863,1863,531,17,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634997,1697101640208.0,120,52.0,20.0,"[211, 1600, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 768, 97, 93, 93, 92, 91, 91, 80, 615]","[1697101635208, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640208]"
1864,1864,532,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623666,1697101626657.0,120,,,"[211, 1817, 115, 97, 69]","[1697101623877, 1697101625694, 1697101625809, 1697101625906, 1697101625975]"
1865,1865,271,23,[],200,llama-13b,128,1,4081.0,1.0,1,H100,1697101642506,1697101646587.0,120,87.0,20.0,"[13, 612, 320, 99, 96, 93, 94, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 78, 74, 741, 98]","[1697101642519, 1697101643131, 1697101643451, 1697101643550, 1697101643646, 1697101643739, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587]"
1866,1866,893,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[239, 1790]","[1697101626902, 1697101628692]"
1867,1867,326,14,[],200,llama-13b,128,1,3438.0,1.0,1,H100,1697101628909,1697101632347.0,120,345.0,12.0,"[214, 1507, 104, 81, 68, 66, 864, 93, 91, 89, 89, 88, 84]","[1697101629123, 1697101630630, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347]"
1868,1868,142,28,[],200,llama-13b,128,1,5765.0,1.0,1,H100,1697101656613,1697101662378.0,120,52.0,20.0,"[59, 1488, 129, 96, 82, 82, 80, 1076, 101, 99, 98, 92, 85, 805, 102, 75, 104, 87, 87, 831, 107]","[1697101656672, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660003, 1697101660095, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378]"
1869,1869,890,18,[],200,llama-13b,128,1,4207.0,1.0,1,H100,1697101640212,1697101644419.0,120,93.0,20.0,"[70, 838, 84, 79, 77, 761, 100, 99, 96, 89, 82, 63, 801, 98, 97, 92, 96, 86, 85, 414]","[1697101640282, 1697101641120, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643549, 1697101643646, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644419]"
1870,1870,155,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619352,1697101623658.0,120,,,"[7, 1019, 197, 95, 94, 93, 92, 91, 74, 644, 81, 79, 78, 61, 615, 86, 87, 84, 83]","[1697101619359, 1697101620378, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620949, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621996, 1697101622057, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1871,1871,489,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[43, 1236]","[1697101633733, 1697101634969]"
1872,1872,250,9,[],200,llama-13b,128,1,1164.0,1.0,1,H100,1697101608940,1697101610104.0,120,31.0,1.0,"[19, 1145]","[1697101608959, 1697101610104]"
1873,1873,152,10,[],200,llama-13b,128,1,3800.0,1.0,1,H100,1697101618257,1697101622057.0,120,87.0,20.0,"[25, 806, 264, 94, 88, 67, 86, 85, 83, 719, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60]","[1697101618282, 1697101619088, 1697101619352, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619855, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
1874,1874,335,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619688,1697101623658.0,120,,,"[7, 684, 196, 95, 94, 93, 92, 92, 73, 644, 81, 79, 78, 61, 615, 87, 86, 84, 83]","[1697101619695, 1697101620379, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620949, 1697101621041, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621996, 1697101622057, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
1875,1875,766,6,[],200,llama-13b,128,1,775.0,1.0,1,H100,1697101610271,1697101611046.0,120,11.0,1.0,"[39, 736]","[1697101610310, 1697101611046]"
1876,1876,901,15,[],200,llama-13b,128,1,2025.0,1.0,1,H100,1697101626667,1697101628692.0,120,17.0,1.0,"[429, 1596]","[1697101627096, 1697101628692]"
1877,1877,307,17,[],200,llama-13b,128,1,2029.0,1.0,1,H100,1697101626664,1697101628693.0,120,26.0,1.0,"[426, 1603]","[1697101627090, 1697101628693]"
1878,1878,665,18,[],200,llama-13b,128,1,4992.0,1.0,1,H100,1697101628695,1697101633687.0,120,90.0,20.0,"[42, 679, 1318, 81, 68, 66, 864, 92, 91, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 92, 70]","[1697101628737, 1697101629416, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631905, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633686]"
1879,1879,318,19,[],200,llama-13b,128,1,1328.0,1.0,1,H100,1697101644421,1697101645749.0,120,6.0,6.0,"[7, 909, 103, 79, 77, 77, 75]","[1697101644428, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748]"
1880,1880,674,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101645750,1697101651605.0,120,,,"[12, 1980, 340, 97, 100, 101, 96, 88, 485, 96, 96, 83, 942, 106, 104, 102, 90, 85]","[1697101645762, 1697101647742, 1697101648082, 1697101648179, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650578, 1697101650668, 1697101650753]"
1881,1881,329,16,[],200,llama-13b,128,1,722.0,1.0,1,H100,1697101628695,1697101629417.0,120,15.0,1.0,"[24, 698]","[1697101628719, 1697101629417]"
1882,1882,659,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101629419,1697101634995.0,120,,,"[23, 2242, 130, 92, 90, 90, 89, 88, 84, 83, 741, 95, 72, 93, 93, 93, 70, 614, 75, 71]","[1697101629442, 1697101631684, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
1883,1883,85,18,[],200,llama-13b,128,1,5210.0,1.0,1,H100,1697101634999,1697101640209.0,120,88.0,20.0,"[388, 1421, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 80, 616]","[1697101635387, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209]"
1884,1884,661,15,[],200,llama-13b,128,1,2541.0,1.0,1,H100,1697101630631,1697101633172.0,120,161.0,10.0,"[49, 1004, 130, 93, 90, 89, 89, 88, 84, 83, 741]","[1697101630680, 1697101631684, 1697101631814, 1697101631907, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171]"
1885,1885,481,11,[],200,llama-13b,128,1,1513.0,1.0,1,H100,1697101622059,1697101623572.0,120,10.0,1.0,"[61, 1452]","[1697101622120, 1697101623572]"
1886,1886,842,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623573,1697101626656.0,120,,,"[36, 265, 733, 1201, 98, 68]","[1697101623609, 1697101623874, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
1887,1887,139,5,[],200,llama-13b,128,1,4578.0,1.0,1,H100,1697101610271,1697101614849.0,120,39.0,21.0,"[149, 626, 46, 1094, 84, 82, 81, 77, 610, 90, 86, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 87]","[1697101610420, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
1888,1888,268,13,[],200,llama-13b,128,1,893.0,1.0,1,H100,1697101626664,1697101627557.0,120,19.0,1.0,"[136, 757]","[1697101626800, 1697101627557]"
1889,1889,90,16,[],200,llama-13b,128,1,1033.0,1.0,1,H100,1697101633173,1697101634206.0,120,19.0,1.0,"[12, 1021]","[1697101633185, 1697101634206]"
1890,1890,787,13,[],200,llama-13b,128,1,1897.0,1.0,1,H100,1697101621116,1697101623013.0,120,123.0,6.0,"[79, 1387, 90, 87, 86, 84, 84]","[1697101621195, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
1891,1891,449,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101634207,1697101634996.0,120,,,"[13, 749]","[1697101634220, 1697101634969]"
1892,1892,807,18,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101635000,1697101640209.0,120,90.0,20.0,"[468, 1340, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 94, 91, 93, 91, 91, 78, 618]","[1697101635468, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639591, 1697101640209]"
1893,1893,432,4,[],200,llama-13b,128,1,2097.0,1.0,1,H100,1697101601196,1697101603293.0,120,13.0,1.0,"[222, 1875]","[1697101601418, 1697101603293]"
1894,1894,763,5,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101603295,1697101604194.0,120,20.0,1.0,"[95, 804]","[1697101603390, 1697101604194]"
1895,1895,438,26,[],200,llama-13b,128,1,1365.0,1.0,1,H100,1697101654633,1697101655998.0,120,9.0,1.0,"[41, 1324]","[1697101654674, 1697101655998]"
1896,1896,98,9,[],200,llama-13b,128,1,626.0,1.0,1,H100,1697101621048,1697101621674.0,120,14.0,1.0,"[59, 566]","[1697101621107, 1697101621673]"
1897,1897,866,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.88 GiB is free. Process 1607256 has 75.21 GiB memory in use. Of the allocated memory 45.11 GiB is allocated by PyTorch, and 29.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101598827,1697101601191.0,120,,,"[42, 1131, 100, 100, 101, 91, 89]","[1697101598869, 1697101600000, 1697101600100, 1697101600200, 1697101600301, 1697101600392, 1697101600481]"
1898,1898,899,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608940,1697101610266.0,120,,,"[24, 1140]","[1697101608964, 1697101610104]"
1899,1899,743,8,[],200,llama-13b,128,1,2232.0,1.0,1,H100,1697101610278,1697101612510.0,120,123.0,6.0,"[502, 1288, 118, 84, 82, 80, 78]","[1697101610780, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510]"
1900,1900,451,10,[],200,llama-13b,128,1,908.0,1.0,1,H100,1697101621674,1697101622582.0,120,286.0,1.0,"[19, 889]","[1697101621693, 1697101622582]"
1901,1901,329,4,[],200,llama-13b,128,1,774.0,1.0,1,H100,1697101610272,1697101611046.0,120,15.0,1.0,"[14, 760]","[1697101610286, 1697101611046]"
1902,1902,683,5,[],200,llama-13b,128,1,1140.0,1.0,1,H100,1697101611046,1697101612186.0,120,874.0,2.0,"[7, 1133]","[1697101611053, 1697101612186]"
1903,1903,191,6,[],200,llama-13b,128,1,4742.0,1.0,1,H100,1697101604195,1697101608937.0,120,85.0,20.0,"[48, 269, 736, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73, 94, 89, 87]","[1697101604243, 1697101604512, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667, 1697101608761, 1697101608850, 1697101608937]"
1904,1904,803,16,[],200,llama-13b,128,1,1541.0,1.0,1,H100,1697101644758,1697101646299.0,120,20.0,1.0,"[26, 1515]","[1697101644784, 1697101646299]"
1905,1905,202,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646300,1697101651605.0,120,,,"[24, 1418, 341, 95, 101, 101, 96, 88, 485, 96, 96, 83, 942, 106, 104, 102, 90, 85]","[1697101646324, 1697101647742, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650578, 1697101650668, 1697101650753]"
1906,1906,212,22,[],200,llama-13b,128,1,566.0,1.0,1,H100,1697101648381,1697101648947.0,120,31.0,1.0,"[24, 542]","[1697101648405, 1697101648947]"
1907,1907,781,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622583,1697101623664.0,120,,,"[6, 983]","[1697101622589, 1697101623572]"
1908,1908,543,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648950,1697101651605.0,120,,,"[65, 896, 355, 106, 104, 101, 91, 85]","[1697101649015, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
1909,1909,108,6,[],200,llama-13b,128,1,934.0,1.0,1,H100,1697101612187,1697101613121.0,120,182.0,2.0,"[7, 820, 107]","[1697101612194, 1697101613014, 1697101613121]"
1910,1910,897,24,[],200,llama-13b,128,1,1862.0,1.0,1,H100,1697101651610,1697101653472.0,120,9.0,1.0,"[382, 1480]","[1697101651992, 1697101653472]"
1911,1911,331,25,[],200,llama-13b,128,1,1152.0,1.0,1,H100,1697101653479,1697101654631.0,120,26.0,1.0,"[24, 1128]","[1697101653503, 1697101654631]"
1912,1912,688,26,[],200,llama-13b,128,1,1701.0,1.0,1,H100,1697101654632,1697101656333.0,120,345.0,4.0,"[6, 1360, 128, 106, 101]","[1697101654638, 1697101655998, 1697101656126, 1697101656232, 1697101656333]"
1913,1913,120,27,[],200,llama-13b,128,1,813.0,1.0,1,H100,1697101656334,1697101657147.0,120,17.0,1.0,"[6, 807]","[1697101656340, 1697101657147]"
1914,1914,446,28,[],200,llama-13b,128,1,1009.0,1.0,1,H100,1697101657151,1697101658160.0,120,26.0,1.0,"[47, 962]","[1697101657198, 1697101658160]"
1915,1915,804,29,[],200,llama-13b,128,1,1212.0,1.0,1,H100,1697101658161,1697101659373.0,120,20.0,1.0,"[54, 1158]","[1697101658215, 1697101659373]"
1916,1916,214,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623672,1697101626658.0,120,,,"[433, 1590, 113, 98, 69]","[1697101624105, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
1917,1917,229,30,[],200,llama-13b,128,1,1272.0,1.0,1,H100,1697101659375,1697101660647.0,120,15.0,1.0,"[36, 1236]","[1697101659411, 1697101660647]"
1918,1918,573,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626670,1697101628905.0,120,,,[541],[1697101627211]
1919,1919,6,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628910,1697101634994.0,120,,,"[357, 1467, 81, 68, 66, 865, 92, 91, 89, 89, 88, 84, 83, 742, 94, 72, 94, 93, 92, 71, 613, 75, 71]","[1697101629267, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688, 1697101634301, 1697101634376, 1697101634447]"
1920,1920,438,7,[],200,llama-13b,128,1,1015.0,1.0,1,H100,1697101613122,1697101614137.0,120,9.0,1.0,"[18, 997]","[1697101613140, 1697101614137]"
1921,1921,796,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614138,1697101616320.0,120,,,"[6, 1256, 84, 78, 79, 76]","[1697101614144, 1697101615400, 1697101615484, 1697101615562, 1697101615641, 1697101615717]"
1922,1922,232,9,[],200,llama-13b,128,1,4710.0,1.0,1,H100,1697101616331,1697101621041.0,120,93.0,20.0,"[180, 914, 939, 81, 80, 79, 78, 669, 94, 89, 67, 86, 85, 82, 720, 96, 94, 93, 91, 92]","[1697101616511, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618604, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040]"
1923,1923,548,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608940,1697101610267.0,120,,,"[41, 1123]","[1697101608981, 1697101610104]"
1924,1924,908,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610272,1697101616320.0,120,,,"[50, 724, 46, 1094, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 69, 87, 634, 79, 78, 76]","[1697101610322, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1925,1925,589,31,[],200,llama-13b,128,1,5325.0,1.0,1,H100,1697101660648,1697101665973.0,120,92.0,20.0,"[24, 1244, 355, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 109, 102, 96, 94, 101, 688, 105]","[1697101660672, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973]"
1926,1926,21,32,[],200,llama-13b,128,1,796.0,1.0,1,H100,1697101665974,1697101666770.0,120,15.0,1.0,"[25, 771]","[1697101665999, 1697101666770]"
1927,1927,469,33,[],200,llama-13b,128,1,1492.0,1.0,1,H100,1697101666771,1697101668263.0,120,17.0,1.0,"[18, 1474]","[1697101666789, 1697101668263]"
1928,1928,168,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612513,1697101616319.0,120,,,"[29, 1595, 186, 95, 93, 92, 90, 69, 87, 634, 80, 77, 77]","[1697101612542, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615563, 1697101615640, 1697101615717]"
1929,1929,828,34,[],200,llama-13b,128,1,1683.0,1.0,1,H100,1697101668264,1697101669947.0,120,182.0,6.0,"[6, 1299, 98, 94, 93, 93]","[1697101668270, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669947]"
1930,1930,298,3,[],200,llama-13b,128,1,2098.0,1.0,1,H100,1697101601194,1697101603292.0,120,17.0,1.0,"[198, 1900]","[1697101601392, 1697101603292]"
1931,1931,9,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614851,1697101616322.0,120,,,"[26, 1418]","[1697101614877, 1697101616295]"
1932,1932,657,4,[],200,llama-13b,128,1,899.0,1.0,1,H100,1697101603295,1697101604194.0,120,10.0,1.0,"[42, 857]","[1697101603337, 1697101604194]"
1933,1933,373,12,[],200,llama-13b,128,1,1052.0,1.0,1,H100,1697101616324,1697101617376.0,120,15.0,1.0,"[45, 1007]","[1697101616369, 1697101617376]"
1934,1934,359,15,[],200,llama-13b,128,1,1811.0,1.0,1,H100,1697101634997,1697101636808.0,120,10.0,1.0,"[180, 1631]","[1697101635177, 1697101636808]"
1935,1935,690,16,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101636810,1697101637479.0,120,39.0,1.0,"[30, 639]","[1697101636840, 1697101637479]"
1936,1936,118,17,[],200,llama-13b,128,1,4937.0,1.0,1,H100,1697101637480,1697101642417.0,120,85.0,20.0,"[24, 1235, 217, 97, 93, 93, 92, 91, 91, 79, 617, 82, 93, 735, 85, 78, 78, 760, 102, 98, 96]","[1697101637504, 1697101638739, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642222, 1697101642320, 1697101642416]"
1937,1937,731,13,[],200,llama-13b,128,1,3736.0,1.0,1,H100,1697101617378,1697101621114.0,120,89.0,20.0,"[18, 860, 108, 82, 80, 78, 79, 668, 94, 89, 67, 86, 85, 82, 721, 94, 95, 93, 91, 92, 74]","[1697101617396, 1697101618256, 1697101618364, 1697101618446, 1697101618526, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114]"
1938,1938,257,35,[],200,llama-13b,128,1,537.0,1.0,1,H100,1697101669948,1697101670485.0,120,14.0,1.0,"[18, 519]","[1697101669966, 1697101670485]"
1939,1939,591,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621048,1697101623660.0,120,,,"[53, 572, 85, 81, 80, 78, 61, 614, 86, 87, 84, 83]","[1697101621101, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1940,1940,614,10,[],200,llama-13b,128,1,1043.0,1.0,1,H100,1697101616333,1697101617376.0,120,15.0,1.0,"[175, 868]","[1697101616508, 1697101617376]"
1941,1941,227,35,[],200,llama-13b,128,1,5772.0,1.0,1,H100,1697101660096,1697101665868.0,120,364.0,25.0,"[18, 533, 338, 102, 75, 104, 87, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 89, 86, 845, 109, 102, 96, 94, 101, 688]","[1697101660114, 1697101660647, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868]"
1942,1942,612,36,[],200,llama-13b,128,1,4495.0,1.0,1,H100,1697101670486,1697101674981.0,120,93.0,20.0,"[24, 1156, 97, 95, 94, 93, 91, 520, 113, 92, 68, 67, 88, 805, 97, 89, 88, 66, 65, 687]","[1697101670510, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673889, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981]"
1943,1943,832,11,[],200,llama-13b,128,1,902.0,1.0,1,H100,1697101623666,1697101624568.0,120,15.0,1.0,"[159, 743]","[1697101623825, 1697101624568]"
1944,1944,535,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649243,1697101651603.0,120,,,"[6, 662, 355, 106, 104, 101, 91, 85]","[1697101649249, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
1945,1945,264,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101624569,1697101626655.0,120,,,"[36, 1090, 114, 97, 69]","[1697101624605, 1697101625695, 1697101625809, 1697101625906, 1697101625975]"
1946,1946,623,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,[233],[1697101626896]
1947,1947,26,14,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,18.0,1.0,"[296, 1424]","[1697101629205, 1697101630629]"
1948,1948,865,22,[],200,llama-13b,128,1,1054.0,1.0,1,H100,1697101651607,1697101652661.0,120,9.0,1.0,"[83, 971]","[1697101651690, 1697101652661]"
1949,1949,37,37,[],200,llama-13b,128,1,807.0,1.0,1,H100,1697101674982,1697101675789.0,120,20.0,1.0,"[31, 776]","[1697101675013, 1697101675789]"
1950,1950,365,38,[],200,llama-13b,128,1,590.0,1.0,1,H100,1697101675790,1697101676380.0,120,23.0,1.0,"[30, 560]","[1697101675820, 1697101676380]"
1951,1951,729,39,[],200,llama-13b,128,1,1096.0,1.0,1,H100,1697101676381,1697101677477.0,120,874.0,2.0,"[31, 1065]","[1697101676412, 1697101677477]"
1952,1952,161,40,[],200,llama-13b,128,1,1281.0,1.0,1,H100,1697101677478,1697101678759.0,120,109.0,7.0,"[25, 852, 90, 85, 65, 83, 81]","[1697101677503, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759]"
1953,1953,519,41,[],200,llama-13b,128,1,10233.0,1.0,1,H100,1697101678760,1697101688993.0,120,58.0,47.0,"[12, 1707, 92, 103, 57, 73, 804, 84, 83, 79, 78, 772, 96, 93, 92, 69, 90, 90, 615, 101, 101, 97, 92, 90, 90, 593, 87, 63, 83, 80, 911, 91, 89, 87, 86, 647, 101, 100, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101678772, 1697101680479, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681932, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
1954,1954,297,23,[],200,llama-13b,128,1,15734.0,1.0,1,H100,1697101652662,1697101668396.0,120,563.0,72.0,"[48, 767, 133, 86, 83, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83, 623, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 108, 103, 96, 94, 91, 697, 106, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 94, 793]","[1697101652710, 1697101653477, 1697101653610, 1697101653696, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665170, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667602, 1697101668395]"
1955,1955,380,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630632,1697101634995.0,120,,,"[29, 1153, 92, 91, 89, 89, 88, 84, 83, 741, 95, 72, 94, 93, 92, 70, 615, 74, 71]","[1697101630661, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
1956,1956,161,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621116,1697101623673.0,120,,,"[74, 1482, 87, 86, 84, 83]","[1697101621190, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
1957,1957,515,15,[],200,llama-13b,128,1,2018.0,1.0,1,H100,1697101623678,1697101625696.0,120,11.0,1.0,"[546, 1471]","[1697101624224, 1697101625695]"
1958,1958,28,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625698,1697101626657.0,120,,,"[107, 761]","[1697101625805, 1697101626566]"
1959,1959,387,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626665,1697101628905.0,120,,,"[430, 1598]","[1697101627095, 1697101628693]"
1960,1960,748,18,[],200,llama-13b,128,1,4261.0,1.0,1,H100,1697101628910,1697101633171.0,120,182.0,14.0,"[422, 1402, 81, 68, 66, 865, 92, 90, 90, 89, 87, 85, 83, 741]","[1697101629332, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632347, 1697101632430, 1697101633171]"
1961,1961,187,14,[],200,llama-13b,128,1,3118.0,1.0,1,H100,1697101628695,1697101631813.0,120,161.0,6.0,"[42, 680, 1317, 81, 68, 66, 864]","[1697101628737, 1697101629417, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813]"
1962,1962,772,34,[],200,llama-13b,128,1,4495.0,1.0,1,H100,1697101670486,1697101674981.0,120,83.0,20.0,"[18, 1055, 107, 97, 95, 94, 93, 90, 521, 113, 92, 68, 67, 88, 805, 97, 89, 88, 66, 65, 687]","[1697101670504, 1697101671559, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673889, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981]"
1963,1963,518,15,[],200,llama-13b,128,1,1151.0,1.0,1,H100,1697101631815,1697101632966.0,120,23.0,1.0,"[15, 1136]","[1697101631830, 1697101632966]"
1964,1964,875,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101632967,1697101634995.0,120,,,"[7, 1232, 95, 75, 71]","[1697101632974, 1697101634206, 1697101634301, 1697101634376, 1697101634447]"
1965,1965,178,19,[],200,llama-13b,128,1,1033.0,1.0,1,H100,1697101633173,1697101634206.0,120,11.0,1.0,"[12, 1021]","[1697101633185, 1697101634206]"
1966,1966,511,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101634208,1697101634996.0,120,,,"[18, 743]","[1697101634226, 1697101634969]"
1967,1967,864,21,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101635000,1697101640209.0,120,83.0,20.0,"[426, 1382, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 94, 91, 93, 91, 91, 80, 616]","[1697101635426, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209]"
1968,1968,264,20,[],200,llama-13b,128,1,4825.0,1.0,1,H100,1697101639595,1697101644420.0,120,86.0,20.0,"[30, 1377, 117, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 800, 100, 95, 93, 95, 86, 86, 415]","[1697101639625, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644005, 1697101644420]"
1969,1969,888,20,[],200,llama-13b,128,1,1540.0,1.0,1,H100,1697101644759,1697101646299.0,120,19.0,1.0,"[66, 1474]","[1697101644825, 1697101646299]"
1970,1970,314,21,[],200,llama-13b,128,1,4072.0,1.0,1,H100,1697101646300,1697101650372.0,120,335.0,13.0,"[12, 1430, 341, 96, 100, 101, 96, 88, 485, 96, 96, 83, 942, 106]","[1697101646312, 1697101647742, 1697101648083, 1697101648179, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372]"
1971,1971,355,16,[],200,llama-13b,128,1,4169.0,1.0,1,H100,1697101642418,1697101646587.0,120,90.0,20.0,"[6, 707, 319, 100, 96, 93, 94, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 78, 74, 741, 98]","[1697101642424, 1697101643131, 1697101643450, 1697101643550, 1697101643646, 1697101643739, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587]"
1972,1972,635,9,[],200,llama-13b,128,1,653.0,1.0,1,H100,1697101609451,1697101610104.0,120,23.0,1.0,"[54, 599]","[1697101609505, 1697101610104]"
1973,1973,766,32,[],200,llama-13b,128,1,1057.0,1.0,1,H100,1697101683850,1697101684907.0,120,11.0,1.0,"[12, 1045]","[1697101683862, 1697101684907]"
1974,1974,673,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101650373,1697101651604.0,120,,,[7],[1697101650380]
1975,1975,63,10,[],200,llama-13b,128,1,375.0,1.0,1,H100,1697101610105,1697101610480.0,120,39.0,1.0,"[42, 333]","[1697101610147, 1697101610480]"
1976,1976,425,11,[],200,llama-13b,128,1,4369.0,1.0,1,H100,1697101610481,1697101614850.0,120,88.0,20.0,"[412, 1176, 117, 84, 82, 81, 78, 609, 90, 86, 84, 81, 62, 80, 721, 94, 93, 92, 90, 70, 87]","[1697101610893, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613120, 1697101613210, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614324, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614850]"
1977,1977,101,23,[],200,llama-13b,128,1,1863.0,1.0,1,H100,1697101651609,1697101653472.0,120,13.0,1.0,"[301, 1562]","[1697101651910, 1697101653472]"
1978,1978,544,24,[],200,llama-13b,128,1,1152.0,1.0,1,H100,1697101653479,1697101654631.0,120,26.0,1.0,"[48, 1104]","[1697101653527, 1697101654631]"
1979,1979,906,25,[],200,llama-13b,128,1,5370.0,1.0,1,H100,1697101654632,1697101660002.0,120,86.0,20.0,"[30, 1336, 129, 105, 101, 98, 98, 83, 622, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97]","[1697101654662, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002]"
1980,1980,199,33,[],200,llama-13b,128,1,1207.0,1.0,1,H100,1697101684909,1697101686116.0,120,13.0,1.0,"[36, 1171]","[1697101684945, 1697101686116]"
1981,1981,107,5,[],200,llama-13b,128,1,1907.0,1.0,1,H100,1697101610279,1697101612186.0,120,216.0,2.0,"[546, 1361]","[1697101610825, 1697101612186]"
1982,1982,440,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612187,1697101616321.0,120,,,"[7, 820, 107, 90, 86, 83, 81, 63, 80, 719, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612194, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613524, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
1983,1983,553,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101686117,1697101689594.0,120,,,"[36, 869, 215, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 91, 90]","[1697101686153, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688994]"
1984,1984,337,17,[],200,llama-13b,128,1,1278.0,1.0,1,H100,1697101633691,1697101634969.0,120,12.0,1.0,"[180, 1098]","[1697101633871, 1697101634969]"
1985,1985,886,10,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616331,1697101617376.0,120,17.0,1.0,"[50, 995]","[1697101616381, 1697101617376]"
1986,1986,401,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601197,1697101604272.0,120,,,"[370, 1726, 115, 101, 74]","[1697101601567, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
1987,1987,316,11,[],200,llama-13b,128,1,4679.0,1.0,1,H100,1697101617378,1697101622057.0,120,86.0,20.0,"[113, 1598, 262, 95, 88, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60]","[1697101617491, 1697101619089, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
1988,1988,257,10,[],200,llama-13b,128,1,1710.0,1.0,1,H100,1697101617378,1697101619088.0,120,14.0,1.0,"[48, 1662]","[1697101617426, 1697101619088]"
1989,1989,616,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619091,1697101623658.0,120,,,"[59, 1229, 196, 95, 94, 93, 92, 91, 74, 644, 80, 80, 78, 61, 615, 86, 87, 84, 83]","[1697101619150, 1697101620379, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620949, 1697101621040, 1697101621114, 1697101621758, 1697101621838, 1697101621918, 1697101621996, 1697101622057, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
1990,1990,729,3,[],200,llama-13b,128,1,2169.0,1.0,1,H100,1697101604276,1697101606445.0,120,874.0,2.0,"[412, 1757]","[1697101604688, 1697101606445]"
1991,1991,296,4,[],200,llama-13b,128,1,1795.0,1.0,1,H100,1697101610273,1697101612068.0,120,6.0,1.0,"[303, 1492]","[1697101610576, 1697101612068]"
1992,1992,12,12,[],200,llama-13b,128,1,904.0,1.0,1,H100,1697101623664,1697101624568.0,120,11.0,1.0,"[19, 885]","[1697101623683, 1697101624568]"
1993,1993,693,17,[],200,llama-13b,128,1,2144.0,1.0,1,H100,1697101623665,1697101625809.0,120,67.0,2.0,"[236, 1793, 115]","[1697101623901, 1697101625694, 1697101625809]"
1994,1994,660,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612070,1697101616320.0,120,,,"[12, 932, 107, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 88, 633, 79, 78, 76]","[1697101612082, 1697101613014, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
1995,1995,372,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101624569,1697101626655.0,120,,,"[12, 1114, 114, 97, 69]","[1697101624581, 1697101625695, 1697101625809, 1697101625906, 1697101625975]"
1996,1996,160,4,[],200,llama-13b,128,1,824.0,1.0,1,H100,1697101606446,1697101607270.0,120,13.0,1.0,"[13, 811]","[1697101606459, 1697101607270]"
1997,1997,517,5,[],200,llama-13b,128,1,1062.0,1.0,1,H100,1697101607271,1697101608333.0,120,15.0,1.0,"[36, 1026]","[1697101607307, 1697101608333]"
1998,1998,584,8,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616331,1697101617376.0,120,10.0,1.0,"[43, 1002]","[1697101616374, 1697101617376]"
1999,1999,875,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608334,1697101610266.0,120,,,"[42, 982, 91, 79, 77]","[1697101608376, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
2000,2000,12,9,[],200,llama-13b,128,1,878.0,1.0,1,H100,1697101617378,1697101618256.0,120,11.0,1.0,"[13, 865]","[1697101617391, 1697101618256]"
2001,2001,732,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626660,1697101628903.0,120,,,"[16, 880, 43]","[1697101626676, 1697101627556, 1697101627599]"
2002,2002,74,21,[],200,llama-13b,128,1,4308.0,1.0,1,H100,1697101636811,1697101641119.0,120,88.0,20.0,"[89, 579, 189, 91, 93, 87, 85, 83, 81, 768, 98, 92, 93, 92, 91, 91, 80, 615, 83, 93, 735]","[1697101636900, 1697101637479, 1697101637668, 1697101637759, 1697101637852, 1697101637939, 1697101638024, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640208, 1697101640291, 1697101640384, 1697101641119]"
2003,2003,742,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607624,1697101610275.0,120,,,"[7, 702, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607631, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
2004,2004,888,9,[],200,llama-13b,128,1,1079.0,1.0,1,H100,1697101616297,1697101617376.0,120,19.0,1.0,"[36, 1043]","[1697101616333, 1697101617376]"
2005,2005,886,27,[],200,llama-13b,128,1,1147.0,1.0,1,H100,1697101655999,1697101657146.0,120,17.0,1.0,"[18, 1129]","[1697101656017, 1697101657146]"
2006,2006,243,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[198, 1081]","[1697101633888, 1697101634969]"
2007,2007,496,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616318.0,120,,,"[44, 1399]","[1697101614896, 1697101616295]"
2008,2008,601,15,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101634999,1697101640208.0,120,83.0,20.0,"[219, 1590, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 768, 97, 93, 93, 92, 91, 91, 79, 616]","[1697101635218, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208]"
2009,2009,868,23,[],200,llama-13b,128,1,4903.0,1.0,1,H100,1697101645675,1697101650578.0,120,85.0,20.0,"[12, 612, 191, 97, 98, 83, 79, 1235, 96, 101, 101, 96, 88, 485, 96, 96, 83, 942, 106, 104, 102]","[1697101645687, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646847, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650578]"
2010,2010,851,7,[],200,llama-13b,128,1,1931.0,1.0,1,H100,1697101616325,1697101618256.0,120,23.0,1.0,"[356, 1575]","[1697101616681, 1697101618256]"
2011,2011,317,28,[],200,llama-13b,128,1,11347.0,1.0,1,H100,1697101657150,1697101668497.0,120,244.0,50.0,"[18, 992, 129, 96, 82, 82, 81, 1075, 101, 99, 97, 92, 86, 805, 102, 75, 104, 87, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 100, 689, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101]","[1697101657168, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658630, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497]"
2012,2012,626,14,[],200,llama-13b,128,1,1136.0,1.0,1,H100,1697101627558,1697101628694.0,120,10.0,1.0,"[30, 1105]","[1697101627588, 1697101628693]"
2013,2013,51,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628695,1697101634994.0,120,,,"[72, 650, 1317, 81, 68, 66, 864, 93, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 93, 70, 614, 75, 71]","[1697101628767, 1697101629417, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2014,2014,285,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101618257,1697101623658.0,120,,,"[37, 794, 264, 94, 88, 68, 85, 85, 83, 719, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60, 614, 87, 87, 84, 83]","[1697101618294, 1697101619088, 1697101619352, 1697101619446, 1697101619534, 1697101619602, 1697101619687, 1697101619772, 1697101619855, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
2015,2015,827,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.77 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.40 GiB is free. Process 1607256 has 69.69 GiB memory in use. Of the allocated memory 41.67 GiB is allocated by PyTorch, and 27.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101601198,1697101604271.0,120,,,"[525, 1570, 115, 101, 74]","[1697101601723, 1697101603293, 1697101603408, 1697101603509, 1697101603583]"
2016,2016,41,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101644759,1697101651605.0,120,,,"[7, 1533, 190, 98, 97, 84, 78, 1237, 95, 101, 100, 97, 88, 485, 97, 96, 82, 942, 106, 103, 102, 91, 85]","[1697101644766, 1697101646299, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
2017,2017,47,11,[],200,llama-13b,128,1,3736.0,1.0,1,H100,1697101617378,1697101621114.0,120,90.0,20.0,"[7, 871, 108, 81, 81, 78, 79, 668, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 92, 74]","[1697101617385, 1697101618256, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114]"
2018,2018,589,36,[],200,llama-13b,128,1,4842.0,1.0,1,H100,1697101665869,1697101670711.0,120,92.0,20.0,"[7, 894, 366, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 93, 88, 676]","[1697101665876, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669947, 1697101670035, 1697101670711]"
2019,2019,7,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628905.0,120,,,"[337, 1692]","[1697101627001, 1697101628693]"
2020,2020,337,11,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628910,1697101630630.0,120,12.0,1.0,"[410, 1309]","[1697101629320, 1697101630629]"
2021,2021,695,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630632,1697101634995.0,120,,,"[14, 1038, 130, 92, 91, 89, 89, 88, 84, 83, 741, 95, 72, 94, 93, 92, 70, 615, 74, 71]","[1697101630646, 1697101631684, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
2022,2022,47,12,[],200,llama-13b,128,1,4709.0,1.0,1,H100,1697101616332,1697101621041.0,120,90.0,20.0,"[247, 798, 48, 939, 81, 81, 78, 78, 668, 96, 88, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93]","[1697101616579, 1697101617377, 1697101617425, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619350, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041]"
2023,2023,401,19,[],200,llama-13b,128,1,4919.0,1.0,1,H100,1697101651610,1697101656529.0,120,84.0,20.0,"[262, 789, 57, 892, 85, 84, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98]","[1697101651872, 1697101652661, 1697101652718, 1697101653610, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529]"
2024,2024,415,18,[],200,llama-13b,128,1,5647.0,1.0,1,H100,1697101641121,1697101646768.0,120,109.0,29.0,"[18, 982, 101, 99, 96, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 87, 84, 83, 83, 683, 78, 78, 77, 75, 741, 98, 97, 84]","[1697101641139, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768]"
2025,2025,308,17,[],200,llama-13b,128,1,4594.0,1.0,1,H100,1697101634998,1697101639592.0,120,87.0,20.0,"[83, 437, 66, 1313, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 92, 93, 91, 91, 78]","[1697101635081, 1697101635518, 1697101635584, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639591]"
2026,2026,190,23,[],200,llama-13b,128,1,2429.0,1.0,1,H100,1697101641121,1697101643550.0,120,335.0,10.0,"[6, 690, 304, 101, 99, 96, 88, 82, 63, 800, 100]","[1697101641127, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550]"
2027,2027,14,37,[],200,llama-13b,128,1,4259.0,1.0,1,H100,1697101670722,1697101674981.0,120,90.0,20.0,"[17, 820, 107, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 66, 686]","[1697101670739, 1697101671559, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981]"
2028,2028,548,24,[],200,llama-13b,128,1,4829.0,1.0,1,H100,1697101643551,1697101648380.0,120,86.0,20.0,"[12, 757, 100, 87, 84, 83, 83, 682, 79, 78, 77, 75, 742, 97, 98, 83, 78, 1236, 96, 101, 100]","[1697101643563, 1697101644320, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379]"
2029,2029,755,20,[],200,llama-13b,128,1,5949.0,1.0,1,H100,1697101656530,1697101662479.0,120,286.0,25.0,"[72, 545, 87, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 102, 98, 98, 92, 85, 805, 101, 76, 98, 93, 87, 831, 107, 101]","[1697101656602, 1697101657147, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660003, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479]"
2030,2030,649,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101629418,1697101634994.0,120,,,"[12, 2254, 130, 92, 90, 90, 89, 88, 84, 83, 741, 95, 72, 93, 93, 93, 70, 614, 75, 71]","[1697101629430, 1697101631684, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2031,2031,907,25,[],200,llama-13b,128,1,566.0,1.0,1,H100,1697101648381,1697101648947.0,120,10.0,1.0,"[18, 548]","[1697101648399, 1697101648947]"
2032,2032,307,26,[],200,llama-13b,128,1,962.0,1.0,1,H100,1697101648949,1697101649911.0,120,26.0,1.0,"[53, 909]","[1697101649002, 1697101649911]"
2033,2033,666,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649913,1697101651604.0,120,,,"[18, 1377]","[1697101649931, 1697101651308]"
2034,2034,849,42,[],200,llama-13b,128,1,981.0,1.0,1,H100,1697101688995,1697101689976.0,120,10.0,1.0,"[6, 975]","[1697101689001, 1697101689976]"
2035,2035,270,43,[],200,llama-13b,128,1,3802.0,1.0,1,H100,1697101689977,1697101693779.0,120,364.0,12.0,"[211, 1842, 244, 213, 212, 78, 111, 93, 94, 403, 102, 100, 99]","[1697101690188, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692888, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779]"
2036,2036,780,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646769,1697101651603.0,120,,,"[6, 967, 341, 96, 100, 101, 96, 88, 485, 97, 95, 83, 941, 107, 103, 102, 91, 85]","[1697101646775, 1697101647742, 1697101648083, 1697101648179, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
2037,2037,629,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693782,1697101697588.0,120,,,"[17, 658, 97, 77, 101, 84, 604, 79, 76, 832, 102, 101, 111, 92, 90]","[1697101693799, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696813, 1697101696903]"
2038,2038,82,14,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634997,1697101640208.0,120,67.0,20.0,"[111, 1699, 90, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 80, 615]","[1697101635108, 1697101636807, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640208]"
2039,2039,588,29,[],200,llama-13b,128,1,958.0,1.0,1,H100,1697101662379,1697101663337.0,120,11.0,1.0,"[12, 946]","[1697101662391, 1697101663337]"
2040,2040,924,13,[],200,llama-13b,128,1,1289.0,1.0,1,H100,1697101619090,1697101620379.0,120,9.0,1.0,"[24, 1264]","[1697101619114, 1697101620378]"
2041,2041,354,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101620379,1697101623659.0,120,,,"[7, 1287, 85, 81, 79, 79, 60, 614, 88, 86, 84, 84]","[1697101620386, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
2042,2042,20,30,[],200,llama-13b,128,1,4265.0,1.0,1,H100,1697101663338,1697101667603.0,120,83.0,20.0,"[36, 1071, 233, 109, 102, 96, 94, 97, 692, 105, 100, 72, 72, 93, 92, 734, 103, 99, 72, 98, 95]","[1697101663374, 1697101664445, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665176, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667508, 1697101667603]"
2043,2043,148,10,[],200,llama-13b,128,1,1795.0,1.0,1,H100,1697101613605,1697101615400.0,120,16.0,1.0,"[18, 1777]","[1697101613623, 1697101615400]"
2044,2044,726,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628695,1697101634994.0,120,,,"[65, 657, 1317, 81, 68, 66, 864, 93, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 93, 70, 614, 75, 71]","[1697101628760, 1697101629417, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2045,2045,711,15,[],200,llama-13b,128,1,2241.0,1.0,1,H100,1697101623665,1697101625906.0,120,457.0,4.0,"[154, 749, 40, 1201, 97]","[1697101623819, 1697101624568, 1697101624608, 1697101625809, 1697101625906]"
2046,2046,139,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625907,1697101626656.0,120,,,"[13, 646]","[1697101625920, 1697101626566]"
2047,2047,856,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101680481,1697101689610.0,120,,,"[24, 997, 106, 84, 83, 79, 79, 771, 97, 92, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90, 594, 85, 64, 83, 80, 911, 91, 89, 87, 86, 647, 101, 100, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101680505, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682801, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685099, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
2048,2048,499,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626662,1697101628904.0,120,,,"[38, 856, 43]","[1697101626700, 1697101627556, 1697101627599]"
2049,2049,509,12,[],200,llama-13b,128,1,2144.0,1.0,1,H100,1697101623664,1697101625808.0,120,286.0,3.0,"[26, 878, 39, 1201]","[1697101623690, 1697101624568, 1697101624607, 1697101625808]"
2050,2050,198,7,[],200,llama-13b,128,1,4489.0,1.0,1,H100,1697101610273,1697101614762.0,120,96.0,20.0,"[111, 662, 46, 1094, 84, 82, 80, 78, 610, 90, 86, 83, 82, 62, 80, 720, 95, 93, 92, 90, 69]","[1697101610384, 1697101611046, 1697101611092, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762]"
2051,2051,828,18,[],200,llama-13b,128,1,2905.0,1.0,1,H100,1697101628909,1697101631814.0,120,182.0,6.0,"[106, 1719, 81, 68, 66, 865]","[1697101629015, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814]"
2052,2052,787,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616319.0,120,,,"[120, 1323]","[1697101614972, 1697101616295]"
2053,2053,302,13,[],200,llama-13b,128,1,4790.0,1.0,1,H100,1697101616324,1697101621114.0,120,85.0,20.0,"[572, 1360, 108, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 98, 68]","[1697101616896, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621046, 1697101621114]"
2054,2054,256,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101631815,1697101634995.0,120,,,"[26, 1125, 206, 95, 71, 94, 93, 92, 70, 615, 74, 71]","[1697101631841, 1697101632966, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
2055,2055,687,18,[],200,llama-13b,128,1,4622.0,1.0,1,H100,1697101634970,1697101639592.0,120,96.0,20.0,"[7, 123, 483, 1313, 88, 55, 629, 90, 93, 87, 86, 82, 82, 767, 98, 93, 92, 92, 92, 91, 73]","[1697101634977, 1697101635100, 1697101635583, 1697101636896, 1697101636984, 1697101637039, 1697101637668, 1697101637758, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639330, 1697101639422, 1697101639513, 1697101639586]"
2056,2056,863,13,[],200,llama-13b,128,1,756.0,1.0,1,H100,1697101625810,1697101626566.0,120,10.0,1.0,"[6, 750]","[1697101625816, 1697101626566]"
2057,2057,294,14,[],200,llama-13b,128,1,1032.0,1.0,1,H100,1697101626567,1697101627599.0,120,9.0,2.0,"[30, 1002]","[1697101626597, 1697101627599]"
2058,2058,654,15,[],200,llama-13b,128,1,3283.0,1.0,1,H100,1697101627600,1697101630883.0,120,47.0,4.0,"[7, 1809, 1317, 81, 69]","[1697101627607, 1697101629416, 1697101630733, 1697101630814, 1697101630883]"
2059,2059,173,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630884,1697101634994.0,120,,,"[7, 793, 131, 92, 90, 89, 89, 88, 84, 83, 742, 94, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101630891, 1697101631684, 1697101631815, 1697101631907, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2060,2060,89,19,[],200,llama-13b,128,1,4826.0,1.0,1,H100,1697101639594,1697101644420.0,120,52.0,20.0,"[23, 1385, 117, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 800, 100, 95, 93, 95, 86, 86, 415]","[1697101639617, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644005, 1697101644420]"
2061,2061,334,26,[],200,llama-13b,128,1,643.0,1.0,1,H100,1697101660004,1697101660647.0,120,15.0,1.0,"[24, 619]","[1697101660028, 1697101660647]"
2062,2062,656,14,[],200,llama-13b,128,1,1466.0,1.0,1,H100,1697101621116,1697101622582.0,120,26.0,1.0,"[69, 1397]","[1697101621185, 1697101622582]"
2063,2063,694,27,[],200,llama-13b,128,1,4030.0,1.0,1,H100,1697101660648,1697101664678.0,120,161.0,13.0,"[42, 1581, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845]","[1697101660690, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678]"
2064,2064,410,24,[],200,llama-13b,128,1,4647.0,1.0,1,H100,1697101650578,1697101655225.0,120,364.0,12.0,"[37, 1261, 842, 891, 86, 83, 82, 81, 896, 107, 106, 100, 75]","[1697101650615, 1697101651876, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225]"
2065,2065,681,13,[],200,llama-13b,128,1,2029.0,1.0,1,H100,1697101626664,1697101628693.0,120,23.0,1.0,"[244, 1784]","[1697101626908, 1697101628692]"
2066,2066,434,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623677,1697101626657.0,120,,,"[529, 1489, 113, 98, 69]","[1697101624206, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
2067,2067,513,12,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628909,1697101633688.0,120,83.0,20.0,"[278, 1442, 105, 81, 68, 66, 865, 92, 91, 89, 89, 88, 84, 82, 743, 94, 72, 94, 93, 92, 71]","[1697101629187, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
2068,2068,788,14,[],200,llama-13b,128,1,2024.0,1.0,1,H100,1697101626669,1697101628693.0,120,31.0,1.0,"[504, 1519]","[1697101627173, 1697101628692]"
2069,2069,584,7,[],200,llama-13b,128,1,1923.0,1.0,1,H100,1697101616333,1697101618256.0,120,10.0,1.0,"[335, 1587]","[1697101616668, 1697101618255]"
2070,2070,615,20,[],200,llama-13b,128,1,5212.0,1.0,1,H100,1697101634997,1697101640209.0,120,93.0,20.0,"[215, 1685, 87, 56, 628, 91, 92, 87, 86, 82, 82, 768, 97, 93, 93, 92, 91, 91, 80, 615]","[1697101635212, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640208]"
2071,2071,13,8,[],200,llama-13b,128,1,3800.0,1.0,1,H100,1697101618257,1697101622057.0,120,90.0,20.0,"[7, 825, 262, 95, 88, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60]","[1697101618264, 1697101619089, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
2072,2072,191,15,[],200,llama-13b,128,1,4992.0,1.0,1,H100,1697101628695,1697101633687.0,120,85.0,20.0,"[12, 710, 1316, 82, 68, 66, 864, 92, 91, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 92, 71]","[1697101628707, 1697101629417, 1697101630733, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631905, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633687]"
2073,2073,578,7,[],200,llama-13b,128,1,1512.0,1.0,1,H100,1697101622060,1697101623572.0,120,31.0,1.0,"[54, 1458]","[1697101622114, 1697101623572]"
2074,2074,579,17,[],200,llama-13b,128,1,791.0,1.0,1,H100,1697101640211,1697101641002.0,120,19.0,1.0,"[37, 754]","[1697101640248, 1697101641002]"
2075,2075,7,18,[],200,llama-13b,128,1,2642.0,1.0,1,H100,1697101641003,1697101643645.0,120,345.0,11.0,"[48, 766, 304, 101, 98, 97, 88, 82, 63, 800, 100, 95]","[1697101641051, 1697101641817, 1697101642121, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645]"
2076,2076,909,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623573,1697101626656.0,120,,,"[18, 283, 733, 1201, 98, 68]","[1697101623591, 1697101623874, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
2077,2077,189,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623665,1697101626656.0,120,,,"[42, 861, 40, 1200, 98, 68]","[1697101623707, 1697101624568, 1697101624608, 1697101625808, 1697101625906, 1697101625974]"
2078,2078,369,19,[],200,llama-13b,128,1,3121.0,1.0,1,H100,1697101643647,1697101646768.0,120,216.0,15.0,"[6, 667, 100, 87, 84, 83, 83, 682, 79, 78, 77, 75, 742, 97, 98, 83]","[1697101643653, 1697101644320, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768]"
2079,2079,794,4,[],200,llama-13b,128,1,1796.0,1.0,1,H100,1697101610272,1697101612068.0,120,11.0,1.0,"[301, 1495]","[1697101610573, 1697101612068]"
2080,2080,86,5,[],200,llama-13b,128,1,4472.0,1.0,1,H100,1697101604195,1697101608667.0,120,335.0,17.0,"[18, 299, 736, 1197, 81, 76, 74, 772, 89, 86, 85, 64, 84, 80, 491, 95, 72, 73]","[1697101604213, 1697101604512, 1697101605248, 1697101606445, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608427, 1697101608522, 1697101608594, 1697101608667]"
2081,2081,562,18,[],200,llama-13b,128,1,9551.0,1.0,1,H100,1697101651610,1697101661161.0,120,67.0,39.0,"[75, 976, 57, 892, 86, 82, 82, 81, 896, 108, 105, 101, 74, 99, 97, 92, 613, 106, 100, 99, 97, 83, 622, 82, 79, 59, 836, 95, 83, 81, 81, 1076, 101, 98, 98, 92, 86, 805, 101, 75]","[1697101651685, 1697101652661, 1697101652718, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657233, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659806, 1697101659904, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161]"
2082,2082,218,5,[],200,llama-13b,128,1,1453.0,1.0,1,H100,1697101612070,1697101613523.0,120,109.0,7.0,"[36, 1015, 90, 86, 83, 81, 62]","[1697101612106, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523]"
2083,2083,581,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613524,1697101616320.0,120,,,"[7, 606, 187, 94, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101613531, 1697101614137, 1697101614324, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
2084,2084,558,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[216, 1063]","[1697101633906, 1697101634969]"
2085,2085,357,17,[],200,llama-13b,128,1,7174.0,1.0,1,H100,1697101639594,1697101646768.0,120,52.0,33.0,"[19, 1389, 117, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 800, 100, 95, 93, 95, 86, 86, 414, 87, 84, 84, 82, 684, 78, 78, 77, 75, 741, 98, 97, 84]","[1697101639613, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644005, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768]"
2086,2086,447,19,[],200,llama-13b,128,1,3240.0,1.0,1,H100,1697101640211,1697101643451.0,120,161.0,13.0,"[25, 883, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 801]","[1697101640236, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451]"
2087,2087,212,14,[],200,llama-13b,128,1,860.0,1.0,1,H100,1697101623014,1697101623874.0,120,31.0,1.0,"[6, 854]","[1697101623020, 1697101623874]"
2088,2088,805,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101643452,1697101651605.0,120,,,"[12, 857, 99, 87, 84, 83, 83, 682, 79, 78, 78, 74, 742, 97, 98, 83, 78, 1227, 105, 101, 100, 97, 88, 485, 96, 96, 83, 941, 107, 103, 102, 90, 86]","[1697101643464, 1697101644321, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645439, 1697101645518, 1697101645596, 1697101645674, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648073, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
2089,2089,349,13,[],200,llama-13b,128,1,4679.0,1.0,1,H100,1697101617378,1697101622057.0,120,88.0,20.0,"[108, 1603, 262, 95, 88, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60]","[1697101617486, 1697101619089, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
2090,2090,572,15,[],200,llama-13b,128,1,1821.0,1.0,1,H100,1697101623875,1697101625696.0,120,16.0,1.0,"[403, 1418]","[1697101624278, 1697101625696]"
2091,2091,926,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625698,1697101626657.0,120,,,"[100, 768]","[1697101625798, 1697101626566]"
2092,2092,183,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628909,1697101634994.0,120,,,"[167, 1553, 105, 81, 68, 66, 864, 93, 90, 90, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101629076, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2093,2093,356,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[231, 1798]","[1697101626894, 1697101628692]"
2094,2094,27,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619352,1697101623658.0,120,,,"[7, 1019, 197, 95, 94, 93, 92, 91, 74, 644, 81, 79, 78, 61, 615, 86, 87, 84, 83]","[1697101619359, 1697101620378, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620949, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621996, 1697101622057, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
2095,2095,613,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623665,1697101626656.0,120,,,"[48, 855, 39, 1201, 98, 68]","[1697101623713, 1697101624568, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
2096,2096,688,18,[],200,llama-13b,128,1,1974.0,1.0,1,H100,1697101628909,1697101630883.0,120,345.0,4.0,"[191, 1529, 105, 81, 68]","[1697101629100, 1697101630629, 1697101630734, 1697101630815, 1697101630883]"
2097,2097,257,2,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101604275,1697101605195.0,120,14.0,1.0,"[108, 812]","[1697101604383, 1697101605195]"
2098,2098,702,3,[],200,llama-13b,128,1,4253.0,1.0,1,H100,1697101605196,1697101609449.0,120,89.0,20.0,"[12, 999, 239, 80, 76, 74, 772, 89, 86, 85, 64, 84, 80, 492, 95, 72, 72, 94, 89, 88, 511]","[1697101605208, 1697101606207, 1697101606446, 1697101606526, 1697101606602, 1697101606676, 1697101607448, 1697101607537, 1697101607623, 1697101607708, 1697101607772, 1697101607856, 1697101607936, 1697101608428, 1697101608523, 1697101608595, 1697101608667, 1697101608761, 1697101608850, 1697101608938, 1697101609449]"
2099,2099,299,24,[],200,llama-13b,128,1,729.0,1.0,1,H100,1697101650579,1697101651308.0,120,14.0,1.0,"[42, 687]","[1697101650621, 1697101651308]"
2100,2100,663,25,[],200,llama-13b,128,1,5219.0,1.0,1,H100,1697101651309,1697101656528.0,120,79.0,20.0,"[18, 550, 841, 891, 86, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 91, 614, 106, 100, 99, 97]","[1697101651327, 1697101651877, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655512, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
2101,2101,414,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[43, 1236]","[1697101633733, 1697101634969]"
2102,2102,371,10,[],200,llama-13b,128,1,1080.0,1.0,1,H100,1697101616296,1697101617376.0,120,13.0,1.0,"[25, 1055]","[1697101616321, 1697101617376]"
2103,2103,43,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626666,1697101628905.0,120,,,"[436, 1590]","[1697101627102, 1697101628692]"
2104,2104,91,26,[],200,llama-13b,128,1,617.0,1.0,1,H100,1697101656530,1697101657147.0,120,23.0,1.0,"[13, 604]","[1697101656543, 1697101657147]"
2105,2105,400,11,[],200,llama-13b,128,1,2996.0,1.0,1,H100,1697101628910,1697101631906.0,120,123.0,7.0,"[313, 1406, 105, 81, 68, 66, 865, 92]","[1697101629223, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906]"
2106,2106,780,30,[],200,llama-13b,128,1,4475.0,1.0,1,H100,1697101684149,1697101688624.0,120,85.0,20.0,"[6, 752, 107, 86, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106]","[1697101684155, 1697101684907, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
2107,2107,732,11,[],200,llama-13b,128,1,2394.0,1.0,1,H100,1697101617378,1697101619772.0,120,345.0,12.0,"[31, 847, 108, 82, 80, 78, 79, 668, 94, 89, 67, 86, 85]","[1697101617409, 1697101618256, 1697101618364, 1697101618446, 1697101618526, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772]"
2108,2108,775,18,[],200,llama-13b,128,1,1808.0,1.0,1,H100,1697101635000,1697101636808.0,120,17.0,1.0,"[417, 1391]","[1697101635417, 1697101636808]"
2109,2109,422,27,[],200,llama-13b,128,1,1009.0,1.0,1,H100,1697101657151,1697101658160.0,120,26.0,1.0,"[53, 956]","[1697101657204, 1697101658160]"
2110,2110,776,28,[],200,llama-13b,128,1,1544.0,1.0,1,H100,1697101658161,1697101659705.0,120,67.0,2.0,"[18, 1194, 332]","[1697101658179, 1697101659373, 1697101659705]"
2111,2111,758,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101631907,1697101634995.0,120,,,"[7, 1052, 206, 95, 71, 94, 93, 92, 70, 614, 75, 71]","[1697101631914, 1697101632966, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2112,2112,201,29,[],200,llama-13b,128,1,5081.0,1.0,1,H100,1697101659706,1697101664787.0,120,67.0,20.0,"[13, 928, 338, 102, 75, 98, 93, 87, 831, 107, 101, 95, 94, 89, 708, 100, 93, 88, 87, 845, 109]","[1697101659719, 1697101660647, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663465, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787]"
2113,2113,206,19,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101636810,1697101637479.0,120,16.0,1.0,"[48, 621]","[1697101636858, 1697101637479]"
2114,2114,531,20,[],200,llama-13b,128,1,4937.0,1.0,1,H100,1697101637480,1697101642417.0,120,52.0,20.0,"[36, 1223, 217, 97, 93, 93, 92, 91, 91, 80, 616, 82, 93, 735, 85, 78, 78, 760, 102, 98, 97]","[1697101637516, 1697101638739, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642222, 1697101642320, 1697101642417]"
2115,2115,843,13,[],200,llama-13b,128,1,1466.0,1.0,1,H100,1697101621116,1697101622582.0,120,14.0,1.0,"[63, 1403]","[1697101621179, 1697101622582]"
2116,2116,381,16,[],200,llama-13b,128,1,1901.0,1.0,1,H100,1697101634996,1697101636897.0,120,140.0,2.0,"[124, 1688, 89]","[1697101635120, 1697101636808, 1697101636897]"
2117,2117,275,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622583,1697101623677.0,120,,,"[18, 972]","[1697101622601, 1697101623573]"
2118,2118,742,17,[],200,llama-13b,128,1,4220.0,1.0,1,H100,1697101636898,1697101641118.0,120,89.0,20.0,"[24, 557, 190, 90, 93, 87, 86, 81, 82, 768, 98, 92, 93, 92, 91, 91, 79, 617, 82, 93, 734]","[1697101636922, 1697101637479, 1697101637669, 1697101637759, 1697101637852, 1697101637939, 1697101638025, 1697101638106, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640384, 1697101641118]"
2119,2119,634,15,[],200,llama-13b,128,1,2018.0,1.0,1,H100,1697101623678,1697101625696.0,120,13.0,1.0,"[552, 1465]","[1697101624230, 1697101625695]"
2120,2120,30,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625698,1697101626657.0,120,,,[101],[1697101625799]
2121,2121,892,10,[],200,llama-13b,128,1,4574.0,1.0,1,H100,1697101610275,1697101614849.0,120,87.0,20.0,"[509, 1284, 118, 84, 82, 80, 78, 610, 90, 86, 83, 82, 63, 79, 720, 95, 93, 92, 90, 69, 87]","[1697101610784, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612432, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613524, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849]"
2122,2122,384,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628903.0,120,,,"[139, 755, 42]","[1697101626802, 1697101627557, 1697101627599]"
2123,2123,671,18,[],200,llama-13b,128,1,1408.0,1.0,1,H100,1697101639594,1697101641002.0,120,12.0,1.0,"[11, 1397]","[1697101639605, 1697101641002]"
2124,2124,744,18,[],200,llama-13b,128,1,2906.0,1.0,1,H100,1697101628908,1697101631814.0,120,161.0,6.0,"[67, 1654, 105, 81, 68, 66, 865]","[1697101628975, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814]"
2125,2125,70,19,[],200,llama-13b,128,1,814.0,1.0,1,H100,1697101641003,1697101641817.0,120,39.0,1.0,"[43, 771]","[1697101641046, 1697101641817]"
2126,2126,424,20,[],200,llama-13b,128,1,4769.0,1.0,1,H100,1697101641818,1697101646587.0,120,88.0,20.0,"[7, 1306, 319, 100, 95, 93, 95, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 77, 75, 741, 98]","[1697101641825, 1697101643131, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587]"
2127,2127,95,28,[],200,llama-13b,128,1,1863.0,1.0,1,H100,1697101651609,1697101653472.0,120,12.0,1.0,"[364, 1499]","[1697101651973, 1697101653472]"
2128,2128,452,29,[],200,llama-13b,128,1,1571.0,1.0,1,H100,1697101653479,1697101655050.0,120,216.0,4.0,"[42, 1110, 206, 108, 105]","[1697101653521, 1697101654631, 1697101654837, 1697101654945, 1697101655050]"
2129,2129,898,30,[],200,llama-13b,128,1,1076.0,1.0,1,H100,1697101655051,1697101656127.0,120,79.0,2.0,"[19, 928, 129]","[1697101655070, 1697101655998, 1697101656127]"
2130,2130,327,31,[],200,llama-13b,128,1,2502.0,1.0,1,H100,1697101656128,1697101658630.0,120,563.0,10.0,"[6, 1012, 88, 81, 79, 60, 835, 96, 82, 82, 80]","[1697101656134, 1697101657146, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629]"
2131,2131,779,21,[],200,llama-13b,128,1,2653.0,1.0,1,H100,1697101646588,1697101649241.0,120,563.0,10.0,"[7, 1147, 342, 94, 101, 101, 96, 88, 485, 96, 96]","[1697101646595, 1697101647742, 1697101648084, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241]"
2132,2132,841,23,[],200,llama-13b,128,1,2847.0,1.0,1,H100,1697101643921,1697101646768.0,120,123.0,15.0,"[6, 494, 86, 84, 83, 83, 683, 78, 78, 77, 75, 742, 97, 98, 83]","[1697101643927, 1697101644421, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768]"
2133,2133,172,19,[],200,llama-13b,128,1,1152.0,1.0,1,H100,1697101631815,1697101632967.0,120,19.0,1.0,"[9, 1143]","[1697101631824, 1697101632967]"
2134,2134,685,32,[],200,llama-13b,128,1,2354.0,1.0,1,H100,1697101658631,1697101660985.0,120,364.0,2.0,"[12, 2003, 339]","[1697101658643, 1697101660646, 1697101660985]"
2135,2135,536,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101632968,1697101634995.0,120,,,"[30, 1208, 95, 75, 71]","[1697101632998, 1697101634206, 1697101634301, 1697101634376, 1697101634447]"
2136,2136,291,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616318.0,120,,,[56],[1697101614908]
2137,2137,58,45,[],200,llama-13b,128,1,1875.0,1.0,1,H100,1697101697600,1697101699475.0,120,15.0,1.0,"[487, 1388]","[1697101698087, 1697101699475]"
2138,2138,560,30,[],200,llama-13b,128,1,2720.0,1.0,1,H100,1697101664788,1697101667508.0,120,161.0,13.0,"[36, 1044, 106, 100, 71, 72, 93, 92, 734, 103, 98, 73, 98]","[1697101664824, 1697101665868, 1697101665974, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508]"
2139,2139,208,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649243,1697101651603.0,120,,,"[12, 656, 355, 106, 104, 101, 91, 86]","[1697101649255, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650754]"
2140,2140,115,33,[],200,llama-13b,128,1,930.0,1.0,1,H100,1697101660986,1697101661916.0,120,13.0,1.0,"[12, 918]","[1697101660998, 1697101661916]"
2141,2141,270,24,[],200,llama-13b,128,1,3497.0,1.0,1,H100,1697101646769,1697101650266.0,120,364.0,12.0,"[24, 949, 341, 96, 100, 101, 96, 88, 485, 97, 96, 82, 942]","[1697101646793, 1697101647742, 1697101648083, 1697101648179, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266]"
2142,2142,569,23,[],200,llama-13b,128,1,1055.0,1.0,1,H100,1697101651606,1697101652661.0,120,16.0,1.0,"[33, 1022]","[1697101651639, 1697101652661]"
2143,2143,422,46,[],200,llama-13b,128,1,1075.0,1.0,1,H100,1697101699477,1697101700552.0,120,26.0,1.0,"[96, 979]","[1697101699573, 1697101700552]"
2144,2144,600,25,[],200,llama-13b,128,1,1041.0,1.0,1,H100,1697101650267,1697101651308.0,120,23.0,1.0,"[6, 1035]","[1697101650273, 1697101651308]"
2145,2145,28,26,[],200,llama-13b,128,1,5219.0,1.0,1,H100,1697101651309,1697101656528.0,120,86.0,20.0,"[24, 544, 841, 891, 86, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 91, 614, 106, 100, 99, 97]","[1697101651333, 1697101651877, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655512, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
2146,2146,753,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700553,1697101703170.0,120,,,"[42, 499, 1188, 69]","[1697101700595, 1697101701094, 1697101702282, 1697101702351]"
2147,2147,181,48,[],200,llama-13b,128,1,8940.0,1.0,1,H100,1697101703174,1697101712114.0,120,91.0,39.0,"[249, 1477, 108, 105, 72, 728, 95, 87, 85, 83, 80, 942, 97, 92, 70, 91, 70, 91, 87, 545, 97, 66, 88, 811, 89, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 497, 99, 91, 84, 83]","[1697101703423, 1697101704900, 1697101705008, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708525, 1697101708591, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711856, 1697101711947, 1697101712031, 1697101712114]"
2148,2148,630,24,[],200,llama-13b,128,1,1154.0,1.0,1,H100,1697101646588,1697101647742.0,120,6.0,1.0,"[42, 1112]","[1697101646630, 1697101647742]"
2149,2149,208,20,[],200,llama-13b,128,1,4922.0,1.0,1,H100,1697101651607,1697101656529.0,120,96.0,20.0,"[101, 953, 57, 892, 86, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 97]","[1697101651708, 1697101652661, 1697101652718, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
2150,2150,58,25,[],200,llama-13b,128,1,1204.0,1.0,1,H100,1697101647743,1697101648947.0,120,15.0,1.0,"[30, 1174]","[1697101647773, 1697101648947]"
2151,2151,416,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651604.0,120,,,"[31, 932, 355, 106, 104, 101, 91, 85]","[1697101648979, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2152,2152,384,27,[],200,llama-13b,128,1,4730.0,1.0,1,H100,1697101656530,1697101661260.0,120,92.0,20.0,"[13, 604, 87, 81, 79, 60, 835, 95, 83, 81, 81, 1076, 102, 98, 97, 93, 85, 805, 101, 76, 98]","[1697101656543, 1697101657147, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260]"
2153,2153,572,12,[],200,llama-13b,128,1,1279.0,1.0,1,H100,1697101633690,1697101634969.0,120,16.0,1.0,"[210, 1069]","[1697101633900, 1697101634969]"
2154,2154,912,35,[],200,llama-13b,128,1,5120.0,1.0,1,H100,1697101689611,1697101694731.0,120,92.0,20.0,"[72, 1429, 93, 1069, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100]","[1697101689683, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
2155,2155,200,7,[],200,llama-13b,128,1,2250.0,1.0,1,H100,1697101611047,1697101613297.0,120,6.0,9.0,"[18, 1004, 117, 84, 82, 81, 78, 609, 90, 86]","[1697101611065, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613120, 1697101613210, 1697101613296]"
2156,2156,926,13,[],200,llama-13b,128,1,7350.0,1.0,1,H100,1697101634970,1697101642320.0,120,563.0,30.0,"[25, 523, 65, 1314, 87, 55, 629, 90, 93, 87, 86, 82, 82, 767, 98, 93, 92, 93, 91, 91, 79, 616, 83, 93, 734, 85, 79, 78, 760, 101, 99]","[1697101634995, 1697101635518, 1697101635583, 1697101636897, 1697101636984, 1697101637039, 1697101637668, 1697101637758, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208, 1697101640291, 1697101640384, 1697101641118, 1697101641203, 1697101641282, 1697101641360, 1697101642120, 1697101642221, 1697101642320]"
2157,2157,287,37,[],200,llama-13b,128,1,2412.0,1.0,1,H100,1697101689618,1697101692030.0,120,10.0,1.0,"[467, 1945]","[1697101690085, 1697101692030]"
2158,2158,644,38,[],200,llama-13b,128,1,1341.0,1.0,1,H100,1697101692031,1697101693372.0,120,19.0,1.0,"[30, 1311]","[1697101692061, 1697101693372]"
2159,2159,83,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622583,1697101623665.0,120,,,"[30, 959]","[1697101622613, 1697101623572]"
2160,2160,46,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693373,1697101697588.0,120,,,"[18, 1163, 77, 101, 84, 604, 78, 77, 831, 103, 101, 111, 91, 91]","[1697101693391, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
2161,2161,775,27,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101651610,1697101652661.0,120,17.0,1.0,"[212, 839]","[1697101651822, 1697101652661]"
2162,2162,291,28,[],200,llama-13b,128,1,3949.0,1.0,1,H100,1697101652662,1697101656611.0,120,79.0,20.0,"[30, 918, 86, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83]","[1697101652692, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611]"
2163,2163,121,28,[],200,llama-13b,128,1,1060.0,1.0,1,H100,1697101664679,1697101665739.0,120,13.0,1.0,"[12, 1048]","[1697101664691, 1697101665739]"
2164,2164,533,17,[],200,llama-13b,128,1,1899.0,1.0,1,H100,1697101634998,1697101636897.0,120,216.0,2.0,"[374, 1525]","[1697101635372, 1697101636897]"
2165,2165,648,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622059,1697101623660.0,120,,,"[43, 1470]","[1697101622102, 1697101623572]"
2166,2166,441,16,[],200,llama-13b,128,1,2019.0,1.0,1,H100,1697101623676,1697101625695.0,120,6.0,1.0,"[422, 1597]","[1697101624098, 1697101625695]"
2167,2167,80,13,[],200,llama-13b,128,1,2018.0,1.0,1,H100,1697101623677,1697101625695.0,120,13.0,1.0,"[517, 1501]","[1697101624194, 1697101625695]"
2168,2168,802,17,[],200,llama-13b,128,1,869.0,1.0,1,H100,1697101625697,1697101626566.0,120,9.0,1.0,"[83, 786]","[1697101625780, 1697101626566]"
2169,2169,440,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625696,1697101626656.0,120,,,"[73, 797]","[1697101625769, 1697101626566]"
2170,2170,693,8,[],200,llama-13b,128,1,943.0,1.0,1,H100,1697101623664,1697101624607.0,120,67.0,2.0,"[14, 890, 39]","[1697101623678, 1697101624568, 1697101624607]"
2171,2171,887,18,[],200,llama-13b,128,1,9786.0,1.0,1,H100,1697101636898,1697101646684.0,120,244.0,50.0,"[12, 569, 190, 90, 93, 87, 85, 82, 82, 768, 98, 92, 93, 92, 91, 91, 79, 616, 83, 93, 735, 85, 78, 78, 761, 100, 99, 96, 89, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 683, 79, 78, 77, 75, 741, 98, 97]","[1697101636910, 1697101637479, 1697101637669, 1697101637759, 1697101637852, 1697101637939, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684]"
2172,2172,284,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616322.0,120,,,[30],[1697101614882]
2173,2173,616,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101616334,1697101623658.0,120,,,"[478, 1444, 108, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 93, 73, 644, 81, 79, 78, 61, 614, 87, 87, 84, 83]","[1697101616812, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621996, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
2174,2174,450,29,[],200,llama-13b,128,1,4971.0,1.0,1,H100,1697101665740,1697101670711.0,120,91.0,20.0,"[30, 1000, 366, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 92, 89, 676]","[1697101665770, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711]"
2175,2175,371,10,[],200,llama-13b,128,1,832.0,1.0,1,H100,1697101618257,1697101619089.0,120,13.0,1.0,"[54, 778]","[1697101618311, 1697101619089]"
2176,2176,729,11,[],200,llama-13b,128,1,1484.0,1.0,1,H100,1697101619090,1697101620574.0,120,874.0,2.0,"[6, 1478]","[1697101619096, 1697101620574]"
2177,2177,729,20,[],200,llama-13b,128,1,1632.0,1.0,1,H100,1697101641818,1697101643450.0,120,874.0,2.0,"[13, 1619]","[1697101641831, 1697101643450]"
2178,2178,118,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101624608,1697101626656.0,120,,,"[15, 1943]","[1697101624623, 1697101626566]"
2179,2179,159,21,[],200,llama-13b,128,1,869.0,1.0,1,H100,1697101643452,1697101644321.0,120,31.0,1.0,"[12, 857]","[1697101643464, 1697101644321]"
2180,2180,220,7,[],200,llama-13b,128,1,1129.0,1.0,1,H100,1697101616296,1697101617425.0,120,67.0,2.0,"[12, 295, 822]","[1697101616308, 1697101616603, 1697101617425]"
2181,2181,579,8,[],200,llama-13b,128,1,1663.0,1.0,1,H100,1697101617426,1697101619089.0,120,19.0,1.0,"[76, 1587]","[1697101617502, 1697101619089]"
2182,2182,517,22,[],200,llama-13b,128,1,1015.0,1.0,1,H100,1697101644322,1697101645337.0,120,15.0,1.0,"[42, 973]","[1697101644364, 1697101645337]"
2183,2183,123,18,[],200,llama-13b,128,1,756.0,1.0,1,H100,1697101625810,1697101626566.0,120,14.0,1.0,"[62, 694]","[1697101625872, 1697101626566]"
2184,2184,908,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619090,1697101623659.0,120,,,"[36, 1252, 196, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 60, 614, 87, 87, 84, 83]","[1697101619126, 1697101620378, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
2185,2185,874,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101645338,1697101651605.0,120,,,"[30, 931, 191, 97, 98, 83, 79, 1234, 97, 101, 101, 96, 88, 486, 96, 96, 82, 942, 106, 104, 101, 91, 85]","[1697101645368, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646847, 1697101648081, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2186,2186,156,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101620576,1697101623659.0,120,,,"[6, 1091, 85, 81, 79, 79, 60, 614, 87, 87, 85, 82]","[1697101620582, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622930, 1697101623012]"
2187,2187,477,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[54, 840, 42]","[1697101626717, 1697101627557, 1697101627599]"
2188,2188,809,30,[],200,llama-13b,128,1,842.0,1.0,1,H100,1697101670717,1697101671559.0,120,16.0,1.0,"[34, 808]","[1697101670751, 1697101671559]"
2189,2189,809,11,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,16.0,1.0,"[26, 1694]","[1697101628935, 1697101630629]"
2190,2190,550,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634995.0,120,,,"[7, 1272]","[1697101633697, 1697101634969]"
2191,2191,238,31,[],200,llama-13b,128,1,1437.0,1.0,1,H100,1697101671560,1697101672997.0,120,563.0,6.0,"[13, 973, 110, 114, 91, 68, 67]","[1697101671573, 1697101672546, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672996]"
2192,2192,911,17,[],200,llama-13b,128,1,3188.0,1.0,1,H100,1697101635000,1697101638188.0,120,335.0,11.0,"[474, 1334, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82]","[1697101635474, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188]"
2193,2193,597,32,[],200,llama-13b,128,1,761.0,1.0,1,H100,1697101672998,1697101673759.0,120,39.0,1.0,"[12, 749]","[1697101673010, 1697101673759]"
2194,2194,336,18,[],200,llama-13b,128,1,3094.0,1.0,1,H100,1697101638189,1697101641283.0,120,58.0,7.0,"[7, 1889, 124, 82, 93, 735, 85, 79]","[1697101638196, 1697101640085, 1697101640209, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641283]"
2195,2195,926,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673760,1697101675811.0,120,,,"[12, 1076, 133, 102, 98, 95, 71, 94]","[1697101673772, 1697101674848, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675347, 1697101675441]"
2196,2196,309,16,[],200,llama-13b,128,1,4748.0,1.0,1,H100,1697101637669,1697101642417.0,120,52.0,20.0,"[7, 1063, 217, 98, 93, 92, 92, 92, 90, 80, 616, 82, 94, 734, 85, 79, 77, 760, 102, 98, 97]","[1697101637676, 1697101638739, 1697101638956, 1697101639054, 1697101639147, 1697101639239, 1697101639331, 1697101639423, 1697101639513, 1697101639593, 1697101640209, 1697101640291, 1697101640385, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642120, 1697101642222, 1697101642320, 1697101642417]"
2197,2197,636,11,[],200,llama-13b,128,1,2018.0,1.0,1,H100,1697101623677,1697101625695.0,120,31.0,1.0,"[529, 1489]","[1697101624206, 1697101625695]"
2198,2198,698,18,[],200,llama-13b,128,1,1910.0,1.0,1,H100,1697101640211,1697101642121.0,120,182.0,6.0,"[13, 778, 117, 85, 78, 78, 761]","[1697101640224, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121]"
2199,2199,696,19,[],200,llama-13b,128,1,3472.0,1.0,1,H100,1697101641284,1697101644756.0,120,83.0,20.0,"[6, 528, 303, 101, 99, 96, 88, 83, 63, 799, 100, 95, 93, 95, 87, 85, 415, 86, 85, 83, 82]","[1697101641290, 1697101641818, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642588, 1697101642651, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644591, 1697101644674, 1697101644756]"
2200,2200,325,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101631815,1697101634995.0,120,,,"[20, 1131, 206, 95, 71, 94, 93, 92, 70, 614, 75, 71]","[1697101631835, 1697101632966, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2201,2201,158,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625698,1697101626657.0,120,,,"[106, 762]","[1697101625804, 1697101626566]"
2202,2202,916,19,[],200,llama-13b,128,1,1808.0,1.0,1,H100,1697101635000,1697101636808.0,120,8.0,1.0,"[485, 1323]","[1697101635485, 1697101636808]"
2203,2203,573,5,[],200,llama-13b,128,1,921.0,1.0,1,H100,1697101606527,1697101607448.0,120,874.0,2.0,"[6, 915]","[1697101606533, 1697101607448]"
2204,2204,346,20,[],200,llama-13b,128,1,4309.0,1.0,1,H100,1697101636810,1697101641119.0,120,85.0,20.0,"[36, 633, 189, 91, 92, 87, 86, 83, 81, 768, 98, 92, 93, 92, 91, 91, 80, 617, 81, 93, 735]","[1697101636846, 1697101637479, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640210, 1697101640291, 1697101640384, 1697101641119]"
2205,2205,2,6,[],200,llama-13b,128,1,1313.0,1.0,1,H100,1697101607449,1697101608762.0,120,58.0,6.0,"[18, 866, 95, 95, 72, 73, 94]","[1697101607467, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762]"
2206,2206,685,17,[],200,llama-13b,128,1,1900.0,1.0,1,H100,1697101634997,1697101636897.0,120,364.0,2.0,"[203, 1608, 89]","[1697101635200, 1697101636808, 1697101636897]"
2207,2207,359,7,[],200,llama-13b,128,1,595.0,1.0,1,H100,1697101608763,1697101609358.0,120,10.0,1.0,"[12, 583]","[1697101608775, 1697101609358]"
2208,2208,200,18,[],200,llama-13b,128,1,2058.0,1.0,1,H100,1697101636898,1697101638956.0,120,6.0,9.0,"[18, 563, 190, 90, 93, 87, 86, 81, 82, 768]","[1697101636916, 1697101637479, 1697101637669, 1697101637759, 1697101637852, 1697101637939, 1697101638025, 1697101638106, 1697101638188, 1697101638956]"
2209,2209,309,10,[],200,llama-13b,128,1,3736.0,1.0,1,H100,1697101617378,1697101621114.0,120,52.0,20.0,"[7, 871, 108, 81, 81, 78, 79, 668, 94, 89, 67, 86, 85, 82, 721, 95, 94, 93, 91, 92, 74]","[1697101617385, 1697101618256, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114]"
2210,2210,475,18,[],200,llama-13b,128,1,4169.0,1.0,1,H100,1697101642418,1697101646587.0,120,89.0,20.0,"[30, 683, 320, 99, 96, 93, 94, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 78, 74, 741, 98]","[1697101642448, 1697101643131, 1697101643451, 1697101643550, 1697101643646, 1697101643739, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587]"
2211,2211,565,19,[],200,llama-13b,128,1,4876.0,1.0,1,H100,1697101638957,1697101643833.0,120,91.0,20.0,"[12, 1117, 123, 83, 92, 735, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 800, 100, 95, 93, 95]","[1697101638969, 1697101640086, 1697101640209, 1697101640292, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833]"
2212,2212,91,6,[],200,llama-13b,128,1,1925.0,1.0,1,H100,1697101616331,1697101618256.0,120,23.0,1.0,"[287, 1637]","[1697101616618, 1697101618255]"
2213,2213,419,7,[],200,llama-13b,128,1,3800.0,1.0,1,H100,1697101618257,1697101622057.0,120,88.0,20.0,"[7, 825, 262, 95, 88, 67, 86, 85, 83, 719, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60]","[1697101618264, 1697101619089, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619855, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
2214,2214,670,11,[],200,llama-13b,128,1,4561.0,1.0,1,H100,1697101616296,1697101620857.0,120,67.0,18.0,"[6, 301, 822, 938, 82, 80, 78, 79, 668, 95, 89, 67, 86, 84, 83, 720, 95, 94, 93]","[1697101616302, 1697101616603, 1697101617425, 1697101618363, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619350, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619771, 1697101619854, 1697101620574, 1697101620669, 1697101620763, 1697101620856]"
2215,2215,96,20,[],200,llama-13b,128,1,1540.0,1.0,1,H100,1697101644759,1697101646299.0,120,31.0,1.0,"[42, 1498]","[1697101644801, 1697101646299]"
2216,2216,453,21,[],200,llama-13b,128,1,1442.0,1.0,1,H100,1697101646300,1697101647742.0,120,26.0,1.0,"[30, 1412]","[1697101646330, 1697101647742]"
2217,2217,812,22,[],200,llama-13b,128,1,1204.0,1.0,1,H100,1697101647743,1697101648947.0,120,16.0,1.0,"[24, 1180]","[1697101647767, 1697101648947]"
2218,2218,240,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651605.0,120,,,"[13, 950, 355, 106, 104, 101, 91, 85]","[1697101648961, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2219,2219,690,24,[],200,llama-13b,128,1,1862.0,1.0,1,H100,1697101651610,1697101653472.0,120,39.0,1.0,"[381, 1481]","[1697101651991, 1697101653472]"
2220,2220,115,25,[],200,llama-13b,128,1,1152.0,1.0,1,H100,1697101653479,1697101654631.0,120,13.0,1.0,"[59, 1093]","[1697101653538, 1697101654631]"
2221,2221,667,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621116,1697101623664.0,120,,,"[15, 1451, 90, 86, 87, 84, 83]","[1697101621131, 1697101622582, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
2222,2222,97,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623676,1697101626658.0,120,,,"[494, 1525, 113, 98, 69]","[1697101624170, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
2223,2223,459,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626669,1697101628904.0,120,,,"[438, 1585]","[1697101627107, 1697101628692]"
2224,2224,792,14,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,11.0,1.0,"[308, 1412]","[1697101629217, 1697101630629]"
2225,2225,706,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622059,1697101623660.0,120,,,"[37, 1476]","[1697101622096, 1697101623572]"
2226,2226,387,19,[],200,llama-13b,128,1,7121.0,1.0,1,H100,1697101635000,1697101642121.0,120,39.0,27.0,"[576, 1233, 88, 87, 56, 628, 91, 92, 87, 87, 82, 81, 767, 99, 93, 92, 92, 91, 91, 79, 618, 81, 93, 734, 86, 78, 78, 761]","[1697101635576, 1697101636809, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638955, 1697101639054, 1697101639147, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642121]"
2227,2227,95,12,[],200,llama-13b,128,1,816.0,1.0,1,H100,1697101620858,1697101621674.0,120,12.0,1.0,"[6, 810]","[1697101620864, 1697101621674]"
2228,2228,454,13,[],200,llama-13b,128,1,1339.0,1.0,1,H100,1697101621674,1697101623013.0,120,182.0,6.0,"[24, 884, 90, 87, 86, 84, 84]","[1697101621698, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
2229,2229,473,26,[],200,llama-13b,128,1,11585.0,1.0,1,H100,1697101654632,1697101666217.0,120,244.0,50.0,"[36, 1330, 129, 105, 101, 98, 98, 83, 622, 81, 79, 60, 836, 95, 82, 82, 80, 1076, 101, 99, 97, 93, 85, 805, 101, 75, 99, 93, 86, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 108, 103, 96, 94, 91, 697, 106, 100, 72, 72]","[1697101654668, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658290, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661353, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665170, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217]"
2230,2230,220,15,[],200,llama-13b,128,1,1184.0,1.0,1,H100,1697101630631,1697101631815.0,120,67.0,2.0,"[66, 988, 130]","[1697101630697, 1697101631685, 1697101631815]"
2231,2231,574,16,[],200,llama-13b,128,1,1356.0,1.0,1,H100,1697101631816,1697101633172.0,120,364.0,2.0,"[26, 1124, 206]","[1697101631842, 1697101632966, 1697101633172]"
2232,2232,815,14,[],200,llama-13b,128,1,2891.0,1.0,1,H100,1697101623014,1697101625905.0,120,52.0,4.0,"[12, 847, 734, 1201, 97]","[1697101623026, 1697101623873, 1697101624607, 1697101625808, 1697101625905]"
2233,2233,1,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633173,1697101634995.0,120,,,"[18, 1015, 96, 74, 71]","[1697101633191, 1697101634206, 1697101634302, 1697101634376, 1697101634447]"
2234,2234,359,18,[],200,llama-13b,128,1,1809.0,1.0,1,H100,1697101634999,1697101636808.0,120,10.0,1.0,"[271, 1538]","[1697101635270, 1697101636808]"
2235,2235,333,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625907,1697101626655.0,120,,,"[7, 652]","[1697101625914, 1697101626566]"
2236,2236,691,19,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101636810,1697101637479.0,120,47.0,1.0,"[72, 597]","[1697101636882, 1697101637479]"
2237,2237,694,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628903.0,120,,,"[18, 918]","[1697101626681, 1697101627599]"
2238,2238,118,17,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628909,1697101633687.0,120,85.0,20.0,"[95, 1625, 105, 81, 68, 66, 865, 92, 90, 90, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70]","[1697101629004, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687]"
2239,2239,121,20,[],200,llama-13b,128,1,1259.0,1.0,1,H100,1697101637480,1697101638739.0,120,13.0,1.0,"[54, 1205]","[1697101637534, 1697101638739]"
2240,2240,134,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609451,1697101610267.0,120,,,"[24, 629]","[1697101609475, 1697101610104]"
2241,2241,483,21,[],200,llama-13b,128,1,5093.0,1.0,1,H100,1697101638740,1697101643833.0,120,84.0,20.0,"[7, 1338, 124, 82, 93, 735, 85, 79, 77, 761, 101, 98, 96, 89, 82, 63, 800, 99, 96, 93, 95]","[1697101638747, 1697101640085, 1697101640209, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642222, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833]"
2242,2242,405,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621116,1697101623664.0,120,,,"[62, 1404, 90, 87, 86, 84, 83]","[1697101621178, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
2243,2243,498,5,[],200,llama-13b,128,1,1792.0,1.0,1,H100,1697101610277,1697101612069.0,120,9.0,1.0,"[407, 1385]","[1697101610684, 1697101612069]"
2244,2244,855,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612070,1697101616320.0,120,,,"[48, 896, 107, 90, 86, 83, 81, 62, 81, 719, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612118, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
2245,2245,740,16,[],200,llama-13b,128,1,4056.0,1.0,1,H100,1697101634997,1697101639053.0,120,563.0,14.0,"[93, 428, 66, 1313, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98]","[1697101635090, 1697101635518, 1697101635584, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053]"
2246,2246,236,21,[],200,llama-13b,128,1,1862.0,1.0,1,H100,1697101651610,1697101653472.0,120,8.0,1.0,"[497, 1365]","[1697101652107, 1697101653472]"
2247,2247,766,13,[],200,llama-13b,128,1,2022.0,1.0,1,H100,1697101623673,1697101625695.0,120,11.0,1.0,"[426, 1596]","[1697101624099, 1697101625695]"
2248,2248,162,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626656.0,120,,,"[42, 827]","[1697101625739, 1697101626566]"
2249,2249,679,22,[],200,llama-13b,128,1,1152.0,1.0,1,H100,1697101653479,1697101654631.0,120,15.0,1.0,"[29, 1123]","[1697101653508, 1697101654631]"
2250,2250,108,23,[],200,llama-13b,128,1,1495.0,1.0,1,H100,1697101654632,1697101656127.0,120,182.0,2.0,"[12, 1354, 129]","[1697101654644, 1697101655998, 1697101656127]"
2251,2251,462,24,[],200,llama-13b,128,1,1019.0,1.0,1,H100,1697101656128,1697101657147.0,120,52.0,1.0,"[12, 1006]","[1697101656140, 1697101657146]"
2252,2252,837,22,[],200,llama-13b,128,1,4546.0,1.0,1,H100,1697101643834,1697101648380.0,120,85.0,20.0,"[20, 467, 100, 86, 84, 83, 83, 683, 78, 78, 77, 75, 742, 97, 98, 83, 78, 1237, 95, 101, 101]","[1697101643854, 1697101644321, 1697101644421, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648380]"
2253,2253,522,15,[],200,llama-13b,128,1,2029.0,1.0,1,H100,1697101626664,1697101628693.0,120,20.0,1.0,"[312, 1717]","[1697101626976, 1697101628693]"
2254,2254,876,16,[],200,llama-13b,128,1,722.0,1.0,1,H100,1697101628695,1697101629417.0,120,11.0,1.0,"[59, 663]","[1697101628754, 1697101629417]"
2255,2255,304,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101629418,1697101634995.0,120,,,"[18, 2248, 130, 92, 90, 90, 89, 88, 84, 83, 741, 95, 72, 93, 93, 93, 70, 614, 75, 71]","[1697101629436, 1697101631684, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2256,2256,168,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101639056,1697101651605.0,120,,,"[11, 1019, 123, 83, 92, 735, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 800, 100, 95, 93, 95, 86, 85, 415, 87, 84, 84, 82, 683, 79, 78, 77, 75, 741, 98, 97, 84, 78, 1226, 106, 101, 100, 97, 87, 486, 96, 96, 83, 941, 107, 103, 102, 90, 86]","[1697101639067, 1697101640086, 1697101640209, 1697101640292, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648563, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
2257,2257,20,21,[],200,llama-13b,128,1,5818.0,1.0,1,H100,1697101644759,1697101650577.0,120,83.0,20.0,"[19, 1521, 190, 98, 98, 83, 78, 1236, 96, 101, 100, 97, 88, 485, 97, 96, 82, 942, 106, 103, 102]","[1697101644778, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577]"
2258,2258,803,17,[],200,llama-13b,128,1,1154.0,1.0,1,H100,1697101646588,1697101647742.0,120,20.0,1.0,"[36, 1118]","[1697101646624, 1697101647742]"
2259,2259,159,13,[],200,llama-13b,128,1,1808.0,1.0,1,H100,1697101635000,1697101636808.0,120,31.0,1.0,"[405, 1403]","[1697101635405, 1697101636808]"
2260,2260,184,21,[],200,llama-13b,128,1,4656.0,1.0,1,H100,1697101662480,1697101667136.0,120,87.0,20.0,"[7, 850, 127, 101, 93, 89, 86, 845, 108, 103, 96, 94, 92, 697, 105, 101, 71, 72, 93, 92, 734]","[1697101662487, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665171, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136]"
2261,2261,513,14,[],200,llama-13b,128,1,4309.0,1.0,1,H100,1697101636810,1697101641119.0,120,83.0,20.0,"[64, 605, 189, 91, 93, 87, 85, 83, 81, 768, 98, 92, 93, 92, 91, 91, 80, 617, 81, 93, 735]","[1697101636874, 1697101637479, 1697101637668, 1697101637759, 1697101637852, 1697101637939, 1697101638024, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640210, 1697101640291, 1697101640384, 1697101641119]"
2262,2262,234,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101647743,1697101651604.0,120,,,"[13, 1191, 102, 97, 96, 82, 942, 106, 103, 102, 91, 85]","[1697101647756, 1697101648947, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
2263,2263,476,34,[],200,llama-13b,128,1,10739.0,1.0,1,H100,1697101661917,1697101672656.0,120,6.0,50.0,"[30, 1390, 127, 101, 93, 88, 87, 845, 108, 104, 95, 94, 92, 696, 106, 100, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521]","[1697101661947, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664890, 1697101664985, 1697101665079, 1697101665171, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656]"
2264,2264,409,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621048,1697101623660.0,120,,,"[58, 567, 85, 81, 80, 78, 61, 614, 86, 87, 84, 83]","[1697101621106, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
2265,2265,377,22,[],200,llama-13b,128,1,729.0,1.0,1,H100,1697101650579,1697101651308.0,120,13.0,1.0,"[36, 693]","[1697101650615, 1697101651308]"
2266,2266,706,23,[],200,llama-13b,128,1,5219.0,1.0,1,H100,1697101651309,1697101656528.0,120,86.0,20.0,"[30, 538, 841, 892, 85, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 91, 614, 106, 100, 99, 97]","[1697101651339, 1697101651877, 1697101652718, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655512, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
2267,2267,507,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101615401,1697101616319.0,120,,,"[12, 882]","[1697101615413, 1697101616295]"
2268,2268,421,19,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634998,1697101640209.0,120,85.0,20.0,"[313, 1497, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 768, 97, 93, 93, 92, 91, 91, 79, 617]","[1697101635311, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209]"
2269,2269,769,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623666,1697101626657.0,120,,,"[247, 1781, 115, 97, 69]","[1697101623913, 1697101625694, 1697101625809, 1697101625906, 1697101625975]"
2270,2270,864,12,[],200,llama-13b,128,1,4776.0,1.0,1,H100,1697101616337,1697101621113.0,120,83.0,20.0,"[452, 1467, 108, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 96, 93, 94, 91, 93, 72]","[1697101616789, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620763, 1697101620857, 1697101620948, 1697101621041, 1697101621113]"
2271,2271,635,17,[],200,llama-13b,128,1,1698.0,1.0,1,H100,1697101637041,1697101638739.0,120,23.0,1.0,"[12, 1686]","[1697101637053, 1697101638739]"
2272,2272,63,18,[],200,llama-13b,128,1,1345.0,1.0,1,H100,1697101638741,1697101640086.0,120,39.0,1.0,"[30, 1315]","[1697101638771, 1697101640086]"
2273,2273,403,40,[],200,llama-13b,128,1,1998.0,1.0,1,H100,1697101697590,1697101699588.0,120,874.0,2.0,"[401, 1484, 113]","[1697101697991, 1697101699475, 1697101699588]"
2274,2274,760,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699589,1697101700635.0,120,,,"[6, 957]","[1697101699595, 1697101700552]"
2275,2275,191,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[47, 1474, 123, 69]","[1697101700685, 1697101702159, 1697101702282, 1697101702351]"
2276,2276,794,15,[],200,llama-13b,128,1,894.0,1.0,1,H100,1697101626663,1697101627557.0,120,11.0,1.0,"[109, 785]","[1697101626772, 1697101627557]"
2277,2277,223,16,[],200,llama-13b,128,1,1136.0,1.0,1,H100,1697101627558,1697101628694.0,120,16.0,1.0,"[36, 1099]","[1697101627594, 1697101628693]"
2278,2278,548,43,[],200,llama-13b,128,1,5354.0,1.0,1,H100,1697101703172,1697101708526.0,120,86.0,20.0,"[261, 1467, 115, 98, 72, 728, 95, 87, 85, 83, 79, 943, 97, 92, 70, 91, 70, 91, 87, 545, 98]","[1697101703433, 1697101704900, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
2279,2279,665,17,[],200,llama-13b,128,1,4993.0,1.0,1,H100,1697101628694,1697101633687.0,120,90.0,20.0,"[7, 715, 1317, 81, 69, 65, 865, 92, 91, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 92, 71]","[1697101628701, 1697101629416, 1697101630733, 1697101630814, 1697101630883, 1697101630948, 1697101631813, 1697101631905, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633687]"
2280,2280,480,19,[],200,llama-13b,128,1,307.0,1.0,1,H100,1697101626567,1697101626874.0,120,26.0,1.0,"[36, 271]","[1697101626603, 1697101626874]"
2281,2281,41,21,[],200,llama-13b,128,1,9030.0,1.0,1,H100,1697101640211,1697101649241.0,120,39.0,43.0,"[13, 778, 117, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 800, 99, 97, 92, 95, 87, 85, 415, 86, 84, 84, 82, 684, 78, 78, 77, 75, 741, 98, 97, 84, 78, 1226, 106, 101, 100, 97, 87, 486, 96, 96]","[1697101640224, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648563, 1697101649049, 1697101649145, 1697101649241]"
2282,2282,165,15,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628909,1697101633687.0,120,83.0,20.0,"[83, 1637, 105, 81, 68, 66, 865, 92, 90, 90, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70]","[1697101628992, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687]"
2283,2283,488,13,[],200,llama-13b,128,1,902.0,1.0,1,H100,1697101623666,1697101624568.0,120,6.0,1.0,"[125, 777]","[1697101623791, 1697101624568]"
2284,2284,846,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101624569,1697101626655.0,120,,,"[6, 1120, 114, 97, 69]","[1697101624575, 1697101625695, 1697101625809, 1697101625906, 1697101625975]"
2285,2285,838,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626875,1697101628905.0,120,,,"[398, 1420]","[1697101627273, 1697101628693]"
2286,2286,337,9,[],200,llama-13b,128,1,893.0,1.0,1,H100,1697101626663,1697101627556.0,120,12.0,1.0,"[25, 868]","[1697101626688, 1697101627556]"
2287,2287,691,10,[],200,llama-13b,128,1,1135.0,1.0,1,H100,1697101627558,1697101628693.0,120,47.0,1.0,"[18, 1117]","[1697101627576, 1697101628693]"
2288,2288,276,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628903.0,120,,,[214],[1697101626878]
2289,2289,239,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628910,1697101634994.0,120,,,"[392, 1327, 105, 81, 68, 66, 865, 92, 90, 90, 89, 87, 84, 83, 743, 95, 71, 93, 94, 91, 71, 614, 75, 71]","[1697101629302, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633172, 1697101633267, 1697101633338, 1697101633431, 1697101633525, 1697101633616, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2290,2290,314,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607857,1697101610275.0,120,,,"[13, 463, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607870, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
2291,2291,118,11,[],200,llama-13b,128,1,4992.0,1.0,1,H100,1697101628695,1697101633687.0,120,85.0,20.0,"[18, 704, 1316, 82, 68, 66, 864, 92, 91, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 92, 71]","[1697101628713, 1697101629417, 1697101630733, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631905, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633687]"
2292,2292,634,16,[],200,llama-13b,128,1,1721.0,1.0,1,H100,1697101628909,1697101630630.0,120,13.0,1.0,"[196, 1525]","[1697101629105, 1697101630630]"
2293,2293,772,10,[],200,llama-13b,128,1,4780.0,1.0,1,H100,1697101616334,1697101621114.0,120,83.0,20.0,"[584, 1338, 108, 81, 81, 78, 79, 668, 94, 89, 67, 86, 85, 82, 721, 95, 94, 92, 92, 93, 73]","[1697101616918, 1697101618256, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575, 1697101620670, 1697101620764, 1697101620856, 1697101620948, 1697101621041, 1697101621114]"
2294,2294,546,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628904.0,120,,,[247],[1697101626911]
2295,2295,905,16,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,11.0,1.0,"[302, 1418]","[1697101629211, 1697101630629]"
2296,2296,476,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[115, 1164]","[1697101633805, 1697101634969]"
2297,2297,645,21,[],200,llama-13b,128,1,4209.0,1.0,1,H100,1697101640211,1697101644420.0,120,86.0,20.0,"[7, 784, 117, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 800, 100, 96, 92, 95, 87, 85, 415]","[1697101640218, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420]"
2298,2298,437,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608669,1697101610266.0,120,,,"[6, 683, 91, 79, 77]","[1697101608675, 1697101609358, 1697101609449, 1697101609528, 1697101609605]"
2299,2299,714,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101620576,1697101623659.0,120,,,"[12, 1085, 85, 81, 79, 79, 60, 614, 87, 87, 85, 82]","[1697101620588, 1697101621673, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622930, 1697101623012]"
2300,2300,401,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101634377,1697101634996.0,120,,,"[12, 580]","[1697101634389, 1697101634969]"
2301,2301,171,7,[],200,llama-13b,128,1,1791.0,1.0,1,H100,1697101610278,1697101612069.0,120,6.0,1.0,"[529, 1261]","[1697101610807, 1697101612068]"
2302,2302,529,8,[],200,llama-13b,128,1,944.0,1.0,1,H100,1697101612070,1697101613014.0,120,10.0,1.0,"[24, 920]","[1697101612094, 1697101613014]"
2303,2303,887,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613015,1697101616319.0,120,,,"[6, 1116, 186, 95, 93, 92, 90, 69, 88, 633, 80, 77, 77]","[1697101613021, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615563, 1697101615640, 1697101615717]"
2304,2304,77,22,[],200,llama-13b,128,1,4821.0,1.0,1,H100,1697101644421,1697101649242.0,120,92.0,20.0,"[49, 867, 103, 79, 77, 78, 75, 740, 98, 97, 84, 78, 1236, 96, 101, 100, 97, 88, 485, 97, 96]","[1697101644470, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645749, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649242]"
2305,2305,758,22,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101635000,1697101640209.0,120,84.0,20.0,"[290, 1518, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 768, 97, 93, 93, 92, 91, 91, 79, 617]","[1697101635290, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209]"
2306,2306,661,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614852,1697101616322.0,120,,,"[13, 1429]","[1697101614865, 1697101616294]"
2307,2307,670,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101632968,1697101634995.0,120,,,"[36, 1202, 96, 74, 71]","[1697101633004, 1697101634206, 1697101634302, 1697101634376, 1697101634447]"
2308,2308,90,7,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616332,1697101617377.0,120,19.0,1.0,"[240, 805]","[1697101616572, 1697101617377]"
2309,2309,448,8,[],200,llama-13b,128,1,2394.0,1.0,1,H100,1697101617378,1697101619772.0,120,335.0,12.0,"[13, 865, 108, 82, 80, 78, 79, 668, 94, 89, 67, 86, 85]","[1697101617391, 1697101618256, 1697101618364, 1697101618446, 1697101618526, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772]"
2310,2310,139,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623666,1697101626656.0,120,,,"[135, 767, 39, 1202, 97, 69]","[1697101623801, 1697101624568, 1697101624607, 1697101625809, 1697101625906, 1697101625975]"
2311,2311,95,20,[],200,llama-13b,128,1,1809.0,1.0,1,H100,1697101635000,1697101636809.0,120,12.0,1.0,"[492, 1316]","[1697101635492, 1697101636808]"
2312,2312,719,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101609359,1697101610267.0,120,,,[6],[1697101609365]
2313,2313,429,21,[],200,llama-13b,128,1,9874.0,1.0,1,H100,1697101636810,1697101646684.0,120,244.0,50.0,"[42, 627, 189, 91, 92, 87, 86, 83, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618, 81, 93, 735, 85, 78, 78, 761, 100, 99, 96, 89, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 683, 79, 78, 77, 75, 741, 98, 97]","[1697101636852, 1697101637479, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684]"
2314,2314,147,9,[],200,llama-13b,128,1,774.0,1.0,1,H100,1697101610272,1697101611046.0,120,182.0,1.0,"[10, 764]","[1697101610282, 1697101611046]"
2315,2315,480,10,[],200,llama-13b,128,1,1022.0,1.0,1,H100,1697101611047,1697101612069.0,120,26.0,1.0,"[36, 986]","[1697101611083, 1697101612069]"
2316,2316,37,16,[],200,llama-13b,128,1,791.0,1.0,1,H100,1697101640211,1697101641002.0,120,20.0,1.0,"[7, 784]","[1697101640218, 1697101641002]"
2317,2317,367,17,[],200,llama-13b,128,1,1502.0,1.0,1,H100,1697101641003,1697101642505.0,120,92.0,6.0,"[13, 801, 304, 101, 98, 97, 88]","[1697101641016, 1697101641817, 1697101642121, 1697101642222, 1697101642320, 1697101642417, 1697101642505]"
2318,2318,502,18,[],200,llama-13b,128,1,2027.0,1.0,1,H100,1697101626666,1697101628693.0,120,19.0,1.0,"[352, 1675]","[1697101627018, 1697101628693]"
2319,2319,315,10,[],200,llama-13b,128,1,4237.0,1.0,1,H100,1697101616337,1697101620574.0,120,335.0,14.0,"[343, 1575, 109, 81, 81, 77, 79, 669, 95, 88, 67, 86, 85, 82, 720]","[1697101616680, 1697101618255, 1697101618364, 1697101618445, 1697101618526, 1697101618603, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574]"
2320,2320,859,19,[],200,llama-13b,128,1,722.0,1.0,1,H100,1697101628695,1697101629417.0,120,23.0,1.0,"[48, 674]","[1697101628743, 1697101629417]"
2321,2321,376,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101629418,1697101634994.0,120,,,"[18, 2248, 130, 92, 90, 90, 89, 88, 84, 83, 741, 95, 72, 93, 93, 93, 70, 614, 75, 71]","[1697101629436, 1697101631684, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2322,2322,725,18,[],200,llama-13b,128,1,4081.0,1.0,1,H100,1697101642506,1697101646587.0,120,90.0,20.0,"[6, 619, 320, 99, 96, 93, 94, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 78, 74, 741, 98]","[1697101642512, 1697101643131, 1697101643451, 1697101643550, 1697101643646, 1697101643739, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587]"
2323,2323,835,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612070,1697101616320.0,120,,,"[36, 908, 107, 90, 86, 83, 81, 62, 80, 720, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612106, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
2324,2324,778,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619773,1697101623658.0,120,,,"[7, 795, 95, 94, 93, 92, 92, 73, 644, 81, 79, 79, 60, 614, 88, 86, 84, 83]","[1697101619780, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620949, 1697101621041, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
2325,2325,263,12,[],200,llama-13b,128,1,1925.0,1.0,1,H100,1697101616331,1697101618256.0,120,15.0,1.0,"[386, 1539]","[1697101616717, 1697101618256]"
2326,2326,617,13,[],200,llama-13b,128,1,3800.0,1.0,1,H100,1697101618257,1697101622057.0,120,87.0,20.0,"[42, 789, 264, 94, 88, 68, 85, 85, 83, 719, 95, 95, 93, 91, 92, 74, 644, 81, 79, 79, 60]","[1697101618299, 1697101619088, 1697101619352, 1697101619446, 1697101619534, 1697101619602, 1697101619687, 1697101619772, 1697101619855, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057]"
2327,2327,207,10,[],200,llama-13b,128,1,2023.0,1.0,1,H100,1697101623671,1697101625694.0,120,10.0,1.0,"[331, 1692]","[1697101624002, 1697101625694]"
2328,2328,566,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625696,1697101626656.0,120,,,"[7, 863]","[1697101625703, 1697101626566]"
2329,2329,646,11,[],200,llama-13b,128,1,1097.0,1.0,1,H100,1697101620576,1697101621673.0,120,14.0,1.0,"[12, 1085]","[1697101620588, 1697101621673]"
2330,2330,121,19,[],200,llama-13b,128,1,801.0,1.0,1,H100,1697101630884,1697101631685.0,120,13.0,1.0,"[13, 788]","[1697101630897, 1697101631685]"
2331,2331,924,12,[],200,llama-13b,128,1,893.0,1.0,1,H100,1697101626664,1697101627557.0,120,9.0,1.0,"[206, 687]","[1697101626870, 1697101627557]"
2332,2332,323,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101627558,1697101628905.0,120,,,"[18, 1117]","[1697101627576, 1697101628693]"
2333,2333,474,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101631685,1697101634994.0,120,,,"[13, 1474, 94, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101631698, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2334,2334,480,20,[],200,llama-13b,128,1,1281.0,1.0,1,H100,1697101631686,1697101632967.0,120,26.0,1.0,"[41, 1240]","[1697101631727, 1697101632967]"
2335,2335,540,11,[],200,llama-13b,128,1,2670.0,1.0,1,H100,1697101634998,1697101637668.0,120,140.0,5.0,"[184, 1626, 89, 87, 56, 628]","[1697101635182, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668]"
2336,2336,833,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101632968,1697101634995.0,120,,,"[12, 1226, 95, 75, 71]","[1697101632980, 1697101634206, 1697101634301, 1697101634376, 1697101634447]"
2337,2337,835,20,[],200,llama-13b,128,1,4465.0,1.0,1,H100,1697101642122,1697101646587.0,120,87.0,20.0,"[7, 1002, 320, 99, 96, 92, 95, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 78, 74, 741, 98]","[1697101642129, 1697101643131, 1697101643451, 1697101643550, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587]"
2338,2338,682,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628910,1697101634994.0,120,,,"[31, 1688, 105, 81, 68, 66, 864, 93, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 93, 70, 614, 75, 71]","[1697101628941, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2339,2339,822,25,[],200,llama-13b,128,1,5228.0,1.0,1,H100,1697101657150,1697101662378.0,120,88.0,20.0,"[42, 968, 129, 96, 82, 82, 81, 1075, 101, 99, 97, 92, 86, 805, 102, 75, 104, 87, 87, 831, 107]","[1697101657192, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658630, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378]"
2340,2340,213,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688625,1697101689595.0,120,,,"[37, 909]","[1697101688662, 1697101689571]"
2341,2341,571,32,[],200,llama-13b,128,1,1590.0,1.0,1,H100,1697101689615,1697101691205.0,120,67.0,2.0,"[183, 1314, 93]","[1697101689798, 1697101691112, 1697101691205]"
2342,2342,898,12,[],200,llama-13b,128,1,1287.0,1.0,1,H100,1697101637669,1697101638956.0,120,79.0,2.0,"[7, 1063, 217]","[1697101637676, 1697101638739, 1697101638956]"
2343,2343,4,33,[],200,llama-13b,128,1,5514.0,1.0,1,H100,1697101691206,1697101696720.0,120,89.0,20.0,"[6, 2160, 107, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 85, 604, 78, 77, 831, 103, 101, 110]","[1697101691212, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720]"
2344,2344,258,22,[],200,llama-13b,128,1,10748.0,1.0,1,H100,1697101635000,1697101645748.0,120,244.0,50.0,"[473, 1335, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 94, 91, 93, 92, 90, 79, 617, 82, 93, 734, 86, 78, 78, 760, 101, 99, 96, 89, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 683, 79, 78, 77, 75]","[1697101635473, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639238, 1697101639331, 1697101639423, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748]"
2345,2345,803,17,[],200,llama-13b,128,1,1811.0,1.0,1,H100,1697101634997,1697101636808.0,120,20.0,1.0,"[119, 1692]","[1697101635116, 1697101636808]"
2346,2346,478,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[88, 1191]","[1697101633778, 1697101634969]"
2347,2347,228,18,[],200,llama-13b,128,1,4309.0,1.0,1,H100,1697101636810,1697101641119.0,120,100.0,20.0,"[18, 651, 189, 91, 92, 87, 87, 82, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618, 81, 93, 735]","[1697101636828, 1697101637479, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641119]"
2348,2348,831,19,[],200,llama-13b,128,1,1808.0,1.0,1,H100,1697101635000,1697101636808.0,120,11.0,1.0,"[429, 1379]","[1697101635429, 1697101636808]"
2349,2349,231,20,[],200,llama-13b,128,1,668.0,1.0,1,H100,1697101636811,1697101637479.0,120,13.0,1.0,"[83, 585]","[1697101636894, 1697101637479]"
2350,2350,512,13,[],200,llama-13b,128,1,1993.0,1.0,1,H100,1697101645749,1697101647742.0,120,11.0,1.0,"[7, 1986]","[1697101645756, 1697101647742]"
2351,2351,594,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101637480,1697101651604.0,120,,,"[48, 1211, 217, 97, 94, 92, 92, 92, 90, 79, 617, 82, 94, 734, 85, 78, 78, 760, 102, 98, 97, 88, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 684, 78, 78, 77, 75, 741, 98, 97, 84, 78, 1226, 106, 101, 100, 97, 87, 486, 96, 96, 83, 941, 107, 103, 102, 90, 86]","[1697101637528, 1697101638739, 1697101638956, 1697101639053, 1697101639147, 1697101639239, 1697101639331, 1697101639423, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640385, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648563, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
2352,2352,872,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101647743,1697101651603.0,120,,,"[6, 1198, 102, 97, 96, 82, 942, 106, 103, 102, 91, 85]","[1697101647749, 1697101648947, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
2353,2353,86,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101607773,1697101610274.0,120,,,"[7, 553, 95, 95, 72, 73, 94, 89, 87, 511, 79, 77]","[1697101607780, 1697101608333, 1697101608428, 1697101608523, 1697101608595, 1697101608668, 1697101608762, 1697101608851, 1697101608938, 1697101609449, 1697101609528, 1697101609605]"
2354,2354,10,16,[],200,llama-13b,128,1,3025.0,1.0,1,H100,1697101635000,1697101638025.0,120,563.0,9.0,"[514, 1295, 88, 87, 56, 628, 91, 92, 87, 87]","[1697101635514, 1697101636809, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025]"
2355,2355,809,22,[],200,llama-13b,128,1,486.0,1.0,1,H100,1697101643835,1697101644321.0,120,16.0,1.0,"[26, 460]","[1697101643861, 1697101644321]"
2356,2356,381,31,[],200,llama-13b,128,1,1964.0,1.0,1,H100,1697101667605,1697101669569.0,120,140.0,2.0,"[36, 1803, 125]","[1697101667641, 1697101669444, 1697101669569]"
2357,2357,740,32,[],200,llama-13b,128,1,3087.0,1.0,1,H100,1697101669570,1697101672657.0,120,563.0,14.0,"[6, 909, 226, 99, 71, 92, 91, 90, 511, 98, 95, 94, 93, 90, 522]","[1697101669576, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657]"
2358,2358,239,23,[],200,llama-13b,128,1,6431.0,1.0,1,H100,1697101644322,1697101650753.0,120,39.0,27.0,"[36, 979, 103, 79, 77, 77, 75, 741, 98, 98, 83, 78, 1235, 97, 101, 100, 97, 88, 485, 96, 97, 82, 942, 106, 103, 102, 90, 86]","[1697101644358, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648081, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
2359,2359,554,8,[],200,llama-13b,128,1,839.0,1.0,1,H100,1697101613298,1697101614137.0,120,26.0,1.0,"[12, 827]","[1697101613310, 1697101614137]"
2360,2360,913,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101614138,1697101616320.0,120,,,"[12, 1250, 84, 78, 79, 76]","[1697101614150, 1697101615400, 1697101615484, 1697101615562, 1697101615641, 1697101615717]"
2361,2361,398,19,[],200,llama-13b,128,1,4333.0,1.0,1,H100,1697101640087,1697101644420.0,120,87.0,20.0,"[6, 909, 117, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 800, 100, 96, 92, 95, 86, 85, 416]","[1697101640093, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643646, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644420]"
2362,2362,764,25,[],200,llama-13b,128,1,772.0,1.0,1,H100,1697101655226,1697101655998.0,120,39.0,1.0,"[6, 766]","[1697101655232, 1697101655998]"
2363,2363,194,26,[],200,llama-13b,128,1,4181.0,1.0,1,H100,1697101655999,1697101660180.0,120,335.0,16.0,"[6, 1141, 88, 81, 79, 60, 836, 95, 82, 82, 80, 1076, 101, 99, 97, 93, 85]","[1697101656005, 1697101657146, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658290, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180]"
2364,2364,591,32,[],200,llama-13b,128,1,10373.0,1.0,1,H100,1697101658290,1697101668663.0,120,874.0,47.0,"[31, 1052, 332, 102, 98, 97, 93, 85, 805, 101, 75, 99, 93, 87, 831, 107, 102, 94, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 100, 689, 105, 100, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71]","[1697101658321, 1697101659373, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662480, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663]"
2365,2365,165,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672659,1697101675812.0,120,,,"[65, 1035, 129, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672724, 1697101673759, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
2366,2366,494,34,[],200,llama-13b,128,1,2943.0,1.0,1,H100,1697101675815,1697101678758.0,120,6.0,10.0,"[484, 1081, 97, 64, 51, 763, 89, 86, 64, 84, 80]","[1697101676299, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758]"
2367,2367,754,20,[],200,llama-13b,128,1,2334.0,1.0,1,H100,1697101651607,1697101653941.0,120,88.0,7.0,"[89, 1022, 892, 86, 82, 82, 81]","[1697101651696, 1697101652718, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941]"
2368,2368,850,35,[],200,llama-13b,128,1,5291.0,1.0,1,H100,1697101678760,1697101684051.0,120,109.0,20.0,"[6, 1713, 92, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 101, 101]","[1697101678766, 1697101680479, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051]"
2369,2369,405,22,[],200,llama-13b,128,1,5950.0,1.0,1,H100,1697101650578,1697101656528.0,120,87.0,20.0,"[25, 1273, 842, 891, 86, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 91, 614, 106, 100, 99, 97]","[1697101650603, 1697101651876, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655512, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
2370,2370,21,33,[],200,llama-13b,128,1,780.0,1.0,1,H100,1697101668664,1697101669444.0,120,15.0,1.0,"[6, 774]","[1697101668670, 1697101669444]"
2371,2371,470,34,[],200,llama-13b,128,1,1266.0,1.0,1,H100,1697101669445,1697101670711.0,120,39.0,2.0,"[18, 1022, 226]","[1697101669463, 1697101670485, 1697101670711]"
2372,2372,864,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[33, 1246]","[1697101633723, 1697101634969]"
2373,2373,547,27,[],200,llama-13b,128,1,1735.0,1.0,1,H100,1697101660181,1697101661916.0,120,12.0,1.0,"[13, 1722]","[1697101660194, 1697101661916]"
2374,2374,881,44,[],200,llama-13b,128,1,1290.0,1.0,1,H100,1697101708533,1697101709823.0,120,58.0,6.0,"[37, 804, 116, 89, 83, 82, 79]","[1697101708570, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823]"
2375,2375,294,14,[],200,llama-13b,128,1,1898.0,1.0,1,H100,1697101635000,1697101636898.0,120,9.0,2.0,"[570, 1327]","[1697101635570, 1697101636897]"
2376,2376,907,28,[],200,llama-13b,128,1,1420.0,1.0,1,H100,1697101661917,1697101663337.0,120,10.0,1.0,"[24, 1396]","[1697101661941, 1697101663337]"
2377,2377,651,15,[],200,llama-13b,128,1,770.0,1.0,1,H100,1697101636899,1697101637669.0,120,457.0,2.0,"[29, 551, 190]","[1697101636928, 1697101637479, 1697101637669]"
2378,2378,426,29,[],200,llama-13b,128,1,7635.0,1.0,1,H100,1697101663338,1697101670973.0,120,79.0,36.0,"[36, 1071, 233, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 99, 72, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 89, 676, 99, 70, 93]","[1697101663374, 1697101664445, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973]"
2379,2379,187,21,[],200,llama-13b,128,1,2587.0,1.0,1,H100,1697101653942,1697101656529.0,120,161.0,6.0,"[7, 2049, 128, 106, 101, 98, 98]","[1697101653949, 1697101655998, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529]"
2380,2380,56,16,[],200,llama-13b,128,1,4747.0,1.0,1,H100,1697101637670,1697101642417.0,120,86.0,20.0,"[12, 1057, 217, 98, 93, 92, 92, 92, 90, 79, 617, 82, 94, 734, 85, 79, 77, 760, 102, 98, 97]","[1697101637682, 1697101638739, 1697101638956, 1697101639054, 1697101639147, 1697101639239, 1697101639331, 1697101639423, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640385, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642120, 1697101642222, 1697101642320, 1697101642417]"
2381,2381,367,20,[],200,llama-13b,128,1,2331.0,1.0,1,H100,1697101651610,1697101653941.0,120,92.0,6.0,"[479, 1388, 133, 86, 82, 82, 81]","[1697101652089, 1697101653477, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941]"
2382,2382,62,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630634,1697101634994.0,120,,,"[69, 982, 130, 92, 90, 89, 89, 88, 84, 83, 742, 94, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101630703, 1697101631685, 1697101631815, 1697101631907, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2383,2383,924,13,[],200,llama-13b,128,1,1808.0,1.0,1,H100,1697101635000,1697101636808.0,120,9.0,1.0,"[480, 1328]","[1697101635480, 1697101636808]"
2384,2384,357,14,[],200,llama-13b,128,1,6836.0,1.0,1,H100,1697101636809,1697101643645.0,120,52.0,33.0,"[7, 663, 189, 91, 92, 87, 87, 82, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618, 81, 93, 735, 85, 78, 78, 761, 100, 99, 96, 89, 82, 63, 800, 99, 96]","[1697101636816, 1697101637479, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645]"
2385,2385,760,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101610278,1697101616320.0,120,,,"[520, 1270, 118, 84, 82, 81, 77, 610, 90, 86, 83, 82, 63, 79, 720, 95, 93, 92, 90, 69, 87, 634, 79, 78, 76]","[1697101610798, 1697101612068, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613524, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
2386,2386,178,11,[],200,llama-13b,128,1,902.0,1.0,1,H100,1697101623666,1697101624568.0,120,11.0,1.0,"[131, 771]","[1697101623797, 1697101624568]"
2387,2387,515,13,[],200,llama-13b,128,1,2030.0,1.0,1,H100,1697101626663,1697101628693.0,120,11.0,1.0,"[356, 1674]","[1697101627019, 1697101628693]"
2388,2388,675,21,[],200,llama-13b,128,1,1296.0,1.0,1,H100,1697101641121,1697101642417.0,120,563.0,5.0,"[42, 654, 304, 101, 99, 96]","[1697101641163, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417]"
2389,2389,876,14,[],200,llama-13b,128,1,722.0,1.0,1,H100,1697101628695,1697101629417.0,120,11.0,1.0,"[12, 710]","[1697101628707, 1697101629417]"
2390,2390,767,7,[],200,llama-13b,128,1,775.0,1.0,1,H100,1697101610271,1697101611046.0,120,11.0,1.0,"[33, 742]","[1697101610304, 1697101611046]"
2391,2391,911,19,[],200,llama-13b,128,1,2584.0,1.0,1,H100,1697101661162,1697101663746.0,120,335.0,11.0,"[7, 747, 355, 107, 101, 95, 94, 89, 707, 100, 94, 88]","[1697101661169, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746]"
2392,2392,195,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101611047,1697101616320.0,120,,,"[30, 992, 117, 84, 82, 81, 78, 610, 89, 86, 84, 81, 62, 80, 720, 95, 93, 92, 90, 70, 87, 633, 79, 78, 76]","[1697101611077, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613121, 1697101613210, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614763, 1697101614850, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
2393,2393,100,22,[],200,llama-13b,128,1,3022.0,1.0,1,H100,1697101642418,1697101645440.0,120,732.0,14.0,"[30, 683, 320, 99, 96, 93, 94, 87, 85, 415, 86, 84, 84, 82, 684]","[1697101642448, 1697101643131, 1697101643451, 1697101643550, 1697101643646, 1697101643739, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440]"
2394,2394,462,23,[],200,llama-13b,128,1,858.0,1.0,1,H100,1697101645441,1697101646299.0,120,52.0,1.0,"[18, 840]","[1697101645459, 1697101646299]"
2395,2395,133,15,[],200,llama-13b,128,1,2024.0,1.0,1,H100,1697101623671,1697101625695.0,120,15.0,1.0,"[416, 1608]","[1697101624087, 1697101625695]"
2396,2396,647,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621046,1697101623659.0,120,,,"[8, 620, 84, 81, 80, 78, 61, 613, 87, 87, 84, 83]","[1697101621054, 1697101621674, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
2397,2397,386,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623664,1697101626656.0,120,,,"[13, 891, 39, 1201, 98, 68]","[1697101623677, 1697101624568, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
2398,2398,494,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626657.0,120,,,"[72, 796]","[1697101625769, 1697101626565]"
2399,2399,799,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613298,1697101616320.0,120,,,"[12, 827, 187, 94, 93, 92, 90, 69, 88, 633, 79, 78, 76]","[1697101613310, 1697101614137, 1697101614324, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
2400,2400,823,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628905.0,120,,,"[324, 1705]","[1697101626988, 1697101628693]"
2401,2401,253,18,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628909,1697101633688.0,120,67.0,20.0,"[266, 1455, 104, 81, 68, 66, 865, 92, 91, 89, 89, 88, 84, 83, 741, 95, 72, 94, 93, 92, 71]","[1697101629175, 1697101630630, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
2402,2402,78,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623673,1697101626657.0,120,,,"[419, 1603, 113, 98, 69]","[1697101624092, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
2403,2403,442,18,[],200,llama-13b,128,1,5467.0,1.0,1,H100,1697101628909,1697101634376.0,120,39.0,22.0,"[161, 1559, 105, 81, 68, 66, 865, 92, 90, 90, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70, 614, 75]","[1697101629070, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376]"
2404,2404,327,13,[],200,llama-13b,128,1,3265.0,1.0,1,H100,1697101638957,1697101642222.0,120,563.0,10.0,"[13, 1116, 123, 83, 92, 735, 85, 78, 78, 761, 101]","[1697101638970, 1697101640086, 1697101640209, 1697101640292, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222]"
2405,2405,228,9,[],200,llama-13b,128,1,4709.0,1.0,1,H100,1697101616331,1697101621040.0,120,100.0,20.0,"[55, 990, 49, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95, 94, 94, 91, 92]","[1697101616386, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620763, 1697101620857, 1697101620948, 1697101621040]"
2406,2406,86,24,[],200,llama-13b,128,1,3671.0,1.0,1,H100,1697101652661,1697101656332.0,120,335.0,17.0,"[7, 804, 138, 86, 82, 82, 81, 896, 107, 106, 101, 74, 99, 97, 92, 614, 105, 100]","[1697101652668, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332]"
2407,2407,304,15,[],200,llama-13b,128,1,4922.0,1.0,1,H100,1697101651607,1697101656529.0,120,86.0,20.0,"[89, 965, 57, 892, 86, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 97]","[1697101651696, 1697101652661, 1697101652718, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
2408,2408,170,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626666,1697101628905.0,120,,,"[412, 1615]","[1697101627078, 1697101628693]"
2409,2409,611,10,[],200,llama-13b,128,1,375.0,1.0,1,H100,1697101610105,1697101610480.0,120,14.0,1.0,"[12, 363]","[1697101610117, 1697101610480]"
2410,2410,448,25,[],200,llama-13b,128,1,3473.0,1.0,1,H100,1697101656334,1697101659807.0,120,335.0,12.0,"[6, 806, 88, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 102]","[1697101656340, 1697101657146, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659807]"
2411,2411,524,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628910,1697101634994.0,120,,,"[399, 1320, 105, 81, 68, 66, 865, 92, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 92, 71, 614, 75, 71]","[1697101629309, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2412,2412,743,26,[],200,llama-13b,128,1,2016.0,1.0,1,H100,1697101656613,1697101658629.0,120,123.0,6.0,"[58, 1489, 129, 96, 82, 82, 80]","[1697101656671, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629]"
2413,2413,150,27,[],200,llama-13b,128,1,2354.0,1.0,1,H100,1697101658631,1697101660985.0,120,216.0,2.0,"[6, 2348]","[1697101658637, 1697101660985]"
2414,2414,508,28,[],200,llama-13b,128,1,4988.0,1.0,1,H100,1697101660986,1697101665974.0,120,86.0,20.0,"[6, 924, 355, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 109, 102, 96, 94, 101, 688, 105]","[1697101660992, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973]"
2415,2415,798,7,[],200,llama-13b,128,1,2345.0,1.0,1,H100,1697101616337,1697101618682.0,120,79.0,6.0,"[446, 1581, 81, 80, 78, 79]","[1697101616783, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682]"
2416,2416,127,11,[],200,llama-13b,128,1,1952.0,1.0,1,H100,1697101610481,1697101612433.0,120,100.0,5.0,"[400, 1188, 117, 84, 82, 81]","[1697101610881, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433]"
2417,2417,309,10,[],200,llama-13b,128,1,4708.0,1.0,1,H100,1697101616332,1697101621040.0,120,52.0,20.0,"[147, 898, 48, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95, 95, 93, 91, 92]","[1697101616479, 1697101617377, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620764, 1697101620857, 1697101620948, 1697101621040]"
2418,2418,229,8,[],200,llama-13b,128,1,1694.0,1.0,1,H100,1697101618684,1697101620378.0,120,15.0,1.0,"[6, 1688]","[1697101618690, 1697101620378]"
2419,2419,582,9,[],200,llama-13b,128,1,1293.0,1.0,1,H100,1697101620380,1697101621673.0,120,19.0,1.0,"[24, 1269]","[1697101620404, 1697101621673]"
2420,2420,7,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621674,1697101623664.0,120,,,"[30, 878, 90, 87, 86, 85, 83]","[1697101621704, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622930, 1697101623013]"
2421,2421,880,17,[],200,llama-13b,128,1,1899.0,1.0,1,H100,1697101634998,1697101636897.0,120,84.0,2.0,"[371, 1438, 90]","[1697101635369, 1697101636807, 1697101636897]"
2422,2422,309,18,[],200,llama-13b,128,1,4221.0,1.0,1,H100,1697101636898,1697101641119.0,120,52.0,20.0,"[8, 573, 189, 91, 93, 87, 85, 83, 81, 768, 98, 92, 93, 92, 91, 91, 79, 617, 82, 93, 735]","[1697101636906, 1697101637479, 1697101637668, 1697101637759, 1697101637852, 1697101637939, 1697101638024, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640384, 1697101641119]"
2423,2423,526,18,[],200,llama-13b,128,1,5002.0,1.0,1,H100,1697101651610,1697101656612.0,120,89.0,20.0,"[471, 1391, 138, 86, 82, 82, 81, 896, 107, 106, 101, 74, 99, 97, 92, 614, 105, 101, 98, 97, 83]","[1697101652081, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611]"
2424,2424,293,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621116,1697101623669.0,120,,,"[9, 1456, 91, 86, 87, 84, 83]","[1697101621125, 1697101622581, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
2425,2425,453,11,[],200,llama-13b,128,1,2025.0,1.0,1,H100,1697101623670,1697101625695.0,120,26.0,1.0,"[411, 1613]","[1697101624081, 1697101625694]"
2426,2426,341,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694733,1697101697587.0,120,,,"[66, 537, 85, 77, 77, 831, 103, 101, 110, 92, 91]","[1697101694799, 1697101695336, 1697101695421, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
2427,2427,831,35,[],200,llama-13b,128,1,843.0,1.0,1,H100,1697101670717,1697101671560.0,120,11.0,1.0,"[40, 802]","[1697101670757, 1697101671559]"
2428,2428,851,23,[],200,llama-13b,128,1,616.0,1.0,1,H100,1697101656530,1697101657146.0,120,23.0,1.0,"[24, 592]","[1697101656554, 1697101657146]"
2429,2429,363,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622059,1697101623660.0,120,,,"[25, 1488]","[1697101622084, 1697101623572]"
2430,2430,669,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697597,1697101700635.0,120,,,"[400, 1479, 112, 83, 82, 81, 81, 82]","[1697101697997, 1697101699476, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
2431,2431,262,36,[],200,llama-13b,128,1,985.0,1.0,1,H100,1697101671561,1697101672546.0,120,39.0,1.0,"[42, 943]","[1697101671603, 1697101672546]"
2432,2432,723,10,[],200,llama-13b,128,1,2028.0,1.0,1,H100,1697101623666,1697101625694.0,120,14.0,1.0,"[234, 1794]","[1697101623900, 1697101625694]"
2433,2433,151,11,[],200,llama-13b,128,1,869.0,1.0,1,H100,1697101625697,1697101626566.0,120,39.0,1.0,"[54, 815]","[1697101625751, 1697101626566]"
2434,2434,279,24,[],200,llama-13b,128,1,4290.0,1.0,1,H100,1697101657150,1697101661440.0,120,67.0,18.0,"[7, 1003, 129, 96, 82, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 102, 75, 104, 87, 87]","[1697101657157, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440]"
2435,2435,45,10,[],200,llama-13b,128,1,2029.0,1.0,1,H100,1697101623665,1697101625694.0,120,19.0,1.0,"[218, 1811]","[1697101623883, 1697101625694]"
2436,2436,616,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672548,1697101675811.0,120,,,"[29, 1181, 130, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672577, 1697101673758, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
2437,2437,40,38,[],200,llama-13b,128,1,4296.0,1.0,1,H100,1697101675814,1697101680110.0,120,86.0,20.0,"[114, 452, 37, 1060, 64, 50, 763, 90, 86, 64, 84, 80, 647, 98, 98, 95, 71, 93, 90, 70, 90]","[1697101675928, 1697101676380, 1697101676417, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679860, 1697101679950, 1697101680020, 1697101680110]"
2438,2438,484,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626567,1697101628903.0,120,,,"[6, 301, 725]","[1697101626573, 1697101626874, 1697101627599]"
2439,2439,404,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625696,1697101626656.0,120,,,"[37, 833]","[1697101625733, 1697101626566]"
2440,2440,370,39,[],200,llama-13b,128,1,1390.0,1.0,1,H100,1697101680112,1697101681502.0,120,31.0,1.0,"[30, 1360]","[1697101680142, 1697101681502]"
2441,2441,847,13,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,10.0,1.0,"[20, 1700]","[1697101628929, 1697101630629]"
2442,2442,275,14,[],200,llama-13b,128,1,1366.0,1.0,1,H100,1697101630631,1697101631997.0,120,161.0,4.0,"[14, 1039, 130, 92, 91]","[1697101630645, 1697101631684, 1697101631814, 1697101631906, 1697101631997]"
2443,2443,754,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628903.0,120,,,"[209, 728]","[1697101626872, 1697101627600]"
2444,2444,184,13,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628909,1697101633687.0,120,87.0,20.0,"[101, 1619, 105, 81, 68, 66, 865, 92, 90, 90, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70]","[1697101629010, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687]"
2445,2445,731,40,[],200,llama-13b,128,1,3824.0,1.0,1,H100,1697101681503,1697101685327.0,120,89.0,20.0,"[18, 990, 193, 96, 93, 92, 69, 90, 90, 615, 101, 101, 97, 92, 90, 90, 593, 87, 63, 83, 81]","[1697101681521, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685327]"
2446,2446,123,19,[],200,llama-13b,128,1,1008.0,1.0,1,H100,1697101642123,1697101643131.0,120,14.0,1.0,"[18, 990]","[1697101642141, 1697101643131]"
2447,2447,486,20,[],200,llama-13b,128,1,5244.0,1.0,1,H100,1697101643136,1697101648380.0,120,14.0,20.0,"[33, 1152, 99, 86, 84, 84, 83, 683, 79, 77, 78, 74, 741, 98, 97, 84, 78, 1236, 96, 101, 100]","[1697101643169, 1697101644321, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644757, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379]"
2448,2448,721,21,[],200,llama-13b,128,1,2489.0,1.0,1,H100,1697101653942,1697101656431.0,120,286.0,5.0,"[7, 2049, 128, 106, 101, 98]","[1697101653949, 1697101655998, 1697101656126, 1697101656232, 1697101656333, 1697101656431]"
2449,2449,426,20,[],200,llama-13b,128,1,9375.0,1.0,1,H100,1697101651610,1697101660985.0,120,79.0,36.0,"[369, 1493, 138, 85, 83, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 97, 83, 623, 81, 79, 59, 836, 95, 83, 81, 81, 1076, 101, 99, 97, 92, 86, 805]","[1697101651979, 1697101653472, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985]"
2450,2450,920,11,[],200,llama-13b,128,1,2298.0,1.0,1,H100,1697101623677,1697101625975.0,120,96.0,4.0,"[511, 1507, 113, 99, 68]","[1697101624188, 1697101625695, 1697101625808, 1697101625907, 1697101625975]"
2451,2451,302,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101629418,1697101634994.0,120,,,"[12, 2254, 130, 92, 90, 90, 89, 88, 84, 83, 741, 95, 72, 93, 93, 93, 70, 615, 74, 71]","[1697101629430, 1697101631684, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
2452,2452,341,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625976,1697101628903.0,120,,,"[7, 891, 725]","[1697101625983, 1697101626874, 1697101627599]"
2453,2453,407,23,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101649243,1697101649912.0,120,16.0,1.0,"[54, 615]","[1697101649297, 1697101649912]"
2454,2454,765,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649913,1697101651604.0,120,,,"[42, 1353]","[1697101649955, 1697101651308]"
2455,2455,820,24,[],200,llama-13b,128,1,2845.0,1.0,1,H100,1697101646300,1697101649145.0,120,161.0,9.0,"[12, 1430, 340, 96, 101, 101, 96, 88, 485, 96]","[1697101646312, 1697101647742, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145]"
2456,2456,185,25,[],200,llama-13b,128,1,4921.0,1.0,1,H100,1697101651608,1697101656529.0,120,93.0,20.0,"[194, 916, 891, 87, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 98]","[1697101651802, 1697101652718, 1697101653609, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656529]"
2457,2457,253,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649146,1697101651605.0,120,,,"[7, 758, 355, 106, 104, 101, 91, 85]","[1697101649153, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2458,2458,808,18,[],200,llama-13b,128,1,1314.0,1.0,1,H100,1697101646769,1697101648083.0,120,286.0,2.0,"[18, 955, 340]","[1697101646787, 1697101647742, 1697101648082]"
2459,2459,744,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628904.0,120,,,"[307, 1722]","[1697101626971, 1697101628693]"
2460,2460,169,17,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,10.0,1.0,"[290, 1430]","[1697101629199, 1697101630629]"
2461,2461,524,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630631,1697101634995.0,120,,,"[18, 1035, 130, 92, 91, 89, 89, 88, 84, 83, 741, 95, 72, 94, 93, 92, 70, 615, 74, 71]","[1697101630649, 1697101631684, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
2462,2462,544,26,[],200,llama-13b,128,1,617.0,1.0,1,H100,1697101656530,1697101657147.0,120,26.0,1.0,"[48, 568]","[1697101656578, 1697101657146]"
2463,2463,902,27,[],200,llama-13b,128,1,2852.0,1.0,1,H100,1697101657150,1697101660002.0,120,335.0,10.0,"[36, 974, 129, 96, 82, 82, 81, 1075, 101, 99, 97]","[1697101657186, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658630, 1697101659705, 1697101659806, 1697101659905, 1697101660002]"
2464,2464,578,26,[],200,llama-13b,128,1,1868.0,1.0,1,H100,1697101651610,1697101653478.0,120,31.0,1.0,"[462, 1405]","[1697101652072, 1697101653477]"
2465,2465,715,15,[],200,llama-13b,128,1,673.0,1.0,1,H100,1697101643647,1697101644320.0,120,20.0,1.0,"[6, 667]","[1697101643653, 1697101644320]"
2466,2466,140,16,[],200,llama-13b,128,1,4921.0,1.0,1,H100,1697101644321,1697101649242.0,120,96.0,20.0,"[7, 1009, 103, 78, 78, 77, 75, 742, 97, 98, 83, 78, 1237, 95, 101, 101, 96, 88, 485, 96, 96]","[1697101644328, 1697101645337, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241]"
2467,2467,173,18,[],200,llama-13b,128,1,3636.0,1.0,1,H100,1697101641121,1697101644757.0,120,96.0,20.0,"[12, 684, 304, 101, 99, 96, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 87, 84, 83, 83]","[1697101641133, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757]"
2468,2468,534,19,[],200,llama-13b,128,1,5818.0,1.0,1,H100,1697101644759,1697101650577.0,120,96.0,20.0,"[60, 1480, 190, 98, 98, 83, 78, 1226, 106, 101, 100, 97, 88, 486, 96, 96, 82, 942, 106, 104, 101]","[1697101644819, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577]"
2469,2469,499,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649243,1697101651603.0,120,,,"[18, 650, 355, 106, 104, 101, 91, 86]","[1697101649261, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650754]"
2470,2470,868,21,[],200,llama-13b,128,1,6118.0,1.0,1,H100,1697101635001,1697101641119.0,120,85.0,20.0,"[593, 1884, 190, 91, 92, 87, 87, 82, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618, 81, 93, 735]","[1697101635594, 1697101637478, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641119]"
2471,2471,655,15,[],200,llama-13b,128,1,2099.0,1.0,1,H100,1697101632348,1697101634447.0,120,335.0,11.0,"[6, 613, 205, 95, 71, 94, 93, 92, 70, 614, 75, 71]","[1697101632354, 1697101632967, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2472,2472,294,22,[],200,llama-13b,128,1,908.0,1.0,1,H100,1697101640212,1697101641120.0,120,9.0,2.0,"[58, 850]","[1697101640270, 1697101641120]"
2473,2473,824,18,[],200,llama-13b,128,1,2089.0,1.0,1,H100,1697101651607,1697101653696.0,120,58.0,4.0,"[95, 959, 57, 892, 86]","[1697101651702, 1697101652661, 1697101652718, 1697101653610, 1697101653696]"
2474,2474,648,23,[],200,llama-13b,128,1,3636.0,1.0,1,H100,1697101641121,1697101644757.0,120,84.0,20.0,"[64, 632, 304, 101, 99, 96, 88, 83, 63, 799, 100, 95, 93, 95, 87, 85, 415, 86, 85, 83, 83]","[1697101641185, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642588, 1697101642651, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644591, 1697101644674, 1697101644757]"
2475,2475,539,21,[],200,llama-13b,128,1,4736.0,1.0,1,H100,1697101656530,1697101661266.0,120,83.0,20.0,"[66, 551, 87, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 102, 98, 97, 93, 85, 805, 101, 76, 102]","[1697101656596, 1697101657147, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661264]"
2476,2476,746,28,[],200,llama-13b,128,1,3912.0,1.0,1,H100,1697101661268,1697101665180.0,120,345.0,18.0,"[36, 612, 356, 106, 101, 95, 94, 90, 706, 101, 93, 88, 87, 845, 109, 103, 95, 94, 101]","[1697101661304, 1697101661916, 1697101662272, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662758, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787, 1697101664890, 1697101664985, 1697101665079, 1697101665180]"
2477,2477,875,15,[],200,llama-13b,128,1,6961.0,1.0,1,H100,1697101641121,1697101648082.0,120,31.0,31.0,"[52, 644, 304, 101, 99, 96, 88, 82, 64, 799, 100, 95, 93, 95, 87, 85, 415, 87, 84, 83, 83, 683, 79, 77, 77, 75, 741, 98, 97, 84, 78, 1231]","[1697101641173, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642651, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648077]"
2478,2478,903,7,[],200,llama-13b,128,1,2074.0,1.0,1,H100,1697101611047,1697101613121.0,120,244.0,7.0,"[18, 1004, 117, 84, 82, 81, 78, 610]","[1697101611065, 1697101612069, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612511, 1697101613121]"
2479,2479,447,8,[],200,llama-13b,128,1,3321.0,1.0,1,H100,1697101610282,1697101613603.0,120,161.0,13.0,"[587, 1317, 84, 82, 81, 77, 610, 90, 86, 83, 82, 62, 80]","[1697101610869, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613120, 1697101613210, 1697101613296, 1697101613379, 1697101613461, 1697101613523, 1697101613603]"
2480,2480,758,20,[],200,llama-13b,128,1,4821.0,1.0,1,H100,1697101644421,1697101649242.0,120,84.0,20.0,"[36, 880, 103, 79, 77, 78, 75, 740, 98, 97, 84, 78, 1237, 95, 101, 100, 97, 88, 485, 96, 97]","[1697101644457, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645749, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242]"
2481,2481,557,8,[],200,llama-13b,128,1,637.0,1.0,1,H100,1697101614763,1697101615400.0,120,31.0,1.0,"[25, 612]","[1697101614788, 1697101615400]"
2482,2482,204,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626567,1697101628903.0,120,,,"[18, 289, 725]","[1697101626585, 1697101626874, 1697101627599]"
2483,2483,914,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101615401,1697101616319.0,120,,,"[30, 864]","[1697101615431, 1697101616295]"
2484,2484,565,19,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628908,1697101633687.0,120,91.0,20.0,"[9, 1712, 105, 81, 68, 66, 864, 93, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 93, 70]","[1697101628917, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687]"
2485,2485,276,7,[],200,llama-13b,128,1,3332.0,1.0,1,H100,1697101610271,1697101613603.0,120,732.0,13.0,"[227, 1688, 84, 82, 81, 77, 611, 90, 85, 84, 81, 62, 80]","[1697101610498, 1697101612186, 1697101612270, 1697101612352, 1697101612433, 1697101612510, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603]"
2486,2486,145,17,[],200,llama-13b,128,1,3026.0,1.0,1,H100,1697101634998,1697101638024.0,120,161.0,9.0,"[325, 1485, 89, 87, 56, 628, 91, 92, 87, 86]","[1697101635323, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024]"
2487,2487,428,10,[],200,llama-13b,128,1,3122.0,1.0,1,H100,1697101616324,1697101619446.0,120,31.0,9.0,"[279, 774, 48, 939, 81, 81, 78, 78, 668, 96]","[1697101616603, 1697101617377, 1697101617425, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619350, 1697101619446]"
2488,2488,424,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613122,1697101616320.0,120,,,"[12, 1003, 186, 95, 93, 92, 90, 69, 88, 633, 79, 78, 76]","[1697101613134, 1697101614137, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
2489,2489,99,21,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101651610,1697101652661.0,120,10.0,1.0,"[218, 833]","[1697101651828, 1697101652661]"
2490,2490,428,22,[],200,llama-13b,128,1,2388.0,1.0,1,H100,1697101652662,1697101655050.0,120,31.0,9.0,"[48, 767, 133, 86, 83, 81, 81, 896, 108, 105]","[1697101652710, 1697101653477, 1697101653610, 1697101653696, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050]"
2491,2491,276,24,[],200,llama-13b,128,1,3811.0,1.0,1,H100,1697101651610,1697101655421.0,120,732.0,13.0,"[399, 1601, 85, 83, 82, 81, 896, 108, 105, 101, 74, 100, 96]","[1697101652009, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421]"
2492,2492,787,23,[],200,llama-13b,128,1,1478.0,1.0,1,H100,1697101655051,1697101656529.0,120,123.0,6.0,"[13, 934, 129, 105, 101, 98, 98]","[1697101655064, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529]"
2493,2493,782,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619447,1697101623658.0,120,,,"[12, 919, 197, 95, 94, 93, 92, 92, 73, 644, 81, 79, 78, 61, 615, 87, 86, 84, 83]","[1697101619459, 1697101620378, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620949, 1697101621041, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621996, 1697101622057, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
2494,2494,505,18,[],200,llama-13b,128,1,5713.0,1.0,1,H100,1697101638025,1697101643738.0,120,100.0,27.0,"[7, 925, 97, 93, 92, 92, 92, 90, 79, 617, 82, 94, 734, 85, 79, 77, 761, 101, 98, 97, 88, 82, 63, 801, 98, 96, 93]","[1697101638032, 1697101638957, 1697101639054, 1697101639147, 1697101639239, 1697101639331, 1697101639423, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640385, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643549, 1697101643645, 1697101643738]"
2495,2495,630,8,[],200,llama-13b,128,1,1795.0,1.0,1,H100,1697101613605,1697101615400.0,120,6.0,1.0,"[6, 1789]","[1697101613611, 1697101615400]"
2496,2496,63,9,[],200,llama-13b,128,1,894.0,1.0,1,H100,1697101615401,1697101616295.0,120,39.0,1.0,"[18, 876]","[1697101615419, 1697101616295]"
2497,2497,421,10,[],200,llama-13b,128,1,4744.0,1.0,1,H100,1697101616296,1697101621040.0,120,85.0,20.0,"[24, 1056, 49, 939, 81, 80, 78, 79, 668, 95, 89, 67, 86, 85, 82, 720, 95, 94, 94, 91, 92]","[1697101616320, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619350, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620763, 1697101620857, 1697101620948, 1697101621040]"
2498,2498,633,25,[],200,llama-13b,128,1,4580.0,1.0,1,H100,1697101655422,1697101660002.0,120,90.0,20.0,"[7, 569, 129, 105, 101, 98, 98, 83, 622, 81, 79, 60, 836, 95, 82, 82, 80, 1076, 101, 99, 97]","[1697101655429, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658290, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002]"
2499,2499,782,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621046,1697101623660.0,120,,,"[19, 608, 85, 81, 80, 78, 61, 614, 86, 87, 84, 83]","[1697101621065, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
2500,2500,100,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703171.0,120,,,"[138, 1383, 123, 69]","[1697101700776, 1697101702159, 1697101702282, 1697101702351]"
2501,2501,63,26,[],200,llama-13b,128,1,643.0,1.0,1,H100,1697101660004,1697101660647.0,120,39.0,1.0,"[30, 613]","[1697101660034, 1697101660647]"
2502,2502,421,27,[],200,llama-13b,128,1,5325.0,1.0,1,H100,1697101660648,1697101665973.0,120,85.0,20.0,"[12, 1256, 355, 107, 101, 95, 94, 89, 707, 100, 94, 89, 86, 845, 109, 102, 96, 94, 101, 688, 105]","[1697101660660, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973]"
2503,2503,782,9,[],200,llama-13b,128,1,4709.0,1.0,1,H100,1697101616331,1697101621040.0,120,90.0,20.0,"[62, 983, 49, 939, 81, 80, 78, 79, 669, 94, 89, 67, 86, 85, 82, 720, 95, 94, 94, 91, 92]","[1697101616393, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620763, 1697101620857, 1697101620948, 1697101621040]"
2504,2504,457,39,[],200,llama-13b,128,1,1841.0,1.0,1,H100,1697101703175,1697101705016.0,120,874.0,2.0,"[31, 1809]","[1697101703206, 1697101705015]"
2505,2505,820,40,[],200,llama-13b,128,1,2363.0,1.0,1,H100,1697101705019,1697101707382.0,120,161.0,9.0,"[10, 749, 135, 95, 87, 85, 83, 80, 942, 96]","[1697101705029, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707381]"
2506,2506,245,41,[],200,llama-13b,128,1,4564.0,1.0,1,H100,1697101707383,1697101711947.0,120,100.0,20.0,"[6, 924, 115, 99, 64, 88, 811, 89, 83, 82, 79, 882, 103, 96, 93, 89, 88, 86, 498, 98, 91]","[1697101707389, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947]"
2507,2507,404,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649243,1697101651605.0,120,,,"[6, 662, 355, 106, 104, 101, 91, 85]","[1697101649249, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2508,2508,351,34,[],200,llama-13b,128,1,2632.0,1.0,1,H100,1697101675813,1697101678445.0,120,216.0,6.0,"[374, 1193, 97, 64, 50, 764, 90]","[1697101676187, 1697101677380, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678445]"
2509,2509,714,35,[],200,llama-13b,128,1,4258.0,1.0,1,H100,1697101678446,1697101682704.0,120,83.0,20.0,"[18, 824, 117, 99, 98, 94, 72, 92, 90, 70, 91, 460, 103, 57, 73, 804, 84, 83, 79, 79, 771]","[1697101678464, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704]"
2510,2510,732,23,[],200,llama-13b,128,1,3715.0,1.0,1,H100,1697101651610,1697101655325.0,120,345.0,12.0,"[507, 1355, 138, 86, 82, 82, 81, 896, 107, 106, 100, 75, 99]","[1697101652117, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324]"
2511,2511,774,28,[],200,llama-13b,128,1,796.0,1.0,1,H100,1697101665974,1697101666770.0,120,8.0,1.0,"[7, 789]","[1697101665981, 1697101666770]"
2512,2512,180,29,[],200,llama-13b,128,1,3175.0,1.0,1,H100,1697101666771,1697101669946.0,120,123.0,12.0,"[18, 1474, 133, 101, 95, 71, 91, 90, 724, 99, 95, 92, 92]","[1697101666789, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669762, 1697101669854, 1697101669946]"
2513,2513,292,11,[],200,llama-13b,128,1,1465.0,1.0,1,H100,1697101621116,1697101622581.0,120,286.0,1.0,"[153, 1312]","[1697101621269, 1697101622581]"
2514,2514,338,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630631,1697101634994.0,120,,,"[55, 999, 129, 93, 90, 89, 89, 88, 84, 83, 741, 95, 72, 94, 93, 92, 70, 615, 74, 71]","[1697101630686, 1697101631685, 1697101631814, 1697101631907, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
2515,2515,649,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622583,1697101623672.0,120,,,"[12, 978]","[1697101622595, 1697101623573]"
2516,2516,597,22,[],200,llama-13b,128,1,521.0,1.0,1,H100,1697101634997,1697101635518.0,120,39.0,1.0,"[95, 426]","[1697101635092, 1697101635518]"
2517,2517,22,23,[],200,llama-13b,128,1,1959.0,1.0,1,H100,1697101635519,1697101637478.0,120,16.0,1.0,"[81, 1878]","[1697101635600, 1697101637478]"
2518,2518,423,9,[],200,llama-13b,128,1,4782.0,1.0,1,H100,1697101616332,1697101621114.0,120,84.0,20.0,"[366, 1558, 108, 81, 81, 77, 79, 669, 95, 88, 67, 86, 85, 82, 720, 96, 93, 94, 91, 93, 73]","[1697101616698, 1697101618256, 1697101618364, 1697101618445, 1697101618526, 1697101618603, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620763, 1697101620857, 1697101620948, 1697101621041, 1697101621114]"
2519,2519,542,22,[],200,llama-13b,128,1,616.0,1.0,1,H100,1697101656531,1697101657147.0,120,11.0,1.0,"[77, 539]","[1697101656608, 1697101657147]"
2520,2520,385,24,[],200,llama-13b,128,1,9204.0,1.0,1,H100,1697101637480,1697101646684.0,120,52.0,43.0,"[48, 1212, 216, 97, 94, 92, 92, 91, 91, 77, 619, 82, 94, 734, 85, 78, 78, 760, 102, 98, 97, 88, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 684, 78, 78, 77, 75, 741, 97, 98]","[1697101637528, 1697101638740, 1697101638956, 1697101639053, 1697101639147, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639590, 1697101640209, 1697101640291, 1697101640385, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646586, 1697101646684]"
2521,2521,910,7,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616332,1697101617377.0,120,8.0,1.0,"[188, 856]","[1697101616520, 1697101617376]"
2522,2522,871,23,[],200,llama-13b,128,1,1479.0,1.0,1,H100,1697101657151,1697101658630.0,120,123.0,6.0,"[47, 962, 129, 96, 82, 82, 81]","[1697101657198, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658630]"
2523,2523,342,8,[],200,llama-13b,128,1,3197.0,1.0,1,H100,1697101617378,1697101620575.0,120,364.0,14.0,"[18, 860, 108, 82, 80, 78, 79, 668, 94, 89, 67, 86, 85, 82, 721]","[1697101617396, 1697101618256, 1697101618364, 1697101618446, 1697101618526, 1697101618604, 1697101618683, 1697101619351, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620575]"
2524,2524,777,10,[],200,llama-13b,128,1,1466.0,1.0,1,H100,1697101621116,1697101622582.0,120,9.0,1.0,"[14, 1452]","[1697101621130, 1697101622582]"
2525,2525,591,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101616296,1697101623658.0,120,,,"[37, 1043, 49, 939, 81, 80, 78, 79, 668, 95, 89, 67, 86, 85, 82, 720, 95, 94, 94, 91, 92, 73, 644, 81, 80, 78, 61, 614, 87, 86, 85, 83]","[1697101616333, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619350, 1697101619445, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620669, 1697101620763, 1697101620857, 1697101620948, 1697101621040, 1697101621113, 1697101621757, 1697101621838, 1697101621918, 1697101621996, 1697101622057, 1697101622671, 1697101622758, 1697101622844, 1697101622929, 1697101623012]"
2526,2526,515,22,[],200,llama-13b,128,1,697.0,1.0,1,H100,1697101641121,1697101641818.0,120,11.0,1.0,"[70, 626]","[1697101641191, 1697101641817]"
2527,2527,869,23,[],200,llama-13b,128,1,2856.0,1.0,1,H100,1697101641818,1697101644674.0,120,244.0,12.0,"[19, 1294, 319, 100, 96, 92, 95, 87, 85, 415, 86, 84, 84]","[1697101641837, 1697101643131, 1697101643450, 1697101643550, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674]"
2528,2528,696,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646769,1697101651603.0,120,,,"[13, 960, 341, 96, 100, 101, 96, 88, 485, 97, 96, 82, 941, 107, 103, 102, 91, 85]","[1697101646782, 1697101647742, 1697101648083, 1697101648179, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
2529,2529,206,19,[],200,llama-13b,128,1,791.0,1.0,1,H100,1697101640211,1697101641002.0,120,16.0,1.0,"[25, 766]","[1697101640236, 1697101641002]"
2530,2530,566,20,[],200,llama-13b,128,1,7561.0,1.0,1,H100,1697101641003,1697101648564.0,120,109.0,36.0,"[54, 760, 304, 101, 98, 97, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 87, 83, 84, 83, 683, 78, 78, 77, 75, 741, 98, 97, 84, 78, 1226, 106, 101, 100, 97, 88]","[1697101641057, 1697101641817, 1697101642121, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644590, 1697101644674, 1697101644757, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564]"
2531,2531,781,21,[],200,llama-13b,128,1,2672.0,1.0,1,H100,1697101660986,1697101663658.0,120,335.0,10.0,"[6, 924, 355, 107, 101, 95, 94, 89, 707, 100, 94]","[1697101660992, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658]"
2532,2532,734,21,[],200,llama-13b,128,1,2762.0,1.0,1,H100,1697101634997,1697101637759.0,120,100.0,6.0,"[205, 1606, 89, 87, 56, 628, 91]","[1697101635202, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759]"
2533,2533,124,21,[],200,llama-13b,128,1,1112.0,1.0,1,H100,1697101651606,1697101652718.0,120,83.0,2.0,"[45, 1067]","[1697101651651, 1697101652718]"
2534,2534,155,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646588,1697101651603.0,120,,,"[25, 1129, 340, 96, 101, 101, 96, 88, 485, 97, 95, 83, 942, 106, 104, 101, 91, 85]","[1697101646613, 1697101647742, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2535,2535,162,22,[],200,llama-13b,128,1,4657.0,1.0,1,H100,1697101637760,1697101642417.0,120,90.0,20.0,"[7, 972, 217, 98, 93, 92, 92, 92, 90, 80, 616, 82, 94, 734, 85, 79, 77, 761, 101, 98, 97]","[1697101637767, 1697101638739, 1697101638956, 1697101639054, 1697101639147, 1697101639239, 1697101639331, 1697101639423, 1697101639513, 1697101639593, 1697101640209, 1697101640291, 1697101640385, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642222, 1697101642320, 1697101642417]"
2536,2536,186,11,[],200,llama-13b,128,1,5508.0,1.0,1,H100,1697101616331,1697101621839.0,120,123.0,22.0,"[441, 1484, 108, 81, 80, 78, 79, 669, 95, 88, 67, 86, 85, 82, 720, 96, 93, 94, 91, 93, 73, 644, 81]","[1697101616772, 1697101618256, 1697101618364, 1697101618445, 1697101618525, 1697101618603, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620763, 1697101620857, 1697101620948, 1697101621041, 1697101621114, 1697101621758, 1697101621839]"
2537,2537,75,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621674,1697101623664.0,120,,,"[25, 883, 90, 87, 86, 85, 83]","[1697101621699, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622930, 1697101623013]"
2538,2538,436,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623670,1697101626657.0,120,,,"[399, 1625, 114, 98, 69]","[1697101624069, 1697101625694, 1697101625808, 1697101625906, 1697101625975]"
2539,2539,545,12,[],200,llama-13b,128,1,1091.0,1.0,1,H100,1697101621839,1697101622930.0,120,216.0,5.0,"[7, 736, 90, 87, 87, 84]","[1697101621846, 1697101622582, 1697101622672, 1697101622759, 1697101622846, 1697101622930]"
2540,2540,791,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626670,1697101628905.0,120,,,[536],[1697101627206]
2541,2541,191,15,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628910,1697101633688.0,120,85.0,20.0,"[368, 1351, 105, 81, 68, 66, 865, 92, 90, 90, 89, 87, 84, 83, 743, 95, 71, 94, 93, 92, 71]","[1697101629278, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
2542,2542,674,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101668498,1697101675811.0,120,,,"[6, 940, 125, 98, 95, 92, 93, 88, 676, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 91, 521, 113, 91, 68, 67, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101668504, 1697101669444, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
2543,2543,920,21,[],200,llama-13b,128,1,1911.0,1.0,1,H100,1697101648565,1697101650476.0,120,96.0,4.0,"[6, 1340, 355, 106, 104]","[1697101648571, 1697101649911, 1697101650266, 1697101650372, 1697101650476]"
2544,2544,506,20,[],200,llama-13b,128,1,1054.0,1.0,1,H100,1697101651607,1697101652661.0,120,16.0,1.0,"[95, 959]","[1697101651702, 1697101652661]"
2545,2545,863,21,[],200,llama-13b,128,1,816.0,1.0,1,H100,1697101652662,1697101653478.0,120,10.0,1.0,"[54, 762]","[1697101652716, 1697101653478]"
2546,2546,264,22,[],200,llama-13b,128,1,4810.0,1.0,1,H100,1697101653479,1697101658289.0,120,86.0,20.0,"[91, 1061, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 82, 623, 81, 79, 60, 835]","[1697101653570, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
2547,2547,899,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622931,1697101623664.0,120,,,"[6, 635]","[1697101622937, 1697101623572]"
2548,2548,924,20,[],200,llama-13b,128,1,486.0,1.0,1,H100,1697101643835,1697101644321.0,120,9.0,1.0,"[26, 460]","[1697101643861, 1697101644321]"
2549,2549,352,21,[],200,llama-13b,128,1,1197.0,1.0,1,H100,1697101644322,1697101645519.0,120,11.0,3.0,"[48, 1070, 79]","[1697101644370, 1697101645440, 1697101645519]"
2550,2550,236,19,[],200,llama-13b,128,1,853.0,1.0,1,H100,1697101648094,1697101648947.0,120,8.0,1.0,"[6, 847]","[1697101648100, 1697101648947]"
2551,2551,591,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648949,1697101651605.0,120,,,"[48, 914, 355, 106, 104, 101, 91, 85]","[1697101648997, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2552,2552,129,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101634999,1697101651604.0,120,,,"[283, 1526, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 768, 97, 93, 93, 92, 91, 91, 79, 617, 82, 93, 734, 86, 78, 78, 760, 101, 99, 96, 89, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 683, 79, 78, 77, 75, 741, 98, 97, 84, 78, 1226, 106, 101, 100, 97, 87, 486, 96, 96, 83, 941, 107, 103, 102, 90, 86]","[1697101635282, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648563, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
2553,2553,266,21,[],200,llama-13b,128,1,1154.0,1.0,1,H100,1697101646588,1697101647742.0,120,9.0,1.0,"[30, 1124]","[1697101646618, 1697101647742]"
2554,2554,627,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101647743,1697101651604.0,120,,,"[24, 1282, 97, 96, 82, 942, 106, 103, 102, 90, 86]","[1697101647767, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
2555,2555,110,15,[],200,llama-13b,128,1,2042.0,1.0,1,H100,1697101634998,1697101637040.0,120,96.0,4.0,"[304, 1506, 89, 87, 56]","[1697101635302, 1697101636808, 1697101636897, 1697101636984, 1697101637040]"
2556,2556,50,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101650578,1697101651604.0,120,,,"[19, 711]","[1697101650597, 1697101651308]"
2557,2557,405,21,[],200,llama-13b,128,1,5001.0,1.0,1,H100,1697101651610,1697101656611.0,120,87.0,20.0,"[370, 1492, 138, 85, 83, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 97, 83]","[1697101651980, 1697101653472, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611]"
2558,2558,439,15,[],200,llama-13b,128,1,1072.0,1.0,1,H100,1697101640211,1697101641283.0,120,13.0,4.0,"[43, 748, 117, 85, 78]","[1697101640254, 1697101641002, 1697101641119, 1697101641204, 1697101641282]"
2559,2559,468,16,[],200,llama-13b,128,1,5375.0,1.0,1,H100,1697101637041,1697101642416.0,120,31.0,20.0,"[6, 1692, 217, 98, 92, 93, 92, 91, 91, 80, 616, 82, 93, 734, 86, 78, 78, 761, 100, 99, 96]","[1697101637047, 1697101638739, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416]"
2560,2560,803,16,[],200,llama-13b,128,1,534.0,1.0,1,H100,1697101641284,1697101641818.0,120,20.0,1.0,"[6, 528]","[1697101641290, 1697101641818]"
2561,2561,227,17,[],200,llama-13b,128,1,6359.0,1.0,1,H100,1697101641819,1697101648178.0,120,364.0,25.0,"[24, 1288, 320, 99, 96, 92, 95, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 77, 75, 741, 98, 97, 84, 78, 1226, 106]","[1697101641843, 1697101643131, 1697101643451, 1697101643550, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178]"
2562,2562,373,38,[],200,llama-13b,128,1,807.0,1.0,1,H100,1697101674982,1697101675789.0,120,15.0,1.0,"[36, 771]","[1697101675018, 1697101675789]"
2563,2563,56,23,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101651608,1697101656528.0,120,86.0,20.0,"[19, 1033, 58, 891, 86, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 92, 613, 106, 100, 99, 97]","[1697101651627, 1697101652660, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
2564,2564,615,21,[],200,llama-13b,128,1,4821.0,1.0,1,H100,1697101644421,1697101649242.0,120,93.0,20.0,"[30, 989, 79, 77, 78, 75, 740, 98, 97, 84, 78, 1237, 95, 101, 100, 97, 88, 485, 96, 97]","[1697101644451, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645749, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242]"
2565,2565,698,39,[],200,llama-13b,128,1,2564.0,1.0,1,H100,1697101675790,1697101678354.0,120,182.0,6.0,"[18, 163, 445, 1061, 64, 50, 763]","[1697101675808, 1697101675971, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354]"
2566,2566,131,40,[],200,llama-13b,128,1,932.0,1.0,1,H100,1697101678356,1697101679288.0,120,8.0,1.0,"[6, 926]","[1697101678362, 1697101679288]"
2567,2567,334,34,[],200,llama-13b,128,1,674.0,1.0,1,H100,1697101696722,1697101697396.0,120,15.0,1.0,"[30, 644]","[1697101696752, 1697101697396]"
2568,2568,688,35,[],200,llama-13b,128,1,2274.0,1.0,1,H100,1697101697397,1697101699671.0,120,345.0,4.0,"[12, 363, 566, 1249, 84]","[1697101697409, 1697101697772, 1697101698338, 1697101699587, 1697101699671]"
2569,2569,759,22,[],200,llama-13b,128,1,5765.0,1.0,1,H100,1697101656613,1697101662378.0,120,92.0,20.0,"[65, 1482, 129, 96, 82, 82, 80, 1077, 100, 99, 97, 93, 85, 805, 102, 75, 104, 87, 87, 831, 107]","[1697101656678, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659706, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378]"
2570,2570,490,41,[],200,llama-13b,128,1,1515.0,1.0,1,H100,1697101679289,1697101680804.0,120,11.0,5.0,"[12, 1270, 103, 58, 72]","[1697101679301, 1697101680571, 1697101680674, 1697101680732, 1697101680804]"
2571,2571,827,17,[],200,llama-13b,128,1,4169.0,1.0,1,H100,1697101642418,1697101646587.0,120,96.0,20.0,"[18, 695, 319, 100, 96, 93, 94, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 78, 74, 741, 98]","[1697101642436, 1697101643131, 1697101643450, 1697101643550, 1697101643646, 1697101643739, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587]"
2572,2572,112,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699672,1697101700635.0,120,,,"[7, 873]","[1697101699679, 1697101700552]"
2573,2573,364,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101638027,1697101651604.0,120,,,"[11, 702, 217, 97, 93, 92, 92, 92, 90, 79, 617, 82, 94, 734, 85, 79, 77, 761, 101, 98, 97, 88, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 683, 79, 78, 77, 75, 741, 98, 97, 84, 78, 1226, 106, 101, 100, 97, 87, 486, 96, 96, 83, 941, 107, 103, 102, 90, 86]","[1697101638038, 1697101638740, 1697101638957, 1697101639054, 1697101639147, 1697101639239, 1697101639331, 1697101639423, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640385, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648563, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
2574,2574,471,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[58, 1463, 123, 69]","[1697101700696, 1697101702159, 1697101702282, 1697101702351]"
2575,2575,261,29,[],200,llama-13b,128,1,1955.0,1.0,1,H100,1697101665181,1697101667136.0,120,874.0,2.0,"[18, 1937]","[1697101665199, 1697101667136]"
2576,2576,571,24,[],200,llama-13b,128,1,1964.0,1.0,1,H100,1697101650754,1697101652718.0,120,67.0,2.0,"[7, 1116, 841]","[1697101650761, 1697101651877, 1697101652718]"
2577,2577,4,25,[],200,llama-13b,128,1,5570.0,1.0,1,H100,1697101652719,1697101658289.0,120,89.0,20.0,"[9, 1903, 206, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83, 623, 81, 79, 60, 835]","[1697101652728, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
2578,2578,807,26,[],200,llama-13b,128,1,4979.0,1.0,1,H100,1697101659808,1697101664787.0,120,90.0,20.0,"[6, 832, 339, 102, 75, 98, 93, 87, 831, 107, 101, 95, 94, 89, 708, 100, 93, 88, 87, 845, 109]","[1697101659814, 1697101660646, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663465, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787]"
2579,2579,159,20,[],200,llama-13b,128,1,1810.0,1.0,1,H100,1697101634998,1697101636808.0,120,31.0,1.0,"[380, 1430]","[1697101635378, 1697101636808]"
2580,2580,518,21,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101636810,1697101637479.0,120,23.0,1.0,"[42, 627]","[1697101636852, 1697101637479]"
2581,2581,847,22,[],200,llama-13b,128,1,1260.0,1.0,1,H100,1697101637480,1697101638740.0,120,10.0,1.0,"[30, 1229]","[1697101637510, 1697101638739]"
2582,2582,267,23,[],200,llama-13b,128,1,5092.0,1.0,1,H100,1697101638741,1697101643833.0,120,83.0,20.0,"[24, 1321, 123, 83, 92, 735, 85, 79, 77, 761, 101, 99, 95, 89, 82, 63, 800, 99, 96, 93, 95]","[1697101638765, 1697101640086, 1697101640209, 1697101640292, 1697101640384, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833]"
2583,2583,652,14,[],200,llama-13b,128,1,2019.0,1.0,1,H100,1697101623676,1697101625695.0,120,14.0,1.0,"[440, 1579]","[1697101624116, 1697101625695]"
2584,2584,814,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626657.0,120,,,"[77, 792]","[1697101625774, 1697101626566]"
2585,2585,52,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626656.0,120,,,"[95, 774]","[1697101625792, 1697101626566]"
2586,2586,413,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626665,1697101628905.0,120,,,"[419, 1609]","[1697101627084, 1697101628693]"
2587,2587,363,26,[],200,llama-13b,128,1,5367.0,1.0,1,H100,1697101658291,1697101663658.0,120,286.0,22.0,"[30, 1052, 332, 102, 98, 97, 93, 85, 805, 101, 75, 99, 93, 87, 831, 107, 102, 94, 94, 89, 707, 101, 93]","[1697101658321, 1697101659373, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662480, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658]"
2588,2588,247,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626673,1697101628905.0,120,,,"[548, 1472]","[1697101627221, 1697101628693]"
2589,2589,495,16,[],200,llama-13b,128,1,1279.0,1.0,1,H100,1697101633690,1697101634969.0,120,13.0,1.0,"[49, 1230]","[1697101633739, 1697101634969]"
2590,2590,769,17,[],200,llama-13b,128,1,4777.0,1.0,1,H100,1697101628910,1697101633687.0,120,47.0,20.0,"[466, 1254, 104, 81, 68, 66, 865, 92, 90, 90, 89, 87, 85, 83, 741, 95, 72, 93, 93, 93, 70]","[1697101629376, 1697101630630, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687]"
2591,2591,849,17,[],200,llama-13b,128,1,547.0,1.0,1,H100,1697101634971,1697101635518.0,120,10.0,1.0,"[44, 503]","[1697101635015, 1697101635518]"
2592,2592,278,18,[],200,llama-13b,128,1,1960.0,1.0,1,H100,1697101635519,1697101637479.0,120,13.0,1.0,"[93, 1867]","[1697101635612, 1697101637479]"
2593,2593,606,14,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628910,1697101630630.0,120,9.0,1.0,"[423, 1297]","[1697101629333, 1697101630630]"
2594,2594,31,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630634,1697101634994.0,120,,,"[58, 993, 129, 93, 90, 89, 89, 88, 84, 83, 741, 95, 72, 94, 93, 92, 70, 615, 74, 71]","[1697101630692, 1697101631685, 1697101631814, 1697101631907, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
2595,2595,633,19,[],200,llama-13b,128,1,4936.0,1.0,1,H100,1697101637480,1697101642416.0,120,90.0,20.0,"[18, 1241, 217, 98, 92, 93, 92, 91, 91, 79, 617, 82, 93, 735, 85, 78, 78, 761, 101, 98, 96]","[1697101637498, 1697101638739, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642320, 1697101642416]"
2596,2596,296,12,[],200,llama-13b,128,1,2023.0,1.0,1,H100,1697101623672,1697101625695.0,120,6.0,1.0,"[439, 1584]","[1697101624111, 1697101625695]"
2597,2597,654,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626656.0,120,,,"[78, 791]","[1697101625775, 1697101626566]"
2598,2598,630,25,[],200,llama-13b,128,1,1896.0,1.0,1,H100,1697101661441,1697101663337.0,120,6.0,1.0,"[7, 1889]","[1697101661448, 1697101663337]"
2599,2599,663,17,[],200,llama-13b,128,1,4169.0,1.0,1,H100,1697101642418,1697101646587.0,120,79.0,20.0,"[6, 707, 319, 100, 96, 93, 94, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 78, 74, 741, 98]","[1697101642424, 1697101643131, 1697101643450, 1697101643550, 1697101643646, 1697101643739, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587]"
2600,2600,60,26,[],200,llama-13b,128,1,7635.0,1.0,1,H100,1697101663338,1697101670973.0,120,93.0,36.0,"[24, 1316, 109, 102, 96, 94, 101, 688, 105, 101, 71, 72, 93, 92, 734, 103, 99, 72, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 89, 676, 99, 70, 93]","[1697101663362, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973]"
2601,2601,79,14,[],200,llama-13b,128,1,892.0,1.0,1,H100,1697101626664,1697101627556.0,120,12.0,1.0,"[126, 766]","[1697101626790, 1697101627556]"
2602,2602,439,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101627557,1697101628905.0,120,,,"[7, 1129]","[1697101627564, 1697101628693]"
2603,2603,799,16,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628910,1697101633688.0,120,84.0,20.0,"[283, 1436, 105, 81, 68, 66, 865, 92, 91, 89, 89, 88, 84, 83, 742, 94, 72, 94, 93, 92, 71]","[1697101629193, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
2604,2604,783,30,[],200,llama-13b,128,1,586.0,1.0,1,H100,1697101670974,1697101671560.0,120,286.0,1.0,"[13, 573]","[1697101670987, 1697101671560]"
2605,2605,218,31,[],200,llama-13b,128,1,1524.0,1.0,1,H100,1697101671561,1697101673085.0,120,109.0,7.0,"[36, 1060, 113, 91, 68, 67, 89]","[1697101671597, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673085]"
2606,2606,537,30,[],200,llama-13b,128,1,3942.0,1.0,1,H100,1697101669947,1697101673889.0,120,83.0,20.0,"[7, 531, 226, 99, 71, 92, 91, 90, 512, 97, 95, 94, 93, 90, 521, 114, 91, 68, 68, 88, 804]","[1697101669954, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673889]"
2607,2607,572,32,[],200,llama-13b,128,1,1762.0,1.0,1,H100,1697101673086,1697101674848.0,120,16.0,1.0,"[18, 1744]","[1697101673104, 1697101674848]"
2608,2608,0,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674849,1697101675810.0,120,,,"[24, 916]","[1697101674873, 1697101675789]"
2609,2609,331,34,[],200,llama-13b,128,1,567.0,1.0,1,H100,1697101675813,1697101676380.0,120,26.0,1.0,"[84, 483]","[1697101675897, 1697101676380]"
2610,2610,83,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623677,1697101626657.0,120,,,"[523, 1495, 113, 99, 68]","[1697101624200, 1697101625695, 1697101625808, 1697101625907, 1697101625975]"
2611,2611,687,35,[],200,llama-13b,128,1,4423.0,1.0,1,H100,1697101676381,1697101680804.0,120,96.0,20.0,"[51, 1815, 107, 90, 86, 64, 84, 81, 646, 98, 98, 95, 71, 92, 92, 69, 90, 470, 94, 57, 73]","[1697101676432, 1697101678247, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679951, 1697101680020, 1697101680110, 1697101680580, 1697101680674, 1697101680731, 1697101680804]"
2612,2612,60,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646588,1697101651603.0,120,,,"[13, 1482, 95, 101, 101, 96, 88, 485, 96, 96, 83, 942, 106, 104, 101, 91, 85]","[1697101646601, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2613,2613,513,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[37, 1242]","[1697101633727, 1697101634969]"
2614,2614,878,15,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101635000,1697101640209.0,120,83.0,20.0,"[423, 1385, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 94, 92, 92, 91, 91, 79, 617]","[1697101635423, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209]"
2615,2615,417,17,[],200,llama-13b,128,1,713.0,1.0,1,H100,1697101642418,1697101643131.0,120,17.0,1.0,"[36, 677]","[1697101642454, 1697101643131]"
2616,2616,775,18,[],200,llama-13b,128,1,1185.0,1.0,1,H100,1697101643136,1697101644321.0,120,17.0,1.0,"[22, 1162]","[1697101643158, 1697101644320]"
2617,2617,205,19,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101644322,1697101649242.0,120,87.0,20.0,"[18, 997, 103, 79, 77, 77, 75, 741, 98, 98, 83, 78, 1237, 95, 101, 101, 96, 88, 485, 96, 97]","[1697101644340, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242]"
2618,2618,118,36,[],200,llama-13b,128,1,4521.0,1.0,1,H100,1697101680806,1697101685327.0,120,85.0,20.0,"[19, 1686, 193, 96, 93, 92, 68, 91, 90, 614, 102, 101, 97, 92, 90, 90, 594, 86, 63, 83, 80]","[1697101680825, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685326]"
2619,2619,418,19,[],200,llama-13b,128,1,2004.0,1.0,1,H100,1697101651606,1697101653610.0,120,286.0,3.0,"[28, 1027, 57, 892]","[1697101651634, 1697101652661, 1697101652718, 1697101653610]"
2620,2620,777,20,[],200,llama-13b,128,1,1020.0,1.0,1,H100,1697101653611,1697101654631.0,120,9.0,1.0,"[6, 1014]","[1697101653617, 1697101654631]"
2621,2621,210,21,[],200,llama-13b,128,1,1495.0,1.0,1,H100,1697101654632,1697101656127.0,120,140.0,2.0,"[12, 1354, 129]","[1697101654644, 1697101655998, 1697101656127]"
2622,2622,742,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646685,1697101651603.0,120,,,"[13, 1044, 340, 96, 101, 101, 96, 88, 485, 97, 95, 83, 942, 106, 103, 102, 91, 85]","[1697101646698, 1697101647742, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
2623,2623,542,22,[],200,llama-13b,128,1,1019.0,1.0,1,H100,1697101656128,1697101657147.0,120,11.0,1.0,"[12, 1006]","[1697101656140, 1697101657146]"
2624,2624,901,23,[],200,llama-13b,128,1,1009.0,1.0,1,H100,1697101657151,1697101658160.0,120,17.0,1.0,"[53, 956]","[1697101657204, 1697101658160]"
2625,2625,701,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101620576,1697101623659.0,120,,,"[18, 1079, 85, 81, 80, 78, 60, 614, 87, 87, 85, 82]","[1697101620594, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622930, 1697101623012]"
2626,2626,326,24,[],200,llama-13b,128,1,3192.0,1.0,1,H100,1697101658161,1697101661353.0,120,345.0,12.0,"[13, 1199, 332, 101, 99, 97, 92, 86, 805, 101, 76, 98, 93]","[1697101658174, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353]"
2627,2627,475,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685328,1697101689605.0,120,,,"[31, 1663, 215, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 92, 88]","[1697101685359, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688993]"
2628,2628,212,11,[],200,llama-13b,128,1,990.0,1.0,1,H100,1697101622583,1697101623573.0,120,31.0,1.0,"[12, 978]","[1697101622595, 1697101623573]"
2629,2629,191,23,[],200,llama-13b,128,1,4207.0,1.0,1,H100,1697101640212,1697101644419.0,120,85.0,20.0,"[64, 726, 118, 84, 79, 77, 761, 100, 99, 96, 89, 82, 63, 800, 99, 97, 92, 95, 87, 85, 414]","[1697101640276, 1697101641002, 1697101641120, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644419]"
2630,2630,569,12,[],200,llama-13b,128,1,300.0,1.0,1,H100,1697101623574,1697101623874.0,120,16.0,1.0,"[41, 259]","[1697101623615, 1697101623874]"
2631,2631,0,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623875,1697101626655.0,120,,,"[415, 1405, 114, 97, 69]","[1697101624290, 1697101625695, 1697101625809, 1697101625906, 1697101625975]"
2632,2632,144,26,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101651608,1697101656528.0,120,96.0,20.0,"[14, 1038, 58, 891, 86, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 92, 613, 106, 100, 99, 97]","[1697101651622, 1697101652660, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
2633,2633,631,16,[],200,llama-13b,128,1,10675.0,1.0,1,H100,1697101634998,1697101645673.0,120,216.0,50.0,"[101, 485, 1313, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 92, 93, 91, 91, 79, 616, 83, 93, 734, 86, 78, 78, 760, 101, 99, 96, 89, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 683, 79, 78, 77]","[1697101635099, 1697101635584, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645439, 1697101645518, 1697101645596, 1697101645673]"
2634,2634,126,10,[],200,llama-13b,128,1,2027.0,1.0,1,H100,1697101623668,1697101625695.0,120,19.0,1.0,"[238, 1788]","[1697101623906, 1697101625694]"
2635,2635,480,11,[],200,llama-13b,128,1,869.0,1.0,1,H100,1697101625697,1697101626566.0,120,26.0,1.0,"[18, 851]","[1697101625715, 1697101626566]"
2636,2636,774,8,[],200,llama-13b,128,1,1513.0,1.0,1,H100,1697101622059,1697101623572.0,120,8.0,1.0,"[49, 1464]","[1697101622108, 1697101623572]"
2637,2637,200,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623573,1697101626656.0,120,,,"[18, 283, 733, 1201, 98, 68]","[1697101623591, 1697101623874, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
2638,2638,810,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626567,1697101628903.0,120,,,"[24, 283, 725]","[1697101626591, 1697101626874, 1697101627599]"
2639,2639,632,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101634377,1697101634996.0,120,,,"[6, 586]","[1697101634383, 1697101634969]"
2640,2640,64,15,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101635000,1697101640209.0,120,89.0,20.0,"[435, 1373, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 94, 91, 93, 91, 91, 76, 620]","[1697101635435, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639589, 1697101640209]"
2641,2641,327,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623672,1697101626657.0,120,,,"[414, 1608, 114, 98, 69]","[1697101624086, 1697101625694, 1697101625808, 1697101625906, 1697101625975]"
2642,2642,242,13,[],200,llama-13b,128,1,3177.0,1.0,1,H100,1697101628909,1697101632086.0,120,345.0,9.0,"[70, 1650, 105, 81, 68, 66, 865, 92, 90, 90]","[1697101628979, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086]"
2643,2643,661,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628903.0,120,,,"[156, 738, 42]","[1697101626819, 1697101627557, 1697101627599]"
2644,2644,93,16,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628909,1697101633687.0,120,88.0,20.0,"[72, 1648, 105, 81, 68, 66, 865, 92, 90, 90, 89, 87, 85, 82, 742, 95, 72, 94, 92, 93, 70]","[1697101628981, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633524, 1697101633617, 1697101633687]"
2645,2645,450,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634995.0,120,,,"[13, 1266]","[1697101633703, 1697101634969]"
2646,2646,806,18,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101635000,1697101640209.0,120,89.0,20.0,"[528, 1281, 88, 87, 56, 628, 91, 92, 87, 87, 81, 82, 767, 98, 94, 91, 93, 92, 90, 80, 616]","[1697101635528, 1697101636809, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639238, 1697101639331, 1697101639423, 1697101639513, 1697101639593, 1697101640209]"
2647,2647,436,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626665,1697101628905.0,120,,,"[407, 1621]","[1697101627072, 1697101628693]"
2648,2648,791,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628910,1697101634994.0,120,,,"[405, 1419, 81, 68, 66, 865, 92, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 93, 70, 614, 75, 71]","[1697101629315, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2649,2649,425,28,[],200,llama-13b,128,1,4784.0,1.0,1,H100,1697101660003,1697101664787.0,120,88.0,20.0,"[19, 624, 339, 102, 75, 98, 93, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 89, 86, 845, 109]","[1697101660022, 1697101660646, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664787]"
2650,2650,419,16,[],200,llama-13b,128,1,4207.0,1.0,1,H100,1697101640212,1697101644419.0,120,88.0,20.0,"[64, 726, 118, 84, 79, 77, 761, 100, 99, 96, 89, 82, 63, 801, 98, 97, 92, 96, 86, 85, 414]","[1697101640276, 1697101641002, 1697101641120, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643549, 1697101643646, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644419]"
2651,2651,703,22,[],200,llama-13b,128,1,779.0,1.0,1,H100,1697101645520,1697101646299.0,120,12.0,1.0,"[6, 773]","[1697101645526, 1697101646299]"
2652,2652,611,19,[],200,llama-13b,128,1,1280.0,1.0,1,H100,1697101633690,1697101634970.0,120,14.0,1.0,"[118, 1162]","[1697101633808, 1697101634970]"
2653,2653,266,23,[],200,llama-13b,128,1,566.0,1.0,1,H100,1697101648381,1697101648947.0,120,9.0,1.0,"[30, 536]","[1697101648411, 1697101648947]"
2654,2654,216,15,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634997,1697101640208.0,120,91.0,20.0,"[193, 1618, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 80, 615]","[1697101635190, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640208]"
2655,2655,658,24,[],200,llama-13b,128,1,1047.0,1.0,1,H100,1697101668397,1697101669444.0,120,11.0,1.0,"[6, 1041]","[1697101668403, 1697101669444]"
2656,2656,104,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646300,1697101651605.0,120,,,"[18, 1765, 96, 100, 101, 96, 88, 485, 96, 96, 83, 942, 106, 104, 102, 90, 85]","[1697101646318, 1697101648083, 1697101648179, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650578, 1697101650668, 1697101650753]"
2657,2657,41,20,[],200,llama-13b,128,1,9535.0,1.0,1,H100,1697101634971,1697101644506.0,120,39.0,43.0,"[48, 499, 66, 1313, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 92, 93, 91, 91, 79, 616, 83, 93, 734, 85, 79, 78, 760, 101, 99, 96, 89, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87]","[1697101635019, 1697101635518, 1697101635584, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208, 1697101640291, 1697101640384, 1697101641118, 1697101641203, 1697101641282, 1697101641360, 1697101642120, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506]"
2658,2658,157,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619773,1697101623658.0,120,,,"[12, 594, 196, 95, 94, 93, 92, 92, 73, 644, 81, 79, 79, 60, 614, 88, 86, 84, 83]","[1697101619785, 1697101620379, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620949, 1697101621041, 1697101621114, 1697101621758, 1697101621839, 1697101621918, 1697101621997, 1697101622057, 1697101622671, 1697101622759, 1697101622845, 1697101622929, 1697101623012]"
2659,2659,88,25,[],200,llama-13b,128,1,4443.0,1.0,1,H100,1697101669445,1697101673888.0,120,58.0,20.0,"[6, 1034, 226, 99, 71, 92, 90, 91, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 67, 88, 804]","[1697101669451, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671063, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888]"
2660,2660,645,12,[],200,llama-13b,128,1,4790.0,1.0,1,H100,1697101616324,1697101621114.0,120,86.0,20.0,"[350, 1581, 109, 81, 81, 77, 79, 669, 95, 88, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93, 73]","[1697101616674, 1697101618255, 1697101618364, 1697101618445, 1697101618526, 1697101618603, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041, 1697101621114]"
2661,2661,797,19,[],200,llama-13b,128,1,592.0,1.0,1,H100,1697101634377,1697101634969.0,120,26.0,1.0,"[13, 579]","[1697101634390, 1697101634969]"
2662,2662,226,20,[],200,llama-13b,128,1,10468.0,1.0,1,H100,1697101634971,1697101645439.0,120,216.0,47.0,"[98, 449, 65, 1314, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 92, 93, 91, 91, 80, 615, 83, 93, 734, 86, 78, 78, 760, 101, 99, 96, 89, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 683]","[1697101635069, 1697101635518, 1697101635583, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640208, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645439]"
2663,2663,462,24,[],200,llama-13b,128,1,1862.0,1.0,1,H100,1697101651610,1697101653472.0,120,52.0,1.0,"[312, 1550]","[1697101651922, 1697101653472]"
2664,2664,821,25,[],200,llama-13b,128,1,4811.0,1.0,1,H100,1697101653478,1697101658289.0,120,85.0,20.0,"[13, 1140, 206, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83, 623, 81, 79, 60, 835]","[1697101653491, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
2665,2665,921,31,[],200,llama-13b,128,1,753.0,1.0,1,H100,1697101667510,1697101668263.0,120,31.0,1.0,"[18, 735]","[1697101667528, 1697101668263]"
2666,2666,323,32,[],200,llama-13b,128,1,4393.0,1.0,1,H100,1697101668264,1697101672657.0,120,84.0,20.0,"[18, 1162, 125, 98, 94, 93, 93, 88, 676, 99, 70, 93, 90, 90, 513, 97, 95, 94, 93, 91, 521]","[1697101668282, 1697101669444, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672657]"
2667,2667,80,13,[],200,llama-13b,128,1,1466.0,1.0,1,H100,1697101621116,1697101622582.0,120,13.0,1.0,"[56, 1410]","[1697101621172, 1697101622582]"
2668,2668,437,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622583,1697101623663.0,120,,,"[24, 965]","[1697101622607, 1697101623572]"
2669,2669,683,33,[],200,llama-13b,128,1,1231.0,1.0,1,H100,1697101672658,1697101673889.0,120,874.0,2.0,"[60, 1170]","[1697101672718, 1697101673888]"
2670,2670,798,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623671,1697101626657.0,120,,,"[330, 1807, 98, 69]","[1697101624001, 1697101625808, 1697101625906, 1697101625975]"
2671,2671,108,34,[],200,llama-13b,128,1,1091.0,1.0,1,H100,1697101673890,1697101674981.0,120,182.0,2.0,"[10, 948, 133]","[1697101673900, 1697101674848, 1697101674981]"
2672,2672,280,7,[],200,llama-13b,128,1,4709.0,1.0,1,H100,1697101616332,1697101621041.0,120,91.0,20.0,"[200, 844, 49, 939, 81, 81, 78, 78, 669, 95, 88, 67, 86, 85, 82, 720, 96, 94, 93, 91, 93]","[1697101616532, 1697101617376, 1697101617425, 1697101618364, 1697101618445, 1697101618526, 1697101618604, 1697101618682, 1697101619351, 1697101619446, 1697101619534, 1697101619601, 1697101619687, 1697101619772, 1697101619854, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621041]"
2673,2673,467,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674982,1697101675810.0,120,,,[13],[1697101674995]
2674,2674,199,16,[],200,llama-13b,128,1,2025.0,1.0,1,H100,1697101626668,1697101628693.0,120,13.0,1.0,"[508, 1517]","[1697101627176, 1697101628693]"
2675,2675,821,36,[],200,llama-13b,128,1,4757.0,1.0,1,H100,1697101675814,1697101680571.0,120,85.0,20.0,"[197, 1369, 97, 64, 51, 762, 90, 86, 65, 83, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101676011, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678354, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
2676,2676,136,24,[],200,llama-13b,128,1,616.0,1.0,1,H100,1697101656530,1697101657146.0,120,31.0,1.0,"[36, 580]","[1697101656566, 1697101657146]"
2677,2677,494,25,[],200,llama-13b,128,1,2852.0,1.0,1,H100,1697101657150,1697101660002.0,120,6.0,10.0,"[30, 980, 129, 96, 82, 82, 81, 1075, 101, 99, 97]","[1697101657180, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658630, 1697101659705, 1697101659806, 1697101659905, 1697101660002]"
2678,2678,295,22,[],200,llama-13b,128,1,1000.0,1.0,1,H100,1697101641121,1697101642121.0,120,52.0,2.0,"[12, 684, 304]","[1697101641133, 1697101641817, 1697101642121]"
2679,2679,651,23,[],200,llama-13b,128,1,1328.0,1.0,1,H100,1697101642122,1697101643450.0,120,457.0,2.0,"[13, 996, 319]","[1697101642135, 1697101643131, 1697101643450]"
2680,2680,848,42,[],200,llama-13b,128,1,1705.0,1.0,1,H100,1697101680806,1697101682511.0,120,47.0,1.0,"[48, 1657]","[1697101680854, 1697101682511]"
2681,2681,276,43,[],200,llama-13b,128,1,2815.0,1.0,1,H100,1697101682512,1697101685327.0,120,732.0,13.0,"[30, 1307, 102, 100, 97, 92, 90, 90, 593, 86, 64, 82, 82]","[1697101682542, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685327]"
2682,2682,584,19,[],200,llama-13b,128,1,696.0,1.0,1,H100,1697101641121,1697101641817.0,120,10.0,1.0,"[24, 672]","[1697101641145, 1697101641817]"
2683,2683,76,24,[],200,llama-13b,128,1,3038.0,1.0,1,H100,1697101643452,1697101646490.0,120,364.0,12.0,"[6, 863, 99, 87, 83, 84, 83, 682, 79, 78, 78, 74, 742]","[1697101643458, 1697101644321, 1697101644420, 1697101644507, 1697101644590, 1697101644674, 1697101644757, 1697101645439, 1697101645518, 1697101645596, 1697101645674, 1697101645748, 1697101646490]"
2684,2684,12,20,[],200,llama-13b,128,1,1313.0,1.0,1,H100,1697101641818,1697101643131.0,120,11.0,1.0,"[18, 1295]","[1697101641836, 1697101643131]"
2685,2685,341,37,[],200,llama-13b,128,1,3839.0,1.0,1,H100,1697101680581,1697101684420.0,120,87.0,20.0,"[13, 908, 106, 84, 83, 79, 79, 771, 97, 92, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680594, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682801, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
2686,2686,371,21,[],200,llama-13b,128,1,1185.0,1.0,1,H100,1697101643136,1697101644321.0,120,13.0,1.0,"[27, 1158]","[1697101643163, 1697101644321]"
2687,2687,621,30,[],200,llama-13b,128,1,4529.0,1.0,1,H100,1697101667137,1697101671666.0,120,88.0,20.0,"[31, 1095, 133, 101, 95, 71, 91, 90, 724, 99, 94, 93, 92, 89, 677, 98, 71, 92, 91, 89, 513]","[1697101667168, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670712, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666]"
2688,2688,606,44,[],200,llama-13b,128,1,1693.0,1.0,1,H100,1697101685329,1697101687022.0,120,9.0,1.0,"[42, 1651]","[1697101685371, 1697101687022]"
2689,2689,35,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687023,1697101689594.0,120,,,"[19, 1343, 133, 106, 97, 92, 92, 89]","[1697101687042, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
2690,2690,112,14,[],200,llama-13b,128,1,2040.0,1.0,1,H100,1697101628694,1697101630734.0,120,16.0,2.0,"[31, 692, 1317]","[1697101628725, 1697101629417, 1697101630734]"
2691,2691,707,22,[],200,llama-13b,128,1,1015.0,1.0,1,H100,1697101644322,1697101645337.0,120,8.0,1.0,"[36, 979]","[1697101644358, 1697101645337]"
2692,2692,135,23,[],200,llama-13b,128,1,1152.0,1.0,1,H100,1697101645338,1697101646490.0,120,52.0,2.0,"[6, 955, 190]","[1697101645344, 1697101646299, 1697101646489]"
2693,2693,495,24,[],200,llama-13b,128,1,1251.0,1.0,1,H100,1697101646491,1697101647742.0,120,13.0,1.0,"[6, 1245]","[1697101646497, 1697101647742]"
2694,2694,849,25,[],200,llama-13b,128,1,1204.0,1.0,1,H100,1697101647743,1697101648947.0,120,10.0,1.0,"[30, 1174]","[1697101647773, 1697101648947]"
2695,2695,392,46,[],200,llama-13b,128,1,1497.0,1.0,1,H100,1697101689615,1697101691112.0,120,20.0,1.0,"[266, 1231]","[1697101689881, 1697101691112]"
2696,2696,274,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651604.0,120,,,"[19, 944, 355, 106, 104, 101, 91, 85]","[1697101648967, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2697,2697,604,27,[],200,llama-13b,128,1,2168.0,1.0,1,H100,1697101651610,1697101653778.0,120,161.0,4.0,"[465, 1402, 133, 86, 82]","[1697101652075, 1697101653477, 1697101653610, 1697101653696, 1697101653778]"
2698,2698,36,28,[],200,llama-13b,128,1,4510.0,1.0,1,H100,1697101653779,1697101658289.0,120,457.0,20.0,"[7, 845, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 83, 622, 81, 79, 60, 835]","[1697101653786, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
2699,2699,469,15,[],200,llama-13b,128,1,949.0,1.0,1,H100,1697101630735,1697101631684.0,120,17.0,1.0,"[13, 936]","[1697101630748, 1697101631684]"
2700,2700,752,47,[],200,llama-13b,128,1,1375.0,1.0,1,H100,1697101691113,1697101692488.0,120,39.0,3.0,"[7, 910, 244, 213]","[1697101691120, 1697101692030, 1697101692274, 1697101692487]"
2701,2701,49,31,[],200,llama-13b,128,1,1103.0,1.0,1,H100,1697101671667,1697101672770.0,120,109.0,3.0,"[24, 855, 111, 113]","[1697101671691, 1697101672546, 1697101672657, 1697101672770]"
2702,2702,268,48,[],200,llama-13b,128,1,883.0,1.0,1,H100,1697101692489,1697101693372.0,120,19.0,1.0,"[6, 877]","[1697101692495, 1697101693372]"
2703,2703,406,32,[],200,llama-13b,128,1,1304.0,1.0,1,H100,1697101672771,1697101674075.0,120,244.0,4.0,"[6, 982, 129, 98, 89]","[1697101672777, 1697101673759, 1697101673888, 1697101673986, 1697101674075]"
2704,2704,231,18,[],200,llama-13b,128,1,1153.0,1.0,1,H100,1697101646589,1697101647742.0,120,13.0,1.0,"[41, 1112]","[1697101646630, 1697101647742]"
2705,2705,765,33,[],200,llama-13b,128,1,905.0,1.0,1,H100,1697101674076,1697101674981.0,120,84.0,2.0,"[7, 765, 133]","[1697101674083, 1697101674848, 1697101674981]"
2706,2706,586,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101647743,1697101651603.0,120,,,"[18, 1186, 102, 97, 96, 82, 942, 106, 103, 102, 90, 86]","[1697101647761, 1697101648947, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
2707,2707,700,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684425,1697101689606.0,120,,,"[31, 1781, 91, 89, 87, 86, 647, 101, 100, 95, 91, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101684456, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
2708,2708,95,19,[],200,llama-13b,128,1,1279.0,1.0,1,H100,1697101633690,1697101634969.0,120,12.0,1.0,"[204, 1075]","[1697101633894, 1697101634969]"
2709,2709,627,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693373,1697101697588.0,120,,,"[30, 1151, 77, 101, 84, 604, 78, 77, 831, 103, 101, 111, 91, 91]","[1697101693403, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
2710,2710,165,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674982,1697101675810.0,120,,,"[25, 782]","[1697101675007, 1697101675789]"
2711,2711,540,20,[],200,llama-13b,128,1,2069.0,1.0,1,H100,1697101634970,1697101637039.0,120,140.0,5.0,"[13, 117, 483, 1314, 87, 55]","[1697101634983, 1697101635100, 1697101635583, 1697101636897, 1697101636984, 1697101637039]"
2712,2712,468,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612071,1697101616321.0,120,,,"[104, 839, 107, 90, 86, 83, 81, 62, 81, 719, 95, 93, 92, 90, 69, 88, 633, 79, 79, 75]","[1697101612175, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613523, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615641, 1697101615716]"
2713,2713,899,21,[],200,llama-13b,128,1,5375.0,1.0,1,H100,1697101637041,1697101642416.0,120,100.0,20.0,"[6, 1692, 217, 98, 92, 93, 92, 91, 91, 80, 616, 82, 93, 734, 86, 78, 78, 761, 100, 99, 96]","[1697101637047, 1697101638739, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416]"
2714,2714,524,35,[],200,llama-13b,128,1,6889.0,1.0,1,H100,1697101675814,1697101682703.0,120,100.0,30.0,"[59, 507, 36, 1061, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98, 95, 71, 92, 91, 70, 90, 461, 103, 57, 73, 803, 85, 82, 80, 78, 771]","[1697101675873, 1697101676380, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681607, 1697101681692, 1697101681774, 1697101681854, 1697101681932, 1697101682703]"
2715,2715,859,19,[],200,llama-13b,128,1,582.0,1.0,1,H100,1697101643739,1697101644321.0,120,23.0,1.0,"[7, 574]","[1697101643746, 1697101644320]"
2716,2716,260,20,[],200,llama-13b,128,1,4921.0,1.0,1,H100,1697101644321,1697101649242.0,120,86.0,20.0,"[13, 1003, 103, 78, 78, 77, 75, 742, 97, 98, 83, 78, 1238, 94, 101, 101, 96, 88, 485, 96, 97]","[1697101644334, 1697101645337, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648084, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242]"
2717,2717,282,36,[],200,llama-13b,128,1,4572.0,1.0,1,H100,1697101684052,1697101688624.0,120,87.0,20.0,"[6, 849, 106, 87, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106]","[1697101684058, 1697101684907, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
2718,2718,336,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623665,1697101626657.0,120,,,"[223, 1806, 115, 97, 69]","[1697101623888, 1697101625694, 1697101625809, 1697101625906, 1697101625975]"
2719,2719,361,16,[],200,llama-13b,128,1,2853.0,1.0,1,H100,1697101634998,1697101637851.0,120,67.0,7.0,"[289, 1521, 89, 87, 56, 628, 91, 92]","[1697101635287, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851]"
2720,2720,207,10,[],200,llama-13b,128,1,626.0,1.0,1,H100,1697101621047,1697101621673.0,120,10.0,1.0,"[30, 596]","[1697101621077, 1697101621673]"
2721,2721,566,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621674,1697101623661.0,120,,,"[13, 895, 90, 87, 86, 84, 84]","[1697101621687, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
2722,2722,614,21,[],200,llama-13b,128,1,668.0,1.0,1,H100,1697101649243,1697101649911.0,120,15.0,1.0,"[12, 656]","[1697101649255, 1697101649911]"
2723,2723,368,19,[],200,llama-13b,128,1,4546.0,1.0,1,H100,1697101643834,1697101648380.0,120,88.0,20.0,"[15, 471, 101, 86, 84, 83, 83, 683, 78, 78, 77, 75, 742, 97, 98, 83, 78, 1236, 96, 101, 101]","[1697101643849, 1697101644320, 1697101644421, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648380]"
2724,2724,729,20,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101648381,1697101649050.0,120,874.0,2.0,"[12, 657]","[1697101648393, 1697101649050]"
2725,2725,160,21,[],200,llama-13b,128,1,860.0,1.0,1,H100,1697101649051,1697101649911.0,120,13.0,1.0,"[22, 838]","[1697101649073, 1697101649911]"
2726,2726,438,14,[],200,llama-13b,128,1,2025.0,1.0,1,H100,1697101626668,1697101628693.0,120,9.0,1.0,"[541, 1484]","[1697101627209, 1697101628693]"
2727,2727,715,17,[],200,llama-13b,128,1,888.0,1.0,1,H100,1697101637852,1697101638740.0,120,20.0,1.0,"[7, 880]","[1697101637859, 1697101638739]"
2728,2728,147,18,[],200,llama-13b,128,1,1346.0,1.0,1,H100,1697101638740,1697101640086.0,120,182.0,1.0,"[13, 1332]","[1697101638753, 1697101640085]"
2729,2729,696,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626670,1697101628904.0,120,,,"[515, 1508]","[1697101627185, 1697101628693]"
2730,2730,125,12,[],200,llama-13b,128,1,1719.0,1.0,1,H100,1697101628910,1697101630629.0,120,13.0,1.0,"[404, 1315]","[1697101629314, 1697101630629]"
2731,2731,486,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630634,1697101634994.0,120,,,"[76, 974, 131, 92, 90, 89, 89, 88, 84, 83, 742, 94, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101630710, 1697101631684, 1697101631815, 1697101631907, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2732,2732,521,22,[],200,llama-13b,128,1,1395.0,1.0,1,H100,1697101649913,1697101651308.0,120,18.0,1.0,"[36, 1359]","[1697101649949, 1697101651308]"
2733,2733,797,15,[],200,llama-13b,128,1,722.0,1.0,1,H100,1697101628695,1697101629417.0,120,26.0,1.0,"[18, 704]","[1697101628713, 1697101629417]"
2734,2734,851,23,[],200,llama-13b,128,1,568.0,1.0,1,H100,1697101651309,1697101651877.0,120,23.0,1.0,"[30, 538]","[1697101651339, 1697101651877]"
2735,2735,506,19,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101640087,1697101641002.0,120,16.0,1.0,"[18, 897]","[1697101640105, 1697101641002]"
2736,2736,838,20,[],200,llama-13b,128,1,3753.0,1.0,1,H100,1697101641003,1697101644756.0,120,90.0,20.0,"[7, 807, 304, 101, 98, 97, 88, 82, 63, 801, 99, 95, 93, 95, 87, 85, 415, 86, 84, 84, 82]","[1697101641010, 1697101641817, 1697101642121, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756]"
2737,2737,276,24,[],200,llama-13b,128,1,3543.0,1.0,1,H100,1697101651878,1697101655421.0,120,732.0,13.0,"[305, 1427, 86, 82, 82, 81, 896, 107, 106, 101, 74, 99, 97]","[1697101652183, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421]"
2738,2738,192,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101629418,1697101634994.0,120,,,"[6, 2390, 92, 90, 90, 89, 87, 85, 83, 741, 95, 72, 93, 93, 93, 70, 614, 75, 71]","[1697101629424, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2739,2739,835,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646588,1697101651603.0,120,,,"[30, 1124, 340, 96, 101, 101, 96, 88, 485, 97, 95, 83, 942, 106, 104, 101, 91, 85]","[1697101646618, 1697101647742, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2740,2740,483,22,[],200,llama-13b,128,1,5570.0,1.0,1,H100,1697101652719,1697101658289.0,120,84.0,20.0,"[9, 1903, 206, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83, 623, 81, 79, 60, 835]","[1697101652728, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
2741,2741,234,20,[],200,llama-13b,128,1,5847.0,1.0,1,H100,1697101651606,1697101657453.0,120,457.0,25.0,"[67, 988, 57, 892, 86, 82, 82, 81, 896, 107, 106, 101, 74, 99, 97, 92, 613, 106, 100, 99, 97, 83, 623, 81, 79, 59]","[1697101651673, 1697101652661, 1697101652718, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453]"
2742,2742,332,14,[],200,llama-13b,128,1,894.0,1.0,1,H100,1697101626663,1697101627557.0,120,39.0,1.0,"[36, 857]","[1697101626699, 1697101627556]"
2743,2743,629,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101655423,1697101675811.0,120,,,"[12, 563, 129, 105, 101, 98, 98, 83, 622, 81, 79, 60, 836, 95, 82, 82, 80, 1076, 101, 99, 97, 93, 85, 805, 101, 75, 99, 93, 86, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 108, 103, 96, 94, 91, 697, 106, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 98, 71, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101655435, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658290, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661353, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665170, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
2744,2744,685,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101627557,1697101628905.0,120,,,"[13, 1123]","[1697101627570, 1697101628693]"
2745,2745,553,17,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634997,1697101640208.0,120,88.0,20.0,"[175, 1636, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 79, 616]","[1697101635172, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208]"
2746,2746,149,22,[],200,llama-13b,128,1,2197.0,1.0,1,H100,1697101656432,1697101658629.0,120,563.0,10.0,"[7, 708, 87, 81, 79, 60, 835, 95, 83, 81, 81]","[1697101656439, 1697101657147, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629]"
2747,2747,115,16,[],200,llama-13b,128,1,1719.0,1.0,1,H100,1697101628910,1697101630629.0,120,13.0,1.0,"[381, 1338]","[1697101629291, 1697101630629]"
2748,2748,469,17,[],200,llama-13b,128,1,1054.0,1.0,1,H100,1697101630631,1697101631685.0,120,17.0,1.0,"[60, 994]","[1697101630691, 1697101631685]"
2749,2749,831,18,[],200,llama-13b,128,1,1281.0,1.0,1,H100,1697101631686,1697101632967.0,120,11.0,1.0,"[36, 1245]","[1697101631722, 1697101632967]"
2750,2750,232,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101632968,1697101634995.0,120,,,"[24, 1310, 74, 71]","[1697101632992, 1697101634302, 1697101634376, 1697101634447]"
2751,2751,593,20,[],200,llama-13b,128,1,2940.0,1.0,1,H100,1697101634998,1697101637938.0,120,335.0,9.0,"[89, 431, 66, 1313, 87, 56, 628, 91, 92, 87]","[1697101635087, 1697101635518, 1697101635584, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938]"
2752,2752,24,21,[],200,llama-13b,128,1,1654.0,1.0,1,H100,1697101637939,1697101639593.0,120,79.0,9.0,"[7, 1010, 98, 93, 92, 92, 92, 90, 80]","[1697101637946, 1697101638956, 1697101639054, 1697101639147, 1697101639239, 1697101639331, 1697101639423, 1697101639513, 1697101639593]"
2753,2753,592,21,[],200,llama-13b,128,1,12113.0,1.0,1,H100,1697101657455,1697101669568.0,120,15.0,50.0,"[6, 1912, 332, 101, 99, 97, 92, 86, 805, 102, 75, 104, 87, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725]","[1697101657461, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568]"
2754,2754,852,19,[],200,llama-13b,128,1,5210.0,1.0,1,H100,1697101634999,1697101640209.0,120,100.0,20.0,"[297, 1512, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 768, 97, 93, 93, 92, 91, 91, 79, 617]","[1697101635296, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209]"
2755,2755,7,27,[],200,llama-13b,128,1,2753.0,1.0,1,H100,1697101653479,1697101656232.0,120,345.0,11.0,"[66, 1086, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106]","[1697101653545, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232]"
2756,2756,21,21,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101651610,1697101652661.0,120,15.0,1.0,"[210, 841]","[1697101651820, 1697101652661]"
2757,2757,374,22,[],200,llama-13b,128,1,3949.0,1.0,1,H100,1697101652662,1697101656611.0,120,85.0,20.0,"[24, 786, 138, 86, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83]","[1697101652686, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611]"
2758,2758,786,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646685,1697101651603.0,120,,,"[7, 1050, 340, 96, 101, 101, 96, 88, 485, 97, 95, 83, 942, 106, 104, 101, 91, 85]","[1697101646692, 1697101647742, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2759,2759,553,17,[],200,llama-13b,128,1,4992.0,1.0,1,H100,1697101628695,1697101633687.0,120,88.0,20.0,"[54, 668, 1317, 81, 68, 66, 864, 92, 91, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 92, 71]","[1697101628749, 1697101629417, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631905, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633687]"
2760,2760,704,23,[],200,llama-13b,128,1,1547.0,1.0,1,H100,1697101656613,1697101658160.0,120,14.0,1.0,"[76, 1471]","[1697101656689, 1697101658160]"
2761,2761,585,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621046,1697101623660.0,120,,,"[15, 612, 85, 81, 80, 78, 61, 613, 87, 87, 84, 83]","[1697101621061, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
2762,2762,137,24,[],200,llama-13b,128,1,5303.0,1.0,1,H100,1697101658161,1697101663464.0,120,86.0,20.0,"[42, 1170, 332, 101, 99, 97, 93, 85, 805, 101, 75, 99, 92, 88, 831, 107, 101, 95, 94, 89, 707]","[1697101658203, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464]"
2763,2763,447,20,[],200,llama-13b,128,1,3757.0,1.0,1,H100,1697101644421,1697101648178.0,120,161.0,13.0,"[42, 977, 79, 77, 78, 75, 740, 98, 97, 84, 78, 1237, 95]","[1697101644463, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645749, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648083, 1697101648178]"
2764,2764,486,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612434,1697101616321.0,120,,,"[24, 556, 107, 90, 86, 83, 81, 63, 80, 719, 95, 93, 92, 90, 69, 87, 634, 80, 78, 75]","[1697101612458, 1697101613014, 1697101613121, 1697101613211, 1697101613297, 1697101613380, 1697101613461, 1697101613524, 1697101613604, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614849, 1697101615483, 1697101615563, 1697101615641, 1697101615716]"
2765,2765,833,38,[],200,llama-13b,128,1,3005.0,1.0,1,H100,1697101703175,1697101706180.0,120,563.0,8.0,"[25, 1699, 116, 98, 72, 728, 95, 87, 85]","[1697101703200, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180]"
2766,2766,356,14,[],200,llama-13b,128,1,1128.0,1.0,1,H100,1697101642322,1697101643450.0,120,874.0,2.0,"[6, 803, 319]","[1697101642328, 1697101643131, 1697101643450]"
2767,2767,241,12,[],200,llama-13b,128,1,1053.0,1.0,1,H100,1697101630631,1697101631684.0,120,19.0,1.0,"[15, 1038]","[1697101630646, 1697101631684]"
2768,2768,236,39,[],200,llama-13b,128,1,864.0,1.0,1,H100,1697101706181,1697101707045.0,120,8.0,1.0,"[7, 857]","[1697101706188, 1697101707045]"
2769,2769,97,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634995.0,120,,,"[13, 1266]","[1697101633703, 1697101634969]"
2770,2770,223,24,[],200,llama-13b,128,1,616.0,1.0,1,H100,1697101656531,1697101657147.0,120,16.0,1.0,"[77, 539]","[1697101656608, 1697101657147]"
2771,2771,600,13,[],200,llama-13b,128,1,1281.0,1.0,1,H100,1697101631685,1697101632966.0,120,23.0,1.0,"[13, 1268]","[1697101631698, 1697101632966]"
2772,2772,720,27,[],200,llama-13b,128,1,1420.0,1.0,1,H100,1697101663659,1697101665079.0,120,286.0,6.0,"[18, 768, 233, 109, 103, 95, 94]","[1697101663677, 1697101664445, 1697101664678, 1697101664787, 1697101664890, 1697101664985, 1697101665079]"
2773,2773,580,25,[],200,llama-13b,128,1,5228.0,1.0,1,H100,1697101657150,1697101662378.0,120,88.0,20.0,"[12, 998, 129, 96, 82, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 102, 75, 104, 87, 87, 831, 107]","[1697101657162, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378]"
2774,2774,25,14,[],200,llama-13b,128,1,1238.0,1.0,1,H100,1697101632968,1697101634206.0,120,12.0,1.0,"[12, 1226]","[1697101632980, 1697101634206]"
2775,2775,641,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688625,1697101689594.0,120,,,[25],[1697101688650]
2776,2776,44,38,[],200,llama-13b,128,1,2411.0,1.0,1,H100,1697101689619,1697101692030.0,120,12.0,1.0,"[499, 1912]","[1697101690118, 1697101692030]"
2777,2777,398,39,[],200,llama-13b,128,1,4689.0,1.0,1,H100,1697101692031,1697101696720.0,120,87.0,20.0,"[24, 1317, 107, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 85, 604, 78, 77, 831, 103, 101, 110]","[1697101692055, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720]"
2778,2778,203,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[103, 1176]","[1697101633793, 1697101634969]"
2779,2779,210,12,[],200,llama-13b,128,1,943.0,1.0,1,H100,1697101623664,1697101624607.0,120,140.0,2.0,"[31, 873, 39]","[1697101623695, 1697101624568, 1697101624607]"
2780,2780,97,26,[],200,llama-13b,128,1,4757.0,1.0,1,H100,1697101662379,1697101667136.0,120,6.0,20.0,"[19, 939, 127, 101, 93, 88, 87, 845, 108, 103, 97, 93, 101, 688, 105, 100, 72, 72, 93, 92, 734]","[1697101662398, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664986, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136]"
2781,2781,570,13,[],200,llama-13b,128,1,1958.0,1.0,1,H100,1697101624608,1697101626566.0,120,18.0,1.0,"[15, 1943]","[1697101624623, 1697101626566]"
2782,2782,47,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649912,1697101651603.0,120,,,"[7, 1389]","[1697101649919, 1697101651308]"
2783,2783,6,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626567,1697101628903.0,120,,,"[12, 1020]","[1697101626579, 1697101627599]"
2784,2784,406,23,[],200,llama-13b,128,1,2090.0,1.0,1,H100,1697101651606,1697101653696.0,120,244.0,4.0,"[72, 983, 57, 892, 86]","[1697101651678, 1697101652661, 1697101652718, 1697101653610, 1697101653696]"
2785,2785,920,12,[],200,llama-13b,128,1,2305.0,1.0,1,H100,1697101623670,1697101625975.0,120,96.0,4.0,"[447, 1578, 113, 98, 69]","[1697101624117, 1697101625695, 1697101625808, 1697101625906, 1697101625975]"
2786,2786,472,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101634207,1697101634996.0,120,,,"[7, 755]","[1697101634214, 1697101634969]"
2787,2787,625,12,[],200,llama-13b,128,1,1240.0,1.0,1,H100,1697101624569,1697101625809.0,120,364.0,2.0,"[12, 1114, 114]","[1697101624581, 1697101625695, 1697101625809]"
2788,2788,127,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628904.0,120,,,"[306, 1722]","[1697101626970, 1697101628692]"
2789,2789,457,11,[],200,llama-13b,128,1,1825.0,1.0,1,H100,1697101628909,1697101630734.0,120,874.0,2.0,"[272, 1553]","[1697101629181, 1697101630734]"
2790,2790,334,15,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,15.0,1.0,"[88, 1632]","[1697101628997, 1697101630629]"
2791,2791,814,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630735,1697101634994.0,120,,,"[18, 931, 131, 92, 90, 89, 89, 88, 84, 83, 742, 94, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101630753, 1697101631684, 1697101631815, 1697101631907, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2792,2792,851,24,[],200,llama-13b,128,1,934.0,1.0,1,H100,1697101653697,1697101654631.0,120,23.0,1.0,"[12, 922]","[1697101653709, 1697101654631]"
2793,2793,419,15,[],200,llama-13b,128,1,4208.0,1.0,1,H100,1697101640212,1697101644420.0,120,88.0,20.0,"[48, 742, 117, 85, 78, 78, 761, 100, 99, 96, 89, 82, 63, 801, 98, 97, 92, 95, 87, 85, 415]","[1697101640260, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643549, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420]"
2794,2794,54,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625810,1697101626655.0,120,,,"[18, 738]","[1697101625828, 1697101626566]"
2795,2795,413,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626660,1697101628903.0,120,,,"[10, 886, 43]","[1697101626670, 1697101627556, 1697101627599]"
2796,2796,285,25,[],200,llama-13b,128,1,6721.0,1.0,1,H100,1697101654632,1697101661353.0,120,100.0,27.0,"[24, 1342, 129, 105, 101, 98, 98, 83, 622, 81, 79, 60, 836, 95, 82, 82, 80, 1076, 101, 99, 97, 93, 85, 805, 101, 75, 99, 93]","[1697101654656, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658290, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661353]"
2797,2797,771,15,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628909,1697101633687.0,120,47.0,20.0,"[78, 1642, 105, 81, 68, 66, 865, 92, 90, 90, 89, 87, 85, 82, 742, 95, 72, 94, 92, 93, 70]","[1697101628987, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633524, 1697101633617, 1697101633687]"
2798,2798,694,16,[],200,llama-13b,128,1,2800.0,1.0,1,H100,1697101630632,1697101633432.0,120,161.0,13.0,"[24, 1158, 92, 91, 89, 89, 88, 84, 83, 741, 95, 72, 94]","[1697101630656, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432]"
2799,2799,826,16,[],200,llama-13b,128,1,5210.0,1.0,1,H100,1697101634999,1697101640209.0,120,87.0,20.0,"[277, 1532, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 768, 97, 93, 93, 92, 91, 91, 80, 616]","[1697101635276, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209]"
2800,2800,254,17,[],200,llama-13b,128,1,791.0,1.0,1,H100,1697101640211,1697101641002.0,120,58.0,1.0,"[101, 690]","[1697101640312, 1697101641002]"
2801,2801,617,18,[],200,llama-13b,128,1,3754.0,1.0,1,H100,1697101641003,1697101644757.0,120,87.0,20.0,"[42, 772, 305, 100, 98, 97, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 87, 83, 84, 83]","[1697101641045, 1697101641817, 1697101642122, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644590, 1697101644674, 1697101644757]"
2802,2802,680,25,[],200,llama-13b,128,1,2392.0,1.0,1,H100,1697101661354,1697101663746.0,120,123.0,11.0,"[6, 556, 356, 106, 102, 95, 93, 90, 706, 101, 93, 88]","[1697101661360, 1697101661916, 1697101662272, 1697101662378, 1697101662480, 1697101662575, 1697101662668, 1697101662758, 1697101663464, 1697101663565, 1697101663658, 1697101663746]"
2803,2803,770,16,[],200,llama-13b,128,1,916.0,1.0,1,H100,1697101644421,1697101645337.0,120,13.0,1.0,"[25, 891]","[1697101644446, 1697101645337]"
2804,2804,308,16,[],200,llama-13b,128,1,4207.0,1.0,1,H100,1697101640212,1697101644419.0,120,87.0,20.0,"[94, 696, 118, 84, 79, 77, 761, 100, 99, 96, 89, 82, 63, 801, 98, 97, 92, 96, 86, 85, 414]","[1697101640306, 1697101641002, 1697101641120, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643549, 1697101643646, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644419]"
2805,2805,200,17,[],200,llama-13b,128,1,2941.0,1.0,1,H100,1697101645338,1697101648279.0,120,6.0,9.0,"[18, 943, 190, 98, 98, 83, 78, 1226, 106, 101]","[1697101645356, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279]"
2806,2806,638,26,[],200,llama-13b,128,1,4619.0,1.0,1,H100,1697101661354,1697101665973.0,120,88.0,20.0,"[6, 556, 356, 106, 102, 94, 94, 90, 706, 101, 93, 88, 87, 845, 108, 104, 95, 94, 101, 687, 106]","[1697101661360, 1697101661916, 1697101662272, 1697101662378, 1697101662480, 1697101662574, 1697101662668, 1697101662758, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664890, 1697101664985, 1697101665079, 1697101665180, 1697101665867, 1697101665973]"
2807,2807,49,19,[],200,llama-13b,128,1,1828.0,1.0,1,H100,1697101644759,1697101646587.0,120,109.0,3.0,"[30, 1510, 190, 98]","[1697101644789, 1697101646299, 1697101646489, 1697101646587]"
2808,2808,110,26,[],200,llama-13b,128,1,1143.0,1.0,1,H100,1697101663747,1697101664890.0,120,96.0,4.0,"[7, 692, 232, 109, 103]","[1697101663754, 1697101664446, 1697101664678, 1697101664787, 1697101664890]"
2809,2809,554,24,[],200,llama-13b,128,1,916.0,1.0,1,H100,1697101644421,1697101645337.0,120,26.0,1.0,"[7, 909]","[1697101644428, 1697101645337]"
2810,2810,67,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101665976,1697101675811.0,120,,,"[35, 759, 366, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 95, 92, 93, 88, 676, 99, 70, 93, 90, 90, 513, 97, 95, 94, 93, 90, 521, 113, 92, 68, 67, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101666011, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
2811,2811,442,27,[],200,llama-13b,128,1,4776.0,1.0,1,H100,1697101664891,1697101669667.0,120,39.0,22.0,"[24, 823, 130, 106, 100, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 96, 70, 91, 90, 725, 98]","[1697101664915, 1697101665738, 1697101665868, 1697101665974, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668593, 1697101668663, 1697101668754, 1697101668844, 1697101669569, 1697101669667]"
2812,2812,558,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628903.0,120,,,"[155, 739, 42]","[1697101626818, 1697101627557, 1697101627599]"
2813,2813,919,11,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,14.0,1.0,"[100, 1620]","[1697101629009, 1697101630629]"
2814,2814,323,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630631,1697101634995.0,120,,,"[25, 1028, 130, 92, 91, 89, 89, 88, 84, 83, 741, 95, 72, 94, 93, 92, 70, 615, 74, 71]","[1697101630656, 1697101631684, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
2815,2815,522,23,[],200,llama-13b,128,1,713.0,1.0,1,H100,1697101642418,1697101643131.0,120,20.0,1.0,"[42, 671]","[1697101642460, 1697101643131]"
2816,2816,211,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101663661,1697101675811.0,120,,,"[22, 763, 232, 109, 103, 95, 94, 101, 688, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 99, 94, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101663683, 1697101664446, 1697101664678, 1697101664787, 1697101664890, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
2817,2817,351,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101650477,1697101651604.0,120,,,"[6, 825]","[1697101650483, 1697101651308]"
2818,2818,51,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622059,1697101623660.0,120,,,"[13, 1500]","[1697101622072, 1697101623572]"
2819,2819,710,23,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101651610,1697101652661.0,120,14.0,1.0,"[206, 845]","[1697101651816, 1697101652661]"
2820,2820,708,24,[],200,llama-13b,128,1,963.0,1.0,1,H100,1697101648949,1697101649912.0,120,140.0,1.0,"[60, 902]","[1697101649009, 1697101649911]"
2821,2821,138,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649912,1697101651604.0,120,,,"[13, 1383]","[1697101649925, 1697101651308]"
2822,2822,496,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623665,1697101626656.0,120,,,"[42, 861, 39, 1201, 98, 68]","[1697101623707, 1697101624568, 1697101624607, 1697101625808, 1697101625906, 1697101625974]"
2823,2823,856,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628904.0,120,,,"[318, 1711]","[1697101626982, 1697101628693]"
2824,2824,288,17,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628909,1697101633688.0,120,93.0,20.0,"[214, 1611, 81, 68, 66, 864, 93, 91, 89, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 71]","[1697101629123, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
2825,2825,114,24,[],200,llama-13b,128,1,3949.0,1.0,1,H100,1697101652662,1697101656611.0,120,88.0,20.0,"[18, 792, 138, 86, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83]","[1697101652680, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611]"
2826,2826,499,26,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101651609,1697101656529.0,120,88.0,20.0,"[175, 877, 57, 891, 87, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 98]","[1697101651784, 1697101652661, 1697101652718, 1697101653609, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656529]"
2827,2827,103,30,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,15.0,1.0,"[555, 1009]","[1697101676371, 1697101677380]"
2828,2828,832,27,[],200,llama-13b,128,1,552.0,1.0,1,H100,1697101666218,1697101666770.0,120,15.0,1.0,"[6, 546]","[1697101666224, 1697101666770]"
2829,2829,642,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[79, 1200]","[1697101633769, 1697101634969]"
2830,2830,42,19,[],200,llama-13b,128,1,1808.0,1.0,1,H100,1697101635000,1697101636808.0,120,10.0,1.0,"[284, 1524]","[1697101635284, 1697101636808]"
2831,2831,396,20,[],200,llama-13b,128,1,4309.0,1.0,1,H100,1697101636810,1697101641119.0,120,89.0,20.0,"[54, 614, 190, 91, 93, 87, 85, 83, 81, 768, 98, 92, 93, 92, 91, 91, 80, 617, 81, 93, 735]","[1697101636864, 1697101637478, 1697101637668, 1697101637759, 1697101637852, 1697101637939, 1697101638024, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640210, 1697101640291, 1697101640384, 1697101641119]"
2832,2832,463,31,[],200,llama-13b,128,1,865.0,1.0,1,H100,1697101677382,1697101678247.0,120,39.0,1.0,"[103, 762]","[1697101677485, 1697101678247]"
2833,2833,790,32,[],200,llama-13b,128,1,4456.0,1.0,1,H100,1697101678248,1697101682704.0,120,84.0,20.0,"[30, 1010, 117, 99, 98, 94, 72, 92, 90, 70, 90, 461, 104, 56, 73, 804, 84, 83, 79, 79, 771]","[1697101678278, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680675, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704]"
2834,2834,366,28,[],200,llama-13b,128,1,2056.0,1.0,1,H100,1697101656233,1697101658289.0,120,85.0,6.0,"[6, 907, 88, 81, 79, 60, 835]","[1697101656239, 1697101657146, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
2835,2835,285,20,[],200,llama-13b,128,1,5385.0,1.0,1,H100,1697101640211,1697101645596.0,120,100.0,27.0,"[31, 760, 117, 85, 78, 78, 761, 100, 100, 95, 89, 82, 63, 800, 99, 97, 92, 95, 87, 85, 415, 86, 84, 84, 82, 684, 78, 78]","[1697101640242, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645518, 1697101645596]"
2836,2836,857,27,[],200,llama-13b,128,1,617.0,1.0,1,H100,1697101656530,1697101657147.0,120,18.0,1.0,"[60, 557]","[1697101656590, 1697101657147]"
2837,2837,80,16,[],200,llama-13b,128,1,651.0,1.0,1,H100,1697101634449,1697101635100.0,120,13.0,1.0,"[6, 645]","[1697101634455, 1697101635100]"
2838,2838,262,28,[],200,llama-13b,128,1,1010.0,1.0,1,H100,1697101657150,1697101658160.0,120,39.0,1.0,"[30, 980]","[1697101657180, 1697101658160]"
2839,2839,440,17,[],200,llama-13b,128,1,6018.0,1.0,1,H100,1697101635101,1697101641119.0,120,84.0,20.0,"[487, 1890, 190, 91, 92, 87, 87, 82, 81, 768, 98, 92, 93, 92, 91, 91, 70, 627, 81, 93, 734]","[1697101635588, 1697101637478, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639583, 1697101640210, 1697101640291, 1697101640384, 1697101641118]"
2840,2840,219,33,[],200,llama-13b,128,1,4634.0,1.0,1,H100,1697101682705,1697101687339.0,120,90.0,20.0,"[18, 918, 208, 102, 100, 97, 92, 90, 90, 593, 86, 64, 82, 82, 910, 91, 89, 87, 86, 647, 102]","[1697101682723, 1697101683641, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339]"
2841,2841,378,22,[],200,llama-13b,128,1,10880.0,1.0,1,H100,1697101639595,1697101650475.0,120,93.0,47.0,"[24, 1500, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 801, 99, 95, 93, 95, 86, 86, 414, 87, 84, 84, 82, 684, 78, 78, 77, 75, 741, 98, 97, 84, 78, 1226, 106, 101, 100, 97, 87, 486, 96, 96, 83, 941, 107, 103]","[1697101639619, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644005, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648563, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475]"
2842,2842,720,29,[],200,llama-13b,128,1,1805.0,1.0,1,H100,1697101658290,1697101660095.0,120,286.0,6.0,"[13, 1071, 331, 102, 98, 97, 93]","[1697101658303, 1697101659374, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095]"
2843,2843,794,18,[],200,llama-13b,128,1,696.0,1.0,1,H100,1697101641121,1697101641817.0,120,11.0,1.0,"[30, 666]","[1697101641151, 1697101641817]"
2844,2844,222,19,[],200,llama-13b,128,1,4769.0,1.0,1,H100,1697101641818,1697101646587.0,120,96.0,20.0,"[7, 1306, 319, 100, 95, 93, 95, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 77, 75, 741, 98]","[1697101641825, 1697101643131, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587]"
2845,2845,257,26,[],200,llama-13b,128,1,1082.0,1.0,1,H100,1697101658291,1697101659373.0,120,14.0,1.0,"[48, 1034]","[1697101658339, 1697101659373]"
2846,2846,45,22,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101649243,1697101649912.0,120,19.0,1.0,"[54, 615]","[1697101649297, 1697101649912]"
2847,2847,403,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649913,1697101651604.0,120,,,"[18, 1377]","[1697101649931, 1697101651308]"
2848,2848,382,24,[],200,llama-13b,128,1,4730.0,1.0,1,H100,1697101656530,1697101661260.0,120,47.0,20.0,"[18, 599, 87, 81, 79, 60, 835, 95, 83, 81, 81, 1076, 102, 98, 97, 93, 85, 805, 101, 76, 98]","[1697101656548, 1697101657147, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260]"
2849,2849,585,27,[],200,llama-13b,128,1,11337.0,1.0,1,H100,1697101659374,1697101670711.0,120,244.0,50.0,"[25, 1248, 338, 101, 76, 98, 93, 87, 831, 107, 101, 95, 94, 89, 708, 100, 93, 88, 87, 845, 108, 103, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677]","[1697101659399, 1697101660647, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663465, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711]"
2850,2850,553,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671859,1697101675810.0,120,,,"[6, 681, 111, 113, 91, 68, 68, 88, 803, 98, 89, 88, 66, 66, 686, 103, 98, 95, 70, 94]","[1697101671865, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675084, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
2851,2851,907,18,[],200,llama-13b,128,1,1279.0,1.0,1,H100,1697101633690,1697101634969.0,120,10.0,1.0,"[19, 1260]","[1697101633709, 1697101634969]"
2852,2852,337,19,[],200,llama-13b,128,1,130.0,1.0,1,H100,1697101634970,1697101635100.0,120,12.0,1.0,"[19, 111]","[1697101634989, 1697101635100]"
2853,2853,699,20,[],200,llama-13b,128,1,2378.0,1.0,1,H100,1697101635101,1697101637479.0,120,39.0,1.0,"[499, 1878]","[1697101635600, 1697101637478]"
2854,2854,100,21,[],200,llama-13b,128,1,3724.0,1.0,1,H100,1697101637480,1697101641204.0,120,732.0,14.0,"[6, 1253, 217, 98, 92, 93, 92, 91, 91, 79, 617, 82, 93, 734, 86]","[1697101637486, 1697101638739, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640384, 1697101641118, 1697101641204]"
2855,2855,907,26,[],200,llama-13b,128,1,1566.0,1.0,1,H100,1697101675814,1697101677380.0,120,10.0,1.0,"[289, 1277]","[1697101676103, 1697101677380]"
2856,2856,311,27,[],200,llama-13b,128,1,3423.0,1.0,1,H100,1697101677381,1697101680804.0,120,93.0,20.0,"[24, 949, 90, 86, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 470, 94, 57, 73]","[1697101677405, 1697101678354, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680580, 1697101680674, 1697101680731, 1697101680804]"
2857,2857,893,22,[],200,llama-13b,128,1,2390.0,1.0,1,H100,1697101661268,1697101663658.0,120,335.0,10.0,"[42, 606, 356, 106, 101, 95, 94, 90, 706, 101, 93]","[1697101661310, 1697101661916, 1697101662272, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662758, 1697101663464, 1697101663565, 1697101663658]"
2858,2858,461,22,[],200,llama-13b,128,1,5641.0,1.0,1,H100,1697101641205,1697101646846.0,120,216.0,30.0,"[6, 910, 101, 99, 96, 88, 83, 63, 799, 100, 95, 93, 95, 87, 85, 415, 86, 85, 83, 82, 684, 79, 77, 77, 75, 741, 98, 97, 84, 78]","[1697101641211, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642588, 1697101642651, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644591, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846]"
2859,2859,670,28,[],200,llama-13b,128,1,4357.0,1.0,1,H100,1697101680806,1697101685163.0,120,67.0,18.0,"[7, 1698, 193, 96, 93, 91, 69, 91, 90, 614, 102, 101, 97, 91, 91, 90, 594, 86, 63]","[1697101680813, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684239, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163]"
2860,2860,318,23,[],200,llama-13b,128,1,1420.0,1.0,1,H100,1697101663659,1697101665079.0,120,6.0,6.0,"[13, 773, 233, 109, 103, 95, 94]","[1697101663672, 1697101664445, 1697101664678, 1697101664787, 1697101664890, 1697101664985, 1697101665079]"
2861,2861,672,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646588,1697101651603.0,120,,,"[7, 1488, 95, 101, 101, 96, 88, 485, 96, 96, 83, 942, 106, 104, 102, 90, 85]","[1697101646595, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650578, 1697101650668, 1697101650753]"
2862,2862,737,25,[],200,llama-13b,128,1,1005.0,1.0,1,H100,1697101661267,1697101662272.0,120,216.0,2.0,"[7, 642, 356]","[1697101661274, 1697101661916, 1697101662272]"
2863,2863,434,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646490,1697101651605.0,120,,,"[7, 1245, 341, 96, 100, 101, 96, 88, 485, 96, 96, 83, 942, 106, 104, 102, 90, 85]","[1697101646497, 1697101647742, 1697101648083, 1697101648179, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650578, 1697101650668, 1697101650753]"
2864,2864,81,24,[],200,llama-13b,128,1,4291.0,1.0,1,H100,1697101644759,1697101649050.0,120,732.0,13.0,"[12, 1528, 190, 98, 97, 84, 78, 1236, 96, 101, 100, 97, 88, 485]","[1697101644771, 1697101646299, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049]"
2865,2865,166,26,[],200,llama-13b,128,1,1064.0,1.0,1,H100,1697101662273,1697101663337.0,120,14.0,1.0,"[12, 1052]","[1697101662285, 1697101663337]"
2866,2866,411,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649051,1697101651605.0,120,,,"[12, 1203, 106, 104, 101, 91, 85]","[1697101649063, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2867,2867,768,26,[],200,llama-13b,128,1,2331.0,1.0,1,H100,1697101651610,1697101653941.0,120,47.0,6.0,"[563, 1299, 138, 86, 82, 82, 81]","[1697101652173, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941]"
2868,2868,100,29,[],200,llama-13b,128,1,3354.0,1.0,1,H100,1697101685164,1697101688518.0,120,732.0,14.0,"[12, 940, 121, 91, 89, 87, 87, 646, 102, 99, 94, 92, 91, 88, 715]","[1697101685176, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518]"
2869,2869,527,27,[],200,llama-13b,128,1,9746.0,1.0,1,H100,1697101663338,1697101673084.0,120,732.0,50.0,"[48, 1059, 233, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88]","[1697101663386, 1697101664445, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084]"
2870,2870,105,21,[],200,llama-13b,128,1,8574.0,1.0,1,H100,1697101651606,1697101660180.0,120,364.0,36.0,"[34, 1021, 57, 892, 85, 83, 82, 81, 896, 107, 106, 101, 74, 99, 97, 92, 613, 106, 100, 99, 97, 83, 623, 81, 79, 59, 836, 95, 83, 81, 81, 1076, 101, 98, 98, 92, 86]","[1697101651640, 1697101652661, 1697101652718, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659806, 1697101659904, 1697101660002, 1697101660094, 1697101660180]"
2871,2871,197,27,[],200,llama-13b,128,1,3292.0,1.0,1,H100,1697101653942,1697101657234.0,120,6.0,8.0,"[13, 2043, 128, 106, 101, 98, 98, 83, 622]","[1697101653955, 1697101655998, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234]"
2872,2872,462,22,[],200,llama-13b,128,1,1735.0,1.0,1,H100,1697101660181,1697101661916.0,120,52.0,1.0,"[7, 1728]","[1697101660188, 1697101661916]"
2873,2873,820,16,[],200,llama-13b,128,1,2616.0,1.0,1,H100,1697101631685,1697101634301.0,120,161.0,9.0,"[7, 1274, 206, 94, 72, 94, 93, 92, 70, 614]","[1697101631692, 1697101632966, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301]"
2874,2874,557,28,[],200,llama-13b,128,1,925.0,1.0,1,H100,1697101657235,1697101658160.0,120,31.0,1.0,"[6, 919]","[1697101657241, 1697101658160]"
2875,2875,914,29,[],200,llama-13b,128,1,5303.0,1.0,1,H100,1697101658161,1697101663464.0,120,84.0,20.0,"[30, 1182, 332, 101, 99, 97, 93, 85, 805, 101, 76, 98, 92, 88, 831, 107, 101, 95, 94, 89, 707]","[1697101658191, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661352, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464]"
2876,2876,221,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101634302,1697101634996.0,120,,,"[7, 660]","[1697101634309, 1697101634969]"
2877,2877,579,18,[],200,llama-13b,128,1,1809.0,1.0,1,H100,1697101635000,1697101636809.0,120,19.0,1.0,"[504, 1305]","[1697101635504, 1697101636809]"
2878,2878,12,19,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101636810,1697101637479.0,120,11.0,1.0,"[70, 599]","[1697101636880, 1697101637479]"
2879,2879,461,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688519,1697101689594.0,120,,,[6],[1697101688525]
2880,2880,373,20,[],200,llama-13b,128,1,1259.0,1.0,1,H100,1697101637480,1697101638739.0,120,15.0,1.0,"[12, 1247]","[1697101637492, 1697101638739]"
2881,2881,811,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648179,1697101651604.0,120,,,"[7, 761, 102, 97, 96, 82, 942, 106, 103, 102, 90, 86]","[1697101648186, 1697101648947, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
2882,2882,786,31,[],200,llama-13b,128,1,5197.0,1.0,1,H100,1697101689619,1697101694816.0,120,87.0,20.0,"[487, 1924, 244, 213, 212, 78, 110, 94, 94, 403, 102, 100, 99, 94, 91, 88, 68, 434, 77, 100, 85]","[1697101690106, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
2883,2883,241,22,[],200,llama-13b,128,1,1863.0,1.0,1,H100,1697101651609,1697101653472.0,120,19.0,1.0,"[304, 1559]","[1697101651913, 1697101653472]"
2884,2884,686,23,[],200,llama-13b,128,1,1153.0,1.0,1,H100,1697101653478,1697101654631.0,120,31.0,1.0,"[13, 1140]","[1697101653491, 1697101654631]"
2885,2885,399,29,[],200,llama-13b,128,1,5173.0,1.0,1,H100,1697101658291,1697101663464.0,120,87.0,20.0,"[42, 1040, 332, 102, 98, 97, 93, 85, 805, 101, 76, 98, 93, 87, 831, 107, 102, 94, 94, 89, 707]","[1697101658333, 1697101659373, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662480, 1697101662574, 1697101662668, 1697101662757, 1697101663464]"
2886,2886,43,19,[],200,llama-13b,128,1,3193.0,1.0,1,H100,1697101656613,1697101659806.0,120,732.0,8.0,"[65, 1482, 129, 96, 82, 82, 80, 1077, 100]","[1697101656678, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659706, 1697101659806]"
2887,2887,55,50,[],200,llama-13b,128,1,1878.0,1.0,1,H100,1697101697597,1697101699475.0,120,12.0,1.0,"[473, 1405]","[1697101698070, 1697101699475]"
2888,2888,595,40,[],200,llama-13b,128,1,1267.0,1.0,1,H100,1697101707046,1697101708313.0,120,8.0,1.0,"[30, 1237]","[1697101707076, 1697101708313]"
2889,2889,24,41,[],200,llama-13b,128,1,2591.0,1.0,1,H100,1697101708314,1697101710905.0,120,79.0,9.0,"[7, 1169, 89, 83, 82, 79, 882, 103, 97]","[1697101708321, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905]"
2890,2890,413,51,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700635.0,120,,,"[12, 1063]","[1697101699489, 1697101700552]"
2891,2891,756,30,[],200,llama-13b,128,1,980.0,1.0,1,H100,1697101663466,1697101664446.0,120,19.0,1.0,"[36, 944]","[1697101663502, 1697101664446]"
2892,2892,181,31,[],200,llama-13b,128,1,8209.0,1.0,1,H100,1697101664447,1697101672656.0,120,91.0,39.0,"[12, 1279, 130, 106, 99, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90, 724, 99, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 513, 97, 95, 94, 93, 90, 521]","[1697101664459, 1697101665738, 1697101665868, 1697101665974, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656]"
2893,2893,772,52,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[529, 1863]","[1697101701171, 1697101703034]"
2894,2894,142,28,[],200,llama-13b,128,1,3763.0,1.0,1,H100,1697101665080,1697101668843.0,120,52.0,20.0,"[7, 652, 129, 106, 100, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89]","[1697101665087, 1697101665739, 1697101665868, 1697101665974, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843]"
2895,2895,171,53,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703176,1697101704900.0,120,6.0,1.0,"[442, 1282]","[1697101703618, 1697101704900]"
2896,2896,380,42,[],200,llama-13b,128,1,2555.0,1.0,1,H100,1697101710906,1697101713461.0,120,216.0,50.0,"[6, 846, 99, 90, 85, 83, 78, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26, 24, 20, 20, 21, 19, 18, 19, 20, 18, 19, 17, 17, 18, 16, 16, 16, 16, 16, 17]","[1697101710912, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114, 1697101713138, 1697101713158, 1697101713178, 1697101713199, 1697101713218, 1697101713236, 1697101713255, 1697101713275, 1697101713293, 1697101713312, 1697101713329, 1697101713346, 1697101713364, 1697101713380, 1697101713396, 1697101713412, 1697101713428, 1697101713444, 1697101713461]"
2897,2897,197,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634995.0,120,,,"[187, 1092]","[1697101633877, 1697101634969]"
2898,2898,561,18,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101635000,1697101640209.0,120,87.0,20.0,"[508, 1301, 88, 87, 56, 628, 91, 92, 87, 87, 81, 82, 767, 98, 94, 91, 93, 92, 90, 79, 617]","[1697101635508, 1697101636809, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639238, 1697101639331, 1697101639423, 1697101639513, 1697101639592, 1697101640209]"
2899,2899,670,19,[],200,llama-13b,128,1,3470.0,1.0,1,H100,1697101641121,1697101644591.0,120,67.0,18.0,"[24, 672, 304, 101, 99, 96, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 87, 84]","[1697101641145, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644591]"
2900,2900,643,19,[],200,llama-13b,128,1,1809.0,1.0,1,H100,1697101634999,1697101636808.0,120,18.0,1.0,"[273, 1536]","[1697101635272, 1697101636808]"
2901,2901,69,20,[],200,llama-13b,128,1,4309.0,1.0,1,H100,1697101636810,1697101641119.0,120,85.0,20.0,"[60, 609, 189, 91, 93, 87, 85, 83, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618, 81, 93, 735]","[1697101636870, 1697101637479, 1697101637668, 1697101637759, 1697101637852, 1697101637939, 1697101638024, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641119]"
2902,2902,764,7,[],200,llama-13b,128,1,1924.0,1.0,1,H100,1697101616332,1697101618256.0,120,39.0,1.0,"[372, 1552]","[1697101616704, 1697101618256]"
2903,2903,296,24,[],200,llama-13b,128,1,2016.0,1.0,1,H100,1697101658631,1697101660647.0,120,6.0,1.0,"[18, 1998]","[1697101658649, 1697101660647]"
2904,2904,882,25,[],200,llama-13b,128,1,3138.0,1.0,1,H100,1697101645338,1697101648476.0,120,345.0,11.0,"[18, 943, 191, 97, 98, 83, 79, 1225, 106, 101, 101, 96]","[1697101645356, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646847, 1697101648072, 1697101648178, 1697101648279, 1697101648380, 1697101648476]"
2905,2905,378,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646588,1697101651603.0,120,,,"[18, 1477, 95, 101, 101, 96, 88, 485, 96, 96, 83, 942, 106, 104, 101, 91, 85]","[1697101646606, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2906,2906,557,9,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101616331,1697101617376.0,120,31.0,1.0,"[44, 1001]","[1697101616375, 1697101617376]"
2907,2907,833,38,[],200,llama-13b,128,1,3456.0,1.0,1,H100,1697101689619,1697101693075.0,120,563.0,8.0,"[472, 1939, 244, 213, 212, 78, 101, 103, 94]","[1697101690091, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692981, 1697101693075]"
2908,2908,666,17,[],200,llama-13b,128,1,4821.0,1.0,1,H100,1697101644421,1697101649242.0,120,84.0,20.0,"[19, 897, 103, 79, 77, 77, 75, 741, 98, 97, 84, 78, 1236, 96, 101, 100, 97, 88, 485, 96, 97]","[1697101644440, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242]"
2909,2909,198,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634995.0,120,,,"[20, 1259]","[1697101633710, 1697101634969]"
2910,2910,512,23,[],200,llama-13b,128,1,2016.0,1.0,1,H100,1697101658631,1697101660647.0,120,11.0,1.0,"[12, 2003]","[1697101658643, 1697101660646]"
2911,2911,501,27,[],200,llama-13b,128,1,617.0,1.0,1,H100,1697101656530,1697101657147.0,120,19.0,1.0,"[66, 551]","[1697101656596, 1697101657147]"
2912,2912,873,24,[],200,llama-13b,128,1,1268.0,1.0,1,H100,1697101660648,1697101661916.0,120,6.0,1.0,"[18, 1250]","[1697101660666, 1697101661916]"
2913,2913,529,17,[],200,llama-13b,128,1,519.0,1.0,1,H100,1697101634999,1697101635518.0,120,10.0,1.0,"[97, 422]","[1697101635096, 1697101635518]"
2914,2914,886,18,[],200,llama-13b,128,1,1960.0,1.0,1,H100,1697101635519,1697101637479.0,120,17.0,1.0,"[93, 1867]","[1697101635612, 1697101637479]"
2915,2915,890,21,[],200,llama-13b,128,1,4169.0,1.0,1,H100,1697101642418,1697101646587.0,120,93.0,20.0,"[24, 1009, 99, 96, 93, 94, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 78, 74, 741, 98]","[1697101642442, 1697101643451, 1697101643550, 1697101643646, 1697101643739, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587]"
2916,2916,838,23,[],200,llama-13b,128,1,5173.0,1.0,1,H100,1697101658291,1697101663464.0,120,90.0,20.0,"[36, 1046, 332, 102, 98, 97, 93, 85, 805, 101, 76, 98, 93, 87, 831, 107, 102, 94, 94, 89, 707]","[1697101658327, 1697101659373, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662480, 1697101662574, 1697101662668, 1697101662757, 1697101663464]"
2917,2917,319,19,[],200,llama-13b,128,1,1259.0,1.0,1,H100,1697101637480,1697101638739.0,120,31.0,1.0,"[24, 1235]","[1697101637504, 1697101638739]"
2918,2918,389,25,[],200,llama-13b,128,1,1420.0,1.0,1,H100,1697101661917,1697101663337.0,120,8.0,1.0,"[12, 1408]","[1697101661929, 1697101663337]"
2919,2919,749,26,[],200,llama-13b,128,1,4265.0,1.0,1,H100,1697101663338,1697101667603.0,120,47.0,20.0,"[6, 1101, 233, 108, 103, 96, 94, 101, 688, 105, 101, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95]","[1697101663344, 1697101664445, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603]"
2920,2920,681,13,[],200,llama-13b,128,1,1809.0,1.0,1,H100,1697101635000,1697101636809.0,120,23.0,1.0,"[516, 1293]","[1697101635516, 1697101636809]"
2921,2921,675,20,[],200,llama-13b,128,1,2379.0,1.0,1,H100,1697101638740,1697101641119.0,120,563.0,5.0,"[13, 1332, 124, 82, 93, 735]","[1697101638753, 1697101640085, 1697101640209, 1697101640291, 1697101640384, 1697101641119]"
2922,2922,656,14,[],200,llama-13b,128,1,908.0,1.0,1,H100,1697101642223,1697101643131.0,120,26.0,1.0,"[7, 901]","[1697101642230, 1697101643131]"
2923,2923,83,15,[],200,llama-13b,128,1,3635.0,1.0,1,H100,1697101643133,1697101646768.0,120,123.0,15.0,"[14, 1173, 100, 86, 84, 84, 83, 683, 79, 77, 78, 74, 741, 98, 97, 84]","[1697101643147, 1697101644320, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644757, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768]"
2924,2924,173,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674982,1697101675810.0,120,,,"[25, 782]","[1697101675007, 1697101675789]"
2925,2925,469,25,[],200,llama-13b,128,1,1547.0,1.0,1,H100,1697101656613,1697101658160.0,120,17.0,1.0,"[13, 1534]","[1697101656626, 1697101658160]"
2926,2926,827,26,[],200,llama-13b,128,1,5303.0,1.0,1,H100,1697101658161,1697101663464.0,120,96.0,20.0,"[18, 1194, 332, 101, 99, 97, 93, 85, 805, 101, 76, 98, 92, 88, 831, 107, 101, 95, 94, 89, 707]","[1697101658179, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661352, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464]"
2927,2927,579,34,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101687340,1697101688385.0,120,19.0,1.0,"[24, 1021]","[1697101687364, 1697101688385]"
2928,2928,7,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688386,1697101689594.0,120,,,"[6, 1178]","[1697101688392, 1697101689570]"
2929,2929,444,16,[],200,llama-13b,128,1,1707.0,1.0,1,H100,1697101646769,1697101648476.0,120,457.0,6.0,"[12, 961, 340, 97, 100, 101, 96]","[1697101646781, 1697101647742, 1697101648082, 1697101648179, 1697101648279, 1697101648380, 1697101648476]"
2930,2930,802,17,[],200,llama-13b,128,1,470.0,1.0,1,H100,1697101648478,1697101648948.0,120,9.0,1.0,"[17, 452]","[1697101648495, 1697101648947]"
2931,2931,610,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621047,1697101623660.0,120,,,"[48, 578, 85, 81, 80, 78, 61, 614, 86, 87, 84, 83]","[1697101621095, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
2932,2932,203,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651605.0,120,,,"[55, 908, 355, 106, 104, 101, 91, 85]","[1697101649003, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
2933,2933,561,19,[],200,llama-13b,128,1,5002.0,1.0,1,H100,1697101651610,1697101656612.0,120,87.0,20.0,"[495, 1373, 132, 86, 82, 82, 81, 896, 107, 106, 100, 75, 99, 97, 92, 614, 105, 101, 98, 97, 84]","[1697101652105, 1697101653478, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656612]"
2934,2934,187,23,[],200,llama-13b,128,1,1454.0,1.0,1,H100,1697101662379,1697101663833.0,120,161.0,6.0,"[18, 940, 127, 101, 93, 88, 87]","[1697101662397, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833]"
2935,2935,35,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623670,1697101626657.0,120,,,"[314, 1710, 115, 97, 69]","[1697101623984, 1697101625694, 1697101625809, 1697101625906, 1697101625975]"
2936,2936,396,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626665,1697101628905.0,120,,,"[419, 1609]","[1697101627084, 1697101628693]"
2937,2937,337,36,[],200,llama-13b,128,1,1499.0,1.0,1,H100,1697101689613,1697101691112.0,120,12.0,1.0,"[87, 1412]","[1697101689700, 1697101691112]"
2938,2938,695,37,[],200,llama-13b,128,1,3702.0,1.0,1,H100,1697101691113,1697101694815.0,120,92.0,20.0,"[30, 887, 244, 214, 211, 79, 111, 92, 94, 403, 102, 100, 99, 95, 91, 87, 68, 433, 78, 100, 84]","[1697101691143, 1697101692030, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692889, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694815]"
2939,2939,755,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628910,1697101634994.0,120,,,"[460, 1260, 104, 81, 68, 66, 865, 92, 90, 90, 89, 87, 85, 83, 741, 95, 72, 93, 93, 93, 70, 614, 75, 71]","[1697101629370, 1697101630630, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
2940,2940,550,24,[],200,llama-13b,128,1,5010.0,1.0,1,H100,1697101663834,1697101668844.0,120,91.0,20.0,"[13, 1891, 130, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89]","[1697101663847, 1697101665738, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843]"
2941,2941,539,22,[],200,llama-13b,128,1,4529.0,1.0,1,H100,1697101667137,1697101671666.0,120,83.0,20.0,"[19, 1107, 133, 101, 95, 71, 91, 90, 724, 99, 94, 93, 92, 89, 677, 98, 71, 92, 91, 89, 513]","[1697101667156, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670712, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666]"
2942,2942,15,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623673,1697101626658.0,120,,,"[503, 1519, 113, 99, 68]","[1697101624176, 1697101625695, 1697101625808, 1697101625907, 1697101625975]"
2943,2943,778,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101613605,1697101616320.0,120,,,"[12, 1867, 78, 79, 76]","[1697101613617, 1697101615484, 1697101615562, 1697101615641, 1697101615717]"
2944,2944,652,29,[],200,llama-13b,128,1,1547.0,1.0,1,H100,1697101656613,1697101658160.0,120,14.0,1.0,"[83, 1464]","[1697101656696, 1697101658160]"
2945,2945,210,10,[],200,llama-13b,128,1,2032.0,1.0,1,H100,1697101616332,1697101618364.0,120,140.0,2.0,"[367, 1557, 108]","[1697101616699, 1697101618256, 1697101618364]"
2946,2946,79,30,[],200,llama-13b,128,1,1212.0,1.0,1,H100,1697101658162,1697101659374.0,120,12.0,1.0,"[59, 1152]","[1697101658221, 1697101659373]"
2947,2947,567,11,[],200,llama-13b,128,1,3693.0,1.0,1,H100,1697101618365,1697101622058.0,120,90.0,20.0,"[6, 718, 263, 94, 88, 68, 85, 85, 83, 719, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 60]","[1697101618371, 1697101619089, 1697101619352, 1697101619446, 1697101619534, 1697101619602, 1697101619687, 1697101619772, 1697101619855, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057]"
2948,2948,437,31,[],200,llama-13b,128,1,6843.0,1.0,1,H100,1697101659374,1697101666217.0,120,91.0,29.0,"[25, 1248, 338, 102, 75, 98, 93, 87, 831, 107, 101, 95, 94, 89, 708, 100, 93, 88, 87, 845, 108, 103, 96, 94, 101, 688, 105, 100, 72, 72]","[1697101659399, 1697101660647, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663465, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217]"
2949,2949,311,30,[],200,llama-13b,128,1,4138.0,1.0,1,H100,1697101663465,1697101667603.0,120,93.0,20.0,"[7, 1206, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 99, 72, 99, 94]","[1697101663472, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603]"
2950,2950,400,20,[],200,llama-13b,128,1,1633.0,1.0,1,H100,1697101659807,1697101661440.0,120,123.0,7.0,"[7, 832, 339, 102, 75, 98, 93, 87]","[1697101659814, 1697101660646, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440]"
2951,2951,790,32,[],200,llama-13b,128,1,4493.0,1.0,1,H100,1697101666218,1697101670711.0,120,84.0,20.0,"[6, 546, 366, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 95, 92, 93, 88, 676]","[1697101666224, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711]"
2952,2952,681,15,[],200,llama-13b,128,1,869.0,1.0,1,H100,1697101643452,1697101644321.0,120,23.0,1.0,"[18, 851]","[1697101643470, 1697101644321]"
2953,2953,113,16,[],200,llama-13b,128,1,1015.0,1.0,1,H100,1697101644322,1697101645337.0,120,13.0,1.0,"[48, 967]","[1697101644370, 1697101645337]"
2954,2954,472,17,[],200,llama-13b,128,1,5240.0,1.0,1,H100,1697101645338,1697101650578.0,120,85.0,20.0,"[24, 937, 191, 97, 98, 83, 79, 1225, 106, 101, 101, 96, 88, 486, 96, 96, 82, 942, 106, 104, 101]","[1697101645362, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646847, 1697101648072, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577]"
2955,2955,331,14,[],200,llama-13b,128,1,903.0,1.0,1,H100,1697101623665,1697101624568.0,120,26.0,1.0,"[106, 797]","[1697101623771, 1697101624568]"
2956,2956,164,24,[],200,llama-13b,128,1,672.0,1.0,1,H100,1697101655326,1697101655998.0,120,15.0,1.0,"[6, 666]","[1697101655332, 1697101655998]"
2957,2957,519,25,[],200,llama-13b,128,1,11137.0,1.0,1,H100,1697101655999,1697101667136.0,120,58.0,47.0,"[24, 1123, 88, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97, 93, 85, 805, 101, 75, 99, 93, 87, 831, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 108, 103, 96, 94, 91, 697, 106, 100, 72, 72, 93, 92, 733]","[1697101656023, 1697101657146, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665170, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135]"
2958,2958,833,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101650578,1697101651604.0,120,,,"[19, 711]","[1697101650597, 1697101651308]"
2959,2959,65,20,[],200,llama-13b,128,1,6631.0,1.0,1,H100,1697101642418,1697101649049.0,120,39.0,30.0,"[18, 695, 320, 99, 96, 93, 94, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 78, 74, 741, 98, 97, 84, 78, 1235, 97, 101, 100, 97, 88, 485]","[1697101642436, 1697101643131, 1697101643451, 1697101643550, 1697101643646, 1697101643739, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648081, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049]"
2960,2960,258,19,[],200,llama-13b,128,1,11954.0,1.0,1,H100,1697101651610,1697101663564.0,120,244.0,50.0,"[200, 851, 57, 891, 86, 83, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 98, 82, 623, 81, 79, 59, 836, 95, 83, 81, 81, 1076, 101, 98, 98, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100]","[1697101651810, 1697101652661, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656529, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659806, 1697101659904, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564]"
2961,2961,307,45,[],200,llama-13b,128,1,1835.0,1.0,1,H100,1697101709825,1697101711660.0,120,26.0,1.0,"[12, 1823]","[1697101709837, 1697101711660]"
2962,2962,472,29,[],200,llama-13b,128,1,5043.0,1.0,1,H100,1697101668845,1697101673888.0,120,85.0,20.0,"[13, 1627, 226, 99, 70, 93, 90, 91, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 67, 88, 804]","[1697101668858, 1697101670485, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888]"
2963,2963,195,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101670726,1697101675812.0,120,,,"[53, 780, 107, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101670779, 1697101671559, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
2964,2964,923,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[127, 1153]","[1697101633817, 1697101634970]"
2965,2965,899,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673892,1697101675812.0,120,,,"[35, 921, 133, 102, 99, 95, 70, 95]","[1697101673927, 1697101674848, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675442]"
2966,2966,348,21,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101635000,1697101640209.0,120,91.0,20.0,"[396, 1412, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 79, 617]","[1697101635396, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209]"
2967,2967,114,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633433,1697101634995.0,120,,,"[6, 767, 96, 74, 71]","[1697101633439, 1697101634206, 1697101634302, 1697101634376, 1697101634447]"
2968,2968,474,18,[],200,llama-13b,128,1,7650.0,1.0,1,H100,1697101635000,1697101642650.0,120,109.0,33.0,"[582, 1315, 87, 56, 628, 91, 92, 87, 87, 82, 81, 768, 98, 93, 92, 92, 91, 91, 79, 618, 81, 93, 734, 86, 78, 78, 761, 100, 99, 96, 89, 82, 63]","[1697101635582, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639147, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650]"
2969,2969,924,12,[],200,llama-13b,128,1,904.0,1.0,1,H100,1697101623664,1697101624568.0,120,9.0,1.0,"[20, 884]","[1697101623684, 1697101624568]"
2970,2970,352,13,[],200,llama-13b,128,1,1338.0,1.0,1,H100,1697101624569,1697101625907.0,120,11.0,3.0,"[30, 1210, 97]","[1697101624599, 1697101625809, 1697101625906]"
2971,2971,329,32,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,15.0,1.0,"[512, 1052]","[1697101676328, 1697101677380]"
2972,2972,235,39,[],200,llama-13b,128,1,3645.0,1.0,1,H100,1697101693076,1697101696721.0,120,161.0,12.0,"[6, 1472, 77, 100, 85, 604, 79, 76, 831, 103, 101, 111]","[1697101693082, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696721]"
2973,2973,769,33,[],200,llama-13b,128,1,3423.0,1.0,1,H100,1697101677381,1697101680804.0,120,47.0,20.0,"[7, 859, 107, 90, 86, 65, 83, 81, 646, 98, 98, 95, 71, 93, 90, 70, 90, 470, 94, 57, 73]","[1697101677388, 1697101678247, 1697101678354, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680580, 1697101680674, 1697101680731, 1697101680804]"
2974,2974,919,10,[],200,llama-13b,128,1,878.0,1.0,1,H100,1697101617378,1697101618256.0,120,14.0,1.0,"[24, 854]","[1697101617402, 1697101618256]"
2975,2975,319,11,[],200,llama-13b,128,1,832.0,1.0,1,H100,1697101618257,1697101619089.0,120,31.0,1.0,"[18, 814]","[1697101618275, 1697101619089]"
2976,2976,194,34,[],200,llama-13b,128,1,4208.0,1.0,1,H100,1697101680806,1697101685014.0,120,335.0,16.0,"[18, 1687, 193, 96, 93, 92, 68, 91, 90, 614, 102, 101, 97, 92, 90, 90, 594]","[1697101680824, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014]"
2977,2977,678,12,[],200,llama-13b,128,1,3922.0,1.0,1,H100,1697101619090,1697101623012.0,120,244.0,18.0,"[48, 1241, 196, 95, 94, 93, 91, 92, 74, 644, 80, 81, 78, 60, 615, 86, 87, 84, 83]","[1697101619138, 1697101620379, 1697101620575, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621838, 1697101621919, 1697101621997, 1697101622057, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
2978,2978,801,28,[],200,llama-13b,128,1,4220.0,1.0,1,H100,1697101669668,1697101673888.0,120,47.0,20.0,"[7, 810, 226, 99, 71, 92, 91, 90, 512, 97, 95, 94, 93, 90, 521, 114, 91, 68, 68, 88, 803]","[1697101669675, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888]"
2979,2979,254,26,[],200,llama-13b,128,1,958.0,1.0,1,H100,1697101662379,1697101663337.0,120,58.0,1.0,"[37, 921]","[1697101662416, 1697101663337]"
2980,2980,585,27,[],200,llama-13b,128,1,9746.0,1.0,1,H100,1697101663338,1697101673084.0,120,244.0,50.0,"[42, 1065, 233, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 99, 72, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88]","[1697101663380, 1697101664445, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084]"
2981,2981,785,29,[],200,llama-13b,128,1,950.0,1.0,1,H100,1697101664788,1697101665738.0,120,10.0,1.0,"[36, 914]","[1697101664824, 1697101665738]"
2982,2982,213,30,[],200,llama-13b,128,1,1770.0,1.0,1,H100,1697101665739,1697101667509.0,120,123.0,6.0,"[7, 1024, 366, 103, 98, 73, 99]","[1697101665746, 1697101666770, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667509]"
2983,2983,573,31,[],200,llama-13b,128,1,887.0,1.0,1,H100,1697101667509,1697101668396.0,120,874.0,2.0,"[13, 874]","[1697101667522, 1697101668396]"
2984,2984,923,32,[],200,llama-13b,128,1,1550.0,1.0,1,H100,1697101668397,1697101669947.0,120,140.0,6.0,"[18, 1029, 125, 98, 95, 92, 93]","[1697101668415, 1697101669444, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947]"
2985,2985,261,28,[],200,llama-13b,128,1,1625.0,1.0,1,H100,1697101666771,1697101668396.0,120,874.0,2.0,"[13, 1612]","[1697101666784, 1697101668396]"
2986,2986,590,29,[],200,llama-13b,128,1,4260.0,1.0,1,H100,1697101668397,1697101672657.0,120,88.0,20.0,"[12, 1035, 125, 98, 95, 92, 93, 88, 676, 99, 70, 93, 90, 90, 513, 97, 95, 94, 93, 91, 521]","[1697101668409, 1697101669444, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672657]"
2987,2987,322,33,[],200,llama-13b,128,1,3941.0,1.0,1,H100,1697101669948,1697101673889.0,120,93.0,20.0,"[12, 751, 99, 71, 92, 91, 90, 512, 97, 95, 94, 93, 90, 521, 114, 91, 68, 68, 87, 805]","[1697101669960, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673084, 1697101673889]"
2988,2988,735,24,[],200,llama-13b,128,1,5002.0,1.0,1,H100,1697101651609,1697101656611.0,120,85.0,20.0,"[299, 1564, 138, 85, 84, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 82]","[1697101651908, 1697101653472, 1697101653610, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656611]"
2989,2989,168,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101656615,1697101675811.0,120,,,"[155, 1390, 129, 96, 82, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 102, 75, 104, 87, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 100, 689, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 98, 71, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101656770, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
2990,2990,704,23,[],200,llama-13b,128,1,1993.0,1.0,1,H100,1697101645749,1697101647742.0,120,14.0,1.0,"[7, 1986]","[1697101645756, 1697101647742]"
2991,2991,133,24,[],200,llama-13b,128,1,1204.0,1.0,1,H100,1697101647743,1697101648947.0,120,15.0,1.0,"[6, 1198]","[1697101647749, 1697101648947]"
2992,2992,498,25,[],200,llama-13b,128,1,963.0,1.0,1,H100,1697101648948,1697101649911.0,120,9.0,1.0,"[37, 926]","[1697101648985, 1697101649911]"
2993,2993,858,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649913,1697101651604.0,120,,,[48],[1697101649961]
2994,2994,866,29,[],200,llama-13b,128,1,4736.0,1.0,1,H100,1697101665975,1697101670711.0,120,93.0,20.0,"[18, 1143, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 95, 92, 93, 88, 676]","[1697101665993, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711]"
2995,2995,543,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101675817,1697101689614.0,120,,,"[585, 978, 97, 64, 51, 762, 90, 86, 64, 84, 80, 647, 99, 98, 94, 72, 92, 91, 69, 90, 463, 101, 57, 73, 804, 84, 83, 79, 78, 771, 97, 93, 91, 69, 91, 90, 615, 101, 101, 97, 92, 90, 90, 594, 85, 64, 82, 81, 911, 91, 89, 87, 86, 647, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 91, 89]","[1697101676402, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680110, 1697101680573, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
2996,2996,257,27,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101651610,1697101652661.0,120,14.0,1.0,"[104, 947]","[1697101651714, 1697101652661]"
2997,2997,616,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101652663,1697101675812.0,120,,,"[59, 1909, 206, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83, 623, 81, 79, 60, 835, 95, 83, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 844, 109, 103, 96, 94, 91, 697, 106, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 94, 793, 102, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 676, 99, 71, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101652722, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664677, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665170, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667602, 1697101668395, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670710, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
2998,2998,817,23,[],200,llama-13b,128,1,5219.0,1.0,1,H100,1697101661917,1697101667136.0,120,86.0,20.0,"[18, 1402, 127, 101, 93, 88, 87, 845, 108, 104, 95, 94, 92, 696, 106, 100, 72, 72, 93, 92, 734]","[1697101661935, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664890, 1697101664985, 1697101665079, 1697101665171, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136]"
2999,2999,676,24,[],200,llama-13b,128,1,659.0,1.0,1,H100,1697101665080,1697101665739.0,120,19.0,1.0,"[7, 652]","[1697101665087, 1697101665739]"
3000,3000,108,25,[],200,llama-13b,128,1,1396.0,1.0,1,H100,1697101665740,1697101667136.0,120,182.0,2.0,"[18, 1012, 366]","[1697101665758, 1697101666770, 1697101667136]"
3001,3001,880,26,[],200,llama-13b,128,1,2000.0,1.0,1,H100,1697101651610,1697101653610.0,120,84.0,2.0,"[274, 1588, 138]","[1697101651884, 1697101653472, 1697101653610]"
3002,3002,124,38,[],200,llama-13b,128,1,1588.0,1.0,1,H100,1697101694818,1697101696406.0,120,83.0,2.0,"[75, 1513]","[1697101694893, 1697101696406]"
3003,3003,559,26,[],200,llama-13b,128,1,4529.0,1.0,1,H100,1697101667137,1697101671666.0,120,86.0,20.0,"[43, 1083, 133, 101, 95, 71, 91, 90, 724, 99, 94, 93, 92, 89, 677, 98, 71, 92, 91, 89, 513]","[1697101667180, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670712, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666]"
3004,3004,316,27,[],200,llama-13b,128,1,4678.0,1.0,1,H100,1697101653611,1697101658289.0,120,86.0,20.0,"[6, 1014, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 83, 622, 81, 79, 60, 835]","[1697101653617, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
3005,3005,483,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696407,1697101697589.0,120,,,"[13, 976]","[1697101696420, 1697101697396]"
3006,3006,734,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.28 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.71 GiB is free. Process 1607256 has 73.38 GiB memory in use. Of the allocated memory 40.99 GiB is allocated by PyTorch, and 31.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101608939,1697101610266.0,120,,,"[30, 1135]","[1697101608969, 1697101610104]"
3007,3007,232,27,[],200,llama-13b,128,1,4056.0,1.0,1,H100,1697101664788,1697101668844.0,120,93.0,20.0,"[6, 1074, 106, 99, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90]","[1697101664794, 1697101665868, 1697101665974, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844]"
3008,3008,675,21,[],200,llama-13b,128,1,1327.0,1.0,1,H100,1697101645441,1697101646768.0,120,563.0,5.0,"[6, 852, 191, 97, 98, 83]","[1697101645447, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768]"
3009,3009,590,28,[],200,llama-13b,128,1,5043.0,1.0,1,H100,1697101668845,1697101673888.0,120,88.0,20.0,"[18, 1621, 227, 99, 70, 93, 90, 91, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 67, 88, 804]","[1697101668863, 1697101670484, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888]"
3010,3010,164,6,[],200,llama-13b,128,1,1796.0,1.0,1,H100,1697101610272,1697101612068.0,120,15.0,1.0,"[235, 1561]","[1697101610507, 1697101612068]"
3011,3011,528,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.25 GiB is free. Process 1607256 has 71.85 GiB memory in use. Of the allocated memory 41.50 GiB is allocated by PyTorch, and 29.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101612070,1697101616320.0,120,,,"[6, 938, 107, 90, 85, 84, 81, 62, 80, 720, 95, 93, 92, 90, 69, 88, 633, 79, 78, 76]","[1697101612076, 1697101613014, 1697101613121, 1697101613211, 1697101613296, 1697101613380, 1697101613461, 1697101613523, 1697101613603, 1697101614323, 1697101614418, 1697101614511, 1697101614603, 1697101614693, 1697101614762, 1697101614850, 1697101615483, 1697101615562, 1697101615640, 1697101615716]"
3012,3012,626,24,[],200,llama-13b,128,1,487.0,1.0,1,H100,1697101643834,1697101644321.0,120,10.0,1.0,"[15, 472]","[1697101643849, 1697101644321]"
3013,3013,55,25,[],200,llama-13b,128,1,1015.0,1.0,1,H100,1697101644322,1697101645337.0,120,12.0,1.0,"[30, 985]","[1697101644352, 1697101645337]"
3014,3014,418,26,[],200,llama-13b,128,1,1249.0,1.0,1,H100,1697101645338,1697101646587.0,120,286.0,3.0,"[6, 955, 190, 98]","[1697101645344, 1697101646299, 1697101646489, 1697101646587]"
3015,3015,327,22,[],200,llama-13b,128,1,2088.0,1.0,1,H100,1697101642418,1697101644506.0,120,563.0,10.0,"[12, 701, 319, 100, 96, 93, 94, 87, 85, 415, 86]","[1697101642430, 1697101643131, 1697101643450, 1697101643550, 1697101643646, 1697101643739, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506]"
3016,3016,498,25,[],200,llama-13b,128,1,981.0,1.0,1,H100,1697101663465,1697101664446.0,120,9.0,1.0,"[25, 956]","[1697101663490, 1697101664446]"
3017,3017,634,15,[],200,llama-13b,128,1,969.0,1.0,1,H100,1697101631998,1697101632967.0,120,13.0,1.0,"[6, 962]","[1697101632004, 1697101632966]"
3018,3018,751,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646588,1697101651603.0,120,,,"[25, 1470, 95, 101, 101, 96, 88, 485, 97, 95, 83, 942, 106, 104, 101, 91, 85]","[1697101646613, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
3019,3019,56,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101632968,1697101634995.0,120,,,"[18, 1220, 96, 74, 71]","[1697101632986, 1697101634206, 1697101634302, 1697101634376, 1697101634447]"
3020,3020,685,23,[],200,llama-13b,128,1,933.0,1.0,1,H100,1697101644507,1697101645440.0,120,364.0,2.0,"[7, 823, 103]","[1697101644514, 1697101645337, 1697101645440]"
3021,3021,86,24,[],200,llama-13b,128,1,4825.0,1.0,1,H100,1697101645441,1697101650266.0,120,335.0,17.0,"[12, 846, 191, 97, 98, 83, 79, 1235, 96, 101, 101, 96, 88, 486, 96, 95, 83, 942]","[1697101645453, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646847, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649241, 1697101649324, 1697101650266]"
3022,3022,655,25,[],200,llama-13b,128,1,3099.0,1.0,1,H100,1697101660648,1697101663747.0,120,335.0,11.0,"[6, 1262, 355, 107, 101, 95, 94, 89, 707, 100, 94, 89]","[1697101660654, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663747]"
3023,3023,556,20,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101649243,1697101649912.0,120,9.0,1.0,"[42, 626]","[1697101649285, 1697101649911]"
3024,3024,71,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649913,1697101651604.0,120,,,"[24, 1371]","[1697101649937, 1697101651308]"
3025,3025,431,22,[],200,llama-13b,128,1,5627.0,1.0,1,H100,1697101651607,1697101657234.0,120,732.0,22.0,"[107, 947, 57, 892, 86, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 97, 83, 623]","[1697101651714, 1697101652661, 1697101652718, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234]"
3026,3026,384,17,[],200,llama-13b,128,1,6118.0,1.0,1,H100,1697101635001,1697101641119.0,120,92.0,20.0,"[587, 1890, 190, 91, 92, 87, 87, 82, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618, 81, 93, 735]","[1697101635588, 1697101637478, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641119]"
3027,3027,242,13,[],200,llama-13b,128,1,3026.0,1.0,1,H100,1697101634998,1697101638024.0,120,345.0,9.0,"[319, 1491, 89, 87, 56, 628, 91, 92, 87, 86]","[1697101635317, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024]"
3028,3028,0,14,[],200,llama-13b,128,1,10676.0,1.0,1,H100,1697101634997,1697101645673.0,120,244.0,50.0,"[87, 434, 66, 1313, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 92, 93, 91, 91, 79, 616, 83, 93, 734, 86, 78, 78, 760, 101, 99, 96, 88, 83, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 683, 79, 78, 77]","[1697101635084, 1697101635518, 1697101635584, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640208, 1697101640291, 1697101640384, 1697101641118, 1697101641204, 1697101641282, 1697101641360, 1697101642120, 1697101642221, 1697101642320, 1697101642416, 1697101642504, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645439, 1697101645518, 1697101645596, 1697101645673]"
3029,3029,528,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648280,1697101651604.0,120,,,"[6, 661, 103, 96, 96, 82, 942, 106, 103, 102, 90, 86]","[1697101648286, 1697101648947, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
3030,3030,893,19,[],200,llama-13b,128,1,3441.0,1.0,1,H100,1697101651609,1697101655050.0,120,335.0,10.0,"[159, 893, 57, 892, 86, 82, 82, 81, 896, 108, 105]","[1697101651768, 1697101652661, 1697101652718, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050]"
3031,3031,795,23,[],200,llama-13b,128,1,925.0,1.0,1,H100,1697101657235,1697101658160.0,120,12.0,1.0,"[6, 919]","[1697101657241, 1697101658160]"
3032,3032,225,24,[],200,llama-13b,128,1,1212.0,1.0,1,H100,1697101658162,1697101659374.0,120,23.0,1.0,"[59, 1152]","[1697101658221, 1697101659373]"
3033,3033,323,20,[],200,llama-13b,128,1,4951.0,1.0,1,H100,1697101655051,1697101660002.0,120,84.0,20.0,"[7, 940, 129, 105, 101, 98, 98, 83, 622, 81, 79, 60, 836, 95, 82, 82, 80, 1076, 101, 99, 97]","[1697101655058, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658290, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002]"
3034,3034,707,14,[],200,llama-13b,128,1,658.0,1.0,1,H100,1697101625908,1697101626566.0,120,8.0,1.0,"[12, 646]","[1697101625920, 1697101626566]"
3035,3035,544,16,[],200,llama-13b,128,1,1280.0,1.0,1,H100,1697101633690,1697101634970.0,120,26.0,1.0,"[106, 1173]","[1697101633796, 1697101634969]"
3036,3036,554,25,[],200,llama-13b,128,1,1272.0,1.0,1,H100,1697101659375,1697101660647.0,120,26.0,1.0,"[36, 1236]","[1697101659411, 1697101660647]"
3037,3037,592,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696722,1697101697589.0,120,,,"[36, 638]","[1697101696758, 1697101697396]"
3038,3038,133,15,[],200,llama-13b,128,1,307.0,1.0,1,H100,1697101626567,1697101626874.0,120,15.0,1.0,"[36, 271]","[1697101626603, 1697101626874]"
3039,3039,876,24,[],200,llama-13b,128,1,1184.0,1.0,1,H100,1697101643136,1697101644320.0,120,11.0,1.0,"[15, 1169]","[1697101643151, 1697101644320]"
3040,3040,24,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697602,1697101700636.0,120,,,"[514, 1472, 83, 82, 81, 81, 82]","[1697101698116, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
3041,3041,234,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673893,1697101675812.0,120,,,"[42, 913, 133, 102, 99, 95, 70, 95]","[1697101673935, 1697101674848, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675442]"
3042,3042,553,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685015,1697101689614.0,120,,,"[18, 1083, 121, 91, 89, 87, 87, 646, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101685033, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3043,3043,491,16,[],200,llama-13b,128,1,1818.0,1.0,1,H100,1697101626875,1697101628693.0,120,14.0,1.0,"[398, 1420]","[1697101627273, 1697101628693]"
3044,3044,821,17,[],200,llama-13b,128,1,4992.0,1.0,1,H100,1697101628695,1697101633687.0,120,85.0,20.0,"[24, 698, 1316, 82, 68, 66, 864, 92, 91, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 92, 71]","[1697101628719, 1697101629417, 1697101630733, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631905, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633616, 1697101633687]"
3045,3045,588,30,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,11.0,1.0,"[572, 992]","[1697101676388, 1697101677380]"
3046,3046,270,24,[],200,llama-13b,128,1,2752.0,1.0,1,H100,1697101663465,1697101666217.0,120,364.0,12.0,"[19, 962, 232, 109, 102, 96, 94, 100, 689, 105, 100, 72, 72]","[1697101663484, 1697101664446, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217]"
3047,3047,380,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700643,1697101703171.0,120,,,[477],[1697101701120]
3048,3048,708,43,[],200,llama-13b,128,1,1723.0,1.0,1,H100,1697101703176,1697101704899.0,120,140.0,1.0,"[295, 1428]","[1697101703471, 1697101704899]"
3049,3049,133,44,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,15.0,1.0,"[12, 865]","[1697101704913, 1697101705778]"
3050,3050,414,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673890,1697101675812.0,120,,,"[21, 937, 133, 102, 99, 95, 70, 94]","[1697101673911, 1697101674848, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
3051,3051,773,27,[],200,llama-13b,128,1,4757.0,1.0,1,H100,1697101675814,1697101680571.0,120,90.0,20.0,"[361, 1205, 97, 64, 50, 764, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101676175, 1697101677380, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
3052,3052,488,13,[],200,llama-13b,128,1,903.0,1.0,1,H100,1697101623665,1697101624568.0,120,6.0,1.0,"[148, 755]","[1697101623813, 1697101624568]"
3053,3053,496,45,[],200,llama-13b,128,1,2747.0,1.0,1,H100,1697101705779,1697101708526.0,120,335.0,11.0,"[36, 1230, 240, 97, 92, 70, 91, 70, 91, 87, 544, 99]","[1697101705815, 1697101707045, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526]"
3054,3054,842,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101624569,1697101626655.0,120,,,"[36, 1090, 114, 97, 69]","[1697101624605, 1697101625695, 1697101625809, 1697101625906, 1697101625975]"
3055,3055,616,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101658163,1697101675811.0,120,,,"[64, 1146, 332, 102, 98, 97, 93, 85, 805, 101, 75, 99, 93, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 100, 689, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 98, 71, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101658227, 1697101659373, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3056,3056,272,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626659,1697101628904.0,120,,,"[46, 852, 42]","[1697101626705, 1697101627557, 1697101627599]"
3057,3057,530,36,[],200,llama-13b,128,1,1566.0,1.0,1,H100,1697101675814,1697101677380.0,120,26.0,1.0,"[200, 1366]","[1697101676014, 1697101677380]"
3058,3058,255,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101653697,1697101675811.0,120,,,"[18, 916, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 83, 622, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 108, 103, 96, 94, 91, 697, 106, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 94, 794, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 98, 71, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101653715, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665170, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667602, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3059,3059,254,18,[],200,llama-13b,128,1,1279.0,1.0,1,H100,1697101633690,1697101634969.0,120,58.0,1.0,"[33, 1246]","[1697101633723, 1697101634969]"
3060,3060,647,21,[],200,llama-13b,128,1,4981.0,1.0,1,H100,1697101645597,1697101650578.0,120,83.0,20.0,"[6, 696, 191, 97, 98, 83, 79, 1235, 96, 101, 101, 96, 88, 486, 96, 95, 83, 942, 106, 104, 102]","[1697101645603, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646847, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650578]"
3061,3061,918,20,[],200,llama-13b,128,1,1546.0,1.0,1,H100,1697101656614,1697101658160.0,120,23.0,1.0,"[93, 1453]","[1697101656707, 1697101658160]"
3062,3062,76,22,[],200,llama-13b,128,1,4646.0,1.0,1,H100,1697101650579,1697101655225.0,120,364.0,12.0,"[30, 1267, 842, 891, 86, 83, 82, 81, 896, 107, 106, 100, 75]","[1697101650609, 1697101651876, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225]"
3063,3063,348,21,[],200,llama-13b,128,1,5303.0,1.0,1,H100,1697101658161,1697101663464.0,120,91.0,20.0,"[30, 1182, 332, 101, 99, 97, 93, 85, 805, 101, 75, 99, 92, 88, 831, 107, 101, 95, 94, 89, 707]","[1697101658191, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464]"
3064,3064,252,27,[],200,llama-13b,128,1,5032.0,1.0,1,H100,1697101663465,1697101668497.0,120,182.0,22.0,"[7, 973, 233, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 99, 72, 99, 94, 793, 101]","[1697101663472, 1697101664445, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497]"
3065,3065,702,21,[],200,llama-13b,128,1,5093.0,1.0,1,H100,1697101638740,1697101643833.0,120,89.0,20.0,"[7, 1338, 124, 82, 93, 735, 85, 79, 77, 761, 101, 98, 96, 89, 82, 63, 800, 99, 96, 93, 95]","[1697101638747, 1697101640085, 1697101640209, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642222, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833]"
3066,3066,464,12,[],200,llama-13b,128,1,2022.0,1.0,1,H100,1697101626671,1697101628693.0,120,12.0,1.0,"[546, 1476]","[1697101627217, 1697101628693]"
3067,3067,111,24,[],200,llama-13b,128,1,1799.0,1.0,1,H100,1697101654632,1697101656431.0,120,79.0,5.0,"[6, 1360, 128, 106, 101, 98]","[1697101654638, 1697101655998, 1697101656126, 1697101656232, 1697101656333, 1697101656431]"
3068,3068,215,32,[],200,llama-13b,128,1,1365.0,1.0,1,H100,1697101694818,1697101696183.0,120,12.0,1.0,"[63, 1302]","[1697101694881, 1697101696183]"
3069,3069,871,23,[],200,llama-13b,128,1,1330.0,1.0,1,H100,1697101671667,1697101672997.0,120,123.0,6.0,"[24, 855, 111, 113, 91, 68, 68]","[1697101671691, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997]"
3070,3070,569,33,[],200,llama-13b,128,1,1212.0,1.0,1,H100,1697101696184,1697101697396.0,120,16.0,1.0,"[37, 1174]","[1697101696221, 1697101697395]"
3071,3071,763,21,[],200,llama-13b,128,1,1896.0,1.0,1,H100,1697101661441,1697101663337.0,120,20.0,1.0,"[7, 1889]","[1697101661448, 1697101663337]"
3072,3072,188,22,[],200,llama-13b,128,1,4265.0,1.0,1,H100,1697101663338,1697101667603.0,120,85.0,20.0,"[12, 1095, 233, 108, 103, 96, 94, 101, 688, 105, 101, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95]","[1697101663350, 1697101664445, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603]"
3073,3073,546,23,[],200,llama-13b,128,1,5052.0,1.0,1,H100,1697101667605,1697101672657.0,120,93.0,20.0,"[30, 1933, 99, 94, 93, 92, 89, 676, 98, 71, 93, 90, 90, 513, 97, 95, 94, 93, 90, 522]","[1697101667635, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657]"
3074,3074,467,25,[],200,llama-13b,128,1,4828.0,1.0,1,H100,1697101656432,1697101661260.0,120,93.0,20.0,"[7, 795, 81, 79, 60, 835, 96, 82, 81, 81, 1076, 102, 98, 97, 93, 85, 805, 101, 76, 98]","[1697101656439, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260]"
3075,3075,2,34,[],200,llama-13b,128,1,2438.0,1.0,1,H100,1697101697396,1697101699834.0,120,58.0,6.0,"[7, 369, 566, 1249, 84, 82, 81]","[1697101697403, 1697101697772, 1697101698338, 1697101699587, 1697101699671, 1697101699753, 1697101699834]"
3076,3076,300,24,[],200,llama-13b,128,1,761.0,1.0,1,H100,1697101672998,1697101673759.0,120,9.0,1.0,"[12, 749]","[1697101673010, 1697101673759]"
3077,3077,857,26,[],200,llama-13b,128,1,1291.0,1.0,1,H100,1697101664447,1697101665738.0,120,18.0,1.0,"[18, 1273]","[1697101664465, 1697101665738]"
3078,3078,282,27,[],200,llama-13b,128,1,4972.0,1.0,1,H100,1697101665739,1697101670711.0,120,87.0,20.0,"[13, 1018, 366, 103, 98, 73, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 92, 89, 676]","[1697101665752, 1697101666770, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711]"
3079,3079,700,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628908,1697101634994.0,120,,,"[61, 1765, 81, 68, 66, 864, 93, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 93, 70, 614, 75, 71]","[1697101628969, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
3080,3080,512,21,[],200,llama-13b,128,1,862.0,1.0,1,H100,1697101649050,1697101649912.0,120,11.0,1.0,"[7, 855]","[1697101649057, 1697101649912]"
3081,3081,298,24,[],200,llama-13b,128,1,662.0,1.0,1,H100,1697101644675,1697101645337.0,120,17.0,1.0,"[6, 656]","[1697101644681, 1697101645337]"
3082,3082,317,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646685,1697101651603.0,120,,,"[7, 1050, 341, 95, 101, 101, 96, 88, 485, 97, 95, 83, 942, 106, 103, 102, 91, 85]","[1697101646692, 1697101647742, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
3083,3083,915,19,[],200,llama-13b,128,1,790.0,1.0,1,H100,1697101640212,1697101641002.0,120,182.0,1.0,"[70, 720]","[1697101640282, 1697101641002]"
3084,3084,340,20,[],200,llama-13b,128,1,3753.0,1.0,1,H100,1697101641003,1697101644756.0,120,85.0,20.0,"[19, 795, 305, 100, 98, 97, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 86, 84, 84, 82]","[1697101641022, 1697101641817, 1697101642122, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756]"
3085,3085,659,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101645340,1697101651605.0,120,,,"[34, 925, 191, 97, 98, 83, 79, 1235, 96, 101, 101, 96, 88, 486, 96, 96, 82, 942, 106, 104, 101, 91, 85]","[1697101645374, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646847, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
3086,3086,733,21,[],200,llama-13b,128,1,1055.0,1.0,1,H100,1697101651606,1697101652661.0,120,31.0,1.0,"[40, 1015]","[1697101651646, 1697101652661]"
3087,3087,158,22,[],200,llama-13b,128,1,3949.0,1.0,1,H100,1697101652662,1697101656611.0,120,85.0,20.0,"[18, 792, 138, 86, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83]","[1697101652680, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611]"
3088,3088,699,21,[],200,llama-13b,128,1,1540.0,1.0,1,H100,1697101644759,1697101646299.0,120,39.0,1.0,"[116, 1424]","[1697101644875, 1697101646299]"
3089,3089,91,18,[],200,llama-13b,128,1,668.0,1.0,1,H100,1697101649243,1697101649911.0,120,23.0,1.0,"[30, 638]","[1697101649273, 1697101649911]"
3090,3090,534,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649912,1697101651603.0,120,,,"[13, 1383]","[1697101649925, 1697101651308]"
3091,3091,743,18,[],200,llama-13b,128,1,1384.0,1.0,1,H100,1697101641121,1697101642505.0,120,123.0,6.0,"[54, 642, 304, 101, 99, 96, 88]","[1697101641175, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505]"
3092,3092,890,20,[],200,llama-13b,128,1,4922.0,1.0,1,H100,1697101651607,1697101656529.0,120,93.0,20.0,"[83, 1028, 892, 86, 82, 82, 81, 896, 108, 105, 101, 74, 99, 97, 92, 613, 106, 100, 99, 97]","[1697101651690, 1697101652718, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
3093,3093,100,22,[],200,llama-13b,128,1,4176.0,1.0,1,H100,1697101646300,1697101650476.0,120,732.0,14.0,"[18, 1424, 341, 95, 101, 101, 96, 88, 485, 96, 96, 83, 942, 106, 104]","[1697101646318, 1697101647742, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476]"
3094,3094,176,19,[],200,llama-13b,128,1,945.0,1.0,1,H100,1697101642506,1697101643451.0,120,216.0,2.0,"[7, 938]","[1697101642513, 1697101643451]"
3095,3095,537,20,[],200,llama-13b,128,1,4928.0,1.0,1,H100,1697101643452,1697101648380.0,120,83.0,20.0,"[6, 863, 99, 87, 84, 83, 83, 682, 79, 78, 78, 74, 742, 97, 98, 83, 78, 1237, 95, 101, 100]","[1697101643458, 1697101644321, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645439, 1697101645518, 1697101645596, 1697101645674, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648379]"
3096,3096,323,21,[],200,llama-13b,128,1,4736.0,1.0,1,H100,1697101656530,1697101661266.0,120,84.0,20.0,"[72, 545, 87, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 102, 98, 98, 92, 85, 805, 101, 76, 103]","[1697101656602, 1697101657147, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660003, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661265]"
3097,3097,681,21,[],200,llama-13b,128,1,643.0,1.0,1,H100,1697101660004,1697101660647.0,120,23.0,1.0,"[30, 613]","[1697101660034, 1697101660647]"
3098,3098,82,22,[],200,llama-13b,128,1,5325.0,1.0,1,H100,1697101660648,1697101665973.0,120,67.0,20.0,"[12, 1256, 355, 107, 101, 95, 94, 89, 707, 100, 94, 89, 86, 845, 109, 102, 96, 94, 101, 688, 105]","[1697101660660, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973]"
3099,3099,518,23,[],200,llama-13b,128,1,1547.0,1.0,1,H100,1697101656613,1697101658160.0,120,23.0,1.0,"[88, 1459]","[1697101656701, 1697101658160]"
3100,3100,433,23,[],200,llama-13b,128,1,1162.0,1.0,1,H100,1697101665974,1697101667136.0,120,109.0,2.0,"[13, 1149]","[1697101665987, 1697101667136]"
3101,3101,790,24,[],200,llama-13b,128,1,4529.0,1.0,1,H100,1697101667137,1697101671666.0,120,84.0,20.0,"[7, 1119, 133, 101, 95, 71, 91, 90, 724, 99, 95, 92, 92, 89, 677, 98, 71, 92, 91, 89, 513]","[1697101667144, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669762, 1697101669854, 1697101669946, 1697101670035, 1697101670712, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666]"
3102,3102,462,23,[],200,llama-13b,128,1,831.0,1.0,1,H100,1697101650477,1697101651308.0,120,52.0,1.0,"[12, 819]","[1697101650489, 1697101651308]"
3103,3103,824,24,[],200,llama-13b,128,1,2386.0,1.0,1,H100,1697101651309,1697101653695.0,120,58.0,4.0,"[7, 561, 841, 891, 86]","[1697101651316, 1697101651877, 1697101652718, 1697101653609, 1697101653695]"
3104,3104,18,31,[],200,llama-13b,128,1,865.0,1.0,1,H100,1697101677382,1697101678247.0,120,15.0,1.0,"[108, 757]","[1697101677490, 1697101678247]"
3105,3105,463,32,[],200,llama-13b,128,1,1040.0,1.0,1,H100,1697101678248,1697101679288.0,120,39.0,1.0,"[36, 1004]","[1697101678284, 1697101679288]"
3106,3106,252,25,[],200,llama-13b,128,1,4771.0,1.0,1,H100,1697101653696,1697101658467.0,120,182.0,22.0,"[7, 928, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 83, 622, 81, 79, 60, 835, 96, 82]","[1697101653703, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467]"
3107,3107,875,24,[],200,llama-13b,128,1,7707.0,1.0,1,H100,1697101658161,1697101665868.0,120,31.0,31.0,"[36, 1176, 332, 101, 99, 97, 93, 85, 805, 101, 75, 99, 92, 88, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 100, 689]","[1697101658197, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868]"
3108,3108,820,33,[],200,llama-13b,128,1,2565.0,1.0,1,H100,1697101679289,1697101681854.0,120,161.0,9.0,"[18, 1172, 92, 103, 58, 72, 804, 84, 83, 79]","[1697101679307, 1697101680479, 1697101680571, 1697101680674, 1697101680732, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854]"
3109,3109,220,25,[],200,llama-13b,128,1,990.0,1.0,1,H100,1697101671667,1697101672657.0,120,67.0,2.0,"[6, 873, 111]","[1697101671673, 1697101672546, 1697101672657]"
3110,3110,585,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672658,1697101675812.0,120,,,"[30, 1071, 129, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672688, 1697101673759, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3111,3111,169,27,[],200,llama-13b,128,1,1839.0,1.0,1,H100,1697101667605,1697101669444.0,120,10.0,1.0,"[36, 1803]","[1697101667641, 1697101669444]"
3112,3112,250,34,[],200,llama-13b,128,1,656.0,1.0,1,H100,1697101681855,1697101682511.0,120,31.0,1.0,"[7, 649]","[1697101681862, 1697101682511]"
3113,3113,529,28,[],200,llama-13b,128,1,1040.0,1.0,1,H100,1697101669445,1697101670485.0,120,10.0,1.0,"[24, 1016]","[1697101669469, 1697101670485]"
3114,3114,578,26,[],200,llama-13b,128,1,905.0,1.0,1,H100,1697101658468,1697101659373.0,120,31.0,1.0,"[6, 899]","[1697101658474, 1697101659373]"
3115,3115,608,35,[],200,llama-13b,128,1,4827.0,1.0,1,H100,1697101682512,1697101687339.0,120,96.0,20.0,"[12, 1117, 208, 102, 100, 97, 92, 90, 90, 593, 86, 64, 82, 81, 911, 91, 89, 87, 86, 647, 102]","[1697101682524, 1697101683641, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339]"
3116,3116,3,27,[],200,llama-13b,128,1,5413.0,1.0,1,H100,1697101659374,1697101664787.0,120,89.0,20.0,"[19, 1254, 338, 101, 76, 98, 93, 87, 831, 107, 101, 95, 94, 89, 708, 100, 93, 88, 87, 845, 109]","[1697101659393, 1697101660647, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663465, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787]"
3117,3117,913,27,[],200,llama-13b,128,1,4755.0,1.0,1,H100,1697101675816,1697101680571.0,120,88.0,20.0,"[377, 1187, 97, 64, 50, 764, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 91, 460]","[1697101676193, 1697101677380, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571]"
3118,3118,667,18,[],200,llama-13b,128,1,4333.0,1.0,1,H100,1697101634998,1697101639331.0,120,364.0,17.0,"[74, 446, 66, 1313, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 92, 93]","[1697101635072, 1697101635518, 1697101635584, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639331]"
3119,3119,282,25,[],200,llama-13b,128,1,4842.0,1.0,1,H100,1697101665869,1697101670711.0,120,87.0,20.0,"[13, 888, 366, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 93, 88, 676]","[1697101665882, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669947, 1697101670035, 1697101670711]"
3120,3120,344,28,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101680582,1697101681502.0,120,13.0,1.0,"[54, 866]","[1697101680636, 1697101681502]"
3121,3121,881,37,[],200,llama-13b,128,1,1297.0,1.0,1,H100,1697101677381,1697101678678.0,120,58.0,6.0,"[7, 859, 107, 90, 86, 64, 84]","[1697101677388, 1697101678247, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678]"
3122,3122,311,38,[],200,llama-13b,128,1,4025.0,1.0,1,H100,1697101678679,1697101682704.0,120,93.0,20.0,"[6, 720, 99, 98, 94, 72, 92, 91, 69, 91, 460, 103, 57, 73, 804, 84, 83, 79, 79, 771]","[1697101678685, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704]"
3123,3123,854,46,[],200,llama-13b,128,1,4341.0,1.0,1,H100,1697101708533,1697101712874.0,120,67.0,29.0,"[148, 1746, 277, 103, 97, 93, 89, 88, 86, 497, 100, 91, 84, 83, 78, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32, 31]","[1697101708681, 1697101710427, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711857, 1697101711948, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874]"
3124,3124,615,19,[],200,llama-13b,128,1,4620.0,1.0,1,H100,1697101634970,1697101639590.0,120,93.0,20.0,"[19, 594, 1314, 87, 55, 629, 91, 92, 87, 86, 82, 82, 767, 98, 93, 92, 92, 92, 91, 69]","[1697101634989, 1697101635583, 1697101636897, 1697101636984, 1697101637039, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639330, 1697101639422, 1697101639513, 1697101639582]"
3125,3125,779,20,[],200,llama-13b,128,1,2293.0,1.0,1,H100,1697101640212,1697101642505.0,120,563.0,10.0,"[18, 772, 117, 85, 78, 78, 761, 101, 99, 95, 89]","[1697101640230, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505]"
3126,3126,288,30,[],200,llama-13b,128,1,4255.0,1.0,1,H100,1697101670726,1697101674981.0,120,93.0,20.0,"[43, 897, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 66, 686]","[1697101670769, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981]"
3127,3127,242,24,[],200,llama-13b,128,1,2530.0,1.0,1,H100,1697101667137,1697101669667.0,120,345.0,9.0,"[49, 1077, 133, 101, 95, 71, 91, 90, 724, 99]","[1697101667186, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667]"
3128,3128,45,20,[],200,llama-13b,128,1,1408.0,1.0,1,H100,1697101639594,1697101641002.0,120,19.0,1.0,"[7, 1401]","[1697101639601, 1697101641002]"
3129,3129,186,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649243,1697101651603.0,120,,,"[42, 626, 355, 106, 104, 101, 91, 86]","[1697101649285, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650754]"
3130,3130,398,21,[],200,llama-13b,128,1,3753.0,1.0,1,H100,1697101641003,1697101644756.0,120,87.0,20.0,"[13, 801, 304, 101, 98, 97, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 86, 84, 84, 82]","[1697101641016, 1697101641817, 1697101642121, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756]"
3131,3131,889,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673086,1697101675812.0,120,,,"[6, 1756, 133, 102, 98, 95, 70, 95]","[1697101673092, 1697101674848, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3132,3132,537,22,[],200,llama-13b,128,1,4921.0,1.0,1,H100,1697101651607,1697101656528.0,120,83.0,20.0,"[77, 977, 57, 892, 86, 82, 82, 81, 896, 108, 105, 101, 74, 99, 97, 92, 613, 106, 100, 99, 97]","[1697101651684, 1697101652661, 1697101652718, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
3133,3133,132,22,[],200,llama-13b,128,1,4546.0,1.0,1,H100,1697101643834,1697101648380.0,120,100.0,20.0,"[9, 477, 101, 86, 84, 83, 83, 682, 79, 78, 77, 75, 742, 97, 98, 83, 78, 1236, 96, 101, 101]","[1697101643843, 1697101644320, 1697101644421, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648380]"
3134,3134,819,13,[],200,llama-13b,128,1,722.0,1.0,1,H100,1697101628695,1697101629417.0,120,13.0,1.0,"[60, 662]","[1697101628755, 1697101629417]"
3135,3135,883,25,[],200,llama-13b,128,1,1640.0,1.0,1,H100,1697101668845,1697101670485.0,120,563.0,1.0,"[7, 1633]","[1697101668852, 1697101670485]"
3136,3136,247,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101629420,1697101634995.0,120,,,"[28, 2236, 130, 92, 91, 89, 89, 88, 84, 83, 741, 95, 72, 94, 92, 93, 70, 615, 74, 71]","[1697101629448, 1697101631684, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633524, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
3137,3137,672,31,[],200,llama-13b,128,1,5052.0,1.0,1,H100,1697101667605,1697101672657.0,120,93.0,20.0,"[6, 1957, 99, 94, 93, 92, 89, 676, 99, 71, 92, 91, 89, 513, 97, 95, 94, 93, 90, 521]","[1697101667611, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656]"
3138,3138,313,26,[],200,llama-13b,128,1,1073.0,1.0,1,H100,1697101670486,1697101671559.0,120,20.0,1.0,"[12, 1061]","[1697101670498, 1697101671559]"
3139,3139,483,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648381,1697101651604.0,120,,,"[18, 548, 103, 96, 96, 83, 941, 106, 103, 102, 91, 85]","[1697101648399, 1697101648947, 1697101649050, 1697101649146, 1697101649242, 1697101649325, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
3140,3140,916,27,[],200,llama-13b,128,1,879.0,1.0,1,H100,1697101671667,1697101672546.0,120,8.0,1.0,"[42, 837]","[1697101671709, 1697101672546]"
3141,3141,341,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672547,1697101675810.0,120,,,"[6, 1205, 130, 98, 89, 88, 66, 66, 686, 102, 98, 95, 71, 94]","[1697101672553, 1697101673758, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675347, 1697101675441]"
3142,3142,840,24,[],200,llama-13b,128,1,1870.0,1.0,1,H100,1697101651608,1697101653478.0,120,17.0,1.0,"[469, 1400]","[1697101652077, 1697101653477]"
3143,3143,671,27,[],200,llama-13b,128,1,986.0,1.0,1,H100,1697101671560,1697101672546.0,120,12.0,1.0,"[19, 967]","[1697101671579, 1697101672546]"
3144,3144,129,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689615,1697101697587.0,120,,,"[203, 1294, 93, 1069, 213, 212, 78, 110, 93, 94, 405, 101, 100, 99, 95, 91, 87, 68, 433, 78, 100, 84, 605, 78, 77, 831, 103, 100, 111, 92, 91]","[1697101689818, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694815, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
3145,3145,573,25,[],200,llama-13b,128,1,1043.0,1.0,1,H100,1697101669668,1697101670711.0,120,874.0,2.0,"[7, 1036]","[1697101669675, 1697101670711]"
3146,3146,1,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101670722,1697101675811.0,120,,,"[39, 798, 107, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101670761, 1697101671559, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3147,3147,886,8,[],200,llama-13b,128,1,1922.0,1.0,1,H100,1697101616334,1697101618256.0,120,17.0,1.0,"[536, 1386]","[1697101616870, 1697101618256]"
3148,3148,287,9,[],200,llama-13b,128,1,831.0,1.0,1,H100,1697101618258,1697101619089.0,120,10.0,1.0,"[60, 771]","[1697101618318, 1697101619089]"
3149,3149,682,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101624569,1697101626655.0,120,,,"[18, 1108, 114, 97, 69]","[1697101624587, 1697101625695, 1697101625809, 1697101625906, 1697101625975]"
3150,3150,271,25,[],200,llama-13b,128,1,4810.0,1.0,1,H100,1697101653479,1697101658289.0,120,87.0,20.0,"[103, 1049, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 82, 623, 81, 79, 60, 835]","[1697101653582, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
3151,3151,366,27,[],200,llama-13b,128,1,2629.0,1.0,1,H100,1697101675815,1697101678444.0,120,85.0,6.0,"[496, 1069, 97, 64, 51, 762, 90]","[1697101676311, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678354, 1697101678444]"
3152,3152,876,24,[],200,llama-13b,128,1,1101.0,1.0,1,H100,1697101672658,1697101673759.0,120,11.0,1.0,"[31, 1070]","[1697101672689, 1697101673759]"
3153,3153,111,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628903.0,120,,,"[13, 880, 43]","[1697101626676, 1697101627556, 1697101627599]"
3154,3154,469,17,[],200,llama-13b,128,1,1720.0,1.0,1,H100,1697101628909,1697101630629.0,120,17.0,1.0,"[107, 1613]","[1697101629016, 1697101630629]"
3155,3155,392,18,[],200,llama-13b,128,1,1810.0,1.0,1,H100,1697101634998,1697101636808.0,120,20.0,1.0,"[377, 1433]","[1697101635375, 1697101636808]"
3156,3156,834,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630631,1697101634994.0,120,,,"[48, 1005, 130, 92, 91, 89, 89, 88, 84, 83, 741, 95, 72, 94, 93, 92, 70, 615, 74, 71]","[1697101630679, 1697101631684, 1697101631814, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
3157,3157,750,19,[],200,llama-13b,128,1,4310.0,1.0,1,H100,1697101636809,1697101641119.0,120,88.0,20.0,"[7, 663, 189, 91, 92, 87, 87, 82, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618, 81, 93, 735]","[1697101636816, 1697101637479, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641119]"
3158,3158,757,40,[],200,llama-13b,128,1,675.0,1.0,1,H100,1697101696721,1697101697396.0,120,20.0,1.0,"[31, 644]","[1697101696752, 1697101697396]"
3159,3159,663,18,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634998,1697101640209.0,120,79.0,20.0,"[386, 1424, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 79, 617]","[1697101635384, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209]"
3160,3160,186,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697397,1697101700636.0,120,,,"[18, 357, 566, 1249, 84, 82, 81, 80, 83]","[1697101697415, 1697101697772, 1697101698338, 1697101699587, 1697101699671, 1697101699753, 1697101699834, 1697101699914, 1697101699997]"
3161,3161,543,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700645,1697101703171.0,120,,,"[481, 1908]","[1697101701126, 1697101703034]"
3162,3162,179,28,[],200,llama-13b,128,1,2089.0,1.0,1,H100,1697101651607,1697101653696.0,120,161.0,4.0,"[71, 983, 57, 892, 86]","[1697101651678, 1697101652661, 1697101652718, 1697101653610, 1697101653696]"
3163,3163,602,14,[],200,llama-13b,128,1,715.0,1.0,1,H100,1697101638025,1697101638740.0,120,15.0,1.0,"[7, 708]","[1697101638032, 1697101638740]"
3164,3164,874,43,[],200,llama-13b,128,1,9531.0,1.0,1,H100,1697101703176,1697101712707.0,120,140.0,50.0,"[112, 1611, 117, 97, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 91, 70, 91, 86, 545, 99, 65, 88, 811, 88, 84, 82, 79, 881, 103, 97, 93, 89, 88, 86, 497, 99, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45, 44, 41, 33]","[1697101703288, 1697101704899, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709578, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712674, 1697101712707]"
3165,3165,30,15,[],200,llama-13b,128,1,5092.0,1.0,1,H100,1697101638741,1697101643833.0,120,93.0,20.0,"[24, 1444, 83, 92, 735, 85, 79, 77, 761, 101, 98, 96, 89, 82, 63, 800, 99, 96, 93, 95]","[1697101638765, 1697101640209, 1697101640292, 1697101640384, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642222, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833]"
3166,3166,93,19,[],200,llama-13b,128,1,4208.0,1.0,1,H100,1697101640212,1697101644420.0,120,88.0,20.0,"[48, 742, 118, 84, 79, 77, 761, 100, 99, 96, 89, 82, 63, 801, 98, 97, 92, 95, 87, 85, 415]","[1697101640260, 1697101641002, 1697101641120, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643549, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420]"
3167,3167,363,16,[],200,llama-13b,128,1,4730.0,1.0,1,H100,1697101643834,1697101648564.0,120,286.0,22.0,"[9, 477, 101, 86, 84, 83, 83, 683, 78, 78, 77, 75, 742, 97, 98, 83, 78, 1236, 96, 101, 101, 96, 88]","[1697101643843, 1697101644320, 1697101644421, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564]"
3168,3168,429,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101641121,1697101651605.0,120,,,"[60, 636, 304, 101, 99, 96, 88, 82, 64, 799, 100, 95, 93, 95, 87, 85, 415, 86, 85, 83, 83, 683, 79, 77, 77, 75, 741, 98, 97, 84, 78, 1226, 106, 101, 100, 97, 88, 485, 96, 96, 83, 941, 107, 103, 102, 90, 86]","[1697101641181, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642651, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644591, 1697101644674, 1697101644757, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
3169,3169,263,21,[],200,llama-13b,128,1,1541.0,1.0,1,H100,1697101644758,1697101646299.0,120,15.0,1.0,"[50, 1491]","[1697101644808, 1697101646299]"
3170,3170,340,20,[],200,llama-13b,128,1,3856.0,1.0,1,H100,1697101663747,1697101667603.0,120,85.0,20.0,"[7, 692, 232, 109, 103, 95, 94, 101, 688, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 99, 94]","[1697101663754, 1697101664446, 1697101664678, 1697101664787, 1697101664890, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667509, 1697101667603]"
3171,3171,623,22,[],200,llama-13b,128,1,1879.0,1.0,1,H100,1697101646300,1697101648179.0,120,140.0,3.0,"[6, 1777, 96]","[1697101646306, 1697101648083, 1697101648179]"
3172,3172,51,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648180,1697101651604.0,120,,,"[12, 755, 102, 97, 96, 83, 941, 106, 103, 102, 90, 86]","[1697101648192, 1697101648947, 1697101649049, 1697101649146, 1697101649242, 1697101649325, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
3173,3173,409,24,[],200,llama-13b,128,1,7019.0,1.0,1,H100,1697101651610,1697101658629.0,120,109.0,30.0,"[198, 853, 57, 892, 86, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 98, 82, 623, 81, 79, 59, 836, 95, 83, 81, 81]","[1697101651808, 1697101652661, 1697101652718, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656529, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629]"
3174,3174,886,29,[],200,llama-13b,128,1,1073.0,1.0,1,H100,1697101670486,1697101671559.0,120,17.0,1.0,"[42, 1031]","[1697101670528, 1697101671559]"
3175,3175,699,21,[],200,llama-13b,128,1,1839.0,1.0,1,H100,1697101667605,1697101669444.0,120,39.0,1.0,"[6, 1833]","[1697101667611, 1697101669444]"
3176,3176,293,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671560,1697101675812.0,120,,,"[7, 979, 110, 114, 91, 68, 67, 88, 804, 98, 89, 88, 66, 66, 686, 102, 99, 94, 71, 94]","[1697101671567, 1697101672546, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675182, 1697101675276, 1697101675347, 1697101675441]"
3177,3177,102,22,[],200,llama-13b,128,1,4443.0,1.0,1,H100,1697101669445,1697101673888.0,120,84.0,20.0,"[12, 1028, 226, 99, 71, 92, 90, 91, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 68, 87, 804]","[1697101669457, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671063, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673084, 1697101673888]"
3178,3178,739,25,[],200,llama-13b,128,1,2015.0,1.0,1,H100,1697101658631,1697101660646.0,120,216.0,1.0,"[6, 2009]","[1697101658637, 1697101660646]"
3179,3179,168,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101660650,1697101675811.0,120,,,"[40, 1226, 355, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 109, 102, 96, 94, 93, 696, 105, 100, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101660690, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665172, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3180,3180,653,31,[],200,llama-13b,128,1,2630.0,1.0,1,H100,1697101675815,1697101678445.0,120,96.0,6.0,"[202, 1363, 97, 64, 51, 763, 89]","[1697101676017, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678444]"
3181,3181,629,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101658292,1697101675810.0,120,,,"[59, 1022, 332, 102, 98, 97, 93, 85, 805, 101, 76, 98, 93, 87, 831, 107, 102, 94, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101658351, 1697101659373, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662480, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3182,3182,74,21,[],200,llama-13b,128,1,3636.0,1.0,1,H100,1697101641121,1697101644757.0,120,88.0,20.0,"[58, 638, 304, 101, 99, 96, 88, 82, 64, 799, 100, 95, 93, 95, 87, 85, 415, 87, 84, 83, 83]","[1697101641179, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642651, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757]"
3183,3183,81,32,[],200,llama-13b,128,1,2286.0,1.0,1,H100,1697101678446,1697101680732.0,120,732.0,13.0,"[12, 830, 117, 99, 98, 94, 72, 92, 90, 70, 91, 460, 103, 57]","[1697101678458, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731]"
3184,3184,464,23,[],200,llama-13b,128,1,955.0,1.0,1,H100,1697101673893,1697101674848.0,120,12.0,1.0,"[52, 903]","[1697101673945, 1697101674848]"
3185,3185,823,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674849,1697101675810.0,120,,,"[30, 910]","[1697101674879, 1697101675789]"
3186,3186,641,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101670717,1697101675811.0,120,,,"[26, 923, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101670743, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3187,3187,758,21,[],200,llama-13b,128,1,3636.0,1.0,1,H100,1697101641121,1697101644757.0,120,84.0,20.0,"[46, 650, 304, 101, 99, 96, 88, 82, 64, 799, 100, 95, 93, 95, 87, 85, 415, 87, 84, 83, 83]","[1697101641167, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642651, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757]"
3188,3188,248,25,[],200,llama-13b,128,1,4047.0,1.0,1,H100,1697101675813,1697101679860.0,120,182.0,17.0,"[121, 446, 39, 1058, 64, 50, 763, 90, 86, 64, 84, 80, 647, 98, 98, 95, 71, 93]","[1697101675934, 1697101676380, 1697101676419, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679860]"
3189,3189,778,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101644421,1697101651604.0,120,,,"[13, 1006, 79, 77, 77, 75, 741, 98, 97, 84, 78, 1236, 96, 101, 100, 97, 88, 485, 96, 97, 82, 942, 106, 103, 102, 91, 85]","[1697101644434, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
3190,3190,207,18,[],200,llama-13b,128,1,1868.0,1.0,1,H100,1697101651610,1697101653478.0,120,10.0,1.0,"[483, 1385]","[1697101652093, 1697101653478]"
3191,3191,533,19,[],200,llama-13b,128,1,1358.0,1.0,1,H100,1697101653479,1697101654837.0,120,216.0,2.0,"[65, 1293]","[1697101653544, 1697101654837]"
3192,3192,188,22,[],200,llama-13b,128,1,5818.0,1.0,1,H100,1697101644759,1697101650577.0,120,85.0,20.0,"[43, 1497, 190, 98, 98, 83, 78, 1226, 106, 101, 100, 97, 88, 486, 96, 96, 82, 942, 106, 103, 102]","[1697101644802, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577]"
3193,3193,894,20,[],200,llama-13b,128,1,1160.0,1.0,1,H100,1697101654838,1697101655998.0,120,14.0,1.0,"[7, 1153]","[1697101654845, 1697101655998]"
3194,3194,322,21,[],200,llama-13b,128,1,5261.0,1.0,1,H100,1697101655999,1697101661260.0,120,93.0,20.0,"[12, 1223, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97, 93, 85, 805, 101, 75, 99]","[1697101656011, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260]"
3195,3195,660,16,[],200,llama-13b,128,1,5229.0,1.0,1,H100,1697101640211,1697101645440.0,120,732.0,25.0,"[83, 708, 118, 84, 79, 77, 761, 100, 99, 96, 89, 82, 63, 801, 98, 97, 92, 96, 86, 85, 414, 87, 84, 84, 82, 684]","[1697101640294, 1697101641002, 1697101641120, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643549, 1697101643646, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440]"
3196,3196,587,19,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101651610,1697101652661.0,120,13.0,1.0,"[188, 863]","[1697101651798, 1697101652661]"
3197,3197,12,20,[],200,llama-13b,128,1,815.0,1.0,1,H100,1697101652662,1697101653477.0,120,11.0,1.0,"[36, 778]","[1697101652698, 1697101653476]"
3198,3198,669,39,[],200,llama-13b,128,1,4634.0,1.0,1,H100,1697101682705,1697101687339.0,120,83.0,20.0,"[24, 912, 208, 102, 100, 97, 92, 90, 90, 593, 86, 64, 82, 82, 910, 91, 89, 87, 86, 647, 102]","[1697101682729, 1697101683641, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339]"
3199,3199,153,30,[],200,llama-13b,128,1,1066.0,1.0,1,H100,1697101660096,1697101661162.0,120,335.0,4.0,"[6, 545, 338, 102, 75]","[1697101660102, 1697101660647, 1697101660985, 1697101661087, 1697101661162]"
3200,3200,659,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101656531,1697101675810.0,120,,,"[83, 1546, 129, 96, 82, 82, 80, 1076, 102, 98, 98, 92, 85, 805, 102, 75, 104, 87, 86, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 108, 103, 96, 94, 92, 697, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 676, 99, 71, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101656614, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660003, 1697101660095, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665171, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670710, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3201,3201,549,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101650578,1697101651604.0,120,,,[13],[1697101650591]
3202,3202,879,24,[],200,llama-13b,128,1,13279.0,1.0,1,H100,1697101651610,1697101664889.0,120,39.0,55.0,"[489, 1379, 132, 86, 82, 82, 81, 896, 107, 106, 100, 75, 99, 97, 92, 614, 105, 101, 98, 97, 84, 622, 81, 79, 60, 835, 95, 83, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 844, 109, 103]","[1697101652099, 1697101653478, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664677, 1697101664786, 1697101664889]"
3203,3203,483,31,[],200,llama-13b,128,1,4811.0,1.0,1,H100,1697101661163,1697101665974.0,120,84.0,20.0,"[12, 741, 356, 106, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 109, 102, 96, 94, 101, 688, 105]","[1697101661175, 1697101661916, 1697101662272, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973]"
3204,3204,645,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621117,1697101623664.0,120,,,"[92, 1373, 90, 87, 86, 84, 84]","[1697101621209, 1697101622582, 1697101622672, 1697101622759, 1697101622845, 1697101622929, 1697101623013]"
3205,3205,430,23,[],200,llama-13b,128,1,772.0,1.0,1,H100,1697101655226,1697101655998.0,120,15.0,1.0,"[6, 766]","[1697101655232, 1697101655998]"
3206,3206,670,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101621047,1697101623660.0,120,,,"[30, 596, 85, 81, 80, 78, 61, 614, 86, 87, 84, 83]","[1697101621077, 1697101621673, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058, 1697101622672, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
3207,3207,877,24,[],200,llama-13b,128,1,5261.0,1.0,1,H100,1697101655999,1697101661260.0,120,85.0,20.0,"[12, 1135, 88, 81, 79, 60, 836, 95, 82, 82, 80, 1076, 101, 99, 97, 93, 85, 805, 101, 75, 99]","[1697101656011, 1697101657146, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658290, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260]"
3208,3208,92,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672547,1697101675811.0,120,,,"[18, 1193, 130, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672565, 1697101673758, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3209,3209,15,20,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101651608,1697101656528.0,120,100.0,20.0,"[13, 1039, 58, 891, 86, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 92, 613, 106, 100, 99, 97]","[1697101651621, 1697101652660, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
3210,3210,361,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699835,1697101700635.0,120,,,"[7, 710]","[1697101699842, 1697101700552]"
3211,3211,810,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703171.0,120,,,"[143, 1378, 123, 69]","[1697101700781, 1697101702159, 1697101702282, 1697101702351]"
3212,3212,451,29,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675815,1697101677379.0,120,286.0,1.0,"[454, 1110]","[1697101676269, 1697101677379]"
3213,3213,779,30,[],200,llama-13b,128,1,2220.0,1.0,1,H100,1697101677381,1697101679601.0,120,563.0,10.0,"[18, 848, 107, 90, 86, 65, 83, 81, 646, 98, 98]","[1697101677399, 1697101678247, 1697101678354, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601]"
3214,3214,101,32,[],200,llama-13b,128,1,1101.0,1.0,1,H100,1697101672658,1697101673759.0,120,13.0,1.0,"[19, 1082]","[1697101672677, 1697101673759]"
3215,3215,240,37,[],200,llama-13b,128,1,5351.0,1.0,1,H100,1697101703175,1697101708526.0,120,83.0,20.0,"[49, 1675, 116, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 91, 70, 91, 86, 545, 99]","[1697101703224, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
3216,3216,461,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673760,1697101675812.0,120,,,"[12, 1209, 102, 98, 95, 71, 94]","[1697101673772, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675347, 1697101675441]"
3217,3217,642,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619090,1697101623659.0,120,,,"[24, 1264, 196, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 60, 614, 87, 87, 84, 83]","[1697101619114, 1697101620378, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
3218,3218,456,19,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101635000,1697101640209.0,120,90.0,20.0,"[420, 1388, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 94, 92, 92, 91, 91, 79, 617]","[1697101635420, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209]"
3219,3219,818,34,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,13.0,1.0,"[269, 1295]","[1697101676085, 1697101677380]"
3220,3220,334,35,[],200,llama-13b,128,1,865.0,1.0,1,H100,1697101677382,1697101678247.0,120,15.0,1.0,"[113, 752]","[1697101677495, 1697101678247]"
3221,3221,882,36,[],200,llama-13b,128,1,2458.0,1.0,1,H100,1697101682705,1697101685163.0,120,345.0,11.0,"[12, 924, 208, 102, 100, 97, 92, 90, 90, 594, 85, 64]","[1697101682717, 1697101683641, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685099, 1697101685163]"
3222,3222,71,20,[],200,llama-13b,128,1,2254.0,1.0,1,H100,1697101644592,1697101646846.0,120,364.0,11.0,"[6, 739, 103, 79, 77, 78, 75, 740, 98, 97, 84, 78]","[1697101644598, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645749, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846]"
3223,3223,68,11,[],200,llama-13b,128,1,2030.0,1.0,1,H100,1697101623665,1697101625695.0,120,12.0,1.0,"[313, 1717]","[1697101623978, 1697101625695]"
3224,3224,426,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625697,1697101626657.0,120,,,"[96, 773]","[1697101625793, 1697101626566]"
3225,3225,432,21,[],200,llama-13b,128,1,2099.0,1.0,1,H100,1697101646848,1697101648947.0,120,13.0,1.0,"[6, 2093]","[1697101646854, 1697101648947]"
3226,3226,787,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[219, 1810]","[1697101626882, 1697101628692]"
3227,3227,842,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648381,1697101651604.0,120,,,"[24, 542, 103, 96, 96, 83, 941, 106, 103, 102, 91, 85]","[1697101648405, 1697101648947, 1697101649050, 1697101649146, 1697101649242, 1697101649325, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
3228,3228,872,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649913,1697101651604.0,120,,,"[30, 1365]","[1697101649943, 1697101651308]"
3229,3229,297,23,[],200,llama-13b,128,1,15993.0,1.0,1,H100,1697101651610,1697101667603.0,120,563.0,72.0,"[216, 835, 57, 892, 85, 84, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 82, 623, 81, 79, 59, 836, 95, 83, 81, 81, 1076, 101, 98, 98, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 844, 109, 103, 96, 94, 91, 697, 106, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 94]","[1697101651826, 1697101652661, 1697101652718, 1697101653610, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659806, 1697101659904, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664677, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665170, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667602]"
3230,3230,786,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651604.0,120,,,"[7, 956, 355, 106, 104, 101, 91, 85]","[1697101648955, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
3231,3231,128,14,[],200,llama-13b,128,1,1811.0,1.0,1,H100,1697101634998,1697101636809.0,120,9.0,1.0,"[328, 1482]","[1697101635326, 1697101636808]"
3232,3232,418,27,[],200,llama-13b,128,1,789.0,1.0,1,H100,1697101670974,1697101671763.0,120,286.0,3.0,"[7, 579, 106, 97]","[1697101670981, 1697101671560, 1697101671666, 1697101671763]"
3233,3233,748,28,[],200,llama-13b,128,1,3218.0,1.0,1,H100,1697101671764,1697101674982.0,120,182.0,14.0,"[6, 887, 113, 91, 68, 68, 88, 803, 98, 89, 88, 66, 66, 686]","[1697101671770, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981]"
3234,3234,719,26,[],200,llama-13b,128,1,1805.0,1.0,1,H100,1697101658290,1697101660095.0,120,182.0,6.0,"[13, 1402, 102, 98, 97, 93]","[1697101658303, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095]"
3235,3235,152,27,[],200,llama-13b,128,1,4691.0,1.0,1,H100,1697101660096,1697101664787.0,120,87.0,20.0,"[12, 539, 338, 102, 75, 98, 93, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 89, 86, 845, 109]","[1697101660108, 1697101660647, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664787]"
3236,3236,216,23,[],200,llama-13b,128,1,4919.0,1.0,1,H100,1697101651610,1697101656529.0,120,91.0,20.0,"[204, 847, 57, 891, 86, 84, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98]","[1697101651814, 1697101652661, 1697101652718, 1697101653609, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529]"
3237,3237,570,24,[],200,llama-13b,128,1,617.0,1.0,1,H100,1697101656530,1697101657147.0,120,18.0,1.0,"[42, 574]","[1697101656572, 1697101657146]"
3238,3238,903,25,[],200,llama-13b,128,1,2555.0,1.0,1,H100,1697101657150,1697101659705.0,120,244.0,7.0,"[7, 1003, 129, 96, 82, 82, 80, 1076]","[1697101657157, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705]"
3239,3239,831,19,[],200,llama-13b,128,1,1669.0,1.0,1,H100,1697101642651,1697101644320.0,120,11.0,1.0,"[7, 1662]","[1697101642658, 1697101644320]"
3240,3240,829,30,[],200,llama-13b,128,1,955.0,1.0,1,H100,1697101673893,1697101674848.0,120,20.0,1.0,"[36, 919]","[1697101673929, 1697101674848]"
3241,3241,265,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674849,1697101675810.0,120,,,"[18, 922]","[1697101674867, 1697101675789]"
3242,3242,625,32,[],200,llama-13b,128,1,1663.0,1.0,1,H100,1697101675814,1697101677477.0,120,364.0,2.0,"[158, 1407, 98]","[1697101675972, 1697101677379, 1697101677477]"
3243,3243,140,33,[],200,llama-13b,128,1,3327.0,1.0,1,H100,1697101677478,1697101680805.0,120,96.0,20.0,"[29, 740, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 103, 58, 73]","[1697101677507, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680732, 1697101680805]"
3244,3244,446,20,[],200,llama-13b,128,1,916.0,1.0,1,H100,1697101644421,1697101645337.0,120,26.0,1.0,"[37, 879]","[1697101644458, 1697101645337]"
3245,3245,809,21,[],200,llama-13b,128,1,961.0,1.0,1,H100,1697101645338,1697101646299.0,120,16.0,1.0,"[12, 949]","[1697101645350, 1697101646299]"
3246,3246,234,20,[],200,llama-13b,128,1,6256.0,1.0,1,H100,1697101644321,1697101650577.0,120,457.0,25.0,"[13, 1003, 103, 78, 78, 77, 75, 742, 97, 98, 83, 78, 1237, 95, 101, 101, 96, 88, 485, 96, 96, 83, 941, 107, 103, 102]","[1697101644334, 1697101645337, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577]"
3247,3247,238,22,[],200,llama-13b,128,1,2176.0,1.0,1,H100,1697101646300,1697101648476.0,120,563.0,6.0,"[24, 1418, 340, 96, 101, 101, 96]","[1697101646324, 1697101647742, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476]"
3248,3248,570,23,[],200,llama-13b,128,1,471.0,1.0,1,H100,1697101648477,1697101648948.0,120,18.0,1.0,"[7, 463]","[1697101648484, 1697101648947]"
3249,3249,1,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648949,1697101651605.0,120,,,"[42, 920, 355, 106, 104, 101, 91, 85]","[1697101648991, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
3250,3250,510,28,[],200,llama-13b,128,1,1080.0,1.0,1,H100,1697101664788,1697101665868.0,120,79.0,2.0,"[24, 927, 129]","[1697101664812, 1697101665739, 1697101665868]"
3251,3251,864,29,[],200,llama-13b,128,1,4842.0,1.0,1,H100,1697101665869,1697101670711.0,120,83.0,20.0,"[13, 888, 366, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 93, 88, 676]","[1697101665882, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669947, 1697101670035, 1697101670711]"
3252,3252,291,30,[],200,llama-13b,128,1,4255.0,1.0,1,H100,1697101670726,1697101674981.0,120,79.0,20.0,"[49, 891, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 66, 686]","[1697101670775, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981]"
3253,3253,907,17,[],200,llama-13b,128,1,547.0,1.0,1,H100,1697101634971,1697101635518.0,120,10.0,1.0,"[48, 499]","[1697101635019, 1697101635518]"
3254,3254,858,28,[],200,llama-13b,128,1,3029.0,1.0,1,H100,1697101657151,1697101660180.0,120,182.0,12.0,"[59, 1079, 96, 82, 82, 81, 1075, 101, 99, 97, 92, 86]","[1697101657210, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658630, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180]"
3255,3255,355,25,[],200,llama-13b,128,1,5001.0,1.0,1,H100,1697101651610,1697101656611.0,120,90.0,20.0,"[569, 1293, 138, 86, 82, 82, 81, 896, 107, 106, 100, 75, 99, 97, 92, 614, 105, 101, 98, 97, 83]","[1697101652179, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611]"
3256,3256,289,29,[],200,llama-13b,128,1,5792.0,1.0,1,H100,1697101660181,1697101665973.0,120,89.0,20.0,"[13, 1722, 355, 107, 101, 95, 94, 89, 707, 101, 93, 89, 86, 845, 109, 102, 96, 94, 101, 688, 105]","[1697101660194, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973]"
3257,3257,336,18,[],200,llama-13b,128,1,2588.0,1.0,1,H100,1697101635519,1697101638107.0,120,58.0,7.0,"[87, 1872, 190, 91, 92, 87, 87, 82]","[1697101635606, 1697101637478, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107]"
3258,3258,99,13,[],200,llama-13b,128,1,860.0,1.0,1,H100,1697101623014,1697101623874.0,120,10.0,1.0,"[12, 848]","[1697101623026, 1697101623874]"
3259,3259,231,19,[],200,llama-13b,128,1,791.0,1.0,1,H100,1697101640211,1697101641002.0,120,13.0,1.0,"[19, 772]","[1697101640230, 1697101641002]"
3260,3260,457,14,[],200,llama-13b,128,1,1934.0,1.0,1,H100,1697101623875,1697101625809.0,120,874.0,2.0,"[393, 1541]","[1697101624268, 1697101625809]"
3261,3261,561,20,[],200,llama-13b,128,1,3754.0,1.0,1,H100,1697101641003,1697101644757.0,120,87.0,20.0,"[67, 747, 304, 101, 99, 96, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 87, 83, 84, 83]","[1697101641070, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644590, 1697101644674, 1697101644757]"
3262,3262,619,31,[],200,llama-13b,128,1,806.0,1.0,1,H100,1697101674983,1697101675789.0,120,10.0,1.0,"[48, 758]","[1697101675031, 1697101675789]"
3263,3263,913,18,[],200,llama-13b,128,1,4207.0,1.0,1,H100,1697101640212,1697101644419.0,120,88.0,20.0,"[82, 708, 118, 84, 79, 77, 761, 100, 99, 96, 89, 82, 63, 801, 98, 97, 92, 96, 86, 85, 414]","[1697101640294, 1697101641002, 1697101641120, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643549, 1697101643646, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644419]"
3264,3264,111,14,[],200,llama-13b,128,1,1129.0,1.0,1,H100,1697101636810,1697101637939.0,120,79.0,5.0,"[66, 603, 189, 91, 93, 87]","[1697101636876, 1697101637479, 1697101637668, 1697101637759, 1697101637852, 1697101637939]"
3265,3265,22,22,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101669570,1697101670485.0,120,16.0,1.0,"[12, 903]","[1697101669582, 1697101670485]"
3266,3266,642,30,[],200,llama-13b,128,1,4737.0,1.0,1,H100,1697101665974,1697101670711.0,120,89.0,20.0,"[7, 789, 366, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 93, 88, 676]","[1697101665981, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669947, 1697101670035, 1697101670711]"
3267,3267,380,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101670486,1697101675812.0,120,,,"[36, 1144, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 805, 97, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101670522, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673889, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3268,3268,465,15,[],200,llama-13b,128,1,1114.0,1.0,1,H100,1697101637940,1697101639054.0,120,364.0,3.0,"[6, 793, 217, 98]","[1697101637946, 1697101638739, 1697101638956, 1697101639054]"
3269,3269,820,16,[],200,llama-13b,128,1,3066.0,1.0,1,H100,1697101639055,1697101642121.0,120,161.0,9.0,"[6, 1025, 123, 83, 92, 735, 85, 78, 78, 761]","[1697101639061, 1697101640086, 1697101640209, 1697101640292, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121]"
3270,3270,71,19,[],200,llama-13b,128,1,2989.0,1.0,1,H100,1697101639332,1697101642321.0,120,364.0,11.0,"[6, 748, 123, 83, 93, 734, 85, 78, 78, 761, 101, 99]","[1697101639338, 1697101640086, 1697101640209, 1697101640292, 1697101640385, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321]"
3271,3271,51,32,[],200,llama-13b,128,1,7444.0,1.0,1,H100,1697101675790,1697101683234.0,120,364.0,36.0,"[31, 559, 36, 1061, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98, 95, 71, 92, 91, 70, 90, 461, 103, 57, 73, 803, 85, 82, 80, 78, 771, 97, 93, 91, 69, 91, 90]","[1697101675821, 1697101676380, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681607, 1697101681692, 1697101681774, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234]"
3272,3272,695,19,[],200,llama-13b,128,1,4309.0,1.0,1,H100,1697101638108,1697101642417.0,120,92.0,20.0,"[6, 626, 217, 97, 93, 92, 92, 92, 90, 79, 617, 82, 94, 734, 85, 79, 77, 761, 101, 98, 97]","[1697101638114, 1697101638740, 1697101638957, 1697101639054, 1697101639147, 1697101639239, 1697101639331, 1697101639423, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640385, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642222, 1697101642320, 1697101642417]"
3273,3273,698,29,[],200,llama-13b,128,1,1551.0,1.0,1,H100,1697101681503,1697101683054.0,120,182.0,6.0,"[6, 1002, 193, 96, 93, 92, 69]","[1697101681509, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054]"
3274,3274,435,33,[],200,llama-13b,128,1,5596.0,1.0,1,H100,1697101680732,1697101686328.0,120,563.0,27.0,"[7, 763, 106, 84, 83, 79, 79, 771, 96, 93, 91, 69, 91, 90, 614, 102, 101, 97, 92, 90, 90, 594, 86, 63, 83, 80, 911, 91]","[1697101680739, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328]"
3275,3275,791,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101686329,1697101689594.0,120,,,"[7, 901, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101686336, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3276,3276,221,17,[],200,llama-13b,128,1,8353.0,1.0,1,H100,1697101642122,1697101650475.0,120,364.0,36.0,"[7, 1002, 319, 100, 96, 92, 95, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 78, 74, 741, 98, 97, 84, 78, 1234, 98, 101, 100, 97, 88, 485, 96, 96, 83, 941, 107, 103]","[1697101642129, 1697101643131, 1697101643450, 1697101643550, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648080, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475]"
3277,3277,216,23,[],200,llama-13b,128,1,4922.0,1.0,1,H100,1697101651606,1697101656528.0,120,91.0,20.0,"[39, 1016, 57, 892, 86, 82, 82, 81, 896, 107, 106, 101, 74, 99, 97, 92, 613, 106, 100, 99, 97]","[1697101651645, 1697101652661, 1697101652718, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
3278,3278,371,21,[],200,llama-13b,128,1,1152.0,1.0,1,H100,1697101653479,1697101654631.0,120,13.0,1.0,"[92, 1060]","[1697101653571, 1697101654631]"
3279,3279,305,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648094,1697101651604.0,120,,,"[12, 841, 102, 97, 96, 82, 942, 106, 103, 102, 90, 86]","[1697101648106, 1697101648947, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
3280,3280,159,12,[],200,llama-13b,128,1,520.0,1.0,1,H100,1697101634998,1697101635518.0,120,31.0,1.0,"[33, 487]","[1697101635031, 1697101635518]"
3281,3281,290,29,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,14.0,1.0,"[562, 1002]","[1697101676378, 1697101677380]"
3282,3282,27,22,[],200,llama-13b,128,1,12048.0,1.0,1,H100,1697101651610,1697101663658.0,120,15.0,50.0,"[503, 1359, 138, 86, 82, 82, 81, 896, 107, 106, 100, 75, 99, 97, 92, 614, 105, 101, 98, 97, 84, 622, 81, 79, 60, 835, 95, 83, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100, 94]","[1697101652113, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658]"
3283,3283,648,30,[],200,llama-13b,128,1,3424.0,1.0,1,H100,1697101677381,1697101680805.0,120,84.0,20.0,"[48, 818, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 104, 56, 73]","[1697101677429, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680675, 1697101680731, 1697101680804]"
3284,3284,101,12,[],200,llama-13b,128,1,902.0,1.0,1,H100,1697101623666,1697101624568.0,120,13.0,1.0,"[137, 765]","[1697101623803, 1697101624568]"
3285,3285,647,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674982,1697101675810.0,120,,,"[13, 794]","[1697101674995, 1697101675789]"
3286,3286,458,13,[],200,llama-13b,128,1,1996.0,1.0,1,H100,1697101624569,1697101626565.0,120,11.0,1.0,"[42, 1954]","[1697101624611, 1697101626565]"
3287,3287,163,32,[],200,llama-13b,128,1,2541.0,1.0,1,H100,1697101675813,1697101678354.0,120,67.0,6.0,"[79, 488, 36, 1061, 64, 50, 763]","[1697101675892, 1697101676380, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354]"
3288,3288,816,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626566,1697101628903.0,120,,,"[7, 301, 725]","[1697101626573, 1697101626874, 1697101627599]"
3289,3289,601,15,[],200,llama-13b,128,1,4595.0,1.0,1,H100,1697101634998,1697101639593.0,120,83.0,20.0,"[80, 440, 66, 1313, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 92, 93, 91, 91, 80]","[1697101635078, 1697101635518, 1697101635584, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639593]"
3290,3290,662,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673760,1697101675812.0,120,,,"[24, 1064, 133, 102, 98, 95, 71, 94]","[1697101673784, 1697101674848, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675347, 1697101675441]"
3291,3291,916,29,[],200,llama-13b,128,1,957.0,1.0,1,H100,1697101673891,1697101674848.0,120,8.0,1.0,"[16, 941]","[1697101673907, 1697101674848]"
3292,3292,305,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673760,1697101675811.0,120,,,"[24, 1064, 133, 102, 99, 94, 71, 94]","[1697101673784, 1697101674848, 1697101674981, 1697101675083, 1697101675182, 1697101675276, 1697101675347, 1697101675441]"
3293,3293,524,33,[],200,llama-13b,128,1,5792.0,1.0,1,H100,1697101678356,1697101684148.0,120,100.0,30.0,"[6, 926, 117, 99, 98, 94, 72, 92, 90, 70, 91, 460, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97]","[1697101678362, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148]"
3294,3294,726,28,[],200,llama-13b,128,1,9087.0,1.0,1,H100,1697101678445,1697101687532.0,120,67.0,47.0,"[7, 836, 117, 99, 98, 94, 72, 92, 90, 70, 91, 460, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97, 92, 90, 90, 593, 86, 64, 83, 80, 911, 91, 89, 87, 86, 647, 101, 99, 95]","[1697101678452, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532]"
3295,3295,348,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674849,1697101675810.0,120,,,"[30, 910]","[1697101674879, 1697101675789]"
3296,3296,693,36,[],200,llama-13b,128,1,1157.0,1.0,1,H100,1697101678248,1697101679405.0,120,67.0,2.0,"[48, 992, 117]","[1697101678296, 1697101679288, 1697101679405]"
3297,3297,121,37,[],200,llama-13b,128,1,1074.0,1.0,1,H100,1697101679406,1697101680480.0,120,13.0,1.0,"[7, 1067]","[1697101679413, 1697101680480]"
3298,3298,706,31,[],200,llama-13b,128,1,4297.0,1.0,1,H100,1697101675813,1697101680110.0,120,86.0,20.0,"[67, 500, 36, 1061, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98, 95, 71, 92, 91, 70, 90]","[1697101675880, 1697101676380, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679950, 1697101680020, 1697101680110]"
3299,3299,663,26,[],200,llama-13b,128,1,4764.0,1.0,1,H100,1697101675815,1697101680579.0,120,79.0,20.0,"[396, 1169, 97, 64, 50, 764, 90, 85, 65, 83, 81, 646, 99, 97, 95, 72, 92, 91, 69, 91, 468]","[1697101676211, 1697101677380, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680111, 1697101680579]"
3300,3300,877,26,[],200,llama-13b,128,1,4529.0,1.0,1,H100,1697101667137,1697101671666.0,120,85.0,20.0,"[25, 1101, 133, 101, 95, 71, 91, 90, 724, 99, 94, 93, 92, 89, 677, 98, 71, 92, 91, 89, 513]","[1697101667162, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670712, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666]"
3301,3301,611,28,[],200,llama-13b,128,1,842.0,1.0,1,H100,1697101670717,1697101671559.0,120,14.0,1.0,"[38, 804]","[1697101670755, 1697101671559]"
3302,3302,38,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671560,1697101675812.0,120,,,"[13, 973, 110, 114, 91, 68, 67, 88, 804, 98, 89, 88, 66, 65, 687, 102, 99, 94, 71, 94]","[1697101671573, 1697101672546, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675182, 1697101675276, 1697101675347, 1697101675441]"
3303,3303,191,14,[],200,llama-13b,128,1,4778.0,1.0,1,H100,1697101628909,1697101633687.0,120,85.0,20.0,"[166, 1554, 105, 81, 68, 66, 865, 92, 90, 90, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70]","[1697101629075, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687]"
3304,3304,444,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101650267,1697101651604.0,120,,,"[6, 1035]","[1697101650273, 1697101651308]"
3305,3305,539,29,[],200,llama-13b,128,1,4592.0,1.0,1,H100,1697101653697,1697101658289.0,120,83.0,20.0,"[6, 928, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 83, 622, 81, 79, 60, 835]","[1697101653703, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
3306,3306,302,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671667,1697101675810.0,120,,,"[18, 861, 111, 113, 91, 68, 68, 88, 803, 98, 89, 88, 66, 66, 686, 102, 99, 95, 70, 94]","[1697101671685, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
3307,3307,719,17,[],200,llama-13b,128,1,2103.0,1.0,1,H100,1697101648565,1697101650668.0,120,182.0,6.0,"[6, 1695, 106, 104, 101, 91]","[1697101648571, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668]"
3308,3308,92,26,[],200,llama-13b,128,1,5002.0,1.0,1,H100,1697101651610,1697101656612.0,120,85.0,20.0,"[474, 1393, 133, 86, 82, 82, 81, 896, 107, 106, 101, 74, 99, 97, 92, 614, 105, 101, 98, 97, 84]","[1697101652084, 1697101653477, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656612]"
3309,3309,147,18,[],200,llama-13b,128,1,1209.0,1.0,1,H100,1697101650668,1697101651877.0,120,182.0,1.0,"[7, 1202]","[1697101650675, 1697101651877]"
3310,3310,501,19,[],200,llama-13b,128,1,1594.0,1.0,1,H100,1697101651878,1697101653472.0,120,19.0,1.0,"[299, 1295]","[1697101652177, 1697101653472]"
3311,3311,863,20,[],200,llama-13b,128,1,1152.0,1.0,1,H100,1697101653479,1697101654631.0,120,10.0,1.0,"[23, 1129]","[1697101653502, 1697101654631]"
3312,3312,789,22,[],200,llama-13b,128,1,12048.0,1.0,1,H100,1697101651610,1697101663658.0,120,6.0,50.0,"[364, 1498, 138, 85, 83, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 97, 83, 623, 81, 79, 59, 836, 95, 83, 81, 81, 1076, 101, 99, 97, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100, 94]","[1697101651974, 1697101653472, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658]"
3313,3313,716,25,[],200,llama-13b,128,1,5827.0,1.0,1,H100,1697101666218,1697101672045.0,120,79.0,30.0,"[12, 540, 366, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 95, 92, 93, 87, 677, 99, 71, 92, 90, 90, 513, 97, 95, 94, 93]","[1697101666230, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670034, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045]"
3314,3314,816,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625810,1697101626655.0,120,,,"[18, 738]","[1697101625828, 1697101626566]"
3315,3315,336,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626659,1697101628903.0,120,,,"[23, 874, 43]","[1697101626682, 1697101627556, 1697101627599]"
3316,3316,697,17,[],200,llama-13b,128,1,3266.0,1.0,1,H100,1697101628909,1697101632175.0,120,123.0,10.0,"[94, 1626, 105, 80, 69, 66, 865, 92, 90, 90, 89]","[1697101629003, 1697101630629, 1697101630734, 1697101630814, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175]"
3317,3317,429,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101642322,1697101651605.0,120,,,"[6, 803, 319, 100, 96, 92, 95, 87, 85, 415, 86, 84, 84, 82, 684, 79, 77, 78, 74, 741, 98, 97, 84, 78, 1229, 103, 101, 100, 97, 88, 485, 96, 96, 83, 941, 107, 103, 102, 90, 86]","[1697101642328, 1697101643131, 1697101643450, 1697101643550, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648075, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
3318,3318,126,18,[],200,llama-13b,128,1,791.0,1.0,1,H100,1697101632176,1697101632967.0,120,19.0,1.0,"[6, 785]","[1697101632182, 1697101632967]"
3319,3319,56,17,[],200,llama-13b,128,1,4904.0,1.0,1,H100,1697101645674,1697101650578.0,120,86.0,20.0,"[7, 618, 191, 97, 98, 83, 79, 1235, 96, 101, 101, 96, 88, 486, 96, 95, 83, 942, 106, 104, 102]","[1697101645681, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646847, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650578]"
3320,3320,686,22,[],200,llama-13b,128,1,649.0,1.0,1,H100,1697101661267,1697101661916.0,120,31.0,1.0,"[13, 636]","[1697101661280, 1697101661916]"
3321,3321,915,36,[],200,llama-13b,128,1,1496.0,1.0,1,H100,1697101689616,1697101691112.0,120,182.0,1.0,"[290, 1206]","[1697101689906, 1697101691112]"
3322,3322,345,37,[],200,llama-13b,128,1,3703.0,1.0,1,H100,1697101691113,1697101694816.0,120,39.0,20.0,"[31, 886, 244, 214, 211, 79, 111, 92, 94, 403, 102, 100, 99, 95, 91, 88, 67, 433, 78, 100, 84]","[1697101691144, 1697101692030, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692889, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694815]"
3323,3323,85,23,[],200,llama-13b,128,1,5219.0,1.0,1,H100,1697101661917,1697101667136.0,120,88.0,20.0,"[30, 1390, 127, 101, 93, 88, 87, 845, 108, 104, 96, 93, 100, 688, 106, 100, 72, 72, 93, 92, 734]","[1697101661947, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664890, 1697101664986, 1697101665079, 1697101665179, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136]"
3324,3324,415,18,[],200,llama-13b,128,1,7970.0,1.0,1,H100,1697101650579,1697101658549.0,120,109.0,29.0,"[48, 2091, 892, 85, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 91, 614, 106, 100, 99, 97, 83, 623, 81, 79, 59, 836, 95, 82, 82]","[1697101650627, 1697101652718, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655512, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658384, 1697101658466, 1697101658548]"
3325,3325,607,26,[],200,llama-13b,128,1,2072.0,1.0,1,H100,1697101679861,1697101681933.0,120,6.0,10.0,"[6, 613, 98, 97, 57, 72, 803, 85, 83, 79, 79]","[1697101679867, 1697101680480, 1697101680578, 1697101680675, 1697101680732, 1697101680804, 1697101681607, 1697101681692, 1697101681775, 1697101681854, 1697101681933]"
3326,3326,215,20,[],200,llama-13b,128,1,713.0,1.0,1,H100,1697101642418,1697101643131.0,120,12.0,1.0,"[36, 677]","[1697101642454, 1697101643131]"
3327,3327,568,21,[],200,llama-13b,128,1,1184.0,1.0,1,H100,1697101643136,1697101644320.0,120,11.0,1.0,"[21, 1163]","[1697101643157, 1697101644320]"
3328,3328,928,22,[],200,llama-13b,128,1,1015.0,1.0,1,H100,1697101644322,1697101645337.0,120,20.0,1.0,"[42, 973]","[1697101644364, 1697101645337]"
3329,3329,353,23,[],200,llama-13b,128,1,1347.0,1.0,1,H100,1697101645338,1697101646685.0,120,52.0,4.0,"[12, 949, 191, 97, 98]","[1697101645350, 1697101646299, 1697101646490, 1697101646587, 1697101646685]"
3330,3330,714,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646686,1697101651603.0,120,,,"[18, 1038, 340, 96, 101, 101, 96, 88, 485, 97, 95, 83, 941, 107, 103, 102, 91, 85]","[1697101646704, 1697101647742, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649241, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
3331,3331,119,27,[],200,llama-13b,128,1,1707.0,1.0,1,H100,1697101681934,1697101683641.0,120,31.0,1.0,"[6, 1701]","[1697101681940, 1697101683641]"
3332,3332,853,26,[],200,llama-13b,128,1,4982.0,1.0,1,H100,1697101660003,1697101664985.0,120,364.0,22.0,"[19, 624, 339, 102, 75, 98, 93, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 89, 86, 845, 109, 102, 96]","[1697101660022, 1697101660646, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985]"
3333,3333,480,28,[],200,llama-13b,128,1,1264.0,1.0,1,H100,1697101683643,1697101684907.0,120,26.0,1.0,"[35, 1229]","[1697101683678, 1697101684907]"
3334,3334,706,22,[],200,llama-13b,128,1,4138.0,1.0,1,H100,1697101663465,1697101667603.0,120,86.0,20.0,"[25, 956, 232, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 99, 72, 99, 94]","[1697101663490, 1697101664446, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603]"
3335,3335,428,22,[],200,llama-13b,128,1,3520.0,1.0,1,H100,1697101644759,1697101648279.0,120,31.0,9.0,"[61, 1479, 190, 98, 98, 83, 78, 1226, 106, 101]","[1697101644820, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279]"
3336,3336,776,19,[],200,llama-13b,128,1,1157.0,1.0,1,H100,1697101658549,1697101659706.0,120,67.0,2.0,"[7, 817, 333]","[1697101658556, 1697101659373, 1697101659706]"
3337,3337,788,23,[],200,llama-13b,128,1,668.0,1.0,1,H100,1697101648280,1697101648948.0,120,31.0,1.0,"[6, 661]","[1697101648286, 1697101648947]"
3338,3338,107,23,[],200,llama-13b,128,1,1964.0,1.0,1,H100,1697101667605,1697101669569.0,120,216.0,2.0,"[42, 1922]","[1697101667647, 1697101669569]"
3339,3339,219,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651605.0,120,,,"[25, 938, 355, 106, 104, 101, 91, 85]","[1697101648973, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
3340,3340,556,18,[],200,llama-13b,128,1,768.0,1.0,1,H100,1697101648179,1697101648947.0,120,9.0,1.0,"[7, 761]","[1697101648186, 1697101648947]"
3341,3341,209,20,[],200,llama-13b,128,1,940.0,1.0,1,H100,1697101659707,1697101660647.0,120,20.0,1.0,"[18, 922]","[1697101659725, 1697101660647]"
3342,3342,539,21,[],200,llama-13b,128,1,5325.0,1.0,1,H100,1697101660648,1697101665973.0,120,83.0,20.0,"[24, 1244, 355, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 109, 103, 95, 94, 101, 688, 105]","[1697101660672, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787, 1697101664890, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973]"
3343,3343,464,24,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101669570,1697101670485.0,120,12.0,1.0,"[24, 891]","[1697101669594, 1697101670485]"
3344,3344,819,25,[],200,llama-13b,128,1,1074.0,1.0,1,H100,1697101670486,1697101671560.0,120,13.0,1.0,"[36, 1037]","[1697101670522, 1697101671559]"
3345,3345,518,13,[],200,llama-13b,128,1,1960.0,1.0,1,H100,1697101635519,1697101637479.0,120,23.0,1.0,"[105, 1855]","[1697101635624, 1697101637479]"
3346,3346,253,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671560,1697101675812.0,120,,,"[31, 955, 110, 114, 91, 68, 67, 88, 804, 98, 89, 88, 66, 65, 687, 102, 99, 94, 71, 94]","[1697101671591, 1697101672546, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675182, 1697101675276, 1697101675347, 1697101675441]"
3347,3347,872,14,[],200,llama-13b,128,1,4936.0,1.0,1,H100,1697101637480,1697101642416.0,120,91.0,20.0,"[12, 1247, 217, 98, 92, 93, 92, 91, 91, 79, 617, 82, 93, 735, 85, 78, 78, 761, 100, 99, 96]","[1697101637492, 1697101638739, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640209, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416]"
3348,3348,840,40,[],200,llama-13b,128,1,1872.0,1.0,1,H100,1697101697603,1697101699475.0,120,17.0,1.0,"[519, 1353]","[1697101698122, 1697101699475]"
3349,3349,81,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623670,1697101626657.0,120,,,"[337, 1687, 114, 98, 69]","[1697101624007, 1697101625694, 1697101625808, 1697101625906, 1697101625975]"
3350,3350,578,25,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101651610,1697101652661.0,120,31.0,1.0,"[168, 883]","[1697101651778, 1697101652661]"
3351,3351,239,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700635.0,120,,,"[66, 1009]","[1697101699543, 1697101700552]"
3352,3352,911,26,[],200,llama-13b,128,1,2564.0,1.0,1,H100,1697101652661,1697101655225.0,120,335.0,11.0,"[7, 804, 138, 86, 82, 82, 81, 896, 107, 106, 101, 74]","[1697101652668, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655151, 1697101655225]"
3353,3353,440,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626669,1697101628905.0,120,,,"[519, 1505]","[1697101627188, 1697101628693]"
3354,3354,599,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[66, 1455, 123, 69]","[1697101700704, 1697101702159, 1697101702282, 1697101702351]"
3355,3355,701,29,[],200,llama-13b,128,1,9199.0,1.0,1,H100,1697101675814,1697101685013.0,120,58.0,43.0,"[188, 1377, 98, 64, 50, 764, 89, 86, 65, 83, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461, 103, 57, 73, 803, 85, 82, 80, 78, 771, 97, 93, 91, 69, 91, 90, 614, 102, 101, 97, 91, 91, 90, 593]","[1697101676002, 1697101677379, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681607, 1697101681692, 1697101681774, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684239, 1697101684330, 1697101684420, 1697101685013]"
3356,3356,24,43,[],200,llama-13b,128,1,3091.0,1.0,1,H100,1697101703172,1697101706263.0,120,79.0,9.0,"[204, 1639, 98, 72, 728, 95, 87, 85, 83]","[1697101703376, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263]"
3357,3357,798,15,[],200,llama-13b,128,1,2905.0,1.0,1,H100,1697101628909,1697101631814.0,120,79.0,6.0,"[260, 1565, 81, 68, 66, 865]","[1697101629169, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814]"
3358,3358,529,54,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,10.0,1.0,"[84, 793]","[1697101704985, 1697101705778]"
3359,3359,888,55,[],200,llama-13b,128,1,1266.0,1.0,1,H100,1697101705779,1697101707045.0,120,19.0,1.0,"[30, 1236]","[1697101705809, 1697101707045]"
3360,3360,35,16,[],200,llama-13b,128,1,4826.0,1.0,1,H100,1697101639594,1697101644420.0,120,87.0,20.0,"[17, 1391, 117, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 800, 100, 95, 93, 95, 86, 86, 414]","[1697101639611, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644005, 1697101644419]"
3361,3361,313,56,[],200,llama-13b,128,1,1268.0,1.0,1,H100,1697101707046,1697101708314.0,120,20.0,1.0,"[30, 1237]","[1697101707076, 1697101708313]"
3362,3362,676,57,[],200,llama-13b,128,1,1059.0,1.0,1,H100,1697101708315,1697101709374.0,120,19.0,1.0,"[29, 1030]","[1697101708344, 1697101709374]"
3363,3363,76,58,[],200,llama-13b,128,1,2657.0,1.0,1,H100,1697101709375,1697101712032.0,120,364.0,12.0,"[19, 1033, 278, 102, 98, 92, 89, 88, 86, 498, 98, 91, 84]","[1697101709394, 1697101710427, 1697101710705, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031]"
3364,3364,387,44,[],200,llama-13b,128,1,5592.0,1.0,1,H100,1697101706264,1697101711856.0,120,39.0,27.0,"[12, 769, 241, 96, 92, 70, 92, 69, 91, 87, 545, 99, 64, 88, 812, 88, 83, 82, 79, 881, 104, 96, 93, 89, 88, 86, 498, 98]","[1697101706276, 1697101707045, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710808, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856]"
3365,3365,126,30,[],200,llama-13b,128,1,1102.0,1.0,1,H100,1697101685014,1697101686116.0,120,19.0,1.0,"[7, 1095]","[1697101685021, 1697101686116]"
3366,3366,335,27,[],200,llama-13b,128,1,13437.0,1.0,1,H100,1697101655226,1697101668663.0,120,58.0,62.0,"[12, 760, 129, 105, 101, 98, 98, 83, 622, 81, 79, 60, 836, 95, 82, 82, 80, 1076, 101, 99, 97, 93, 85, 805, 101, 75, 99, 93, 86, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 108, 103, 96, 94, 91, 697, 106, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 792, 102, 95, 71]","[1697101655238, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658290, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661353, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665170, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668395, 1697101668497, 1697101668592, 1697101668663]"
3367,3367,458,31,[],200,llama-13b,128,1,904.0,1.0,1,H100,1697101686118,1697101687022.0,120,11.0,1.0,"[35, 869]","[1697101686153, 1697101687022]"
3368,3368,816,32,[],200,llama-13b,128,1,1698.0,1.0,1,H100,1697101687023,1697101688721.0,120,182.0,4.0,"[13, 1349, 133, 106, 97]","[1697101687036, 1697101688385, 1697101688518, 1697101688624, 1697101688721]"
3369,3369,594,38,[],200,llama-13b,128,1,5885.0,1.0,1,H100,1697101708534,1697101714419.0,120,216.0,119.0,"[159, 1734, 277, 103, 97, 93, 89, 88, 86, 497, 100, 91, 84, 83, 78, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26, 24, 20, 20, 21, 19, 18, 19, 20, 18, 19, 17, 17, 18, 16, 16, 16, 16, 16, 17, 16, 15, 16, 15, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 16, 15, 16, 15, 16, 15, 16]","[1697101708693, 1697101710427, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711857, 1697101711948, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114, 1697101713138, 1697101713158, 1697101713178, 1697101713199, 1697101713218, 1697101713236, 1697101713255, 1697101713275, 1697101713293, 1697101713312, 1697101713329, 1697101713346, 1697101713364, 1697101713380, 1697101713396, 1697101713412, 1697101713428, 1697101713444, 1697101713461, 1697101713477, 1697101713492, 1697101713508, 1697101713523, 1697101713538, 1697101713554, 1697101713569, 1697101713584, 1697101713600, 1697101713615, 1697101713631, 1697101713646, 1697101713661, 1697101713677, 1697101713692, 1697101713707, 1697101713723, 1697101713738, 1697101713753, 1697101713769, 1697101713784, 1697101713800, 1697101713815, 1697101713830, 1697101713846, 1697101713861, 1697101713877, 1697101713892, 1697101713907, 1697101713923, 1697101713938, 1697101713954, 1697101713969, 1697101713984, 1697101714000, 1697101714015, 1697101714031, 1697101714046, 1697101714062, 1697101714077, 1697101714093, 1697101714108, 1697101714124, 1697101714139, 1697101714155, 1697101714170, 1697101714186, 1697101714201, 1697101714217, 1697101714232, 1697101714248, 1697101714263, 1697101714279, 1697101714294, 1697101714310, 1697101714326, 1697101714341, 1697101714357, 1697101714372, 1697101714388, 1697101714403, 1697101714419]"
3370,3370,364,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101644422,1697101651605.0,120,,,"[59, 856, 103, 79, 77, 78, 75, 740, 98, 97, 84, 78, 1237, 95, 101, 100, 97, 88, 485, 97, 96, 82, 942, 106, 103, 102, 91, 85]","[1697101644481, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645749, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
3371,3371,87,26,[],200,llama-13b,128,1,4295.0,1.0,1,H100,1697101675816,1697101680111.0,120,335.0,19.0,"[469, 1095, 97, 64, 51, 763, 90, 85, 65, 82, 82, 646, 99, 97, 95, 72, 92, 90, 70, 91]","[1697101676285, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678677, 1697101678759, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111]"
3372,3372,249,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688722,1697101689611.0,120,,,"[7, 841]","[1697101688729, 1697101689570]"
3373,3373,603,34,[],200,llama-13b,128,1,1496.0,1.0,1,H100,1697101689616,1697101691112.0,120,9.0,1.0,"[301, 1195]","[1697101689917, 1697101691112]"
3374,3374,33,35,[],200,llama-13b,128,1,1868.0,1.0,1,H100,1697101691113,1697101692981.0,120,140.0,7.0,"[49, 868, 244, 214, 211, 79, 101, 101]","[1697101691162, 1697101692030, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692879, 1697101692980]"
3375,3375,486,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700637.0,120,,,"[206, 1674, 113, 83, 82, 81, 81, 82]","[1697101697801, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
3376,3376,480,38,[],200,llama-13b,128,1,1021.0,1.0,1,H100,1697101680481,1697101681502.0,120,26.0,1.0,"[24, 997]","[1697101680505, 1697101681502]"
3377,3377,809,39,[],200,llama-13b,128,1,1008.0,1.0,1,H100,1697101681503,1697101682511.0,120,16.0,1.0,"[42, 966]","[1697101681545, 1697101682511]"
3378,3378,234,40,[],200,llama-13b,128,1,5291.0,1.0,1,H100,1697101682512,1697101687803.0,120,457.0,25.0,"[18, 1111, 208, 102, 100, 97, 92, 90, 90, 594, 85, 64, 82, 81, 911, 91, 89, 87, 86, 647, 102, 99, 94, 92, 91, 88]","[1697101682530, 1697101683641, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803]"
3379,3379,845,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[483, 1909]","[1697101701125, 1697101703034]"
3380,3380,235,19,[],200,llama-13b,128,1,3959.0,1.0,1,H100,1697101634997,1697101638956.0,120,161.0,12.0,"[209, 1691, 87, 56, 628, 91, 92, 87, 86, 82, 82, 768]","[1697101635206, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638956]"
3381,3381,828,9,[],200,llama-13b,128,1,2272.0,1.0,1,H100,1697101616332,1697101618604.0,120,182.0,6.0,"[159, 934, 939, 81, 80, 78]","[1697101616491, 1697101617425, 1697101618364, 1697101618445, 1697101618525, 1697101618603]"
3382,3382,597,41,[],200,llama-13b,128,1,1766.0,1.0,1,H100,1697101687804,1697101689570.0,120,39.0,1.0,"[7, 1759]","[1697101687811, 1697101689570]"
3383,3383,25,42,[],200,llama-13b,128,1,404.0,1.0,1,H100,1697101689572,1697101689976.0,120,12.0,1.0,"[12, 392]","[1697101689584, 1697101689976]"
3384,3384,260,10,[],200,llama-13b,128,1,3454.0,1.0,1,H100,1697101618604,1697101622058.0,120,86.0,20.0,"[7, 478, 263, 94, 88, 68, 85, 85, 83, 719, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 61]","[1697101618611, 1697101619089, 1697101619352, 1697101619446, 1697101619534, 1697101619602, 1697101619687, 1697101619772, 1697101619855, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622058]"
3385,3385,278,8,[],200,llama-13b,128,1,832.0,1.0,1,H100,1697101618257,1697101619089.0,120,13.0,1.0,"[13, 819]","[1697101618270, 1697101619089]"
3386,3386,387,43,[],200,llama-13b,128,1,6743.0,1.0,1,H100,1697101689977,1697101696720.0,120,39.0,27.0,"[219, 1834, 244, 213, 212, 78, 111, 93, 94, 403, 102, 100, 99, 95, 91, 88, 67, 434, 77, 100, 85, 604, 79, 76, 831, 103, 101, 110]","[1697101690196, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692888, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720]"
3387,3387,636,9,[],200,llama-13b,128,1,1287.0,1.0,1,H100,1697101619091,1697101620378.0,120,31.0,1.0,"[59, 1228]","[1697101619150, 1697101620378]"
3388,3388,162,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685329,1697101689593.0,120,,,"[48, 1645, 215, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101685377, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
3389,3389,398,30,[],200,llama-13b,128,1,4295.0,1.0,1,H100,1697101675815,1697101680110.0,120,87.0,20.0,"[100, 465, 38, 1059, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98, 95, 71, 93, 90, 70, 90]","[1697101675915, 1697101676380, 1697101676418, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679860, 1697101679950, 1697101680020, 1697101680110]"
3390,3390,68,10,[],200,llama-13b,128,1,1294.0,1.0,1,H100,1697101620379,1697101621673.0,120,12.0,1.0,"[13, 1281]","[1697101620392, 1697101621673]"
3391,3391,307,26,[],200,llama-13b,128,1,470.0,1.0,1,H100,1697101648477,1697101648947.0,120,26.0,1.0,"[12, 458]","[1697101648489, 1697101648947]"
3392,3392,908,26,[],200,llama-13b,128,1,11017.0,1.0,1,H100,1697101660648,1697101671665.0,120,6.0,50.0,"[30, 1238, 355, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 109, 102, 96, 94, 98, 691, 105, 100, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 98, 71, 93, 90, 90, 512]","[1697101660678, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665177, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665]"
3393,3393,757,31,[],200,llama-13b,128,1,1390.0,1.0,1,H100,1697101680112,1697101681502.0,120,20.0,1.0,"[36, 1354]","[1697101680148, 1697101681502]"
3394,3394,263,21,[],200,llama-13b,128,1,1366.0,1.0,1,H100,1697101654632,1697101655998.0,120,15.0,1.0,"[18, 1348]","[1697101654650, 1697101655998]"
3395,3395,708,26,[],200,llama-13b,128,1,1547.0,1.0,1,H100,1697101656613,1697101658160.0,120,140.0,1.0,"[25, 1522]","[1697101656638, 1697101658160]"
3396,3396,480,19,[],200,llama-13b,128,1,1238.0,1.0,1,H100,1697101632968,1697101634206.0,120,26.0,1.0,"[41, 1197]","[1697101633009, 1697101634206]"
3397,3397,836,20,[],200,llama-13b,128,1,762.0,1.0,1,H100,1697101634207,1697101634969.0,120,11.0,1.0,"[13, 749]","[1697101634220, 1697101634969]"
3398,3398,236,21,[],200,llama-13b,128,1,547.0,1.0,1,H100,1697101634971,1697101635518.0,120,8.0,1.0,"[53, 494]","[1697101635024, 1697101635518]"
3399,3399,220,23,[],200,llama-13b,128,1,1019.0,1.0,1,H100,1697101663659,1697101664678.0,120,67.0,2.0,"[6, 780, 233]","[1697101663665, 1697101664445, 1697101664678]"
3400,3400,198,28,[],200,llama-13b,128,1,3840.0,1.0,1,H100,1697101680581,1697101684421.0,120,96.0,20.0,"[49, 872, 106, 84, 83, 79, 79, 770, 97, 93, 91, 69, 91, 90, 615, 102, 100, 97, 92, 90, 91]","[1697101680630, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684421]"
3401,3401,597,22,[],200,llama-13b,128,1,1960.0,1.0,1,H100,1697101635519,1697101637479.0,120,39.0,1.0,"[87, 1872]","[1697101635606, 1697101637478]"
3402,3402,28,23,[],200,llama-13b,128,1,4936.0,1.0,1,H100,1697101637480,1697101642416.0,120,86.0,20.0,"[6, 1253, 217, 98, 92, 93, 92, 91, 91, 80, 616, 82, 93, 735, 85, 78, 78, 761, 100, 99, 96]","[1697101637486, 1697101638739, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209, 1697101640291, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416]"
3403,3403,546,24,[],200,llama-13b,128,1,4165.0,1.0,1,H100,1697101664679,1697101668844.0,120,93.0,20.0,"[6, 1183, 106, 99, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90]","[1697101664685, 1697101665868, 1697101665974, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844]"
3404,3404,622,22,[],200,llama-13b,128,1,1147.0,1.0,1,H100,1697101655999,1697101657146.0,120,20.0,1.0,"[18, 1129]","[1697101656017, 1697101657146]"
3405,3405,55,23,[],200,llama-13b,128,1,1010.0,1.0,1,H100,1697101657150,1697101658160.0,120,12.0,1.0,"[24, 986]","[1697101657174, 1697101658160]"
3406,3406,409,24,[],200,llama-13b,128,1,7019.0,1.0,1,H100,1697101658161,1697101665180.0,120,109.0,30.0,"[25, 1187, 332, 101, 99, 97, 93, 85, 805, 101, 76, 98, 92, 88, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 100]","[1697101658186, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661352, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665179]"
3407,3407,20,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672658,1697101675811.0,120,,,"[55, 1045, 130, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672713, 1697101673758, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3408,3408,633,16,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628909,1697101633688.0,120,90.0,20.0,"[185, 1535, 104, 82, 68, 66, 864, 93, 91, 89, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 70]","[1697101629094, 1697101630629, 1697101630733, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687]"
3409,3409,905,25,[],200,llama-13b,128,1,1640.0,1.0,1,H100,1697101668845,1697101670485.0,120,11.0,1.0,"[30, 1610]","[1697101668875, 1697101670485]"
3410,3410,330,26,[],200,llama-13b,128,1,3403.0,1.0,1,H100,1697101670486,1697101673889.0,120,345.0,14.0,"[18, 1055, 107, 97, 95, 94, 93, 91, 520, 113, 92, 68, 67, 88, 805]","[1697101670504, 1697101671559, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673889]"
3411,3411,209,21,[],200,llama-13b,128,1,625.0,1.0,1,H100,1697101642506,1697101643131.0,120,20.0,1.0,"[13, 612]","[1697101642519, 1697101643131]"
3412,3412,567,22,[],200,llama-13b,128,1,5246.0,1.0,1,H100,1697101643133,1697101648379.0,120,90.0,20.0,"[14, 1173, 100, 86, 84, 84, 83, 683, 79, 77, 78, 74, 741, 98, 97, 84, 78, 1236, 96, 101, 100]","[1697101643147, 1697101644320, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644757, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379]"
3413,3413,530,27,[],200,llama-13b,128,1,1566.0,1.0,1,H100,1697101675814,1697101677380.0,120,26.0,1.0,"[361, 1205]","[1697101676175, 1697101677380]"
3414,3414,526,26,[],200,llama-13b,128,1,4297.0,1.0,1,H100,1697101675813,1697101680110.0,120,89.0,20.0,"[73, 494, 36, 1061, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98, 95, 71, 92, 91, 70, 90]","[1697101675886, 1697101676380, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679950, 1697101680020, 1697101680110]"
3415,3415,885,28,[],200,llama-13b,128,1,7865.0,1.0,1,H100,1697101677381,1697101685246.0,120,84.0,43.0,"[49, 817, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93, 91, 69, 91, 90, 615, 101, 101, 97, 92, 90, 90, 593, 86, 64, 82]","[1697101677430, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245]"
3416,3416,15,28,[],200,llama-13b,128,1,4266.0,1.0,1,H100,1697101670715,1697101674981.0,120,100.0,20.0,"[11, 833, 107, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 805, 97, 89, 88, 66, 65, 687]","[1697101670726, 1697101671559, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673889, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981]"
3417,3417,105,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646769,1697101651603.0,120,,,"[18, 955, 340, 97, 100, 101, 96, 88, 485, 97, 96, 82, 941, 107, 103, 102, 91, 85]","[1697101646787, 1697101647742, 1697101648082, 1697101648179, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650265, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
3418,3418,880,27,[],200,llama-13b,128,1,1496.0,1.0,1,H100,1697101680112,1697101681608.0,120,84.0,2.0,"[13, 1377, 106]","[1697101680125, 1697101681502, 1697101681608]"
3419,3419,306,28,[],200,llama-13b,128,1,1445.0,1.0,1,H100,1697101681609,1697101683054.0,120,140.0,6.0,"[6, 1089, 96, 93, 92, 69]","[1697101681615, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054]"
3420,3420,366,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674982,1697101675810.0,120,,,"[19, 788]","[1697101675001, 1697101675789]"
3421,3421,447,27,[],200,llama-13b,128,1,3121.0,1.0,1,H100,1697101680113,1697101683234.0,120,161.0,13.0,"[41, 1454, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90]","[1697101680154, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234]"
3422,3422,705,20,[],200,llama-13b,128,1,6003.0,1.0,1,H100,1697101663566,1697101669569.0,120,79.0,27.0,"[6, 873, 233, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 99, 94, 793, 101, 95, 71, 91, 89, 725]","[1697101663572, 1697101664445, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568]"
3423,3423,92,27,[],200,llama-13b,128,1,3840.0,1.0,1,H100,1697101680581,1697101684421.0,120,85.0,20.0,"[43, 878, 106, 84, 83, 79, 79, 770, 97, 93, 91, 70, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680624, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
3424,3424,552,34,[],200,llama-13b,128,1,4764.0,1.0,1,H100,1697101675816,1697101680580.0,120,87.0,20.0,"[499, 1065, 97, 64, 51, 762, 90, 86, 64, 84, 80, 647, 99, 98, 94, 72, 92, 90, 70, 91, 469]","[1697101676315, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680580]"
3425,3425,523,42,[],200,llama-13b,128,1,4180.0,1.0,1,H100,1697101689599,1697101693779.0,120,345.0,13.0,"[23, 1490, 93, 1068, 214, 212, 78, 101, 102, 94, 404, 102, 100, 99]","[1697101689622, 1697101691112, 1697101691205, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779]"
3426,3426,775,28,[],200,llama-13b,128,1,1672.0,1.0,1,H100,1697101683235,1697101684907.0,120,17.0,1.0,"[6, 1666]","[1697101683241, 1697101684907]"
3427,3427,184,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674982,1697101675810.0,120,,,"[43, 764]","[1697101675025, 1697101675789]"
3428,3428,204,29,[],200,llama-13b,128,1,1683.0,1.0,1,H100,1697101684908,1697101686591.0,120,67.0,6.0,"[6, 1202, 121, 91, 89, 87, 87]","[1697101684914, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591]"
3429,3429,541,30,[],200,llama-13b,128,1,4765.0,1.0,1,H100,1697101675814,1697101680579.0,120,90.0,20.0,"[465, 1100, 98, 64, 51, 763, 90, 85, 65, 82, 82, 646, 99, 98, 94, 72, 92, 90, 70, 91, 468]","[1697101676279, 1697101677379, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678677, 1697101678759, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680579]"
3430,3430,912,35,[],200,llama-13b,128,1,3839.0,1.0,1,H100,1697101680582,1697101684421.0,120,92.0,20.0,"[86, 834, 106, 84, 83, 79, 79, 771, 96, 93, 91, 69, 91, 90, 615, 102, 100, 97, 92, 90, 91]","[1697101680668, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684421]"
3431,3431,563,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101686592,1697101689594.0,120,,,"[6, 1787, 133, 106, 97, 92, 92, 89]","[1697101686598, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3432,3432,921,31,[],200,llama-13b,128,1,1498.0,1.0,1,H100,1697101689614,1697101691112.0,120,31.0,1.0,"[160, 1338]","[1697101689774, 1697101691112]"
3433,3433,346,32,[],200,llama-13b,128,1,3703.0,1.0,1,H100,1697101691113,1697101694816.0,120,85.0,20.0,"[37, 880, 244, 214, 211, 79, 111, 92, 94, 404, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 85]","[1697101691150, 1697101692030, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692889, 1697101692981, 1697101693075, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816]"
3434,3434,160,32,[],200,llama-13b,128,1,1008.0,1.0,1,H100,1697101681503,1697101682511.0,120,13.0,1.0,"[18, 990]","[1697101681521, 1697101682511]"
3435,3435,514,33,[],200,llama-13b,128,1,4827.0,1.0,1,H100,1697101682512,1697101687339.0,120,85.0,20.0,"[24, 1105, 208, 102, 100, 97, 92, 90, 90, 594, 85, 64, 82, 81, 911, 91, 89, 87, 86, 647, 102]","[1697101682536, 1697101683641, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339]"
3436,3436,134,21,[],200,llama-13b,128,1,4318.0,1.0,1,H100,1697101669570,1697101673888.0,120,86.0,20.0,"[6, 909, 226, 99, 71, 92, 91, 90, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 68, 88, 803]","[1697101669576, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888]"
3437,3437,671,20,[],200,llama-13b,128,1,1053.0,1.0,1,H100,1697101651608,1697101652661.0,120,12.0,1.0,"[172, 881]","[1697101651780, 1697101652661]"
3438,3438,919,21,[],200,llama-13b,128,1,1540.0,1.0,1,H100,1697101644759,1697101646299.0,120,14.0,1.0,"[54, 1486]","[1697101644813, 1697101646299]"
3439,3439,124,30,[],200,llama-13b,128,1,794.0,1.0,1,H100,1697101683055,1697101683849.0,120,83.0,2.0,"[6, 788]","[1697101683061, 1697101683849]"
3440,3440,355,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101646300,1697101651605.0,120,,,"[30, 1412, 341, 96, 100, 101, 96, 88, 485, 96, 96, 83, 942, 106, 104, 102, 90, 85]","[1697101646330, 1697101647742, 1697101648083, 1697101648179, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650578, 1697101650668, 1697101650753]"
3441,3441,389,24,[],200,llama-13b,128,1,713.0,1.0,1,H100,1697101642418,1697101643131.0,120,8.0,1.0,"[24, 689]","[1697101642442, 1697101643131]"
3442,3442,115,25,[],200,llama-13b,128,1,1864.0,1.0,1,H100,1697101651608,1697101653472.0,120,13.0,1.0,"[288, 1576]","[1697101651896, 1697101653472]"
3443,3443,476,26,[],200,llama-13b,128,1,11410.0,1.0,1,H100,1697101653479,1697101664889.0,120,6.0,50.0,"[30, 1122, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 97, 83, 623, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 108, 103]","[1697101653509, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889]"
3444,3444,743,25,[],200,llama-13b,128,1,1625.0,1.0,1,H100,1697101643132,1697101644757.0,120,123.0,6.0,"[15, 1173, 100, 86, 84, 84, 83]","[1697101643147, 1697101644320, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644757]"
3445,3445,685,22,[],200,llama-13b,128,1,1004.0,1.0,1,H100,1697101661268,1697101662272.0,120,364.0,2.0,"[30, 618, 356]","[1697101661298, 1697101661916, 1697101662272]"
3446,3446,666,17,[],200,llama-13b,128,1,5003.0,1.0,1,H100,1697101651608,1697101656611.0,120,84.0,20.0,"[270, 1594, 137, 86, 84, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 82]","[1697101651878, 1697101653472, 1697101653609, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656611]"
3447,3447,144,26,[],200,llama-13b,128,1,5818.0,1.0,1,H100,1697101644759,1697101650577.0,120,96.0,20.0,"[24, 1516, 190, 98, 98, 83, 78, 1237, 95, 101, 100, 97, 88, 485, 97, 96, 82, 942, 106, 103, 102]","[1697101644783, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577]"
3448,3448,110,23,[],200,llama-13b,128,1,1385.0,1.0,1,H100,1697101662273,1697101663658.0,120,96.0,4.0,"[6, 1058, 127, 101, 93]","[1697101662279, 1697101663337, 1697101663464, 1697101663565, 1697101663658]"
3449,3449,923,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622059,1697101623660.0,120,,,"[7, 1506]","[1697101622066, 1697101623572]"
3450,3450,439,24,[],200,llama-13b,128,1,1231.0,1.0,1,H100,1697101663659,1697101664890.0,120,13.0,4.0,"[12, 774, 233, 109, 103]","[1697101663671, 1697101664445, 1697101664678, 1697101664787, 1697101664890]"
3451,3451,798,25,[],200,llama-13b,128,1,1327.0,1.0,1,H100,1697101664890,1697101666217.0,120,79.0,6.0,"[19, 959, 106, 100, 71, 72]","[1697101664909, 1697101665868, 1697101665974, 1697101666074, 1697101666145, 1697101666217]"
3452,3452,497,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101650578,1697101651604.0,120,,,"[7, 723]","[1697101650585, 1697101651308]"
3453,3453,859,28,[],200,llama-13b,128,1,1862.0,1.0,1,H100,1697101651610,1697101653472.0,120,23.0,1.0,"[501, 1361]","[1697101652111, 1697101653472]"
3454,3454,289,29,[],200,llama-13b,128,1,4810.0,1.0,1,H100,1697101653479,1697101658289.0,120,89.0,20.0,"[18, 1134, 206, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83, 623, 81, 79, 60, 835]","[1697101653497, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
3455,3455,751,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101683055,1697101689613.0,120,,,"[6, 788, 102, 100, 97, 92, 90, 90, 593, 86, 64, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 91, 90]","[1697101683061, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688994]"
3456,3456,843,13,[],200,llama-13b,128,1,1044.0,1.0,1,H100,1697101616332,1697101617376.0,120,14.0,1.0,"[184, 860]","[1697101616516, 1697101617376]"
3457,3457,273,14,[],200,llama-13b,128,1,1710.0,1.0,1,H100,1697101617378,1697101619088.0,120,19.0,1.0,"[49, 1661]","[1697101617427, 1697101619088]"
3458,3458,307,37,[],200,llama-13b,128,1,952.0,1.0,1,H100,1697101685164,1697101686116.0,120,26.0,1.0,"[13, 939]","[1697101685177, 1697101686116]"
3459,3459,670,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101686117,1697101689594.0,120,,,"[18, 888, 214, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 91, 90]","[1697101686135, 1697101687023, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688994]"
3460,3460,631,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101619090,1697101623659.0,120,,,"[30, 1454, 96, 94, 93, 91, 92, 74, 644, 81, 80, 78, 60, 614, 87, 87, 84, 83]","[1697101619120, 1697101620574, 1697101620670, 1697101620764, 1697101620857, 1697101620948, 1697101621040, 1697101621114, 1697101621758, 1697101621839, 1697101621919, 1697101621997, 1697101622057, 1697101622671, 1697101622758, 1697101622845, 1697101622929, 1697101623012]"
3461,3461,628,32,[],200,llama-13b,128,1,2523.0,1.0,1,H100,1697101672658,1697101675181.0,120,732.0,10.0,"[25, 1076, 129, 98, 89, 88, 66, 66, 686, 102, 98]","[1697101672683, 1697101673759, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181]"
3462,3462,70,39,[],200,llama-13b,128,1,2411.0,1.0,1,H100,1697101689619,1697101692030.0,120,39.0,1.0,"[478, 1933]","[1697101690097, 1697101692030]"
3463,3463,430,40,[],200,llama-13b,128,1,1341.0,1.0,1,H100,1697101692031,1697101693372.0,120,15.0,1.0,"[24, 1317]","[1697101692055, 1697101693372]"
3464,3464,786,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693373,1697101697588.0,120,,,"[13, 1071, 97, 77, 101, 84, 604, 78, 77, 831, 103, 101, 111, 91, 91]","[1697101693386, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
3465,3465,87,26,[],200,llama-13b,128,1,3760.0,1.0,1,H100,1697101663748,1697101667508.0,120,335.0,19.0,"[12, 686, 232, 109, 103, 95, 94, 100, 689, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98]","[1697101663760, 1697101664446, 1697101664678, 1697101664787, 1697101664890, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508]"
3466,3466,488,15,[],200,llama-13b,128,1,668.0,1.0,1,H100,1697101636811,1697101637479.0,120,6.0,1.0,"[87, 581]","[1697101636898, 1697101637479]"
3467,3467,593,20,[],200,llama-13b,128,1,3164.0,1.0,1,H100,1697101638957,1697101642121.0,120,335.0,9.0,"[6, 1123, 123, 83, 92, 735, 85, 79, 77, 761]","[1697101638963, 1697101640086, 1697101640209, 1697101640292, 1697101640384, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642121]"
3468,3468,246,22,[],200,llama-13b,128,1,11147.0,1.0,1,H100,1697101651610,1697101662757.0,120,58.0,47.0,"[375, 1487, 138, 85, 83, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 97, 83, 623, 81, 79, 59, 836, 95, 83, 81, 81, 1076, 101, 98, 98, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89]","[1697101651985, 1697101653472, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659806, 1697101659904, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757]"
3469,3469,801,26,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101651609,1697101656529.0,120,47.0,20.0,"[177, 875, 57, 891, 87, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 98]","[1697101651786, 1697101652661, 1697101652718, 1697101653609, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656529]"
3470,3470,31,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623665,1697101626657.0,120,,,"[130, 773, 39, 1202, 97, 68]","[1697101623795, 1697101624568, 1697101624607, 1697101625809, 1697101625906, 1697101625974]"
3471,3471,390,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626664,1697101628905.0,120,,,"[409, 1620]","[1697101627073, 1697101628693]"
3472,3472,18,21,[],200,llama-13b,128,1,1009.0,1.0,1,H100,1697101642122,1697101643131.0,120,15.0,1.0,"[13, 996]","[1697101642135, 1697101643131]"
3473,3473,748,18,[],200,llama-13b,128,1,4261.0,1.0,1,H100,1697101628910,1697101633171.0,120,182.0,14.0,"[461, 1363, 81, 68, 66, 865, 92, 90, 90, 89, 87, 85, 83, 741]","[1697101629371, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632347, 1697101632430, 1697101633171]"
3474,3474,549,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,[49],[1697101633739]
3475,3475,903,16,[],200,llama-13b,128,1,2852.0,1.0,1,H100,1697101634999,1697101637851.0,120,244.0,7.0,"[382, 1427, 89, 87, 56, 628, 91, 92]","[1697101635381, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851]"
3476,3476,373,22,[],200,llama-13b,128,1,1185.0,1.0,1,H100,1697101643136,1697101644321.0,120,15.0,1.0,"[27, 1158]","[1697101643163, 1697101644321]"
3477,3477,333,17,[],200,llama-13b,128,1,2439.0,1.0,1,H100,1697101637852,1697101640291.0,120,563.0,11.0,"[13, 874, 217, 98, 93, 92, 92, 92, 90, 79, 617, 82]","[1697101637865, 1697101638739, 1697101638956, 1697101639054, 1697101639147, 1697101639239, 1697101639331, 1697101639423, 1697101639513, 1697101639592, 1697101640209, 1697101640291]"
3478,3478,173,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633172,1697101634995.0,120,,,"[7, 1027, 96, 74, 71]","[1697101633179, 1697101634206, 1697101634302, 1697101634376, 1697101634447]"
3479,3479,717,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696721,1697101697589.0,120,,,"[7, 667]","[1697101696728, 1697101697395]"
3480,3480,141,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697598,1697101700636.0,120,,,"[483, 1394, 113, 83, 82, 81, 81, 82]","[1697101698081, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
3481,3481,901,31,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101680582,1697101681502.0,120,17.0,1.0,"[54, 866]","[1697101680636, 1697101681502]"
3482,3482,279,25,[],200,llama-13b,128,1,4728.0,1.0,1,H100,1697101644321,1697101649049.0,120,67.0,18.0,"[19, 997, 103, 78, 78, 77, 75, 741, 98, 98, 83, 78, 1227, 105, 101, 101, 96, 88, 485]","[1697101644340, 1697101645337, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648073, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049]"
3483,3483,749,28,[],200,llama-13b,128,1,4757.0,1.0,1,H100,1697101675814,1697101680571.0,120,47.0,20.0,"[173, 1392, 98, 64, 50, 763, 90, 86, 64, 84, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101675987, 1697101677379, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
3484,3484,495,46,[],200,llama-13b,128,1,1522.0,1.0,1,H100,1697101700638,1697101702160.0,120,13.0,1.0,"[186, 1336]","[1697101700824, 1697101702160]"
3485,3485,855,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703171.0,120,,,"[42, 831]","[1697101702203, 1697101703034]"
3486,3486,36,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687340,1697101689594.0,120,,,"[6, 1039, 133, 106, 97, 92, 92, 89]","[1697101687346, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3487,3487,287,48,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703176,1697101704900.0,120,10.0,1.0,"[234, 1490]","[1697101703410, 1697101704900]"
3488,3488,638,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649050,1697101651605.0,120,,,"[7, 854, 355, 106, 104, 101, 91, 85]","[1697101649057, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
3489,3489,322,32,[],200,llama-13b,128,1,3824.0,1.0,1,H100,1697101681503,1697101685327.0,120,93.0,20.0,"[12, 1189, 96, 93, 92, 69, 90, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 81]","[1697101681515, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327]"
3490,3490,617,49,[],200,llama-13b,128,1,4590.0,1.0,1,H100,1697101704901,1697101709491.0,120,87.0,20.0,"[96, 781, 135, 95, 87, 85, 83, 80, 942, 96, 93, 70, 91, 70, 91, 87, 544, 99, 65, 88, 811]","[1697101704997, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490]"
3491,3491,445,24,[],200,llama-13b,128,1,1259.0,1.0,1,H100,1697101667137,1697101668396.0,120,457.0,2.0,"[37, 1089, 133]","[1697101667174, 1697101668263, 1697101668396]"
3492,3492,341,19,[],200,llama-13b,128,1,4821.0,1.0,1,H100,1697101644421,1697101649242.0,120,87.0,20.0,"[25, 891, 103, 79, 77, 78, 74, 741, 98, 97, 84, 78, 1236, 96, 101, 100, 97, 88, 485, 96, 97]","[1697101644446, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242]"
3493,3493,181,29,[],200,llama-13b,128,1,8043.0,1.0,1,H100,1697101680581,1697101688624.0,120,91.0,39.0,"[19, 902, 106, 84, 83, 79, 79, 770, 98, 92, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90, 594, 86, 63, 83, 80, 911, 91, 89, 87, 86, 647, 101, 100, 94, 92, 91, 88, 715, 106]","[1697101680600, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682703, 1697101682801, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
3494,3494,74,21,[],200,llama-13b,128,1,3949.0,1.0,1,H100,1697101652662,1697101656611.0,120,88.0,20.0,"[12, 798, 138, 86, 82, 82, 81, 896, 108, 105, 101, 74, 99, 97, 92, 614, 105, 100, 99, 97, 83]","[1697101652674, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611]"
3495,3495,699,20,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101649243,1697101649912.0,120,39.0,1.0,"[48, 621]","[1697101649291, 1697101649912]"
3496,3496,100,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649913,1697101651604.0,120,,,"[36, 1359]","[1697101649949, 1697101651308]"
3497,3497,458,22,[],200,llama-13b,128,1,1862.0,1.0,1,H100,1697101651610,1697101653472.0,120,11.0,1.0,"[282, 1580]","[1697101651892, 1697101653472]"
3498,3498,678,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[64, 1301, 223, 103, 101, 110, 92, 91]","[1697101694882, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
3499,3499,703,22,[],200,llama-13b,128,1,1366.0,1.0,1,H100,1697101654632,1697101655998.0,120,12.0,1.0,"[36, 1330]","[1697101654668, 1697101655998]"
3500,3500,136,23,[],200,llama-13b,128,1,1147.0,1.0,1,H100,1697101655999,1697101657146.0,120,31.0,1.0,"[24, 1123]","[1697101656023, 1697101657146]"
3501,3501,819,23,[],200,llama-13b,128,1,1152.0,1.0,1,H100,1697101653479,1697101654631.0,120,13.0,1.0,"[54, 1098]","[1697101653533, 1697101654631]"
3502,3502,245,24,[],200,llama-13b,128,1,5370.0,1.0,1,H100,1697101654632,1697101660002.0,120,100.0,20.0,"[30, 1336, 129, 105, 101, 98, 98, 83, 622, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97]","[1697101654662, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002]"
3503,3503,482,31,[],200,llama-13b,128,1,4774.0,1.0,1,H100,1697101683850,1697101688624.0,120,91.0,20.0,"[18, 1039, 106, 87, 63, 83, 81, 910, 91, 88, 88, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106]","[1697101683868, 1697101684907, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686416, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
3504,3504,783,21,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101651610,1697101652661.0,120,286.0,1.0,"[224, 827]","[1697101651834, 1697101652661]"
3505,3505,208,22,[],200,llama-13b,128,1,3949.0,1.0,1,H100,1697101652662,1697101656611.0,120,96.0,20.0,"[36, 774, 138, 86, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83]","[1697101652698, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611]"
3506,3506,844,32,[],200,llama-13b,128,1,795.0,1.0,1,H100,1697101665975,1697101666770.0,120,10.0,1.0,"[24, 771]","[1697101665999, 1697101666770]"
3507,3507,273,33,[],200,llama-13b,128,1,1492.0,1.0,1,H100,1697101666771,1697101668263.0,120,19.0,1.0,"[24, 1468]","[1697101666795, 1697101668263]"
3508,3508,628,34,[],200,llama-13b,128,1,2616.0,1.0,1,H100,1697101668264,1697101670880.0,120,732.0,10.0,"[24, 1156, 125, 98, 95, 92, 93, 88, 676, 99, 70]","[1697101668288, 1697101669444, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880]"
3509,3509,819,23,[],200,llama-13b,128,1,2099.0,1.0,1,H100,1697101646848,1697101648947.0,120,13.0,1.0,"[6, 2093]","[1697101646854, 1697101648947]"
3510,3510,246,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651604.0,120,,,"[19, 944, 355, 106, 104, 101, 91, 85]","[1697101648967, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
3511,3511,675,28,[],200,llama-13b,128,1,1712.0,1.0,1,H100,1697101658290,1697101660002.0,120,563.0,5.0,"[25, 1058, 332, 102, 98, 97]","[1697101658315, 1697101659373, 1697101659705, 1697101659807, 1697101659905, 1697101660002]"
3512,3512,599,25,[],200,llama-13b,128,1,13279.0,1.0,1,H100,1697101651610,1697101664889.0,120,58.0,55.0,"[399, 1463, 138, 85, 83, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 614, 105, 101, 98, 97, 83, 623, 81, 79, 60, 835, 95, 83, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 844, 109, 103]","[1697101652009, 1697101653472, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664677, 1697101664786, 1697101664889]"
3513,3513,104,29,[],200,llama-13b,128,1,4784.0,1.0,1,H100,1697101660003,1697101664787.0,120,93.0,20.0,"[7, 975, 102, 75, 98, 93, 87, 831, 107, 101, 95, 94, 89, 708, 100, 93, 88, 87, 845, 109]","[1697101660010, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663465, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787]"
3514,3514,467,23,[],200,llama-13b,128,1,5002.0,1.0,1,H100,1697101651609,1697101656611.0,120,93.0,20.0,"[292, 1709, 85, 84, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 82]","[1697101651901, 1697101653610, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656611]"
3515,3515,67,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101656615,1697101675810.0,120,,,"[98, 1447, 129, 96, 82, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 102, 75, 104, 87, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 92, 697, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 98, 71, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101656713, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665171, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3516,3516,824,26,[],200,llama-13b,128,1,1212.0,1.0,1,H100,1697101661268,1697101662480.0,120,58.0,4.0,"[30, 618, 356, 106, 102]","[1697101661298, 1697101661916, 1697101662272, 1697101662378, 1697101662480]"
3517,3517,255,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101662481,1697101675811.0,120,,,"[18, 838, 127, 101, 93, 89, 86, 845, 108, 103, 96, 94, 100, 689, 105, 101, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101662499, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3518,3518,69,31,[],200,llama-13b,128,1,4521.0,1.0,1,H100,1697101680806,1697101685327.0,120,85.0,20.0,"[43, 1662, 193, 96, 93, 92, 69, 90, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 81]","[1697101680849, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327]"
3519,3519,199,16,[],200,llama-13b,128,1,1150.0,1.0,1,H100,1697101631816,1697101632966.0,120,13.0,1.0,"[20, 1130]","[1697101631836, 1697101632966]"
3520,3520,550,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101632967,1697101634995.0,120,,,"[7, 1232, 95, 75, 71]","[1697101632974, 1697101634206, 1697101634301, 1697101634376, 1697101634447]"
3521,3521,245,42,[],200,llama-13b,128,1,5351.0,1.0,1,H100,1697101703175,1697101708526.0,120,100.0,20.0,"[37, 1687, 117, 97, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 91, 70, 91, 86, 545, 99]","[1697101703212, 1697101704899, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
3522,3522,53,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101675182,1697101675811.0,120,,,"[7, 600]","[1697101675189, 1697101675789]"
3523,3523,887,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684149,1697101689611.0,120,,,"[12, 746, 107, 86, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101684161, 1697101684907, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3524,3524,906,18,[],200,llama-13b,128,1,5210.0,1.0,1,H100,1697101634998,1697101640208.0,120,86.0,20.0,"[106, 1703, 90, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 93, 92, 91, 91, 80, 615]","[1697101635104, 1697101636807, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640208]"
3525,3525,180,20,[],200,llama-13b,128,1,2617.0,1.0,1,H100,1697101641121,1697101643738.0,120,123.0,12.0,"[36, 660, 304, 101, 99, 96, 88, 82, 64, 799, 100, 95, 93]","[1697101641157, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642651, 1697101643450, 1697101643550, 1697101643645, 1697101643738]"
3526,3526,824,16,[],200,llama-13b,128,1,1667.0,1.0,1,H100,1697101637480,1697101639147.0,120,58.0,4.0,"[18, 1241, 217, 97, 93]","[1697101637498, 1697101638739, 1697101638956, 1697101639053, 1697101639146]"
3527,3527,600,43,[],200,llama-13b,128,1,840.0,1.0,1,H100,1697101708534,1697101709374.0,120,23.0,1.0,"[89, 751]","[1697101708623, 1697101709374]"
3528,3528,430,11,[],200,llama-13b,128,1,908.0,1.0,1,H100,1697101621674,1697101622582.0,120,15.0,1.0,"[7, 901]","[1697101621681, 1697101622582]"
3529,3529,667,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651604.0,120,,,"[43, 920, 355, 106, 104, 101, 91, 85]","[1697101648991, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
3530,3530,448,27,[],200,llama-13b,128,1,2437.0,1.0,1,H100,1697101667509,1697101669946.0,120,335.0,12.0,"[7, 747, 133, 101, 95, 71, 92, 89, 724, 99, 94, 93, 92]","[1697101667516, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668755, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946]"
3531,3531,337,19,[],200,llama-13b,128,1,790.0,1.0,1,H100,1697101640212,1697101641002.0,120,12.0,1.0,"[30, 760]","[1697101640242, 1697101641002]"
3532,3532,759,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.22 GiB. GPU 0 has a total capacty of 79.11 GiB of which 5.77 GiB is free. Process 1607256 has 73.32 GiB memory in use. Of the allocated memory 41.54 GiB is allocated by PyTorch, and 30.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101622583,1697101623664.0,120,,,"[24, 965]","[1697101622607, 1697101623572]"
3533,3533,184,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623673,1697101626658.0,120,,,"[498, 1524, 113, 99, 68]","[1697101624171, 1697101625695, 1697101625808, 1697101625907, 1697101625975]"
3534,3534,702,20,[],200,llama-13b,128,1,3754.0,1.0,1,H100,1697101641003,1697101644757.0,120,89.0,20.0,"[49, 765, 304, 101, 98, 97, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 87, 83, 84, 83]","[1697101641052, 1697101641817, 1697101642121, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644507, 1697101644590, 1697101644674, 1697101644757]"
3535,3535,544,14,[],200,llama-13b,128,1,2023.0,1.0,1,H100,1697101626670,1697101628693.0,120,26.0,1.0,"[521, 1502]","[1697101627191, 1697101628693]"
3536,3536,897,15,[],200,llama-13b,128,1,722.0,1.0,1,H100,1697101628695,1697101629417.0,120,9.0,1.0,"[78, 644]","[1697101628773, 1697101629417]"
3537,3537,874,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687340,1697101689594.0,120,,,"[18, 1027, 133, 106, 97, 92, 92, 89]","[1697101687358, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3538,3538,328,16,[],200,llama-13b,128,1,2757.0,1.0,1,H100,1697101629418,1697101632175.0,120,109.0,6.0,"[24, 2242, 130, 92, 90, 90, 89]","[1697101629442, 1697101631684, 1697101631814, 1697101631906, 1697101631996, 1697101632086, 1697101632175]"
3539,3539,660,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101632176,1697101634995.0,120,,,"[12, 779, 205, 95, 71, 94, 93, 92, 70, 615, 74, 71]","[1697101632188, 1697101632967, 1697101633172, 1697101633267, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634302, 1697101634376, 1697101634447]"
3540,3540,314,22,[],200,llama-13b,128,1,3784.0,1.0,1,H100,1697101646588,1697101650372.0,120,335.0,13.0,"[13, 1141, 340, 96, 101, 101, 96, 88, 485, 96, 96, 83, 942, 106]","[1697101646601, 1697101647742, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372]"
3541,3541,217,21,[],200,llama-13b,128,1,5818.0,1.0,1,H100,1697101644759,1697101650577.0,120,85.0,20.0,"[48, 1492, 190, 98, 98, 83, 78, 1226, 106, 101, 100, 97, 88, 486, 96, 96, 82, 942, 106, 103, 102]","[1697101644807, 1697101646299, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577]"
3542,3542,303,35,[],200,llama-13b,128,1,5205.0,1.0,1,H100,1697101689611,1697101694816.0,120,88.0,20.0,"[465, 1954, 244, 213, 212, 78, 101, 103, 94, 403, 102, 100, 99, 94, 91, 88, 68, 434, 77, 100, 85]","[1697101690076, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
3543,3543,162,31,[],200,llama-13b,128,1,4264.0,1.0,1,H100,1697101670717,1697101674981.0,120,90.0,20.0,"[28, 814, 107, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 66, 686]","[1697101670745, 1697101671559, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981]"
3544,3544,650,33,[],200,llama-13b,128,1,1694.0,1.0,1,H100,1697101685328,1697101687022.0,120,13.0,1.0,"[13, 1681]","[1697101685341, 1697101687022]"
3545,3545,91,18,[],200,llama-13b,128,1,1808.0,1.0,1,H100,1697101635000,1697101636808.0,120,23.0,1.0,"[393, 1415]","[1697101635393, 1697101636808]"
3546,3546,451,19,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101636810,1697101637479.0,120,286.0,1.0,"[24, 645]","[1697101636834, 1697101637479]"
3547,3547,675,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101650373,1697101651604.0,120,,,"[6, 929]","[1697101650379, 1697101651308]"
3548,3548,106,24,[],200,llama-13b,128,1,5002.0,1.0,1,H100,1697101651609,1697101656611.0,120,161.0,20.0,"[289, 1574, 137, 86, 84, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 82]","[1697101651898, 1697101653472, 1697101653609, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656611]"
3549,3549,80,34,[],200,llama-13b,128,1,1361.0,1.0,1,H100,1697101687024,1697101688385.0,120,13.0,1.0,"[41, 1320]","[1697101687065, 1697101688385]"
3550,3550,442,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688386,1697101689594.0,120,,,"[18, 1166]","[1697101688404, 1697101689570]"
3551,3551,805,20,[],200,llama-13b,128,1,10996.0,1.0,1,H100,1697101637480,1697101648476.0,120,286.0,50.0,"[60, 1199, 217, 98, 93, 92, 92, 92, 90, 78, 618, 82, 94, 734, 85, 79, 77, 760, 102, 98, 97, 88, 82, 63, 800, 99, 96, 93, 95, 86, 85, 415, 87, 84, 84, 82, 683, 79, 78, 77, 75, 741, 98, 97, 84, 78, 1226, 106, 101, 100, 97]","[1697101637540, 1697101638739, 1697101638956, 1697101639054, 1697101639147, 1697101639239, 1697101639331, 1697101639423, 1697101639513, 1697101639591, 1697101640209, 1697101640291, 1697101640385, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642120, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644004, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178, 1697101648279, 1697101648379, 1697101648476]"
3552,3552,799,25,[],200,llama-13b,128,1,4260.0,1.0,1,H100,1697101668397,1697101672657.0,120,84.0,20.0,"[12, 1035, 125, 98, 95, 92, 93, 88, 676, 99, 70, 93, 90, 90, 513, 97, 95, 94, 93, 91, 521]","[1697101668409, 1697101669444, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672657]"
3553,3553,69,27,[],200,llama-13b,128,1,4765.0,1.0,1,H100,1697101675814,1697101680579.0,120,85.0,20.0,"[391, 1175, 97, 64, 50, 764, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 91, 460]","[1697101676205, 1697101677380, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571]"
3554,3554,422,27,[],200,llama-13b,128,1,1546.0,1.0,1,H100,1697101656614,1697101658160.0,120,26.0,1.0,"[76, 1470]","[1697101656690, 1697101658160]"
3555,3555,489,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673892,1697101675810.0,120,,,"[47, 909, 133, 102, 99, 95, 70, 95]","[1697101673939, 1697101674848, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675442]"
3556,3556,780,28,[],200,llama-13b,128,1,5303.0,1.0,1,H100,1697101658161,1697101663464.0,120,85.0,20.0,"[42, 1170, 332, 101, 99, 97, 93, 85, 805, 101, 75, 99, 92, 88, 831, 107, 101, 95, 94, 89, 707]","[1697101658203, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464]"
3557,3557,420,28,[],200,llama-13b,128,1,3839.0,1.0,1,H100,1697101680581,1697101684420.0,120,52.0,20.0,"[13, 908, 106, 84, 83, 79, 79, 770, 98, 92, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680594, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682703, 1697101682801, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
3558,3558,779,29,[],200,llama-13b,128,1,3106.0,1.0,1,H100,1697101684426,1697101687532.0,120,563.0,10.0,"[42, 1648, 121, 91, 89, 87, 87, 646, 101, 99, 95]","[1697101684468, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687338, 1697101687437, 1697101687532]"
3559,3559,554,25,[],200,llama-13b,128,1,1547.0,1.0,1,H100,1697101656613,1697101658160.0,120,26.0,1.0,"[70, 1477]","[1697101656683, 1697101658160]"
3560,3560,912,26,[],200,llama-13b,128,1,5303.0,1.0,1,H100,1697101658161,1697101663464.0,120,92.0,20.0,"[36, 1176, 332, 101, 99, 97, 93, 85, 805, 101, 75, 99, 92, 88, 831, 107, 101, 95, 94, 89, 707]","[1697101658197, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464]"
3561,3561,377,31,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,13.0,1.0,"[567, 997]","[1697101676383, 1697101677380]"
3562,3562,179,30,[],200,llama-13b,128,1,1188.0,1.0,1,H100,1697101687533,1697101688721.0,120,161.0,4.0,"[7, 845, 133, 106, 97]","[1697101687540, 1697101688385, 1697101688518, 1697101688624, 1697101688721]"
3563,3563,736,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101677383,1697101689615.0,120,,,"[118, 746, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 104, 57, 73, 803, 84, 83, 79, 79, 771, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97, 92, 90, 90, 594, 85, 64, 82, 81, 911, 91, 89, 87, 86, 647, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 91, 89]","[1697101677501, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680675, 1697101680732, 1697101680805, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
3564,3564,66,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[85, 1194]","[1697101633775, 1697101634969]"
3565,3565,540,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688722,1697101689600.0,120,,,"[7, 841]","[1697101688729, 1697101689570]"
3566,3566,207,29,[],200,llama-13b,128,1,981.0,1.0,1,H100,1697101663465,1697101664446.0,120,10.0,1.0,"[13, 968]","[1697101663478, 1697101664446]"
3567,3567,512,18,[],200,llama-13b,128,1,1809.0,1.0,1,H100,1697101635000,1697101636809.0,120,11.0,1.0,"[532, 1277]","[1697101635532, 1697101636809]"
3568,3568,1,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688625,1697101689595.0,120,,,"[31, 915]","[1697101688656, 1697101689571]"
3569,3569,865,19,[],200,llama-13b,128,1,668.0,1.0,1,H100,1697101636811,1697101637479.0,120,9.0,1.0,"[93, 575]","[1697101636904, 1697101637479]"
3570,3570,290,20,[],200,llama-13b,128,1,1260.0,1.0,1,H100,1697101637480,1697101638740.0,120,14.0,1.0,"[42, 1218]","[1697101637522, 1697101638740]"
3571,3571,300,15,[],200,llama-13b,128,1,713.0,1.0,1,H100,1697101642418,1697101643131.0,120,9.0,1.0,"[12, 701]","[1697101642430, 1697101643131]"
3572,3572,656,16,[],200,llama-13b,128,1,1184.0,1.0,1,H100,1697101643136,1697101644320.0,120,26.0,1.0,"[15, 1169]","[1697101643151, 1697101644320]"
3573,3573,59,17,[],200,llama-13b,128,1,4921.0,1.0,1,H100,1697101644321,1697101649242.0,120,91.0,20.0,"[7, 1009, 103, 78, 78, 77, 75, 742, 97, 98, 83, 78, 1237, 95, 101, 101, 96, 88, 485, 96, 97]","[1697101644328, 1697101645337, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242]"
3574,3574,217,15,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628908,1697101633687.0,120,85.0,20.0,"[15, 1705, 106, 81, 68, 66, 864, 93, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 93, 70]","[1697101628923, 1697101630628, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687]"
3575,3575,416,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649243,1697101651603.0,120,,,"[24, 644, 355, 106, 104, 101, 91, 86]","[1697101649267, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650754]"
3576,3576,779,19,[],200,llama-13b,128,1,3442.0,1.0,1,H100,1697101651608,1697101655050.0,120,563.0,10.0,"[266, 787, 57, 892, 85, 84, 81, 81, 896, 108, 105]","[1697101651874, 1697101652661, 1697101652718, 1697101653610, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050]"
3577,3577,454,30,[],200,llama-13b,128,1,1429.0,1.0,1,H100,1697101664788,1697101666217.0,120,182.0,6.0,"[24, 927, 129, 106, 99, 72, 72]","[1697101664812, 1697101665739, 1697101665868, 1697101665974, 1697101666073, 1697101666145, 1697101666217]"
3578,3578,323,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625976,1697101628903.0,120,,,"[13, 885, 725]","[1697101625989, 1697101626874, 1697101627599]"
3579,3579,538,21,[],200,llama-13b,128,1,4641.0,1.0,1,H100,1697101643739,1697101648380.0,120,89.0,20.0,"[7, 574, 100, 87, 84, 83, 83, 682, 79, 78, 77, 75, 742, 97, 98, 83, 78, 1236, 96, 101, 101]","[1697101643746, 1697101644320, 1697101644420, 1697101644507, 1697101644591, 1697101644674, 1697101644757, 1697101645439, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648380]"
3580,3580,33,44,[],200,llama-13b,128,1,1799.0,1.0,1,H100,1697101709375,1697101711174.0,120,140.0,7.0,"[7, 1045, 277, 103, 97, 93, 89, 88]","[1697101709382, 1697101710427, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174]"
3581,3581,702,22,[],200,llama-13b,128,1,4208.0,1.0,1,H100,1697101640212,1697101644420.0,120,89.0,20.0,"[106, 684, 118, 84, 79, 78, 760, 100, 99, 97, 88, 82, 63, 801, 99, 96, 92, 96, 86, 85, 415]","[1697101640318, 1697101641002, 1697101641120, 1697101641204, 1697101641283, 1697101641361, 1697101642121, 1697101642221, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643550, 1697101643646, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644420]"
3582,3582,92,28,[],200,llama-13b,128,1,5002.0,1.0,1,H100,1697101651610,1697101656612.0,120,85.0,20.0,"[405, 1457, 138, 86, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 614, 105, 101, 98, 97, 83]","[1697101652015, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611]"
3583,3583,125,29,[],200,llama-13b,128,1,851.0,1.0,1,H100,1697101687534,1697101688385.0,120,13.0,1.0,"[12, 839]","[1697101687546, 1697101688385]"
3584,3584,422,28,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,26.0,1.0,"[517, 1047]","[1697101676333, 1697101677380]"
3585,3585,479,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688386,1697101689594.0,120,,,"[31, 1153]","[1697101688417, 1697101689570]"
3586,3586,834,31,[],200,llama-13b,128,1,5117.0,1.0,1,H100,1697101689615,1697101694732.0,120,85.0,20.0,"[189, 1308, 93, 1069, 213, 212, 78, 110, 93, 94, 405, 101, 100, 99, 95, 91, 87, 68, 433, 78, 100]","[1697101689804, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
3587,3587,731,23,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101644322,1697101649242.0,120,89.0,20.0,"[24, 991, 103, 79, 77, 77, 75, 741, 98, 98, 83, 78, 1237, 95, 101, 101, 96, 88, 485, 96, 97]","[1697101644346, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242]"
3588,3588,687,18,[],200,llama-13b,128,1,4128.0,1.0,1,H100,1697101640292,1697101644420.0,120,96.0,20.0,"[37, 673, 118, 84, 79, 78, 760, 100, 99, 97, 88, 82, 63, 801, 99, 96, 92, 96, 86, 85, 415]","[1697101640329, 1697101641002, 1697101641120, 1697101641204, 1697101641283, 1697101641361, 1697101642121, 1697101642221, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643550, 1697101643646, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644420]"
3589,3589,104,23,[],200,llama-13b,128,1,4821.0,1.0,1,H100,1697101644421,1697101649242.0,120,93.0,20.0,"[31, 988, 79, 77, 78, 75, 740, 98, 97, 84, 78, 1236, 96, 101, 100, 97, 88, 485, 96, 97]","[1697101644452, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645749, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242]"
3590,3590,37,29,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,20.0,1.0,"[580, 984]","[1697101676396, 1697101677380]"
3591,3591,603,14,[],200,llama-13b,128,1,880.0,1.0,1,H100,1697101632087,1697101632967.0,120,9.0,1.0,"[7, 872]","[1697101632094, 1697101632966]"
3592,3592,357,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101645674,1697101651605.0,120,,,"[7, 618, 191, 97, 98, 83, 79, 1235, 96, 101, 101, 96, 88, 486, 95, 96, 83, 942, 106, 104, 102, 90, 85]","[1697101645681, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646847, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649050, 1697101649145, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650578, 1697101650668, 1697101650753]"
3593,3593,70,27,[],200,llama-13b,128,1,1864.0,1.0,1,H100,1697101651610,1697101653474.0,120,39.0,1.0,"[394, 1468]","[1697101652004, 1697101653472]"
3594,3594,425,28,[],200,llama-13b,128,1,4810.0,1.0,1,H100,1697101653479,1697101658289.0,120,88.0,20.0,"[17, 1135, 206, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83, 623, 81, 79, 60, 835]","[1697101653496, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
3595,3595,207,19,[],200,llama-13b,128,1,916.0,1.0,1,H100,1697101644421,1697101645337.0,120,10.0,1.0,"[54, 862]","[1697101644475, 1697101645337]"
3596,3596,224,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672658,1697101675812.0,120,,,"[42, 1059, 129, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672700, 1697101673759, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3597,3597,190,35,[],200,llama-13b,128,1,3864.0,1.0,1,H100,1697101689615,1697101693479.0,120,335.0,10.0,"[213, 1284, 93, 1069, 213, 212, 79, 110, 92, 94, 405]","[1697101689828, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692888, 1697101692980, 1697101693074, 1697101693479]"
3598,3598,839,29,[],200,llama-13b,128,1,1596.0,1.0,1,H100,1697101684908,1697101686504.0,120,58.0,5.0,"[13, 1195, 121, 91, 89, 87]","[1697101684921, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504]"
3599,3599,566,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101645338,1697101651605.0,120,,,"[24, 937, 191, 97, 98, 83, 79, 1225, 106, 101, 101, 96, 88, 486, 96, 96, 82, 942, 106, 104, 101, 91, 85]","[1697101645362, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646847, 1697101648072, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
3600,3600,556,29,[],200,llama-13b,128,1,1690.0,1.0,1,H100,1697101684426,1697101686116.0,120,9.0,1.0,"[60, 1630]","[1697101684486, 1697101686116]"
3601,3601,551,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693480,1697101697588.0,120,,,"[6, 971, 97, 77, 101, 84, 604, 78, 77, 831, 103, 101, 111, 91, 91]","[1697101693486, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
3602,3602,583,27,[],200,llama-13b,128,1,4761.0,1.0,1,H100,1697101675816,1697101680577.0,120,96.0,20.0,"[574, 990, 97, 64, 51, 762, 90, 86, 64, 84, 80, 647, 99, 98, 94, 72, 92, 91, 69, 91, 463]","[1697101676390, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680111, 1697101680574]"
3603,3603,914,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697596,1697101700635.0,120,,,"[295, 1584, 113, 83, 82, 82, 80, 82]","[1697101697891, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
3604,3604,106,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697590,1697101700636.0,120,,,"[21, 684, 43, 1250, 83, 82, 81, 81, 82]","[1697101697611, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
3605,3605,689,27,[],200,llama-13b,128,1,955.0,1.0,1,H100,1697101673893,1697101674848.0,120,15.0,1.0,"[80, 875]","[1697101673973, 1697101674848]"
3606,3606,343,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[325, 1192, 123, 69]","[1697101700967, 1697101702159, 1697101702282, 1697101702351]"
3607,3607,122,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674849,1697101675810.0,120,,,"[42, 898]","[1697101674891, 1697101675789]"
3608,3608,803,35,[],200,llama-13b,128,1,1101.0,1.0,1,H100,1697101672658,1697101673759.0,120,20.0,1.0,"[7, 1094]","[1697101672665, 1697101673759]"
3609,3609,673,39,[],200,llama-13b,128,1,5352.0,1.0,1,H100,1697101703174,1697101708526.0,120,93.0,20.0,"[144, 1697, 98, 72, 728, 95, 87, 85, 83, 79, 943, 97, 92, 70, 91, 70, 91, 87, 544, 99]","[1697101703318, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526]"
3610,3610,231,36,[],200,llama-13b,128,1,1088.0,1.0,1,H100,1697101673760,1697101674848.0,120,13.0,1.0,"[36, 1052]","[1697101673796, 1697101674848]"
3611,3611,94,17,[],200,llama-13b,128,1,5137.0,1.0,1,H100,1697101645441,1697101650578.0,120,86.0,20.0,"[6, 852, 191, 97, 98, 83, 79, 1235, 96, 101, 101, 96, 88, 486, 96, 95, 83, 942, 106, 104, 102]","[1697101645447, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646847, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649050, 1697101649146, 1697101649241, 1697101649324, 1697101650266, 1697101650372, 1697101650476, 1697101650578]"
3612,3612,560,30,[],200,llama-13b,128,1,3061.0,1.0,1,H100,1697101664447,1697101667508.0,120,161.0,13.0,"[24, 1397, 106, 99, 72, 72, 93, 92, 733, 104, 98, 73, 98]","[1697101664471, 1697101665868, 1697101665974, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508]"
3613,3613,585,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674849,1697101675810.0,120,,,"[36, 904]","[1697101674885, 1697101675789]"
3614,3614,731,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101650477,1697101651604.0,120,,,"[6, 825]","[1697101650483, 1697101651308]"
3615,3615,98,40,[],200,llama-13b,128,1,1894.0,1.0,1,H100,1697101708534,1697101710428.0,120,14.0,1.0,"[165, 1728]","[1697101708699, 1697101710427]"
3616,3616,452,41,[],200,llama-13b,128,1,1519.0,1.0,1,H100,1697101710429,1697101711948.0,120,216.0,4.0,"[36, 1195, 98, 99, 90]","[1697101710465, 1697101711660, 1697101711758, 1697101711857, 1697101711947]"
3617,3617,132,24,[],200,llama-13b,128,1,4919.0,1.0,1,H100,1697101651610,1697101656529.0,120,100.0,20.0,"[164, 886, 58, 891, 87, 82, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 98]","[1697101651774, 1697101652660, 1697101652718, 1697101653609, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656529]"
3618,3618,915,19,[],200,llama-13b,128,1,963.0,1.0,1,H100,1697101648948,1697101649911.0,120,182.0,1.0,"[31, 932]","[1697101648979, 1697101649911]"
3619,3619,918,31,[],200,llama-13b,128,1,753.0,1.0,1,H100,1697101667510,1697101668263.0,120,23.0,1.0,"[18, 735]","[1697101667528, 1697101668263]"
3620,3620,323,32,[],200,llama-13b,128,1,4393.0,1.0,1,H100,1697101668264,1697101672657.0,120,84.0,20.0,"[18, 1162, 125, 98, 94, 93, 93, 88, 676, 99, 70, 93, 90, 90, 513, 97, 95, 94, 93, 91, 521]","[1697101668282, 1697101669444, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672657]"
3621,3621,343,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649912,1697101651604.0,120,,,"[7, 1389]","[1697101649919, 1697101651308]"
3622,3622,19,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101675814,1697101689595.0,120,,,"[182, 1384, 97, 64, 50, 763, 90, 86, 65, 83, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461, 103, 57, 73, 803, 85, 82, 80, 78, 771, 97, 93, 91, 69, 91, 89, 615, 102, 101, 97, 91, 91, 90, 593, 86, 64, 82, 81, 911, 91, 88, 88, 86, 647, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 91, 89]","[1697101675996, 1697101677380, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681607, 1697101681692, 1697101681774, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683233, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684239, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686416, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
3623,3623,903,32,[],200,llama-13b,128,1,3364.0,1.0,1,H100,1697101689617,1697101692981.0,120,244.0,7.0,"[362, 2050, 244, 214, 212, 78, 111, 92]","[1697101689979, 1697101692029, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692888, 1697101692980]"
3624,3624,702,21,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101651608,1697101656528.0,120,89.0,20.0,"[20, 1033, 57, 891, 86, 83, 82, 81, 896, 107, 106, 101, 74, 99, 97, 92, 613, 106, 100, 99, 97]","[1697101651628, 1697101652661, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
3625,3625,302,25,[],200,llama-13b,128,1,4706.0,1.0,1,H100,1697101661267,1697101665973.0,120,85.0,20.0,"[19, 630, 356, 106, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 109, 102, 96, 94, 101, 688, 105]","[1697101661286, 1697101661916, 1697101662272, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973]"
3626,3626,896,23,[],200,llama-13b,128,1,616.0,1.0,1,H100,1697101656530,1697101657146.0,120,15.0,1.0,"[48, 568]","[1697101656578, 1697101657146]"
3627,3627,332,33,[],200,llama-13b,128,1,390.0,1.0,1,H100,1697101692982,1697101693372.0,120,39.0,1.0,"[12, 378]","[1697101692994, 1697101693372]"
3628,3628,412,24,[],200,llama-13b,128,1,2755.0,1.0,1,H100,1697101657150,1697101659905.0,120,244.0,9.0,"[18, 992, 129, 96, 82, 82, 81, 1075, 101, 99]","[1697101657168, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658630, 1697101659705, 1697101659806, 1697101659905]"
3629,3629,682,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101628909,1697101634994.0,120,,,"[20, 1699, 106, 81, 68, 66, 864, 93, 90, 90, 89, 87, 84, 83, 742, 95, 72, 93, 93, 93, 70, 614, 75, 71]","[1697101628929, 1697101630628, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631996, 1697101632086, 1697101632175, 1697101632262, 1697101632346, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633431, 1697101633524, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
3630,3630,182,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689615,1697101697587.0,120,,,"[186, 1311, 93, 1069, 213, 212, 78, 110, 93, 94, 405, 101, 100, 99, 95, 91, 87, 68, 433, 78, 100, 84, 605, 78, 77, 831, 103, 100, 111, 92, 91]","[1697101689801, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694815, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
3631,3631,252,17,[],200,llama-13b,128,1,4858.0,1.0,1,H100,1697101639147,1697101644005.0,120,182.0,22.0,"[7, 932, 123, 83, 92, 735, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 800, 100, 95, 93, 95, 86, 86]","[1697101639154, 1697101640086, 1697101640209, 1697101640292, 1697101640384, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644005]"
3632,3632,543,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697601,1697101700636.0,120,,,"[487, 1387, 113, 83, 82, 81, 81, 82]","[1697101698088, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
3633,3633,500,34,[],200,llama-13b,128,1,3245.0,1.0,1,H100,1697101680806,1697101684051.0,120,335.0,11.0,"[36, 1669, 193, 96, 93, 92, 69, 90, 90, 615, 101, 101]","[1697101680842, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051]"
3634,3634,725,18,[],200,llama-13b,128,1,4919.0,1.0,1,H100,1697101651610,1697101656529.0,120,90.0,20.0,"[258, 793, 57, 891, 86, 84, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98]","[1697101651868, 1697101652661, 1697101652718, 1697101653609, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529]"
3635,3635,232,27,[],200,llama-13b,128,1,4730.0,1.0,1,H100,1697101656530,1697101661260.0,120,93.0,20.0,"[30, 674, 81, 79, 60, 835, 95, 83, 81, 81, 1076, 102, 98, 97, 93, 85, 805, 101, 76, 98]","[1697101656560, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260]"
3636,3636,250,24,[],200,llama-13b,128,1,668.0,1.0,1,H100,1697101649243,1697101649911.0,120,31.0,1.0,"[36, 632]","[1697101649279, 1697101649911]"
3637,3637,156,19,[],200,llama-13b,128,1,4733.0,1.0,1,H100,1697101656530,1697101661263.0,120,86.0,20.0,"[54, 562, 88, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 102, 98, 97, 93, 85, 805, 101, 76, 98]","[1697101656584, 1697101657146, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260]"
3638,3638,613,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649913,1697101651604.0,120,,,"[24, 1371]","[1697101649937, 1697101651308]"
3639,3639,41,26,[],200,llama-13b,128,1,10768.0,1.0,1,H100,1697101651610,1697101662378.0,120,39.0,43.0,"[376, 1486, 138, 85, 83, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 97, 83, 623, 81, 79, 59, 836, 95, 83, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107]","[1697101651986, 1697101653472, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658384, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378]"
3640,3640,536,20,[],200,llama-13b,128,1,5209.0,1.0,1,H100,1697101635000,1697101640209.0,120,83.0,20.0,"[510, 1299, 88, 87, 56, 628, 91, 92, 87, 87, 81, 82, 767, 98, 94, 91, 93, 92, 90, 79, 617]","[1697101635510, 1697101636809, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639147, 1697101639238, 1697101639331, 1697101639423, 1697101639513, 1697101639592, 1697101640209]"
3641,3641,593,28,[],200,llama-13b,128,1,2298.0,1.0,1,H100,1697101661267,1697101663565.0,120,335.0,9.0,"[19, 630, 356, 106, 101, 95, 94, 90, 706, 101]","[1697101661286, 1697101661916, 1697101662272, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662758, 1697101663464, 1697101663565]"
3642,3642,450,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684426,1697101689611.0,120,,,"[66, 1624, 121, 91, 89, 87, 87, 646, 101, 99, 95, 92, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101684492, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3643,3643,850,35,[],200,llama-13b,128,1,4572.0,1.0,1,H100,1697101684052,1697101688624.0,120,109.0,20.0,"[12, 843, 107, 86, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106]","[1697101684064, 1697101684907, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
3644,3644,510,20,[],200,llama-13b,128,1,1005.0,1.0,1,H100,1697101661267,1697101662272.0,120,79.0,2.0,"[8, 641, 356]","[1697101661275, 1697101661916, 1697101662272]"
3645,3645,863,21,[],200,llama-13b,128,1,1064.0,1.0,1,H100,1697101662273,1697101663337.0,120,10.0,1.0,"[6, 1058]","[1697101662279, 1697101663337]"
3646,3646,264,22,[],200,llama-13b,128,1,4265.0,1.0,1,H100,1697101663338,1697101667603.0,120,86.0,20.0,"[30, 1077, 233, 109, 102, 96, 94, 97, 692, 105, 101, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95]","[1697101663368, 1697101664445, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665176, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603]"
3647,3647,139,27,[],200,llama-13b,128,1,5404.0,1.0,1,H100,1697101658161,1697101663565.0,120,39.0,21.0,"[7, 1205, 332, 101, 99, 97, 92, 86, 805, 101, 76, 98, 93, 87, 831, 107, 101, 95, 94, 89, 707, 101]","[1697101658168, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565]"
3648,3648,279,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688625,1697101689595.0,120,,,"[19, 927]","[1697101688644, 1697101689571]"
3649,3649,638,37,[],200,llama-13b,128,1,5198.0,1.0,1,H100,1697101689618,1697101694816.0,120,88.0,20.0,"[461, 1951, 244, 213, 212, 78, 101, 103, 94, 403, 102, 100, 99, 94, 91, 88, 68, 434, 77, 100, 85]","[1697101690079, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
3650,3650,625,23,[],200,llama-13b,128,1,1964.0,1.0,1,H100,1697101667605,1697101669569.0,120,364.0,2.0,"[30, 1809, 124]","[1697101667635, 1697101669444, 1697101669568]"
3651,3651,577,22,[],200,llama-13b,128,1,4365.0,1.0,1,H100,1697101650579,1697101654944.0,120,93.0,9.0,"[42, 2097, 891, 86, 83, 82, 81, 896, 107]","[1697101650621, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944]"
3652,3652,2,23,[],200,llama-13b,128,1,1583.0,1.0,1,H100,1697101654946,1697101656529.0,120,58.0,6.0,"[6, 1046, 129, 105, 101, 98, 98]","[1697101654952, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529]"
3653,3653,357,24,[],200,llama-13b,128,1,7303.0,1.0,1,H100,1697101656530,1697101663833.0,120,52.0,33.0,"[30, 586, 88, 81, 79, 60, 835, 95, 83, 82, 80, 1076, 102, 98, 97, 93, 85, 805, 101, 76, 98, 93, 87, 831, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87]","[1697101656560, 1697101657146, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833]"
3654,3654,44,38,[],200,llama-13b,128,1,1365.0,1.0,1,H100,1697101694818,1697101696183.0,120,12.0,1.0,"[153, 1212]","[1697101694971, 1697101696183]"
3655,3655,401,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696184,1697101697589.0,120,,,"[25, 1186]","[1697101696209, 1697101697395]"
3656,3656,761,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697598,1697101700636.0,120,,,"[418, 1459, 113, 83, 82, 81, 81, 82]","[1697101698016, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
3657,3657,272,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101686505,1697101689594.0,120,,,"[6, 512, 214, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101686511, 1697101687023, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3658,3658,681,34,[],200,llama-13b,128,1,955.0,1.0,1,H100,1697101673893,1697101674848.0,120,23.0,1.0,"[48, 907]","[1697101673941, 1697101674848]"
3659,3659,117,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674849,1697101675810.0,120,,,"[12, 928]","[1697101674861, 1697101675789]"
3660,3660,568,23,[],200,llama-13b,128,1,1547.0,1.0,1,H100,1697101656613,1697101658160.0,120,11.0,1.0,"[19, 1528]","[1697101656632, 1697101658160]"
3661,3661,475,36,[],200,llama-13b,128,1,4757.0,1.0,1,H100,1697101675814,1697101680571.0,120,89.0,20.0,"[185, 1380, 98, 64, 50, 764, 89, 86, 65, 83, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101675999, 1697101677379, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
3662,3662,896,24,[],200,llama-13b,128,1,1212.0,1.0,1,H100,1697101658161,1697101659373.0,120,15.0,1.0,"[25, 1187]","[1697101658186, 1697101659373]"
3663,3663,332,25,[],200,llama-13b,128,1,1272.0,1.0,1,H100,1697101659375,1697101660647.0,120,39.0,1.0,"[30, 1242]","[1697101659405, 1697101660647]"
3664,3664,490,14,[],200,llama-13b,128,1,2170.0,1.0,1,H100,1697101651609,1697101653779.0,120,11.0,5.0,"[195, 914, 891, 87, 82]","[1697101651804, 1697101652718, 1697101653609, 1697101653696, 1697101653778]"
3665,3665,691,26,[],200,llama-13b,128,1,1268.0,1.0,1,H100,1697101660648,1697101661916.0,120,47.0,1.0,"[30, 1238]","[1697101660678, 1697101661916]"
3666,3666,6,15,[],200,llama-13b,128,1,6316.0,1.0,1,H100,1697101653779,1697101660095.0,120,100.0,29.0,"[7, 1051, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 83, 622, 81, 79, 59, 836, 96, 82, 82, 80, 1076, 101, 99, 97, 93]","[1697101653786, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095]"
3667,3667,626,31,[],200,llama-13b,128,1,1501.0,1.0,1,H100,1697101689611,1697101691112.0,120,10.0,1.0,"[65, 1436]","[1697101689676, 1697101691112]"
3668,3668,27,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101691113,1697101697588.0,120,,,"[19, 898, 244, 213, 212, 79, 111, 92, 94, 403, 102, 100, 99, 95, 91, 87, 68, 433, 78, 100, 85, 604, 79, 76, 831, 103, 101, 110, 92, 91]","[1697101691132, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692889, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
3669,3669,493,25,[],200,llama-13b,128,1,4730.0,1.0,1,H100,1697101656530,1697101661260.0,120,83.0,20.0,"[36, 580, 88, 81, 79, 60, 835, 95, 83, 82, 80, 1076, 102, 98, 97, 93, 85, 805, 101, 76, 98]","[1697101656566, 1697101657146, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260]"
3670,3670,611,28,[],200,llama-13b,128,1,946.0,1.0,1,H100,1697101668498,1697101669444.0,120,14.0,1.0,"[12, 934]","[1697101668510, 1697101669444]"
3671,3671,574,24,[],200,llama-13b,128,1,704.0,1.0,1,H100,1697101656530,1697101657234.0,120,364.0,2.0,"[24, 592, 88]","[1697101656554, 1697101657146, 1697101657234]"
3672,3672,89,25,[],200,llama-13b,128,1,5143.0,1.0,1,H100,1697101657235,1697101662378.0,120,52.0,20.0,"[12, 913, 129, 96, 82, 82, 81, 1075, 101, 99, 97, 92, 86, 805, 102, 75, 104, 87, 87, 831, 107]","[1697101657247, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658630, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378]"
3673,3673,684,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672658,1697101675811.0,120,,,"[13, 1088, 129, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672671, 1697101673759, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3674,3674,449,26,[],200,llama-13b,128,1,4757.0,1.0,1,H100,1697101662379,1697101667136.0,120,86.0,20.0,"[31, 927, 127, 101, 93, 88, 87, 845, 108, 103, 96, 94, 100, 689, 105, 101, 71, 72, 93, 92, 734]","[1697101662410, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136]"
3675,3675,112,34,[],200,llama-13b,128,1,1664.0,1.0,1,H100,1697101675813,1697101677477.0,120,16.0,2.0,"[288, 1279, 97]","[1697101676101, 1697101677380, 1697101677477]"
3676,3676,471,35,[],200,llama-13b,128,1,3327.0,1.0,1,H100,1697101677478,1697101680805.0,120,86.0,20.0,"[35, 734, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 103, 57, 74]","[1697101677513, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680805]"
3677,3677,215,31,[],200,llama-13b,128,1,878.0,1.0,1,H100,1697101679602,1697101680480.0,120,12.0,1.0,"[7, 871]","[1697101679609, 1697101680480]"
3678,3678,574,32,[],200,llama-13b,128,1,1127.0,1.0,1,H100,1697101680481,1697101681608.0,120,364.0,2.0,"[6, 1015, 106]","[1697101680487, 1697101681502, 1697101681608]"
3679,3679,3,33,[],200,llama-13b,128,1,3717.0,1.0,1,H100,1697101681609,1697101685326.0,120,89.0,20.0,"[12, 890, 193, 97, 92, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90, 593, 86, 64, 82, 81]","[1697101681621, 1697101682511, 1697101682704, 1697101682801, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326]"
3680,3680,854,26,[],200,llama-13b,128,1,6142.0,1.0,1,H100,1697101661268,1697101667410.0,120,67.0,29.0,"[24, 624, 356, 106, 101, 95, 94, 90, 706, 101, 93, 88, 87, 845, 109, 103, 95, 94, 101, 687, 106, 100, 72, 72, 93, 92, 734, 103, 98, 73]","[1697101661292, 1697101661916, 1697101662272, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662758, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787, 1697101664890, 1697101664985, 1697101665079, 1697101665180, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410]"
3681,3681,649,21,[],200,llama-13b,128,1,5092.0,1.0,1,H100,1697101638741,1697101643833.0,120,244.0,20.0,"[30, 1315, 123, 83, 92, 735, 85, 79, 77, 761, 101, 99, 95, 89, 82, 63, 800, 100, 95, 93, 95]","[1697101638771, 1697101640086, 1697101640209, 1697101640292, 1697101640384, 1697101641119, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833]"
3682,3682,822,36,[],200,llama-13b,128,1,4521.0,1.0,1,H100,1697101680806,1697101685327.0,120,88.0,20.0,"[77, 1628, 193, 96, 93, 92, 69, 90, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 81]","[1697101680883, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327]"
3683,3683,223,37,[],200,llama-13b,128,1,1693.0,1.0,1,H100,1697101685329,1697101687022.0,120,16.0,1.0,"[6, 1687]","[1697101685335, 1697101687022]"
3684,3684,581,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687023,1697101689594.0,120,,,"[7, 1355, 133, 106, 97, 92, 92, 89]","[1697101687030, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3685,3685,285,27,[],200,llama-13b,128,1,5359.0,1.0,1,H100,1697101667411,1697101672770.0,120,100.0,27.0,"[6, 846, 133, 101, 95, 71, 92, 89, 724, 99, 94, 93, 92, 89, 676, 99, 71, 92, 91, 89, 513, 97, 95, 94, 93, 90, 521, 114]","[1697101667417, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668755, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672770]"
3686,3686,13,39,[],200,llama-13b,128,1,5120.0,1.0,1,H100,1697101689611,1697101694731.0,120,90.0,20.0,"[71, 1430, 93, 1069, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100]","[1697101689682, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
3687,3687,313,29,[],200,llama-13b,128,1,869.0,1.0,1,H100,1697101685247,1697101686116.0,120,20.0,1.0,"[6, 863]","[1697101685253, 1697101686116]"
3688,3688,759,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101686117,1697101689594.0,120,,,"[30, 876, 214, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 91, 90]","[1697101686147, 1697101687023, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688994]"
3689,3689,357,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685328,1697101689607.0,120,,,"[25, 1669, 215, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 92, 88]","[1697101685353, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688993]"
3690,3690,227,26,[],200,llama-13b,128,1,5917.0,1.0,1,H100,1697101666218,1697101672135.0,120,364.0,25.0,"[18, 2027, 133, 101, 95, 71, 91, 90, 725, 98, 95, 92, 93, 88, 676, 99, 71, 92, 90, 90, 513, 97, 95, 94, 93, 90]","[1697101666236, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135]"
3691,3691,375,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694733,1697101697587.0,120,,,"[54, 634, 78, 76, 831, 103, 101, 110, 92, 91]","[1697101694787, 1697101695421, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
3692,3692,188,31,[],200,llama-13b,128,1,5119.0,1.0,1,H100,1697101689612,1697101694731.0,120,85.0,20.0,"[83, 1417, 93, 1069, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 95, 90, 88, 68, 433, 78, 100]","[1697101689695, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
3693,3693,795,33,[],200,llama-13b,128,1,1365.0,1.0,1,H100,1697101694818,1697101696183.0,120,12.0,1.0,"[18, 1347]","[1697101694836, 1697101696183]"
3694,3694,224,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696184,1697101697589.0,120,,,"[13, 1198]","[1697101696197, 1697101697395]"
3695,3695,165,33,[],200,llama-13b,128,1,5116.0,1.0,1,H100,1697101689616,1697101694732.0,120,83.0,20.0,"[283, 1213, 93, 1069, 213, 212, 79, 109, 93, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101]","[1697101689899, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692887, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732]"
3696,3696,585,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697598,1697101700636.0,120,,,"[501, 1376, 113, 83, 82, 81, 81, 82]","[1697101698099, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
3697,3697,684,35,[],200,llama-13b,128,1,5197.0,1.0,1,H100,1697101689619,1697101694816.0,120,100.0,20.0,"[563, 1848, 244, 213, 212, 78, 110, 94, 94, 403, 102, 100, 99, 94, 92, 87, 68, 434, 77, 100, 85]","[1697101690182, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693965, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
3698,3698,574,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101633690,1697101634996.0,120,,,"[193, 1086]","[1697101633883, 1697101634969]"
3699,3699,12,36,[],200,llama-13b,128,1,1517.0,1.0,1,H100,1697101700643,1697101702160.0,120,11.0,1.0,"[378, 1139]","[1697101701021, 1697101702160]"
3700,3700,112,36,[],200,llama-13b,128,1,1589.0,1.0,1,H100,1697101694818,1697101696407.0,120,16.0,2.0,"[94, 1271, 223]","[1697101694912, 1697101696183, 1697101696406]"
3701,3701,471,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696407,1697101697589.0,120,,,"[13, 976]","[1697101696420, 1697101697396]"
3702,3702,836,38,[],200,llama-13b,128,1,1879.0,1.0,1,H100,1697101697596,1697101699475.0,120,11.0,1.0,"[217, 1662]","[1697101697813, 1697101699475]"
3703,3703,352,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700635.0,120,,,[60],[1697101699537]
3704,3704,448,27,[],200,llama-13b,128,1,2809.0,1.0,1,H100,1697101667137,1697101669946.0,120,335.0,12.0,"[7, 1119, 133, 101, 95, 71, 91, 90, 724, 99, 95, 92, 92]","[1697101667144, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669762, 1697101669854, 1697101669946]"
3705,3705,711,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[446, 1072, 122, 69]","[1697101701088, 1697101702160, 1697101702282, 1697101702351]"
3706,3706,546,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694733,1697101697587.0,120,,,"[19, 668, 78, 77, 831, 103, 100, 111, 92, 91]","[1697101694752, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
3707,3707,136,41,[],200,llama-13b,128,1,1726.0,1.0,1,H100,1697101703173,1697101704899.0,120,31.0,1.0,"[404, 1322]","[1697101703577, 1697101704899]"
3708,3708,3,17,[],200,llama-13b,128,1,5210.0,1.0,1,H100,1697101635000,1697101640210.0,120,89.0,20.0,"[582, 1227, 89, 86, 56, 628, 91, 92, 87, 87, 82, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618]","[1697101635582, 1697101636809, 1697101636898, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210]"
3709,3709,490,42,[],200,llama-13b,128,1,1279.0,1.0,1,H100,1697101704901,1697101706180.0,120,11.0,5.0,"[36, 976, 95, 87, 85]","[1697101704937, 1697101705913, 1697101706008, 1697101706095, 1697101706180]"
3710,3710,723,30,[],200,llama-13b,128,1,567.0,1.0,1,H100,1697101675813,1697101676380.0,120,14.0,1.0,"[91, 476]","[1697101675904, 1697101676380]"
3711,3711,154,31,[],200,llama-13b,128,1,1866.0,1.0,1,H100,1697101676381,1697101678247.0,120,13.0,1.0,"[88, 1778]","[1697101676469, 1697101678247]"
3712,3712,598,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101650578,1697101651604.0,120,,,"[7, 723]","[1697101650585, 1697101651308]"
3713,3713,333,26,[],200,llama-13b,128,1,2868.0,1.0,1,H100,1697101659706,1697101662574.0,120,563.0,11.0,"[7, 934, 338, 102, 75, 98, 93, 87, 831, 107, 101, 95]","[1697101659713, 1697101660647, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574]"
3714,3714,26,22,[],200,llama-13b,128,1,1868.0,1.0,1,H100,1697101651610,1697101653478.0,120,18.0,1.0,"[477, 1390]","[1697101652087, 1697101653477]"
3715,3715,380,23,[],200,llama-13b,128,1,11409.0,1.0,1,H100,1697101653480,1697101664889.0,120,216.0,50.0,"[97, 1260, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 82, 623, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 101, 75, 99, 93, 86, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 108, 103]","[1697101653577, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661353, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889]"
3716,3716,807,28,[],200,llama-13b,128,1,3942.0,1.0,1,H100,1697101669947,1697101673889.0,120,90.0,20.0,"[7, 531, 226, 99, 71, 92, 91, 90, 512, 97, 95, 94, 93, 90, 521, 114, 91, 68, 68, 87, 805]","[1697101669954, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673084, 1697101673889]"
3717,3717,362,18,[],200,llama-13b,128,1,790.0,1.0,1,H100,1697101640212,1697101641002.0,120,14.0,1.0,"[76, 714]","[1697101640288, 1697101641002]"
3718,3718,807,19,[],200,llama-13b,128,1,3753.0,1.0,1,H100,1697101641003,1697101644756.0,120,90.0,20.0,"[25, 789, 305, 100, 98, 97, 88, 82, 63, 800, 100, 95, 93, 95, 87, 85, 415, 86, 84, 84, 82]","[1697101641028, 1697101641817, 1697101642122, 1697101642222, 1697101642320, 1697101642417, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420, 1697101644506, 1697101644590, 1697101644674, 1697101644756]"
3719,3719,414,34,[],200,llama-13b,128,1,4758.0,1.0,1,H100,1697101675813,1697101680571.0,120,87.0,20.0,"[356, 1211, 97, 64, 50, 764, 89, 86, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101676169, 1697101677380, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
3720,3720,386,23,[],200,llama-13b,128,1,1420.0,1.0,1,H100,1697101663659,1697101665079.0,120,140.0,6.0,"[6, 780, 233, 109, 102, 96, 94]","[1697101663665, 1697101664445, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079]"
3721,3721,710,24,[],200,llama-13b,128,1,659.0,1.0,1,H100,1697101665080,1697101665739.0,120,14.0,1.0,"[13, 646]","[1697101665093, 1697101665739]"
3722,3722,135,25,[],200,llama-13b,128,1,1396.0,1.0,1,H100,1697101665740,1697101667136.0,120,52.0,2.0,"[24, 1006, 366]","[1697101665764, 1697101666770, 1697101667136]"
3723,3723,495,26,[],200,llama-13b,128,1,1126.0,1.0,1,H100,1697101667137,1697101668263.0,120,13.0,1.0,"[25, 1101]","[1697101667162, 1697101668263]"
3724,3724,852,27,[],200,llama-13b,128,1,4393.0,1.0,1,H100,1697101668264,1697101672657.0,120,100.0,20.0,"[12, 1168, 125, 98, 94, 93, 93, 88, 676, 98, 71, 93, 90, 90, 513, 97, 95, 94, 93, 90, 522]","[1697101668276, 1697101669444, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657]"
3725,3725,395,30,[],200,llama-13b,128,1,3423.0,1.0,1,H100,1697101677382,1697101680805.0,120,88.0,20.0,"[96, 769, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 104, 57, 73]","[1697101677478, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680675, 1697101680732, 1697101680805]"
3726,3726,288,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672658,1697101675811.0,120,,,"[54, 1176, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672712, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3727,3727,421,19,[],200,llama-13b,128,1,4297.0,1.0,1,H100,1697101675813,1697101680110.0,120,85.0,20.0,"[55, 512, 36, 1061, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98, 95, 71, 92, 91, 70, 90]","[1697101675868, 1697101676380, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679950, 1697101680020, 1697101680110]"
3728,3728,367,37,[],200,llama-13b,128,1,3177.0,1.0,1,H100,1697101689600,1697101692777.0,120,92.0,6.0,"[23, 1489, 93, 1069, 213, 212, 78]","[1697101689623, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777]"
3729,3729,618,29,[],200,llama-13b,128,1,1565.0,1.0,1,H100,1697101675815,1697101677380.0,120,9.0,1.0,"[372, 1193]","[1697101676187, 1697101677380]"
3730,3730,47,30,[],200,llama-13b,128,1,3423.0,1.0,1,H100,1697101677382,1697101680805.0,120,90.0,20.0,"[101, 764, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 103, 58, 73]","[1697101677483, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680732, 1697101680805]"
3731,3731,724,38,[],200,llama-13b,128,1,593.0,1.0,1,H100,1697101692779,1697101693372.0,120,11.0,1.0,"[18, 575]","[1697101692797, 1697101693372]"
3732,3732,155,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693373,1697101697588.0,120,,,"[7, 1077, 97, 77, 101, 84, 604, 78, 77, 831, 103, 101, 111, 91, 91]","[1697101693380, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
3733,3733,401,31,[],200,llama-13b,128,1,4521.0,1.0,1,H100,1697101680806,1697101685327.0,120,84.0,20.0,"[31, 1674, 193, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 81]","[1697101680837, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327]"
3734,3734,660,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[17, 1348, 223, 103, 101, 110, 92, 91]","[1697101694835, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
3735,3735,775,20,[],200,llama-13b,128,1,1390.0,1.0,1,H100,1697101680112,1697101681502.0,120,17.0,1.0,"[25, 1365]","[1697101680137, 1697101681502]"
3736,3736,512,40,[],200,llama-13b,128,1,700.0,1.0,1,H100,1697101697595,1697101698295.0,120,11.0,1.0,"[130, 570]","[1697101697725, 1697101698295]"
3737,3737,757,32,[],200,llama-13b,128,1,1694.0,1.0,1,H100,1697101685329,1697101687023.0,120,20.0,1.0,"[42, 1651]","[1697101685371, 1697101687022]"
3738,3738,185,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687024,1697101689594.0,120,,,"[29, 1465, 106, 97, 92, 92, 89]","[1697101687053, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3739,3739,178,37,[],200,llama-13b,128,1,700.0,1.0,1,H100,1697101697595,1697101698295.0,120,11.0,1.0,"[124, 576]","[1697101697719, 1697101698295]"
3740,3740,535,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101698296,1697101700636.0,120,,,"[24, 1156, 112, 84, 81, 82, 80, 82]","[1697101698320, 1697101699476, 1697101699588, 1697101699672, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
3741,3741,841,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101698296,1697101700637.0,120,,,"[30, 1262, 84, 81, 82, 80, 82]","[1697101698326, 1697101699588, 1697101699672, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
3742,3742,864,21,[],200,llama-13b,128,1,4208.0,1.0,1,H100,1697101640212,1697101644420.0,120,83.0,20.0,"[106, 684, 118, 84, 79, 78, 760, 100, 99, 96, 89, 82, 63, 801, 98, 97, 92, 96, 86, 85, 414]","[1697101640318, 1697101641002, 1697101641120, 1697101641204, 1697101641283, 1697101641361, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643549, 1697101643646, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644419]"
3743,3743,271,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[452, 1940]","[1697101701094, 1697101703034]"
3744,3744,892,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[275, 1242, 123, 69]","[1697101700917, 1697101702159, 1697101702282, 1697101702351]"
3745,3745,16,28,[],200,llama-13b,128,1,1762.0,1.0,1,H100,1697101673086,1697101674848.0,120,9.0,1.0,"[12, 1750]","[1697101673098, 1697101674848]"
3746,3746,370,29,[],200,llama-13b,128,1,940.0,1.0,1,H100,1697101674849,1697101675789.0,120,31.0,1.0,"[6, 934]","[1697101674855, 1697101675789]"
3747,3747,628,43,[],200,llama-13b,128,1,3167.0,1.0,1,H100,1697101703175,1697101706342.0,120,732.0,10.0,"[7, 1717, 115, 99, 72, 728, 95, 87, 85, 83, 79]","[1697101703182, 1697101704899, 1697101705014, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342]"
3748,3748,631,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689613,1697101697587.0,120,,,"[100, 1492, 1069, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 92, 87, 68, 433, 78, 100, 84, 605, 79, 75, 832, 103, 100, 111, 92, 91]","[1697101689713, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694815, 1697101695420, 1697101695499, 1697101695574, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
3749,3749,296,22,[],200,llama-13b,128,1,916.0,1.0,1,H100,1697101644421,1697101645337.0,120,6.0,1.0,"[54, 862]","[1697101644475, 1697101645337]"
3750,3750,729,30,[],200,llama-13b,128,1,626.0,1.0,1,H100,1697101675790,1697101676416.0,120,874.0,2.0,"[6, 620]","[1697101675796, 1697101676416]"
3751,3751,322,40,[],200,llama-13b,128,1,5351.0,1.0,1,H100,1697101703176,1697101708527.0,120,93.0,20.0,"[228, 1604, 105, 72, 728, 95, 87, 85, 83, 80, 942, 97, 92, 70, 91, 70, 91, 87, 545, 98]","[1697101703404, 1697101705008, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
3752,3752,919,30,[],200,llama-13b,128,1,906.0,1.0,1,H100,1697101686117,1697101687023.0,120,14.0,1.0,"[24, 882]","[1697101686141, 1697101687023]"
3753,3753,57,44,[],200,llama-13b,128,1,1969.0,1.0,1,H100,1697101706344,1697101708313.0,120,13.0,1.0,"[6, 1963]","[1697101706350, 1697101708313]"
3754,3754,321,31,[],200,llama-13b,128,1,1697.0,1.0,1,H100,1697101687024,1697101688721.0,120,182.0,4.0,"[35, 1326, 133, 106, 97]","[1697101687059, 1697101688385, 1697101688518, 1697101688624, 1697101688721]"
3755,3755,50,50,[],200,llama-13b,128,1,1413.0,1.0,1,H100,1697101709492,1697101710905.0,120,90.0,4.0,"[36, 899, 278, 103, 97]","[1697101709528, 1697101710427, 1697101710705, 1697101710808, 1697101710905]"
3756,3756,407,51,[],200,llama-13b,128,1,754.0,1.0,1,H100,1697101710906,1697101711660.0,120,16.0,1.0,"[12, 742]","[1697101710918, 1697101711660]"
3757,3757,679,32,[],200,llama-13b,128,1,848.0,1.0,1,H100,1697101688723,1697101689571.0,120,15.0,1.0,"[12, 836]","[1697101688735, 1697101689571]"
3758,3758,105,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689572,1697101697587.0,120,,,"[18, 386, 1228, 1069, 214, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100, 84, 605, 78, 77, 831, 103, 100, 111, 92, 91]","[1697101689590, 1697101689976, 1697101691204, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694815, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
3759,3759,542,30,[],200,llama-13b,128,1,946.0,1.0,1,H100,1697101688625,1697101689571.0,120,11.0,1.0,"[25, 921]","[1697101688650, 1697101689571]"
3760,3760,805,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101680581,1697101689605.0,120,,,"[19, 902, 106, 84, 83, 79, 79, 770, 98, 92, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90, 594, 86, 63, 83, 80, 911, 91, 89, 87, 86, 647, 101, 100, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101680600, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682703, 1697101682801, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
3761,3761,901,31,[],200,llama-13b,128,1,1540.0,1.0,1,H100,1697101689572,1697101691112.0,120,17.0,1.0,"[31, 1509]","[1697101689603, 1697101691112]"
3762,3762,297,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101691113,1697101697588.0,120,,,"[36, 881, 244, 214, 211, 79, 111, 92, 94, 404, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 84, 605, 79, 76, 831, 103, 101, 110, 92, 91]","[1697101691149, 1697101692030, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692889, 1697101692981, 1697101693075, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694815, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
3763,3763,715,25,[],200,llama-13b,128,1,1904.0,1.0,1,H100,1697101663834,1697101665738.0,120,20.0,1.0,"[7, 1897]","[1697101663841, 1697101665738]"
3764,3764,53,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101670882,1697101675812.0,120,,,"[6, 672, 106, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 66, 686, 102, 98, 95, 71, 94]","[1697101670888, 1697101671560, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675347, 1697101675441]"
3765,3765,455,18,[],200,llama-13b,128,1,5949.0,1.0,1,H100,1697101650579,1697101656528.0,120,91.0,20.0,"[54, 1244, 841, 891, 86, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 91, 614, 106, 100, 99, 97]","[1697101650633, 1697101651877, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655512, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528]"
3766,3766,44,30,[],200,llama-13b,128,1,567.0,1.0,1,H100,1697101675813,1697101676380.0,120,12.0,1.0,"[25, 542]","[1697101675838, 1697101676380]"
3767,3767,118,26,[],200,llama-13b,128,1,4972.0,1.0,1,H100,1697101665739,1697101670711.0,120,85.0,20.0,"[13, 1018, 366, 103, 98, 73, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 92, 89, 676]","[1697101665752, 1697101666770, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711]"
3768,3768,399,31,[],200,llama-13b,128,1,4423.0,1.0,1,H100,1697101676381,1697101680804.0,120,87.0,20.0,"[51, 1815, 107, 90, 86, 64, 84, 81, 646, 98, 98, 95, 72, 92, 91, 69, 90, 470, 94, 57, 73]","[1697101676432, 1697101678247, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680110, 1697101680580, 1697101680674, 1697101680731, 1697101680804]"
3769,3769,723,22,[],200,llama-13b,128,1,1541.0,1.0,1,H100,1697101644758,1697101646299.0,120,14.0,1.0,"[55, 1486]","[1697101644813, 1697101646299]"
3770,3770,230,38,[],200,llama-13b,128,1,3159.0,1.0,1,H100,1697101689618,1697101692777.0,120,86.0,5.0,"[391, 2021, 244, 213, 212, 78]","[1697101690009, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777]"
3771,3771,154,23,[],200,llama-13b,128,1,1442.0,1.0,1,H100,1697101646300,1697101647742.0,120,13.0,1.0,"[36, 1406]","[1697101646336, 1697101647742]"
3772,3772,586,39,[],200,llama-13b,128,1,3942.0,1.0,1,H100,1697101692779,1697101696721.0,120,85.0,20.0,"[12, 581, 107, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 85, 604, 79, 76, 831, 103, 101, 110]","[1697101692791, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720]"
3773,3773,760,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101680806,1697101689595.0,120,,,"[42, 1663, 193, 96, 93, 92, 69, 90, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101680848, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
3774,3774,14,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696721,1697101697589.0,120,,,"[13, 662]","[1697101696734, 1697101697396]"
3775,3775,515,24,[],200,llama-13b,128,1,1204.0,1.0,1,H100,1697101647743,1697101648947.0,120,11.0,1.0,"[36, 1168]","[1697101647779, 1697101648947]"
3776,3776,874,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651605.0,120,,,"[49, 914, 355, 106, 104, 101, 91, 85]","[1697101648997, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
3777,3777,778,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693373,1697101697588.0,120,,,"[25, 1156, 77, 101, 84, 604, 79, 76, 831, 103, 101, 111, 91, 91]","[1697101693398, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
3778,3778,813,19,[],200,llama-13b,128,1,4730.0,1.0,1,H100,1697101656530,1697101661260.0,120,85.0,20.0,"[18, 599, 87, 81, 79, 60, 835, 95, 83, 81, 81, 1076, 102, 98, 97, 93, 85, 805, 101, 76, 98]","[1697101656548, 1697101657147, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260]"
3779,3779,803,27,[],200,llama-13b,128,1,1126.0,1.0,1,H100,1697101667137,1697101668263.0,120,20.0,1.0,"[49, 1077]","[1697101667186, 1697101668263]"
3780,3780,773,25,[],200,llama-13b,128,1,4881.0,1.0,1,H100,1697101659906,1697101664787.0,120,90.0,20.0,"[6, 734, 339, 102, 75, 98, 93, 87, 831, 107, 101, 95, 94, 89, 708, 100, 93, 88, 87, 845, 109]","[1697101659912, 1697101660646, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663465, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787]"
3781,3781,369,21,[],200,llama-13b,128,1,3565.0,1.0,1,H100,1697101656530,1697101660095.0,120,216.0,15.0,"[6, 611, 87, 81, 79, 60, 835, 95, 83, 81, 81, 1076, 102, 98, 97, 93]","[1697101656536, 1697101657147, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658548, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095]"
3782,3782,204,20,[],200,llama-13b,128,1,1478.0,1.0,1,H100,1697101655051,1697101656529.0,120,67.0,6.0,"[13, 934, 129, 105, 101, 98, 98]","[1697101655064, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529]"
3783,3783,564,21,[],200,llama-13b,128,1,4736.0,1.0,1,H100,1697101656530,1697101661266.0,120,84.0,20.0,"[54, 562, 88, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 102, 98, 97, 93, 85, 805, 101, 76, 98]","[1697101656584, 1697101657146, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260]"
3784,3784,341,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703171.0,120,,,"[36, 837]","[1697101702197, 1697101703034]"
3785,3785,215,20,[],200,llama-13b,128,1,648.0,1.0,1,H100,1697101661268,1697101661916.0,120,12.0,1.0,"[36, 612]","[1697101661304, 1697101661916]"
3786,3786,700,38,[],200,llama-13b,128,1,7998.0,1.0,1,H100,1697101703176,1697101711174.0,120,140.0,33.0,"[229, 1603, 105, 72, 728, 95, 87, 85, 83, 80, 942, 97, 92, 70, 91, 70, 91, 87, 545, 98, 65, 88, 811, 88, 84, 82, 79, 881, 103, 97, 93, 89, 88]","[1697101703405, 1697101705008, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709578, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174]"
3787,3787,486,32,[],200,llama-13b,128,1,4456.0,1.0,1,H100,1697101678248,1697101682704.0,120,14.0,20.0,"[42, 998, 117, 99, 98, 94, 72, 92, 90, 70, 91, 460, 104, 56, 73, 804, 84, 83, 79, 79, 771]","[1697101678290, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571, 1697101680675, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704]"
3788,3788,754,29,[],200,llama-13b,128,1,1378.0,1.0,1,H100,1697101677381,1697101678759.0,120,88.0,7.0,"[13, 960, 90, 86, 65, 83, 81]","[1697101677394, 1697101678354, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678759]"
3789,3789,619,11,[],200,llama-13b,128,1,1512.0,1.0,1,H100,1697101622060,1697101623572.0,120,10.0,1.0,"[66, 1446]","[1697101622126, 1697101623572]"
3790,3790,895,28,[],200,llama-13b,128,1,537.0,1.0,1,H100,1697101669948,1697101670485.0,120,15.0,1.0,"[12, 525]","[1697101669960, 1697101670485]"
3791,3791,320,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101670486,1697101675811.0,120,,,"[30, 1150, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 805, 97, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101670516, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673889, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3792,3792,183,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101678760,1697101689606.0,120,,,"[6, 1713, 92, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 101, 101, 97, 92, 90, 90, 593, 86, 64, 83, 80, 911, 91, 89, 87, 86, 647, 101, 100, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101678766, 1697101680479, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
3793,3793,849,33,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101682705,1697101683641.0,120,10.0,1.0,"[36, 900]","[1697101682741, 1697101683641]"
3794,3794,175,17,[],200,llama-13b,128,1,2781.0,1.0,1,H100,1697101675814,1697101678595.0,120,140.0,8.0,"[179, 1484, 64, 50, 763, 90, 86, 65]","[1697101675993, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678595]"
3795,3795,131,12,[],200,llama-13b,128,1,301.0,1.0,1,H100,1697101623573,1697101623874.0,120,8.0,1.0,"[30, 271]","[1697101623603, 1697101623874]"
3796,3796,485,13,[],200,llama-13b,128,1,2031.0,1.0,1,H100,1697101623875,1697101625906.0,120,67.0,3.0,"[409, 1411, 114, 97]","[1697101624284, 1697101625695, 1697101625809, 1697101625906]"
3797,3797,528,18,[],200,llama-13b,128,1,4109.0,1.0,1,H100,1697101678595,1697101682704.0,120,52.0,20.0,"[7, 686, 117, 99, 98, 94, 72, 92, 90, 70, 91, 460, 103, 57, 73, 804, 84, 83, 79, 79, 771]","[1697101678602, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704]"
3798,3798,616,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101675817,1697101689607.0,120,,,"[583, 980, 97, 64, 51, 762, 90, 86, 64, 84, 80, 647, 99, 98, 94, 72, 92, 91, 69, 90, 462, 102, 57, 73, 804, 84, 83, 79, 78, 771, 97, 93, 91, 69, 91, 90, 615, 101, 101, 97, 92, 90, 90, 594, 85, 64, 82, 81, 911, 91, 89, 87, 86, 647, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 91, 89]","[1697101676400, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680110, 1697101680572, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
3799,3799,845,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101625907,1697101626655.0,120,,,"[7, 652]","[1697101625914, 1697101626566]"
3800,3800,905,33,[],200,llama-13b,128,1,702.0,1.0,1,H100,1697101697593,1697101698295.0,120,11.0,1.0,"[21, 681]","[1697101697614, 1697101698295]"
3801,3801,276,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626663,1697101628904.0,120,,,"[114, 822]","[1697101626777, 1697101627599]"
3802,3802,695,28,[],200,llama-13b,128,1,3993.0,1.0,1,H100,1697101668664,1697101672657.0,120,92.0,20.0,"[12, 768, 125, 98, 95, 92, 93, 88, 676, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 91, 521]","[1697101668676, 1697101669444, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672657]"
3803,3803,237,20,[],200,llama-13b,128,1,5818.0,1.0,1,H100,1697101644759,1697101650577.0,120,87.0,20.0,"[13, 1527, 190, 98, 97, 84, 78, 1236, 96, 101, 100, 97, 88, 485, 97, 96, 82, 942, 106, 103, 102]","[1697101644772, 1697101646299, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577]"
3804,3804,330,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101698296,1697101700636.0,120,,,"[12, 1168, 112, 83, 82, 82, 80, 82]","[1697101698308, 1697101699476, 1697101699588, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
3805,3805,891,19,[],200,llama-13b,128,1,1144.0,1.0,1,H100,1697101682705,1697101683849.0,120,52.0,2.0,"[30, 906, 208]","[1697101682735, 1697101683641, 1697101683849]"
3806,3806,320,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101683850,1697101689595.0,120,,,"[18, 1146, 86, 63, 83, 81, 910, 91, 89, 87, 87, 646, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101683868, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3807,3807,663,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700643,1697101703170.0,120,,,"[372, 1145, 122, 69]","[1697101701015, 1697101702160, 1697101702282, 1697101702351]"
3808,3808,92,36,[],200,llama-13b,128,1,5353.0,1.0,1,H100,1697101703173,1697101708526.0,120,85.0,20.0,"[352, 1374, 116, 98, 72, 728, 95, 87, 85, 83, 79, 944, 96, 92, 70, 92, 69, 91, 87, 545, 98]","[1697101703525, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
3809,3809,344,36,[],200,llama-13b,128,1,1689.0,1.0,1,H100,1697101684427,1697101686116.0,120,13.0,1.0,"[71, 1618]","[1697101684498, 1697101686116]"
3810,3810,205,21,[],200,llama-13b,128,1,3824.0,1.0,1,H100,1697101681503,1697101685327.0,120,87.0,20.0,"[12, 996, 193, 96, 93, 92, 69, 90, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 81]","[1697101681515, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327]"
3811,3811,651,23,[],200,llama-13b,128,1,1152.0,1.0,1,H100,1697101645338,1697101646490.0,120,457.0,2.0,"[30, 931, 191]","[1697101645368, 1697101646299, 1697101646490]"
3812,3812,80,24,[],200,llama-13b,128,1,1251.0,1.0,1,H100,1697101646491,1697101647742.0,120,13.0,1.0,"[12, 1239]","[1697101646503, 1697101647742]"
3813,3813,669,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101686117,1697101689594.0,120,,,"[24, 882, 214, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 91, 90]","[1697101686141, 1697101687023, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688994]"
3814,3814,361,28,[],200,llama-13b,128,1,1522.0,1.0,1,H100,1697101664788,1697101666310.0,120,67.0,7.0,"[12, 939, 129, 106, 99, 72, 72, 93]","[1697101664800, 1697101665739, 1697101665868, 1697101665974, 1697101666073, 1697101666145, 1697101666217, 1697101666310]"
3815,3815,453,37,[],200,llama-13b,128,1,1893.0,1.0,1,H100,1697101708534,1697101710427.0,120,26.0,1.0,"[153, 1740]","[1697101708687, 1697101710427]"
3816,3816,809,38,[],200,llama-13b,128,1,1231.0,1.0,1,H100,1697101710429,1697101711660.0,120,16.0,1.0,"[12, 1219]","[1697101710441, 1697101711660]"
3817,3817,411,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101647743,1697101651603.0,120,,,"[13, 1293, 97, 96, 82, 942, 106, 103, 102, 91, 85]","[1697101647756, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
3818,3818,335,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671667,1697101675810.0,120,,,"[18, 861, 111, 113, 91, 68, 68, 88, 803, 98, 89, 88, 66, 65, 687, 102, 99, 95, 70, 94]","[1697101671685, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
3819,3819,720,29,[],200,llama-13b,128,1,2443.0,1.0,1,H100,1697101666311,1697101668754.0,120,286.0,6.0,"[6, 1946, 133, 101, 95, 71, 91]","[1697101666317, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754]"
3820,3820,765,26,[],200,llama-13b,128,1,1112.0,1.0,1,H100,1697101651606,1697101652718.0,120,84.0,2.0,"[66, 989, 57]","[1697101651672, 1697101652661, 1697101652718]"
3821,3821,197,27,[],200,llama-13b,128,1,2702.0,1.0,1,H100,1697101652719,1697101655421.0,120,6.0,8.0,"[15, 1897, 206, 108, 105, 101, 74, 100, 96]","[1697101652734, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421]"
3822,3822,434,22,[],200,llama-13b,128,1,5765.0,1.0,1,H100,1697101656613,1697101662378.0,120,85.0,20.0,"[7, 1540, 129, 96, 82, 82, 80, 1076, 102, 97, 99, 92, 85, 805, 102, 75, 103, 88, 87, 831, 107]","[1697101656620, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659807, 1697101659904, 1697101660003, 1697101660095, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661265, 1697101661353, 1697101661440, 1697101662271, 1697101662378]"
3823,3823,156,30,[],200,llama-13b,128,1,3902.0,1.0,1,H100,1697101668755,1697101672657.0,120,86.0,20.0,"[7, 682, 125, 98, 95, 92, 93, 88, 676, 99, 70, 93, 90, 91, 511, 98, 95, 94, 93, 91, 521]","[1697101668762, 1697101669444, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672657]"
3824,3824,714,23,[],200,llama-13b,128,1,5001.0,1.0,1,H100,1697101651610,1697101656611.0,120,83.0,20.0,"[387, 1475, 138, 85, 83, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 614, 105, 101, 98, 97, 83]","[1697101651997, 1697101653472, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611]"
3825,3825,462,40,[],200,llama-13b,128,1,1517.0,1.0,1,H100,1697101700642,1697101702159.0,120,52.0,1.0,"[337, 1180]","[1697101700979, 1697101702159]"
3826,3826,692,28,[],200,llama-13b,128,1,4297.0,1.0,1,H100,1697101675813,1697101680110.0,120,88.0,20.0,"[156, 411, 38, 1059, 64, 50, 763, 90, 86, 64, 84, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90]","[1697101675969, 1697101676380, 1697101676418, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110]"
3827,3827,816,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703171.0,120,,,"[54, 820]","[1697101702215, 1697101703035]"
3828,3828,855,25,[],200,llama-13b,128,1,5530.0,1.0,1,H100,1697101665181,1697101670711.0,120,83.0,20.0,"[12, 1577, 366, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 92, 89, 676]","[1697101665193, 1697101666770, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711]"
3829,3829,244,42,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703176,1697101704900.0,120,9.0,1.0,"[411, 1312]","[1697101703587, 1697101704899]"
3830,3830,578,43,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,31.0,1.0,"[104, 773]","[1697101705005, 1697101705778]"
3831,3831,10,44,[],200,llama-13b,128,1,2104.0,1.0,1,H100,1697101705779,1697101707883.0,120,563.0,9.0,"[54, 1212, 241, 96, 92, 70, 91, 70, 91, 87]","[1697101705833, 1697101707045, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883]"
3832,3832,124,29,[],200,llama-13b,128,1,1496.0,1.0,1,H100,1697101680112,1697101681608.0,120,83.0,2.0,"[13, 1483]","[1697101680125, 1697101681608]"
3833,3833,455,30,[],200,llama-13b,128,1,3717.0,1.0,1,H100,1697101681609,1697101685326.0,120,91.0,20.0,"[6, 896, 193, 97, 92, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90, 593, 86, 64, 83, 80]","[1697101681615, 1697101682511, 1697101682704, 1697101682801, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685246, 1697101685326]"
3834,3834,361,33,[],200,llama-13b,128,1,3273.0,1.0,1,H100,1697101689614,1697101692887.0,120,67.0,7.0,"[166, 1332, 93, 1069, 213, 212, 78, 110]","[1697101689780, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887]"
3835,3835,14,29,[],200,llama-13b,128,1,4443.0,1.0,1,H100,1697101669445,1697101673888.0,120,90.0,20.0,"[18, 1022, 226, 99, 71, 92, 91, 90, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 68, 87, 804]","[1697101669463, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673084, 1697101673888]"
3836,3836,897,23,[],200,llama-13b,128,1,566.0,1.0,1,H100,1697101648381,1697101648947.0,120,9.0,1.0,"[6, 560]","[1697101648387, 1697101648947]"
3837,3837,348,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.51 GiB is free. Process 1607256 has 67.58 GiB memory in use. Of the allocated memory 40.84 GiB is allocated by PyTorch, and 25.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101623671,1697101626657.0,120,,,"[325, 1698, 115, 97, 69]","[1697101623996, 1697101625694, 1697101625809, 1697101625906, 1697101625975]"
3838,3838,366,16,[],200,llama-13b,128,1,1257.0,1.0,1,H100,1697101660096,1697101661353.0,120,85.0,6.0,"[6, 545, 338, 102, 75, 98, 93]","[1697101660102, 1697101660647, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353]"
3839,3839,716,17,[],200,llama-13b,128,1,6154.0,1.0,1,H100,1697101661354,1697101667508.0,120,79.0,30.0,"[12, 550, 356, 106, 102, 94, 94, 90, 706, 101, 93, 88, 87, 845, 108, 104, 95, 94, 100, 688, 106, 100, 72, 72, 93, 92, 734, 103, 98, 73, 98]","[1697101661366, 1697101661916, 1697101662272, 1697101662378, 1697101662480, 1697101662574, 1697101662668, 1697101662758, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664890, 1697101664985, 1697101665079, 1697101665179, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508]"
3840,3840,375,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673892,1697101675812.0,120,,,"[23, 1066, 102, 99, 94, 71, 94]","[1697101673915, 1697101674981, 1697101675083, 1697101675182, 1697101675276, 1697101675347, 1697101675441]"
3841,3841,144,18,[],200,llama-13b,128,1,4157.0,1.0,1,H100,1697101667509,1697101671666.0,120,96.0,20.0,"[7, 747, 133, 101, 95, 71, 92, 89, 724, 99, 94, 93, 92, 89, 676, 99, 71, 92, 91, 89, 513]","[1697101667516, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668755, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666]"
3842,3842,678,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.68 GiB. GPU 0 has a total capacty of 79.11 GiB of which 6.30 GiB is free. Process 1607256 has 72.80 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 30.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101626665,1697101628904.0,120,,,"[318, 1710]","[1697101626983, 1697101628693]"
3843,3843,734,31,[],200,llama-13b,128,1,2629.0,1.0,1,H100,1697101675815,1697101678444.0,120,100.0,6.0,"[489, 1076, 97, 64, 51, 762, 90]","[1697101676304, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678354, 1697101678444]"
3844,3844,110,15,[],200,llama-13b,128,1,1974.0,1.0,1,H100,1697101628909,1697101630883.0,120,96.0,4.0,"[278, 1442, 105, 81, 68]","[1697101629187, 1697101630629, 1697101630734, 1697101630815, 1697101630883]"
3845,3845,214,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[106, 594, 43, 1250, 83, 82, 81, 81, 82]","[1697101697701, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
3846,3846,160,32,[],200,llama-13b,128,1,842.0,1.0,1,H100,1697101678446,1697101679288.0,120,13.0,1.0,"[12, 830]","[1697101678458, 1697101679288]"
3847,3847,514,33,[],200,llama-13b,128,1,4762.0,1.0,1,H100,1697101679289,1697101684051.0,120,85.0,20.0,"[24, 1166, 92, 103, 58, 72, 804, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 102, 100]","[1697101679313, 1697101680479, 1697101680571, 1697101680674, 1697101680732, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051]"
3848,3848,471,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.37 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.61 GiB is free. Process 1607256 has 67.49 GiB memory in use. Of the allocated memory 40.42 GiB is allocated by PyTorch, and 26.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101630884,1697101634994.0,120,,,"[7, 793, 131, 92, 90, 90, 88, 88, 84, 83, 742, 94, 72, 94, 93, 92, 70, 614, 75, 71]","[1697101630891, 1697101631684, 1697101631815, 1697101631907, 1697101631997, 1697101632087, 1697101632175, 1697101632263, 1697101632347, 1697101632430, 1697101633172, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633687, 1697101634301, 1697101634376, 1697101634447]"
3849,3849,503,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671667,1697101675810.0,120,,,"[36, 843, 111, 113, 91, 68, 68, 88, 803, 98, 89, 88, 66, 66, 686, 102, 99, 95, 70, 94]","[1697101671703, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
3850,3850,586,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672137,1697101675810.0,120,,,"[6, 1615, 130, 98, 89, 88, 66, 66, 686, 102, 98, 96, 70, 94]","[1697101672143, 1697101673758, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675277, 1697101675347, 1697101675441]"
3851,3851,569,43,[],200,llama-13b,128,1,1518.0,1.0,1,H100,1697101700642,1697101702160.0,120,16.0,1.0,"[372, 1145]","[1697101701014, 1697101702159]"
3852,3852,316,35,[],200,llama-13b,128,1,5198.0,1.0,1,H100,1697101689617,1697101694815.0,120,86.0,20.0,"[362, 2050, 244, 214, 212, 78, 110, 93, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101, 83]","[1697101689979, 1697101692029, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732, 1697101694815]"
3853,3853,831,17,[],200,llama-13b,128,1,1811.0,1.0,1,H100,1697101634997,1697101636808.0,120,11.0,1.0,"[117, 1694]","[1697101635114, 1697101636808]"
3854,3854,346,18,[],200,llama-13b,128,1,4310.0,1.0,1,H100,1697101636809,1697101641119.0,120,85.0,20.0,"[13, 657, 189, 91, 92, 87, 87, 82, 81, 768, 98, 92, 93, 92, 91, 91, 79, 618, 81, 93, 735]","[1697101636822, 1697101637479, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639592, 1697101640210, 1697101640291, 1697101640384, 1697101641119]"
3855,3855,899,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703171.0,120,,,"[60, 814]","[1697101702221, 1697101703035]"
3856,3856,31,34,[],200,llama-13b,128,1,4572.0,1.0,1,H100,1697101684052,1697101688624.0,120,84.0,20.0,"[18, 837, 107, 86, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106]","[1697101684070, 1697101684907, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
3857,3857,331,45,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703176,1697101704900.0,120,26.0,1.0,"[241, 1483]","[1697101703417, 1697101704900]"
3858,3858,453,29,[],200,llama-13b,128,1,1547.0,1.0,1,H100,1697101656613,1697101658160.0,120,26.0,1.0,"[89, 1458]","[1697101656702, 1697101658160]"
3859,3859,690,46,[],200,llama-13b,128,1,876.0,1.0,1,H100,1697101704902,1697101705778.0,120,39.0,1.0,"[114, 762]","[1697101705016, 1697101705778]"
3860,3860,783,30,[],200,llama-13b,128,1,1212.0,1.0,1,H100,1697101658162,1697101659374.0,120,286.0,1.0,"[65, 1147]","[1697101658227, 1697101659374]"
3861,3861,123,47,[],200,llama-13b,128,1,1265.0,1.0,1,H100,1697101705780,1697101707045.0,120,14.0,1.0,"[95, 1169]","[1697101705875, 1697101707044]"
3862,3862,563,48,[],200,llama-13b,128,1,4713.0,1.0,1,H100,1697101707045,1697101711758.0,120,874.0,18.0,"[7, 1261, 115, 99, 64, 88, 812, 88, 83, 82, 79, 881, 104, 96, 93, 89, 88, 86, 498]","[1697101707052, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710808, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758]"
3863,3863,215,31,[],200,llama-13b,128,1,1272.0,1.0,1,H100,1697101659375,1697101660647.0,120,12.0,1.0,"[30, 1242]","[1697101659405, 1697101660647]"
3864,3864,574,32,[],200,llama-13b,128,1,1623.0,1.0,1,H100,1697101660648,1697101662271.0,120,364.0,2.0,"[6, 1262, 355]","[1697101660654, 1697101661916, 1697101662271]"
3865,3865,928,33,[],200,llama-13b,128,1,1064.0,1.0,1,H100,1697101662273,1697101663337.0,120,20.0,1.0,"[12, 1052]","[1697101662285, 1697101663337]"
3866,3866,697,19,[],200,llama-13b,128,1,2429.0,1.0,1,H100,1697101641121,1697101643550.0,120,123.0,10.0,"[36, 660, 304, 101, 99, 96, 88, 82, 64, 799, 100]","[1697101641157, 1697101641817, 1697101642121, 1697101642222, 1697101642321, 1697101642417, 1697101642505, 1697101642587, 1697101642651, 1697101643450, 1697101643550]"
3867,3867,889,30,[],200,llama-13b,128,1,5174.0,1.0,1,H100,1697101658290,1697101663464.0,120,86.0,20.0,"[7, 1077, 331, 102, 98, 97, 93, 85, 805, 101, 75, 99, 93, 87, 831, 107, 101, 95, 94, 89, 707]","[1697101658297, 1697101659374, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464]"
3868,3868,278,34,[],200,llama-13b,128,1,1264.0,1.0,1,H100,1697101683643,1697101684907.0,120,13.0,1.0,"[29, 1235]","[1697101683672, 1697101684907]"
3869,3869,357,34,[],200,llama-13b,128,1,7373.0,1.0,1,H100,1697101663338,1697101670711.0,120,52.0,33.0,"[42, 1065, 233, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 89, 676]","[1697101663380, 1697101664445, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711]"
3870,3870,631,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684908,1697101689611.0,120,,,"[30, 1299, 91, 89, 87, 87, 646, 101, 100, 94, 92, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101684938, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3871,3871,595,21,[],200,llama-13b,128,1,730.0,1.0,1,H100,1697101650578,1697101651308.0,120,8.0,1.0,"[13, 717]","[1697101650591, 1697101651308]"
3872,3872,904,32,[],200,llama-13b,128,1,1644.0,1.0,1,H100,1697101700638,1697101702282.0,120,563.0,2.0,"[54, 1467, 123]","[1697101700692, 1697101702159, 1697101702282]"
3873,3873,114,15,[],200,llama-13b,128,1,5211.0,1.0,1,H100,1697101634998,1697101640209.0,120,88.0,20.0,"[296, 1514, 89, 87, 56, 628, 91, 92, 87, 86, 82, 82, 768, 97, 93, 93, 92, 91, 91, 80, 616]","[1697101635294, 1697101636808, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638956, 1697101639053, 1697101639146, 1697101639239, 1697101639331, 1697101639422, 1697101639513, 1697101639593, 1697101640209]"
3874,3874,23,22,[],200,llama-13b,128,1,568.0,1.0,1,H100,1697101651309,1697101651877.0,120,26.0,1.0,"[18, 550]","[1697101651327, 1697101651877]"
3875,3875,383,23,[],200,llama-13b,128,1,1594.0,1.0,1,H100,1697101651878,1697101653472.0,120,15.0,1.0,"[313, 1281]","[1697101652191, 1697101653472]"
3876,3876,305,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702283,1697101703172.0,120,,,"[12, 740]","[1697101702295, 1697101703035]"
3877,3877,658,34,[],200,llama-13b,128,1,1723.0,1.0,1,H100,1697101703177,1697101704900.0,120,11.0,1.0,"[491, 1232]","[1697101703668, 1697101704900]"
3878,3878,84,35,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,26.0,1.0,"[118, 759]","[1697101705019, 1697101705778]"
3879,3879,443,36,[],200,llama-13b,128,1,1266.0,1.0,1,H100,1697101705779,1697101707045.0,120,19.0,1.0,"[54, 1212]","[1697101705833, 1697101707045]"
3880,3880,711,24,[],200,llama-13b,128,1,1572.0,1.0,1,H100,1697101653478,1697101655050.0,120,457.0,4.0,"[7, 1146, 206, 108, 105]","[1697101653485, 1697101654631, 1697101654837, 1697101654945, 1697101655050]"
3881,3881,805,37,[],200,llama-13b,128,1,6132.0,1.0,1,H100,1697101707046,1697101713178.0,120,286.0,50.0,"[36, 1231, 115, 99, 64, 88, 811, 89, 83, 82, 79, 882, 103, 96, 93, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45, 44, 41, 33, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26, 24, 20, 20]","[1697101707082, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712674, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114, 1697101713138, 1697101713158, 1697101713178]"
3882,3882,139,25,[],200,llama-13b,128,1,5044.0,1.0,1,H100,1697101655051,1697101660095.0,120,39.0,21.0,"[7, 940, 129, 105, 101, 98, 98, 83, 622, 81, 79, 60, 836, 95, 82, 82, 80, 1076, 101, 99, 97, 93]","[1697101655058, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658290, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095]"
3883,3883,57,36,[],200,llama-13b,128,1,1496.0,1.0,1,H100,1697101689616,1697101691112.0,120,13.0,1.0,"[319, 1177]","[1697101689935, 1697101691112]"
3884,3884,502,37,[],200,llama-13b,128,1,917.0,1.0,1,H100,1697101691113,1697101692030.0,120,19.0,1.0,"[54, 863]","[1697101691167, 1697101692030]"
3885,3885,864,38,[],200,llama-13b,128,1,4689.0,1.0,1,H100,1697101692031,1697101696720.0,120,83.0,20.0,"[18, 1323, 107, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 85, 604, 78, 77, 831, 103, 101, 110]","[1697101692049, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720]"
3886,3886,877,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693780,1697101697588.0,120,,,"[7, 670, 97, 77, 101, 84, 604, 79, 76, 832, 102, 101, 111, 91, 91]","[1697101693787, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
3887,3887,601,23,[],200,llama-13b,128,1,4845.0,1.0,1,H100,1697101662758,1697101667603.0,120,83.0,20.0,"[7, 1680, 233, 108, 103, 96, 94, 101, 688, 105, 101, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95]","[1697101662765, 1697101664445, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603]"
3888,3888,30,24,[],200,llama-13b,128,1,5052.0,1.0,1,H100,1697101667605,1697101672657.0,120,93.0,20.0,"[18, 1945, 99, 94, 93, 92, 89, 676, 98, 71, 93, 90, 90, 513, 97, 95, 94, 93, 90, 522]","[1697101667623, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657]"
3889,3889,278,44,[],200,llama-13b,128,1,1873.0,1.0,1,H100,1697101697602,1697101699475.0,120,13.0,1.0,"[522, 1351]","[1697101698124, 1697101699475]"
3890,3890,636,45,[],200,llama-13b,128,1,1075.0,1.0,1,H100,1697101699477,1697101700552.0,120,31.0,1.0,"[96, 979]","[1697101699573, 1697101700552]"
3891,3891,64,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700553,1697101703170.0,120,,,"[36, 505, 1188, 69]","[1697101700589, 1697101701094, 1697101702282, 1697101702351]"
3892,3892,294,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696721,1697101697589.0,120,,,[19],[1697101696740]
3893,3893,498,26,[],200,llama-13b,128,1,551.0,1.0,1,H100,1697101660096,1697101660647.0,120,9.0,1.0,"[18, 533]","[1697101660114, 1697101660647]"
3894,3894,852,27,[],200,llama-13b,128,1,5325.0,1.0,1,H100,1697101660648,1697101665973.0,120,100.0,20.0,"[18, 1250, 355, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 109, 102, 96, 94, 101, 688, 105]","[1697101660666, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973]"
3895,3895,263,32,[],200,llama-13b,128,1,603.0,1.0,1,H100,1697101694733,1697101695336.0,120,15.0,1.0,"[48, 555]","[1697101694781, 1697101695336]"
3896,3896,780,29,[],200,llama-13b,128,1,5117.0,1.0,1,H100,1697101689615,1697101694732.0,120,85.0,20.0,"[210, 1287, 93, 1069, 213, 212, 79, 110, 92, 94, 405, 101, 100, 99, 95, 91, 87, 68, 433, 78, 101]","[1697101689825, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692888, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694732]"
3897,3897,624,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695338,1697101697588.0,120,,,"[41, 804, 224, 102, 101, 111, 92, 91]","[1697101695379, 1697101696183, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696813, 1697101696904]"
3898,3898,401,27,[],200,llama-13b,128,1,4757.0,1.0,1,H100,1697101662379,1697101667136.0,120,84.0,20.0,"[6, 952, 127, 101, 93, 88, 87, 845, 108, 103, 97, 93, 100, 689, 105, 100, 72, 72, 93, 92, 734]","[1697101662385, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664986, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136]"
3899,3899,754,31,[],200,llama-13b,128,1,2338.0,1.0,1,H100,1697101680806,1697101683144.0,120,88.0,7.0,"[71, 1827, 96, 93, 92, 69, 90]","[1697101680877, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144]"
3900,3900,161,32,[],200,llama-13b,128,1,1185.0,1.0,1,H100,1697101683145,1697101684330.0,120,109.0,7.0,"[6, 698, 102, 100, 97, 92, 90]","[1697101683151, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330]"
3901,3901,519,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684331,1697101689595.0,120,,,"[7, 569, 107, 86, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101684338, 1697101684907, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3902,3902,209,30,[],200,llama-13b,128,1,603.0,1.0,1,H100,1697101694733,1697101695336.0,120,20.0,1.0,"[60, 543]","[1697101694793, 1697101695336]"
3903,3903,755,28,[],200,llama-13b,128,1,4998.0,1.0,1,H100,1697101667137,1697101672135.0,120,286.0,25.0,"[13, 1113, 133, 101, 95, 71, 91, 90, 724, 99, 94, 93, 92, 89, 677, 98, 71, 92, 91, 89, 513, 97, 95, 94, 93, 90]","[1697101667150, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670712, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135]"
3904,3904,29,34,[],200,llama-13b,128,1,2320.0,1.0,1,H100,1697101697595,1697101699915.0,120,161.0,6.0,"[420, 1461, 112, 83, 82, 81, 81]","[1697101698015, 1697101699476, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915]"
3905,3905,570,31,[],200,llama-13b,128,1,846.0,1.0,1,H100,1697101695337,1697101696183.0,120,18.0,1.0,"[30, 816]","[1697101695367, 1697101696183]"
3906,3906,926,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696184,1697101697589.0,120,,,"[19, 1192]","[1697101696203, 1697101697395]"
3907,3907,325,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[174, 526, 43, 1250, 83, 82, 81, 81, 82]","[1697101697769, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
3908,3908,679,34,[],200,llama-13b,128,1,1518.0,1.0,1,H100,1697101700642,1697101702160.0,120,15.0,1.0,"[240, 1278]","[1697101700882, 1697101702160]"
3909,3909,113,35,[],200,llama-13b,128,1,873.0,1.0,1,H100,1697101702162,1697101703035.0,120,13.0,1.0,"[77, 796]","[1697101702239, 1697101703035]"
3910,3910,878,34,[],200,llama-13b,128,1,5197.0,1.0,1,H100,1697101689619,1697101694816.0,120,83.0,20.0,"[571, 1840, 244, 213, 212, 78, 111, 93, 94, 403, 102, 100, 99, 95, 91, 87, 68, 434, 77, 100, 85]","[1697101690190, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692888, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
3911,3911,387,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699916,1697101700635.0,120,,,"[6, 630]","[1697101699922, 1697101700552]"
3912,3912,746,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[138, 1383, 123, 69]","[1697101700776, 1697101702159, 1697101702282, 1697101702351]"
3913,3913,171,37,[],200,llama-13b,128,1,1727.0,1.0,1,H100,1697101703172,1697101704899.0,120,6.0,1.0,"[305, 1422]","[1697101703477, 1697101704899]"
3914,3914,525,38,[],200,llama-13b,128,1,8058.0,1.0,1,H100,1697101704901,1697101712959.0,120,216.0,55.0,"[48, 829, 135, 95, 87, 85, 83, 80, 942, 96, 93, 70, 91, 70, 91, 87, 544, 99, 65, 88, 811, 89, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32, 31, 31, 25, 29]","[1697101704949, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959]"
3915,3915,587,28,[],200,llama-13b,128,1,879.0,1.0,1,H100,1697101663566,1697101664445.0,120,13.0,1.0,"[12, 867]","[1697101663578, 1697101664445]"
3916,3916,154,31,[],200,llama-13b,128,1,1826.0,1.0,1,H100,1697101676421,1697101678247.0,120,13.0,1.0,"[81, 1745]","[1697101676502, 1697101678247]"
3917,3917,19,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101664447,1697101675811.0,120,,,"[12, 1279, 130, 106, 99, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90, 724, 99, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 513, 97, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101664459, 1697101665738, 1697101665868, 1697101665974, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3918,3918,483,32,[],200,llama-13b,128,1,4456.0,1.0,1,H100,1697101678248,1697101682704.0,120,84.0,20.0,"[24, 1016, 117, 99, 97, 95, 72, 92, 90, 70, 90, 461, 103, 57, 73, 804, 84, 83, 79, 79, 771]","[1697101678272, 1697101679288, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704]"
3919,3919,282,27,[],200,llama-13b,128,1,3857.0,1.0,1,H100,1697101664986,1697101668843.0,120,87.0,20.0,"[7, 746, 129, 106, 100, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 96, 70, 91, 89]","[1697101664993, 1697101665739, 1697101665868, 1697101665974, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668593, 1697101668663, 1697101668754, 1697101668843]"
3920,3920,574,25,[],200,llama-13b,128,1,982.0,1.0,1,H100,1697101660003,1697101660985.0,120,364.0,2.0,"[13, 630, 339]","[1697101660016, 1697101660646, 1697101660985]"
3921,3921,4,26,[],200,llama-13b,128,1,4988.0,1.0,1,H100,1697101660986,1697101665974.0,120,89.0,20.0,"[13, 917, 355, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 109, 102, 96, 94, 101, 688, 105]","[1697101660999, 1697101661916, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973]"
3922,3922,614,28,[],200,llama-13b,128,1,1640.0,1.0,1,H100,1697101668845,1697101670485.0,120,15.0,1.0,"[7, 1633]","[1697101668852, 1697101670485]"
3923,3923,41,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101670486,1697101675811.0,120,,,"[24, 1049, 107, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 805, 97, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101670510, 1697101671559, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673889, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3924,3924,566,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685329,1697101689594.0,120,,,"[54, 1639, 215, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101685383, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
3925,3925,821,24,[],200,llama-13b,128,1,5765.0,1.0,1,H100,1697101656613,1697101662378.0,120,85.0,20.0,"[71, 1476, 129, 96, 82, 82, 80, 1076, 101, 99, 97, 93, 85, 805, 102, 75, 98, 93, 87, 831, 107]","[1697101656684, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378]"
3926,3926,326,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651604.0,120,,,"[13, 950, 355, 106, 104, 101, 91, 85]","[1697101648961, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
3927,3927,687,25,[],200,llama-13b,128,1,5002.0,1.0,1,H100,1697101651610,1697101656612.0,120,96.0,20.0,"[459, 1403, 138, 86, 82, 82, 81, 896, 107, 106, 101, 74, 99, 97, 91, 615, 105, 101, 98, 97, 83]","[1697101652069, 1697101653472, 1697101653610, 1697101653696, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655151, 1697101655225, 1697101655324, 1697101655421, 1697101655512, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611]"
3928,3928,660,26,[],200,llama-13b,128,1,5179.0,1.0,1,H100,1697101665974,1697101671153.0,120,732.0,25.0,"[13, 783, 366, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 93, 88, 676, 99, 70, 93, 90, 90]","[1697101665987, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153]"
3929,3929,362,27,[],200,llama-13b,128,1,795.0,1.0,1,H100,1697101665975,1697101666770.0,120,14.0,1.0,"[30, 765]","[1697101666005, 1697101666770]"
3930,3930,719,28,[],200,llama-13b,128,1,1983.0,1.0,1,H100,1697101666771,1697101668754.0,120,182.0,6.0,"[6, 1619, 101, 95, 71, 91]","[1697101666777, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754]"
3931,3931,113,26,[],200,llama-13b,128,1,1546.0,1.0,1,H100,1697101656614,1697101658160.0,120,13.0,1.0,"[94, 1452]","[1697101656708, 1697101658160]"
3932,3932,558,27,[],200,llama-13b,128,1,5303.0,1.0,1,H100,1697101658161,1697101663464.0,120,58.0,20.0,"[48, 1164, 332, 101, 99, 97, 93, 85, 805, 101, 75, 99, 92, 88, 831, 107, 101, 95, 94, 89, 707]","[1697101658209, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464]"
3933,3933,218,22,[],200,llama-13b,128,1,1855.0,1.0,1,H100,1697101656530,1697101658385.0,120,109.0,7.0,"[42, 662, 81, 79, 60, 835, 96]","[1697101656572, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385]"
3934,3934,235,28,[],200,llama-13b,128,1,2799.0,1.0,1,H100,1697101668264,1697101671063.0,120,161.0,12.0,"[24, 1281, 98, 94, 93, 93, 88, 676, 99, 70, 93, 90]","[1697101668288, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063]"
3935,3935,576,23,[],200,llama-13b,128,1,987.0,1.0,1,H100,1697101658386,1697101659373.0,120,14.0,1.0,"[6, 981]","[1697101658392, 1697101659373]"
3936,3936,912,28,[],200,llama-13b,128,1,4138.0,1.0,1,H100,1697101663465,1697101667603.0,120,92.0,20.0,"[19, 962, 232, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 99, 72, 99, 94]","[1697101663484, 1697101664446, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603]"
3937,3937,6,24,[],200,llama-13b,128,1,6843.0,1.0,1,H100,1697101659374,1697101666217.0,120,100.0,29.0,"[13, 1598, 101, 76, 98, 93, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 101, 688, 105, 100, 72, 72]","[1697101659387, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217]"
3938,3938,428,32,[],200,llama-13b,128,1,3190.0,1.0,1,H100,1697101685328,1697101688518.0,120,31.0,9.0,"[7, 1687, 215, 102, 99, 94, 92, 91, 88, 715]","[1697101685335, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518]"
3939,3939,849,43,[],200,llama-13b,128,1,863.0,1.0,1,H100,1697101706182,1697101707045.0,120,10.0,1.0,"[6, 857]","[1697101706188, 1697101707045]"
3940,3940,253,44,[],200,llama-13b,128,1,4901.0,1.0,1,H100,1697101707046,1697101711947.0,120,67.0,20.0,"[18, 1249, 115, 99, 64, 88, 812, 88, 83, 82, 79, 882, 103, 96, 93, 89, 88, 86, 498, 98, 91]","[1697101707064, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947]"
3941,3941,389,35,[],200,llama-13b,128,1,946.0,1.0,1,H100,1697101688625,1697101689571.0,120,8.0,1.0,"[49, 897]","[1697101688674, 1697101689571]"
3942,3942,786,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688519,1697101689594.0,120,,,"[6, 1045]","[1697101688525, 1697101689570]"
3943,3943,752,36,[],200,llama-13b,128,1,2703.0,1.0,1,H100,1697101689571,1697101692274.0,120,39.0,3.0,"[13, 392, 1229, 1068]","[1697101689584, 1697101689976, 1697101691205, 1697101692273]"
3944,3944,346,29,[],200,llama-13b,128,1,5052.0,1.0,1,H100,1697101667605,1697101672657.0,120,85.0,20.0,"[42, 1797, 125, 98, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 513, 97, 95, 94, 93, 90, 522]","[1697101667647, 1697101669444, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657]"
3945,3945,184,37,[],200,llama-13b,128,1,4445.0,1.0,1,H100,1697101692275,1697101696720.0,120,87.0,20.0,"[6, 1091, 107, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 85, 604, 78, 77, 831, 103, 101, 110]","[1697101692281, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720]"
3946,3946,86,23,[],200,llama-13b,128,1,4521.0,1.0,1,H100,1697101689599,1697101694120.0,120,335.0,17.0,"[11, 1502, 93, 1068, 214, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68]","[1697101689610, 1697101691112, 1697101691205, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120]"
3947,3947,192,34,[],200,llama-13b,128,1,5198.0,1.0,1,H100,1697101689618,1697101694816.0,120,93.0,20.0,"[397, 2259, 213, 212, 78, 101, 102, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 100, 85]","[1697101690015, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
3948,3948,691,27,[],200,llama-13b,128,1,762.0,1.0,1,H100,1697101662575,1697101663337.0,120,47.0,1.0,"[7, 755]","[1697101662582, 1697101663337]"
3949,3949,125,28,[],200,llama-13b,128,1,1108.0,1.0,1,H100,1697101663338,1697101664446.0,120,13.0,1.0,"[48, 1059]","[1697101663386, 1697101664445]"
3950,3950,479,29,[],200,llama-13b,128,1,7505.0,1.0,1,H100,1697101664447,1697101671952.0,120,140.0,36.0,"[6, 1285, 130, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90, 724, 99, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 512, 98, 95, 94]","[1697101664453, 1697101665738, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952]"
3951,3951,618,30,[],200,llama-13b,128,1,1082.0,1.0,1,H100,1697101658291,1697101659373.0,120,9.0,1.0,"[54, 1028]","[1697101658345, 1697101659373]"
3952,3952,52,31,[],200,llama-13b,128,1,1979.0,1.0,1,H100,1697101659374,1697101661353.0,120,58.0,6.0,"[13, 1259, 339, 102, 75, 98, 93]","[1697101659387, 1697101660646, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353]"
3953,3953,552,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[57, 1308, 223, 103, 101, 110, 92, 91]","[1697101694875, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
3954,3954,127,20,[],200,llama-13b,128,1,1123.0,1.0,1,H100,1697101643551,1697101644674.0,120,100.0,5.0,"[6, 763, 100, 87, 84, 83]","[1697101643557, 1697101644320, 1697101644420, 1697101644507, 1697101644591, 1697101644674]"
3955,3955,674,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101675816,1697101689611.0,120,,,"[556, 1008, 97, 64, 51, 762, 90, 86, 64, 84, 80, 647, 99, 98, 94, 72, 92, 91, 69, 91, 469, 95, 56, 73, 804, 84, 83, 79, 78, 771, 97, 93, 91, 69, 91, 90, 614, 102, 101, 97, 91, 91, 90, 594, 85, 64, 82, 81, 911, 91, 89, 87, 86, 647, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 91, 89]","[1697101676372, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680111, 1697101680580, 1697101680675, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684239, 1697101684330, 1697101684420, 1697101685014, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
3956,3956,34,15,[],200,llama-13b,128,1,1238.0,1.0,1,H100,1697101632968,1697101634206.0,120,12.0,1.0,"[42, 1196]","[1697101633010, 1697101634206]"
3957,3957,465,24,[],200,llama-13b,128,1,1130.0,1.0,1,H100,1697101649243,1697101650373.0,120,364.0,3.0,"[24, 644, 355, 106]","[1697101649267, 1697101649911, 1697101650266, 1697101650372]"
3958,3958,362,16,[],200,llama-13b,128,1,761.0,1.0,1,H100,1697101634208,1697101634969.0,120,14.0,1.0,"[24, 737]","[1697101634232, 1697101634969]"
3959,3959,823,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101650373,1697101651604.0,120,,,"[12, 923]","[1697101650385, 1697101651308]"
3960,3960,718,17,[],200,llama-13b,128,1,548.0,1.0,1,H100,1697101634970,1697101635518.0,120,13.0,1.0,"[37, 511]","[1697101635007, 1697101635518]"
3961,3961,384,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672658,1697101675812.0,120,,,"[18, 1083, 129, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672676, 1697101673759, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3962,3962,922,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671953,1697101675810.0,120,,,"[6, 587, 111, 113, 91, 68, 68, 88, 803, 98, 89, 88, 66, 66, 686, 103, 98, 95, 70, 94]","[1697101671959, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675084, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
3963,3963,257,26,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101651610,1697101652661.0,120,14.0,1.0,"[186, 865]","[1697101651796, 1697101652661]"
3964,3964,611,27,[],200,llama-13b,128,1,815.0,1.0,1,H100,1697101652662,1697101653477.0,120,14.0,1.0,"[42, 773]","[1697101652704, 1697101653477]"
3965,3965,349,31,[],200,llama-13b,128,1,4297.0,1.0,1,H100,1697101675813,1697101680110.0,120,88.0,20.0,"[96, 471, 37, 1060, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98, 95, 71, 93, 90, 70, 90]","[1697101675909, 1697101676380, 1697101676417, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679860, 1697101679950, 1697101680020, 1697101680110]"
3966,3966,920,29,[],200,llama-13b,128,1,1324.0,1.0,1,H100,1697101663566,1697101664890.0,120,96.0,4.0,"[12, 867, 233, 109, 102]","[1697101663578, 1697101664445, 1697101664678, 1697101664787, 1697101664889]"
3967,3967,399,21,[],200,llama-13b,128,1,4735.0,1.0,1,H100,1697101644507,1697101649242.0,120,87.0,20.0,"[7, 823, 103, 79, 77, 78, 75, 740, 98, 97, 84, 78, 1236, 96, 101, 100, 97, 88, 485, 97, 96]","[1697101644514, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645749, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649146, 1697101649242]"
3968,3968,60,26,[],200,llama-13b,128,1,8034.0,1.0,1,H100,1697101675815,1697101683849.0,120,93.0,36.0,"[477, 1185, 64, 51, 763, 89, 86, 64, 84, 80, 647, 99, 98, 94, 72, 92, 90, 70, 91, 468, 95, 57, 73, 803, 85, 83, 79, 78, 771, 97, 93, 91, 69, 91, 90, 614]","[1697101676292, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680579, 1697101680674, 1697101680731, 1697101680804, 1697101681607, 1697101681692, 1697101681775, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848]"
3969,3969,348,30,[],200,llama-13b,128,1,3954.0,1.0,1,H100,1697101664890,1697101668844.0,120,91.0,20.0,"[19, 830, 129, 106, 100, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90]","[1697101664909, 1697101665739, 1697101665868, 1697101665974, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844]"
3970,3970,56,24,[],200,llama-13b,128,1,4318.0,1.0,1,H100,1697101669570,1697101673888.0,120,86.0,20.0,"[18, 897, 226, 99, 71, 92, 91, 90, 511, 98, 95, 94, 93, 90, 521, 114, 91, 68, 68, 88, 803]","[1697101669588, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888]"
3971,3971,495,24,[],200,llama-13b,128,1,1010.0,1.0,1,H100,1697101657150,1697101658160.0,120,13.0,1.0,"[36, 974]","[1697101657186, 1697101658160]"
3972,3972,853,25,[],200,llama-13b,128,1,5497.0,1.0,1,H100,1697101658161,1697101663658.0,120,364.0,22.0,"[13, 1199, 332, 101, 99, 97, 92, 86, 805, 101, 76, 98, 93, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93]","[1697101658174, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658]"
3973,3973,739,24,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,216.0,1.0,"[518, 1046]","[1697101676334, 1697101677380]"
3974,3974,255,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101677382,1697101689614.0,120,,,"[97, 768, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 104, 57, 73, 803, 84, 83, 79, 79, 771, 96, 93, 91, 69, 91, 90, 615, 101, 101, 97, 92, 90, 90, 593, 86, 64, 82, 81, 911, 91, 89, 87, 86, 647, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 91, 89]","[1697101677479, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680675, 1697101680732, 1697101680805, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
3975,3975,711,32,[],200,llama-13b,128,1,1663.0,1.0,1,H100,1697101680112,1697101681775.0,120,457.0,4.0,"[18, 1372, 106, 84, 83]","[1697101680130, 1697101681502, 1697101681608, 1697101681692, 1697101681775]"
3976,3976,843,22,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101649243,1697101649912.0,120,14.0,1.0,"[60, 609]","[1697101649303, 1697101649912]"
3977,3977,273,23,[],200,llama-13b,128,1,1395.0,1.0,1,H100,1697101649913,1697101651308.0,120,19.0,1.0,"[54, 1341]","[1697101649967, 1697101651308]"
3978,3978,915,28,[],200,llama-13b,128,1,921.0,1.0,1,H100,1697101680581,1697101681502.0,120,182.0,1.0,"[31, 890]","[1697101680612, 1697101681502]"
3979,3979,72,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687340,1697101689594.0,120,,,"[12, 1033, 133, 106, 97, 92, 92, 89]","[1697101687352, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3980,3980,631,24,[],200,llama-13b,128,1,12255.0,1.0,1,H100,1697101651309,1697101663564.0,120,216.0,50.0,"[24, 1385, 892, 85, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 91, 614, 106, 100, 99, 97, 83, 623, 81, 79, 59, 836, 95, 82, 82, 81, 1076, 101, 98, 98, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100]","[1697101651333, 1697101652718, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655512, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658384, 1697101658466, 1697101658548, 1697101658629, 1697101659705, 1697101659806, 1697101659904, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564]"
3981,3981,139,33,[],200,llama-13b,128,1,4461.0,1.0,1,H100,1697101681776,1697101686237.0,120,39.0,21.0,"[6, 729, 193, 97, 92, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90, 593, 86, 64, 82, 81, 911]","[1697101681782, 1697101682511, 1697101682704, 1697101682801, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237]"
3982,3982,347,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101681503,1697101689595.0,120,,,"[30, 1171, 96, 93, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90, 593, 86, 64, 83, 80, 911, 91, 89, 87, 86, 647, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101681533, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
3983,3983,120,27,[],200,llama-13b,128,1,1420.0,1.0,1,H100,1697101661917,1697101663337.0,120,17.0,1.0,"[24, 1396]","[1697101661941, 1697101663337]"
3984,3984,602,31,[],200,llama-13b,128,1,1101.0,1.0,1,H100,1697101672658,1697101673759.0,120,15.0,1.0,"[48, 1052]","[1697101672706, 1697101673758]"
3985,3985,470,28,[],200,llama-13b,128,1,1340.0,1.0,1,H100,1697101663338,1697101664678.0,120,39.0,2.0,"[6, 1101, 233]","[1697101663344, 1697101664445, 1697101664678]"
3986,3986,656,33,[],200,llama-13b,128,1,1880.0,1.0,1,H100,1697101697595,1697101699475.0,120,26.0,1.0,"[200, 1680]","[1697101697795, 1697101699475]"
3987,3987,81,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699476,1697101700637.0,120,,,"[7, 1069]","[1697101699483, 1697101700552]"
3988,3988,31,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673759,1697101675812.0,120,,,"[7, 1082, 133, 102, 98, 95, 70, 95]","[1697101673766, 1697101674848, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
3989,3989,439,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[288, 1229, 123, 69]","[1697101700930, 1697101702159, 1697101702282, 1697101702351]"
3990,3990,481,27,[],200,llama-13b,128,1,834.0,1.0,1,H100,1697101670726,1697101671560.0,120,10.0,1.0,"[47, 787]","[1697101670773, 1697101671560]"
3991,3991,385,33,[],200,llama-13b,128,1,9197.0,1.0,1,H100,1697101675816,1697101685013.0,120,52.0,43.0,"[299, 1265, 97, 64, 50, 764, 89, 86, 65, 83, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461, 103, 57, 73, 803, 85, 83, 79, 78, 771, 97, 92, 92, 69, 91, 90, 614, 102, 101, 97, 91, 91, 90, 593]","[1697101676115, 1697101677380, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681607, 1697101681692, 1697101681775, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682892, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684239, 1697101684330, 1697101684420, 1697101685013]"
3992,3992,142,36,[],200,llama-13b,128,1,4634.0,1.0,1,H100,1697101682705,1697101687339.0,120,52.0,20.0,"[30, 906, 208, 102, 100, 97, 92, 90, 90, 594, 85, 64, 83, 81, 910, 91, 89, 87, 86, 647, 102]","[1697101682735, 1697101683641, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685099, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339]"
3993,3993,838,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671560,1697101675812.0,120,,,"[31, 955, 111, 113, 91, 68, 68, 87, 804, 98, 89, 88, 66, 65, 687, 102, 99, 94, 71, 94]","[1697101671591, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675182, 1697101675276, 1697101675347, 1697101675441]"
3994,3994,924,21,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101651610,1697101652661.0,120,9.0,1.0,"[98, 953]","[1697101651708, 1697101652661]"
3995,3995,357,22,[],200,llama-13b,128,1,7340.0,1.0,1,H100,1697101652662,1697101660002.0,120,52.0,33.0,"[42, 773, 133, 86, 83, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 614, 105, 100, 99, 97, 83, 623, 81, 79, 60, 835, 95, 83, 82, 80, 1076, 101, 99, 97]","[1697101652704, 1697101653477, 1697101653610, 1697101653696, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658384, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002]"
3996,3996,802,36,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703175,1697101704899.0,120,9.0,1.0,"[37, 1687]","[1697101703212, 1697101704899]"
3997,3997,503,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687340,1697101689594.0,120,,,"[12, 1033, 133, 106, 97, 92, 92, 89]","[1697101687352, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
3998,3998,263,29,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,15.0,1.0,"[506, 1058]","[1697101676322, 1697101677380]"
3999,3999,830,38,[],200,llama-13b,128,1,3861.0,1.0,1,H100,1697101689618,1697101693479.0,120,140.0,9.0,"[373, 2039, 244, 213, 212, 78, 101, 102, 94, 405]","[1697101689991, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693479]"
4000,4000,259,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693480,1697101697588.0,120,,,"[12, 965, 97, 77, 101, 84, 604, 78, 77, 832, 102, 101, 111, 91, 91]","[1697101693492, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
4001,4001,623,30,[],200,llama-13b,128,1,1064.0,1.0,1,H100,1697101677381,1697101678445.0,120,140.0,3.0,"[25, 948, 91]","[1697101677406, 1697101678354, 1697101678445]"
4002,4002,19,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",9999,llama-13b,128,1,,,1,H100,1697101678446,,120,,,"[30, 812, 117, 99, 98, 94, 72, 92, 91, 69, 91, 460, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97, 92, 90, 90, 593, 86, 64, 83, 80, 911, 91, 89, 87, 86, 647, 101, 99, 95, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101678476, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
4003,4003,205,37,[],200,llama-13b,128,1,4589.0,1.0,1,H100,1697101704901,1697101709490.0,120,87.0,20.0,"[12, 865, 135, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 87, 544, 99, 65, 88, 811]","[1697101704913, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490]"
4004,4004,563,38,[],200,llama-13b,128,1,2940.0,1.0,1,H100,1697101709492,1697101712432.0,120,874.0,18.0,"[12, 923, 278, 103, 97, 92, 89, 88, 86, 498, 98, 91, 85, 82, 79, 59, 69, 51, 60]","[1697101709504, 1697101710427, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432]"
4005,4005,394,26,[],200,llama-13b,128,1,1868.0,1.0,1,H100,1697101651610,1697101653478.0,120,11.0,1.0,"[486, 1382]","[1697101652096, 1697101653478]"
4006,4006,721,34,[],200,llama-13b,128,1,891.0,1.0,1,H100,1697101692889,1697101693780.0,120,286.0,5.0,"[11, 472, 107, 101, 100, 100]","[1697101692900, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693780]"
4007,4007,146,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693781,1697101697588.0,120,,,"[12, 664, 97, 77, 101, 84, 604, 78, 77, 832, 102, 101, 111, 91, 91]","[1697101693793, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
4008,4008,748,27,[],200,llama-13b,128,1,3049.0,1.0,1,H100,1697101653480,1697101656529.0,120,182.0,14.0,"[96, 1261, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98]","[1697101653576, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529]"
4009,4009,506,36,[],200,llama-13b,128,1,1875.0,1.0,1,H100,1697101697600,1697101699475.0,120,16.0,1.0,"[482, 1393]","[1697101698082, 1697101699475]"
4010,4010,162,33,[],200,llama-13b,128,1,5116.0,1.0,1,H100,1697101689616,1697101694732.0,120,90.0,20.0,"[300, 1196, 93, 1069, 214, 211, 79, 109, 93, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101]","[1697101689916, 1697101691112, 1697101691205, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692887, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732]"
4011,4011,670,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[76, 1289, 223, 103, 101, 110, 92, 91]","[1697101694894, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4012,4012,67,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697603,1697101700636.0,120,,,"[569, 1303, 113, 83, 82, 81, 81, 82]","[1697101698172, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4013,4013,914,28,[],200,llama-13b,128,1,4757.0,1.0,1,H100,1697101675814,1697101680571.0,120,84.0,20.0,"[277, 1289, 97, 64, 51, 763, 89, 86, 65, 83, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101676091, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
4014,4014,839,20,[],200,llama-13b,128,1,2540.0,1.0,1,H100,1697101675814,1697101678354.0,120,58.0,5.0,"[164, 1401, 98, 64, 50, 763]","[1697101675978, 1697101677379, 1697101677477, 1697101677541, 1697101677591, 1697101678354]"
4015,4015,565,21,[],200,llama-13b,128,1,5219.0,1.0,1,H100,1697101661917,1697101667136.0,120,91.0,20.0,"[6, 1414, 127, 101, 93, 88, 87, 845, 108, 104, 95, 94, 91, 698, 105, 100, 72, 72, 93, 92, 734]","[1697101661923, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664890, 1697101664985, 1697101665079, 1697101665170, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136]"
4016,4016,343,29,[],200,llama-13b,128,1,3840.0,1.0,1,H100,1697101680581,1697101684421.0,120,84.0,20.0,"[93, 828, 106, 84, 83, 79, 79, 771, 96, 93, 91, 69, 91, 90, 615, 101, 101, 97, 92, 90, 91]","[1697101680674, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684421]"
4017,4017,612,18,[],200,llama-13b,128,1,5235.0,1.0,1,H100,1697101644006,1697101649241.0,120,93.0,20.0,"[6, 1428, 78, 78, 77, 75, 742, 97, 98, 83, 78, 1236, 96, 101, 101, 96, 88, 485, 96, 96]","[1697101644012, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648082, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649241]"
4018,4018,269,21,[],200,llama-13b,128,1,932.0,1.0,1,H100,1697101678356,1697101679288.0,120,11.0,1.0,"[12, 920]","[1697101678368, 1697101679288]"
4019,4019,80,22,[],200,llama-13b,128,1,643.0,1.0,1,H100,1697101661273,1697101661916.0,120,13.0,1.0,"[37, 606]","[1697101661310, 1697101661916]"
4020,4020,437,23,[],200,llama-13b,128,1,6746.0,1.0,1,H100,1697101661917,1697101668663.0,120,91.0,29.0,"[18, 1402, 127, 101, 93, 88, 87, 845, 108, 104, 95, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71]","[1697101661935, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664890, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663]"
4021,4021,627,22,[],200,llama-13b,128,1,4762.0,1.0,1,H100,1697101679289,1697101684051.0,120,93.0,20.0,"[30, 1252, 103, 58, 72, 804, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 102, 100]","[1697101679319, 1697101680571, 1697101680674, 1697101680732, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051]"
4022,4022,445,24,[],200,llama-13b,128,1,1299.0,1.0,1,H100,1697101694121,1697101695420.0,120,457.0,2.0,"[7, 1208, 84]","[1697101694128, 1697101695336, 1697101695420]"
4023,4023,803,25,[],200,llama-13b,128,1,760.0,1.0,1,H100,1697101695423,1697101696183.0,120,20.0,1.0,"[23, 737]","[1697101695446, 1697101696183]"
4024,4024,37,19,[],200,llama-13b,128,1,668.0,1.0,1,H100,1697101649243,1697101649911.0,120,20.0,1.0,"[36, 632]","[1697101649279, 1697101649911]"
4025,4025,909,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697590,1697101700635.0,120,,,"[221, 1664, 113, 83, 82, 81, 81, 82]","[1697101697811, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4026,4026,390,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649913,1697101651604.0,120,,,"[42, 1353]","[1697101649955, 1697101651308]"
4027,4027,143,18,[],200,llama-13b,128,1,3720.0,1.0,1,H100,1697101635519,1697101639239.0,120,6.0,12.0,"[75, 1884, 190, 91, 92, 87, 87, 82, 81, 768, 98, 92, 93]","[1697101635594, 1697101637478, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638025, 1697101638107, 1697101638188, 1697101638956, 1697101639054, 1697101639146, 1697101639239]"
4028,4028,224,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696184,1697101697589.0,120,,,"[37, 1174]","[1697101696221, 1697101697395]"
4029,4029,836,21,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101651610,1697101652661.0,120,11.0,1.0,"[182, 869]","[1697101651792, 1697101652661]"
4030,4030,269,22,[],200,llama-13b,128,1,815.0,1.0,1,H100,1697101652662,1697101653477.0,120,11.0,1.0,"[30, 780]","[1697101652692, 1697101653472]"
4031,4031,583,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697596,1697101700635.0,120,,,"[307, 1572, 112, 84, 82, 82, 80, 82]","[1697101697903, 1697101699475, 1697101699587, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
4032,4032,632,23,[],200,llama-13b,128,1,4810.0,1.0,1,H100,1697101653479,1697101658289.0,120,91.0,20.0,"[41, 1111, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 98, 82, 623, 81, 79, 60, 835]","[1697101653520, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656529, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
4033,4033,501,19,[],200,llama-13b,128,1,846.0,1.0,1,H100,1697101639240,1697101640086.0,120,19.0,1.0,"[6, 840]","[1697101639246, 1697101640086]"
4034,4034,860,20,[],200,llama-13b,128,1,4333.0,1.0,1,H100,1697101640087,1697101644420.0,120,85.0,20.0,"[12, 903, 117, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 801, 99, 96, 92, 95, 86, 86, 415]","[1697101640099, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643550, 1697101643646, 1697101643738, 1697101643833, 1697101643919, 1697101644005, 1697101644420]"
4035,4035,11,28,[],200,llama-13b,128,1,3836.0,1.0,1,H100,1697101653479,1697101657315.0,120,732.0,17.0,"[53, 1099, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 98, 82, 623, 81]","[1697101653532, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656529, 1697101656611, 1697101657234, 1697101657315]"
4036,4036,911,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[378, 1139, 123, 69]","[1697101701020, 1697101702159, 1697101702282, 1697101702351]"
4037,4037,754,29,[],200,llama-13b,128,1,1889.0,1.0,1,H100,1697101658291,1697101660180.0,120,88.0,7.0,"[18, 1396, 102, 98, 97, 93, 85]","[1697101658309, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180]"
4038,4038,681,21,[],200,llama-13b,128,1,1496.0,1.0,1,H100,1697101689616,1697101691112.0,120,23.0,1.0,"[295, 1201]","[1697101689911, 1697101691112]"
4039,4039,151,29,[],200,llama-13b,128,1,1622.0,1.0,1,H100,1697101672137,1697101673759.0,120,39.0,1.0,"[6, 1615]","[1697101672143, 1697101673758]"
4040,4040,179,30,[],200,llama-13b,128,1,2298.0,1.0,1,H100,1697101660181,1697101662479.0,120,161.0,4.0,"[7, 1728, 355, 107, 101]","[1697101660188, 1697101661916, 1697101662271, 1697101662378, 1697101662479]"
4041,4041,82,22,[],200,llama-13b,128,1,3703.0,1.0,1,H100,1697101691113,1697101694816.0,120,67.0,20.0,"[25, 892, 244, 213, 212, 79, 110, 93, 94, 403, 102, 100, 99, 95, 91, 87, 68, 433, 78, 100, 85]","[1697101691138, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692888, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816]"
4042,4042,510,30,[],200,llama-13b,128,1,1222.0,1.0,1,H100,1697101673759,1697101674981.0,120,79.0,2.0,"[7, 1082, 133]","[1697101673766, 1697101674848, 1697101674981]"
4043,4043,871,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674982,1697101675811.0,120,,,"[37, 770]","[1697101675019, 1697101675789]"
4044,4044,303,32,[],200,llama-13b,128,1,4764.0,1.0,1,H100,1697101675816,1697101680580.0,120,88.0,20.0,"[592, 972, 97, 65, 50, 762, 90, 86, 64, 84, 80, 647, 99, 97, 95, 72, 92, 90, 70, 90, 470]","[1697101676408, 1697101677380, 1697101677477, 1697101677542, 1697101677592, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680580]"
4045,4045,539,31,[],200,llama-13b,128,1,4656.0,1.0,1,H100,1697101662480,1697101667136.0,120,83.0,20.0,"[7, 850, 127, 101, 93, 89, 86, 845, 108, 103, 96, 94, 101, 687, 106, 101, 71, 72, 93, 92, 734]","[1697101662487, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665867, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136]"
4046,4046,805,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689618,1697101697588.0,120,,,"[397, 2015, 244, 213, 212, 78, 101, 102, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 100, 84, 605, 79, 76, 831, 103, 101, 110, 92, 91]","[1697101690015, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694815, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4047,4047,60,24,[],200,llama-13b,128,1,8020.0,1.0,1,H100,1697101658290,1697101666310.0,120,93.0,36.0,"[7, 1408, 102, 98, 97, 93, 85, 805, 101, 75, 99, 93, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 101, 688, 105, 100, 72, 72, 93]","[1697101658297, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310]"
4048,4048,45,21,[],200,llama-13b,128,1,1500.0,1.0,1,H100,1697101689612,1697101691112.0,120,19.0,1.0,"[95, 1405]","[1697101689707, 1697101691112]"
4049,4049,378,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101691113,1697101697588.0,120,,,"[48, 1113, 214, 211, 79, 111, 91, 95, 404, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 85, 604, 78, 77, 831, 103, 101, 110, 92, 91]","[1697101691161, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692889, 1697101692980, 1697101693075, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4050,4050,186,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[144, 1377, 123, 69]","[1697101700782, 1697101702159, 1697101702282, 1697101702351]"
4051,4051,67,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697602,1697101700636.0,120,,,"[503, 1370, 113, 83, 82, 81, 81, 82]","[1697101698105, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4052,4052,900,32,[],200,llama-13b,128,1,1618.0,1.0,1,H100,1697101667137,1697101668755.0,120,67.0,6.0,"[31, 1095, 133, 101, 95, 71, 91]","[1697101667168, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754]"
4053,4053,416,45,[],200,llama-13b,128,1,4941.0,1.0,1,H100,1697101708314,1697101713255.0,120,286.0,50.0,"[19, 1041, 116, 89, 83, 82, 79, 882, 103, 97, 92, 89, 88, 86, 498, 99, 90, 85, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26, 24, 20, 20, 21, 19, 18, 19]","[1697101708333, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114, 1697101713138, 1697101713158, 1697101713178, 1697101713199, 1697101713218, 1697101713236, 1697101713255]"
4054,4054,542,42,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703176,1697101704900.0,120,11.0,1.0,"[142, 1582]","[1697101703318, 1697101704900]"
4055,4055,716,16,[],200,llama-13b,128,1,8095.0,1.0,1,H100,1697101651610,1697101659705.0,120,79.0,30.0,"[393, 1469, 138, 85, 83, 82, 81, 896, 108, 105, 101, 74, 100, 96, 92, 614, 105, 101, 98, 97, 83, 623, 81, 79, 59, 836, 95, 83, 82, 80, 1076]","[1697101652003, 1697101653472, 1697101653610, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657453, 1697101658289, 1697101658384, 1697101658467, 1697101658549, 1697101658629, 1697101659705]"
4056,4056,425,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703171.0,120,,,"[155, 1366, 123, 69]","[1697101700793, 1697101702159, 1697101702282, 1697101702351]"
4057,4057,871,43,[],200,llama-13b,128,1,1362.0,1.0,1,H100,1697101704901,1697101706263.0,120,123.0,6.0,"[36, 841, 135, 95, 87, 85, 83]","[1697101704937, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263]"
4058,4058,846,33,[],200,llama-13b,128,1,1535.0,1.0,1,H100,1697101682705,1697101684240.0,120,140.0,6.0,"[12, 924, 208, 102, 100, 97, 92]","[1697101682717, 1697101683641, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240]"
4059,4059,302,44,[],200,llama-13b,128,1,4544.0,1.0,1,H100,1697101706264,1697101710808.0,120,85.0,20.0,"[13, 768, 241, 96, 92, 70, 92, 69, 91, 87, 545, 99, 64, 88, 812, 88, 83, 82, 79, 881, 104]","[1697101706277, 1697101707045, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710808]"
4060,4060,680,41,[],200,llama-13b,128,1,2554.0,1.0,1,H100,1697101708533,1697101711087.0,120,123.0,11.0,"[43, 798, 116, 89, 83, 82, 79, 881, 103, 98, 92, 89]","[1697101708576, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710905, 1697101710997, 1697101711086]"
4061,4061,662,45,[],200,llama-13b,128,1,2002.0,1.0,1,H100,1697101710809,1697101712811.0,120,83.0,20.0,"[30, 821, 98, 99, 90, 85, 83, 78, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32]","[1697101710839, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811]"
4062,4062,57,24,[],200,llama-13b,128,1,1566.0,1.0,1,H100,1697101675814,1697101677380.0,120,13.0,1.0,"[170, 1395]","[1697101675984, 1697101677379]"
4063,4063,916,29,[],200,llama-13b,128,1,1060.0,1.0,1,H100,1697101664679,1697101665739.0,120,8.0,1.0,"[6, 1053]","[1697101664685, 1697101665738]"
4064,4064,897,22,[],200,llama-13b,128,1,795.0,1.0,1,H100,1697101665975,1697101666770.0,120,9.0,1.0,"[18, 777]","[1697101665993, 1697101666770]"
4065,4065,323,23,[],200,llama-13b,128,1,4895.0,1.0,1,H100,1697101666771,1697101671666.0,120,84.0,20.0,"[6, 1486, 133, 101, 95, 71, 91, 90, 725, 98, 95, 92, 92, 89, 676, 99, 71, 92, 91, 89, 513]","[1697101666777, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666]"
4066,4066,305,25,[],200,llama-13b,128,1,3954.0,1.0,1,H100,1697101664890,1697101668844.0,120,86.0,20.0,"[7, 841, 130, 106, 100, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90]","[1697101664897, 1697101665738, 1697101665868, 1697101665974, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844]"
4067,4067,345,30,[],200,llama-13b,128,1,4971.0,1.0,1,H100,1697101665740,1697101670711.0,120,39.0,20.0,"[30, 1000, 366, 103, 98, 73, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 92, 89, 676]","[1697101665770, 1697101666770, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711]"
4068,4068,499,36,[],200,llama-13b,128,1,4764.0,1.0,1,H100,1697101675816,1697101680580.0,120,88.0,20.0,"[475, 1089, 97, 64, 51, 763, 89, 86, 65, 82, 81, 647, 99, 98, 94, 72, 92, 90, 70, 91, 469]","[1697101676291, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678444, 1697101678530, 1697101678595, 1697101678677, 1697101678758, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680580]"
4069,4069,375,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697602,1697101700636.0,120,,,"[510, 1476, 83, 82, 81, 81, 82]","[1697101698112, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4070,4070,835,27,[],200,llama-13b,128,1,3954.0,1.0,1,H100,1697101664890,1697101668844.0,120,87.0,20.0,"[7, 841, 130, 106, 100, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90]","[1697101664897, 1697101665738, 1697101665868, 1697101665974, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844]"
4071,4071,207,26,[],200,llama-13b,128,1,950.0,1.0,1,H100,1697101664788,1697101665738.0,120,10.0,1.0,"[30, 920]","[1697101664818, 1697101665738]"
4072,4072,709,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700645,1697101703171.0,120,,,"[538, 1852]","[1697101701183, 1697101703035]"
4073,4073,261,28,[],200,llama-13b,128,1,1866.0,1.0,1,H100,1697101668845,1697101670711.0,120,874.0,2.0,"[37, 1829]","[1697101668882, 1697101670711]"
4074,4074,594,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671065,1697101675812.0,120,,,"[6, 489, 106, 97, 95, 94, 93, 91, 520, 113, 92, 68, 67, 88, 804, 98, 89, 88, 66, 66, 686, 102, 98, 95, 71, 94]","[1697101671071, 1697101671560, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675347, 1697101675441]"
4075,4075,615,29,[],200,llama-13b,128,1,4264.0,1.0,1,H100,1697101670717,1697101674981.0,120,93.0,20.0,"[20, 929, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687]","[1697101670737, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981]"
4076,4076,817,20,[],200,llama-13b,128,1,4208.0,1.0,1,H100,1697101640212,1697101644420.0,120,86.0,20.0,"[42, 748, 117, 85, 78, 78, 761, 100, 99, 96, 89, 82, 63, 800, 99, 97, 92, 95, 87, 85, 415]","[1697101640254, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643549, 1697101643646, 1697101643738, 1697101643833, 1697101643920, 1697101644005, 1697101644420]"
4077,4077,861,37,[],200,llama-13b,128,1,920.0,1.0,1,H100,1697101680582,1697101681502.0,120,10.0,1.0,"[36, 884]","[1697101680618, 1697101681502]"
4078,4078,293,38,[],200,llama-13b,128,1,3824.0,1.0,1,H100,1697101681503,1697101685327.0,120,91.0,20.0,"[6, 1002, 193, 96, 93, 92, 69, 90, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 81]","[1697101681509, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327]"
4079,4079,784,31,[],200,llama-13b,128,1,4494.0,1.0,1,H100,1697101666218,1697101670712.0,120,89.0,20.0,"[13, 539, 366, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 726, 98, 95, 92, 93, 88, 676]","[1697101666231, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711]"
4080,4080,140,32,[],200,llama-13b,128,1,4308.0,1.0,1,H100,1697101680112,1697101684420.0,120,96.0,20.0,"[19, 1371, 106, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680131, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
4081,4081,250,25,[],200,llama-13b,128,1,958.0,1.0,1,H100,1697101662379,1697101663337.0,120,31.0,1.0,"[25, 933]","[1697101662404, 1697101663337]"
4082,4082,575,26,[],200,llama-13b,128,1,4265.0,1.0,1,H100,1697101663338,1697101667603.0,120,86.0,20.0,"[30, 1077, 233, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95]","[1697101663368, 1697101664445, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603]"
4083,4083,924,22,[],200,llama-13b,128,1,1126.0,1.0,1,H100,1697101667137,1697101668263.0,120,9.0,1.0,"[13, 1113]","[1697101667150, 1697101668263]"
4084,4084,891,22,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101648381,1697101649050.0,120,52.0,2.0,"[12, 554, 103]","[1697101648393, 1697101648947, 1697101649050]"
4085,4085,353,23,[],200,llama-13b,128,1,1498.0,1.0,1,H100,1697101668264,1697101669762.0,120,52.0,4.0,"[6, 1174, 125, 98, 94]","[1697101668270, 1697101669444, 1697101669569, 1697101669667, 1697101669761]"
4086,4086,717,24,[],200,llama-13b,128,1,4127.0,1.0,1,H100,1697101669762,1697101673889.0,120,89.0,20.0,"[7, 716, 226, 99, 71, 92, 91, 90, 512, 97, 95, 94, 93, 90, 521, 114, 91, 68, 68, 88, 803]","[1697101669769, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888]"
4087,4087,655,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697601,1697101700636.0,120,,,"[529, 1345, 113, 83, 82, 81, 81, 82]","[1697101698130, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4088,4088,80,41,[],200,llama-13b,128,1,1518.0,1.0,1,H100,1697101700642,1697101702160.0,120,13.0,1.0,"[252, 1266]","[1697101700894, 1697101702160]"
4089,4089,345,29,[],200,llama-13b,128,1,5351.0,1.0,1,H100,1697101703175,1697101708526.0,120,39.0,20.0,"[352, 1372, 116, 98, 72, 728, 95, 87, 85, 83, 79, 944, 96, 92, 70, 92, 69, 91, 87, 545, 98]","[1697101703527, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
4090,4090,800,35,[],200,llama-13b,128,1,4266.0,1.0,1,H100,1697101670715,1697101674981.0,120,140.0,20.0,"[10, 941, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 805, 97, 89, 88, 66, 65, 687]","[1697101670725, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673889, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981]"
4091,4091,470,36,[],200,llama-13b,128,1,1979.0,1.0,1,H100,1697101703036,1697101705015.0,120,39.0,2.0,"[18, 634, 1320]","[1697101703054, 1697101703688, 1697101705008]"
4092,4092,832,37,[],200,llama-13b,128,1,760.0,1.0,1,H100,1697101705018,1697101705778.0,120,15.0,1.0,"[57, 703]","[1697101705075, 1697101705778]"
4093,4093,349,38,[],200,llama-13b,128,1,5028.0,1.0,1,H100,1697101705779,1697101710807.0,120,88.0,20.0,"[60, 1206, 241, 96, 92, 70, 91, 70, 91, 87, 544, 99, 66, 87, 812, 88, 83, 82, 79, 881, 103]","[1697101705839, 1697101707045, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708592, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807]"
4094,4094,148,26,[],200,llama-13b,128,1,500.0,1.0,1,H100,1697101672046,1697101672546.0,120,16.0,1.0,"[12, 488]","[1697101672058, 1697101672546]"
4095,4095,339,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[331, 1186, 123, 69]","[1697101700973, 1697101702159, 1697101702282, 1697101702351]"
4096,4096,365,29,[],200,llama-13b,128,1,844.0,1.0,1,H100,1697101657316,1697101658160.0,120,23.0,1.0,"[7, 837]","[1697101657323, 1697101658160]"
4097,4097,521,32,[],200,llama-13b,128,1,807.0,1.0,1,H100,1697101674982,1697101675789.0,120,18.0,1.0,"[42, 765]","[1697101675024, 1697101675789]"
4098,4098,332,33,[],200,llama-13b,128,1,688.0,1.0,1,H100,1697101668756,1697101669444.0,120,39.0,1.0,"[12, 676]","[1697101668768, 1697101669444]"
4099,4099,303,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[100, 1265, 223, 103, 101, 110, 92, 92]","[1697101694918, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696904]"
4100,4100,662,34,[],200,llama-13b,128,1,4443.0,1.0,1,H100,1697101669445,1697101673888.0,120,83.0,20.0,"[24, 1016, 226, 99, 71, 92, 91, 90, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 68, 87, 804]","[1697101669469, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673084, 1697101673888]"
4101,4101,659,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697603,1697101700636.0,120,,,"[581, 1291, 113, 83, 82, 81, 81, 82]","[1697101698184, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4102,4102,205,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697590,1697101700636.0,120,,,"[420, 1466, 112, 83, 82, 81, 81, 82]","[1697101698010, 1697101699476, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4103,4103,558,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[60, 1461, 123, 69]","[1697101700698, 1697101702159, 1697101702282, 1697101702351]"
4104,4104,918,39,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703175,1697101704899.0,120,23.0,1.0,"[95, 1629]","[1697101703270, 1697101704899]"
4105,4105,342,40,[],200,llama-13b,128,1,2895.0,1.0,1,H100,1697101704901,1697101707796.0,120,364.0,14.0,"[42, 835, 135, 95, 87, 85, 83, 80, 942, 96, 93, 70, 91, 70, 91]","[1697101704943, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796]"
4106,4106,453,29,[],200,llama-13b,128,1,565.0,1.0,1,H100,1697101675815,1697101676380.0,120,26.0,1.0,"[70, 495]","[1697101675885, 1697101676380]"
4107,4107,812,30,[],200,llama-13b,128,1,1866.0,1.0,1,H100,1697101676381,1697101678247.0,120,16.0,1.0,"[97, 1769]","[1697101676478, 1697101678247]"
4108,4108,417,25,[],200,llama-13b,128,1,866.0,1.0,1,H100,1697101677381,1697101678247.0,120,17.0,1.0,"[30, 836]","[1697101677411, 1697101678247]"
4109,4109,771,26,[],200,llama-13b,128,1,4456.0,1.0,1,H100,1697101678248,1697101682704.0,120,47.0,20.0,"[36, 1004, 117, 99, 98, 94, 72, 92, 90, 70, 91, 460, 103, 57, 73, 804, 84, 83, 79, 79, 771]","[1697101678284, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704]"
4110,4110,459,34,[],200,llama-13b,128,1,2158.0,1.0,1,H100,1697101697595,1697101699753.0,120,58.0,5.0,"[25, 675, 43, 1250, 83, 82]","[1697101697620, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753]"
4111,4111,819,35,[],200,llama-13b,128,1,798.0,1.0,1,H100,1697101699754,1697101700552.0,120,13.0,1.0,"[6, 792]","[1697101699760, 1697101700552]"
4112,4112,335,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700553,1697101703170.0,120,,,"[36, 505, 1188, 69]","[1697101700589, 1697101701094, 1697101702282, 1697101702351]"
4113,4113,699,37,[],200,llama-13b,128,1,1726.0,1.0,1,H100,1697101703174,1697101704900.0,120,39.0,1.0,"[212, 1514]","[1697101703386, 1697101704900]"
4114,4114,130,38,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,14.0,1.0,"[86, 791]","[1697101704987, 1697101705778]"
4115,4115,487,39,[],200,llama-13b,128,1,3965.0,1.0,1,H100,1697101705779,1697101709744.0,120,123.0,17.0,"[12, 1494, 97, 92, 70, 91, 70, 91, 87, 544, 99, 65, 88, 812, 88, 83, 82]","[1697101705791, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744]"
4116,4116,250,26,[],200,llama-13b,128,1,787.0,1.0,1,H100,1697101663659,1697101664446.0,120,31.0,1.0,"[18, 769]","[1697101663677, 1697101664446]"
4117,4117,435,41,[],200,llama-13b,128,1,6995.0,1.0,1,H100,1697101689615,1697101696610.0,120,563.0,27.0,"[255, 1242, 93, 1069, 213, 212, 79, 110, 92, 94, 405, 101, 100, 99, 95, 91, 87, 68, 433, 78, 101, 83, 605, 79, 76, 831, 103, 101]","[1697101689870, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692888, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694732, 1697101694815, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610]"
4118,4118,791,23,[],200,llama-13b,128,1,12802.0,1.0,1,H100,1697101662379,1697101675181.0,120,182.0,64.0,"[36, 1049, 101, 93, 89, 86, 845, 108, 103, 96, 94, 100, 689, 105, 101, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 98, 71, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98]","[1697101662415, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181]"
4119,4119,284,27,[],200,llama-13b,128,1,6289.0,1.0,1,H100,1697101682705,1697101688994.0,120,90.0,31.0,"[24, 1120, 102, 100, 97, 92, 90, 90, 593, 86, 64, 82, 82, 910, 91, 89, 87, 86, 647, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 91, 90]","[1697101682729, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688994]"
4120,4120,113,24,[],200,llama-13b,128,1,1547.0,1.0,1,H100,1697101656613,1697101658160.0,120,13.0,1.0,"[25, 1522]","[1697101656638, 1697101658160]"
4121,4121,792,42,[],200,llama-13b,128,1,785.0,1.0,1,H100,1697101696611,1697101697396.0,120,11.0,1.0,"[6, 779]","[1697101696617, 1697101697396]"
4122,4122,223,43,[],200,llama-13b,128,1,376.0,1.0,1,H100,1697101697397,1697101697773.0,120,16.0,1.0,"[30, 345]","[1697101697427, 1697101697772]"
4123,4123,573,44,[],200,llama-13b,128,1,1815.0,1.0,1,H100,1697101697773,1697101699588.0,120,874.0,2.0,"[423, 1392]","[1697101698196, 1697101699588]"
4124,4124,901,45,[],200,llama-13b,128,1,963.0,1.0,1,H100,1697101699589,1697101700552.0,120,17.0,1.0,"[12, 951]","[1697101699601, 1697101700552]"
4125,4125,332,46,[],200,llama-13b,128,1,541.0,1.0,1,H100,1697101700554,1697101701095.0,120,39.0,1.0,"[53, 488]","[1697101700607, 1697101701095]"
4126,4126,693,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101701096,1697101703171.0,120,,,"[99, 1840]","[1697101701195, 1697101703035]"
4127,4127,564,27,[],200,llama-13b,128,1,4972.0,1.0,1,H100,1697101665739,1697101670711.0,120,84.0,20.0,"[7, 1024, 366, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 92, 89, 676]","[1697101665746, 1697101666770, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711]"
4128,4128,79,22,[],200,llama-13b,128,1,486.0,1.0,1,H100,1697101643835,1697101644321.0,120,12.0,1.0,"[31, 455]","[1697101643866, 1697101644321]"
4129,4129,414,23,[],200,llama-13b,128,1,4920.0,1.0,1,H100,1697101644322,1697101649242.0,120,87.0,20.0,"[24, 991, 103, 79, 77, 77, 75, 741, 98, 98, 83, 78, 1237, 95, 101, 101, 96, 88, 485, 96, 97]","[1697101644346, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646685, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648380, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242]"
4130,4130,724,18,[],200,llama-13b,128,1,1868.0,1.0,1,H100,1697101651610,1697101653478.0,120,11.0,1.0,"[491, 1377]","[1697101652101, 1697101653478]"
4131,4131,152,19,[],200,llama-13b,128,1,4809.0,1.0,1,H100,1697101653480,1697101658289.0,120,87.0,20.0,"[108, 1043, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 82, 623, 81, 79, 60, 835]","[1697101653588, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
4132,4132,927,30,[],200,llama-13b,128,1,4756.0,1.0,1,H100,1697101675815,1697101680571.0,120,83.0,20.0,"[166, 1398, 98, 64, 50, 763, 90, 86, 64, 84, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101675981, 1697101677379, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
4133,4133,413,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649051,1697101651605.0,120,,,"[12, 849, 354, 106, 104, 101, 91, 85]","[1697101649063, 1697101649912, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
4134,4134,770,24,[],200,llama-13b,128,1,1862.0,1.0,1,H100,1697101651610,1697101653472.0,120,13.0,1.0,"[270, 1592]","[1697101651880, 1697101653472]"
4135,4135,202,25,[],200,llama-13b,128,1,15364.0,1.0,1,H100,1697101653479,1697101668843.0,120,874.0,72.0,"[60, 1092, 206, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 98, 82, 623, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97, 92, 86, 805, 101, 75, 99, 92, 87, 832, 107, 101, 95, 94, 89, 707, 100, 94, 88, 87, 845, 108, 103, 96, 94, 91, 697, 106, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 792, 102, 95, 71, 91, 89]","[1697101653539, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656529, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661439, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663564, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665170, 1697101665867, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668395, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843]"
4136,4136,510,20,[],200,llama-13b,128,1,1415.0,1.0,1,H100,1697101658290,1697101659705.0,120,79.0,2.0,"[19, 1064, 332]","[1697101658309, 1697101659373, 1697101659705]"
4137,4137,406,32,[],200,llama-13b,128,1,1126.0,1.0,1,H100,1697101661354,1697101662480.0,120,244.0,4.0,"[12, 550, 356, 106, 102]","[1697101661366, 1697101661916, 1697101662272, 1697101662378, 1697101662480]"
4138,4138,542,31,[],200,llama-13b,128,1,2413.0,1.0,1,H100,1697101689617,1697101692030.0,120,11.0,1.0,"[374, 2039]","[1697101689991, 1697101692030]"
4139,4139,840,21,[],200,llama-13b,128,1,940.0,1.0,1,H100,1697101659707,1697101660647.0,120,17.0,1.0,"[12, 928]","[1697101659719, 1697101660647]"
4140,4140,268,22,[],200,llama-13b,128,1,1268.0,1.0,1,H100,1697101660648,1697101661916.0,120,19.0,1.0,"[36, 1232]","[1697101660684, 1697101661916]"
4141,4141,900,32,[],200,llama-13b,128,1,1843.0,1.0,1,H100,1697101692031,1697101693874.0,120,67.0,6.0,"[12, 1329, 107, 101, 100, 99, 95]","[1697101692043, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874]"
4142,4142,744,26,[],200,llama-13b,128,1,2628.0,1.0,1,H100,1697101675816,1697101678444.0,120,161.0,6.0,"[500, 1064, 97, 64, 51, 762, 90]","[1697101676316, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678354, 1697101678444]"
4143,4143,739,24,[],200,llama-13b,128,1,848.0,1.0,1,H100,1697101664890,1697101665738.0,120,216.0,1.0,"[13, 835]","[1697101664903, 1697101665738]"
4144,4144,147,27,[],200,llama-13b,128,1,843.0,1.0,1,H100,1697101678445,1697101679288.0,120,182.0,1.0,"[7, 836]","[1697101678452, 1697101679288]"
4145,4145,506,28,[],200,llama-13b,128,1,1191.0,1.0,1,H100,1697101679289,1697101680480.0,120,16.0,1.0,"[36, 1155]","[1697101679325, 1697101680480]"
4146,4146,867,29,[],200,llama-13b,128,1,3939.0,1.0,1,H100,1697101680481,1697101684420.0,120,91.0,20.0,"[12, 1009, 106, 84, 83, 79, 79, 771, 97, 92, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680493, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682801, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
4147,4147,135,25,[],200,llama-13b,128,1,1397.0,1.0,1,H100,1697101665739,1697101667136.0,120,52.0,2.0,"[19, 1012, 366]","[1697101665758, 1697101666770, 1697101667136]"
4148,4148,773,35,[],200,llama-13b,128,1,3839.0,1.0,1,H100,1697101680581,1697101684420.0,120,90.0,20.0,"[25, 896, 106, 84, 83, 79, 79, 770, 98, 92, 91, 70, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680606, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682703, 1697101682801, 1697101682893, 1697101682984, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
4149,4149,496,26,[],200,llama-13b,128,1,2717.0,1.0,1,H100,1697101667137,1697101669854.0,120,335.0,11.0,"[55, 1071, 133, 101, 95, 71, 91, 90, 724, 99, 94, 93]","[1697101667192, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854]"
4150,4150,701,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101668845,1697101675812.0,120,,,"[24, 1616, 226, 99, 70, 93, 90, 91, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 67, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101668869, 1697101670485, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4151,4151,292,30,[],200,llama-13b,128,1,1691.0,1.0,1,H100,1697101684425,1697101686116.0,120,286.0,1.0,"[13, 1678]","[1697101684438, 1697101686116]"
4152,4152,690,38,[],200,llama-13b,128,1,1727.0,1.0,1,H100,1697101703173,1697101704900.0,120,39.0,1.0,"[410, 1317]","[1697101703583, 1697101704900]"
4153,4153,206,39,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,16.0,1.0,"[115, 762]","[1697101705016, 1697101705778]"
4154,4154,232,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674982,1697101675810.0,120,,,[7],[1697101674989]
4155,4155,622,31,[],200,llama-13b,128,1,906.0,1.0,1,H100,1697101686117,1697101687023.0,120,20.0,1.0,"[12, 894]","[1697101686129, 1697101687023]"
4156,4156,589,37,[],200,llama-13b,128,1,4758.0,1.0,1,H100,1697101675813,1697101680571.0,120,92.0,20.0,"[296, 1271, 97, 65, 49, 764, 89, 86, 65, 83, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101676109, 1697101677380, 1697101677477, 1697101677542, 1697101677591, 1697101678355, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
4157,4157,47,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687024,1697101689595.0,120,,,"[30, 1331, 133, 106, 97, 92, 92, 89]","[1697101687054, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4158,4158,502,27,[],200,llama-13b,128,1,1212.0,1.0,1,H100,1697101672547,1697101673759.0,120,19.0,1.0,"[24, 1187]","[1697101672571, 1697101673758]"
4159,4159,861,28,[],200,llama-13b,128,1,1088.0,1.0,1,H100,1697101673760,1697101674848.0,120,10.0,1.0,"[18, 1070]","[1697101673778, 1697101674848]"
4160,4160,633,33,[],200,llama-13b,128,1,3839.0,1.0,1,H100,1697101680582,1697101684421.0,120,90.0,20.0,"[98, 822, 106, 84, 83, 79, 79, 771, 96, 93, 91, 69, 91, 90, 614, 102, 101, 97, 92, 90, 91]","[1697101680680, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684421]"
4161,4161,417,25,[],200,llama-13b,128,1,955.0,1.0,1,H100,1697101673893,1697101674848.0,120,17.0,1.0,"[53, 902]","[1697101673946, 1697101674848]"
4162,4162,286,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674849,1697101675810.0,120,,,[54],[1697101674903]
4163,4163,746,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674849,1697101675810.0,120,,,"[12, 928]","[1697101674861, 1697101675789]"
4164,4164,58,34,[],200,llama-13b,128,1,1689.0,1.0,1,H100,1697101684427,1697101686116.0,120,15.0,1.0,"[64, 1625]","[1697101684491, 1697101686116]"
4165,4165,100,38,[],200,llama-13b,128,1,4260.0,1.0,1,H100,1697101689614,1697101693874.0,120,732.0,14.0,"[110, 1388, 93, 1069, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 95]","[1697101689724, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874]"
4166,4166,405,33,[],200,llama-13b,128,1,5116.0,1.0,1,H100,1697101689616,1697101694732.0,120,87.0,20.0,"[307, 1189, 93, 1069, 214, 211, 79, 109, 93, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101]","[1697101689923, 1697101691112, 1697101691205, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692887, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732]"
4167,4167,339,27,[],200,llama-13b,128,1,4138.0,1.0,1,H100,1697101663465,1697101667603.0,120,87.0,20.0,"[13, 968, 232, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 99, 72, 99, 94]","[1697101663478, 1697101664446, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603]"
4168,4168,173,27,[],200,llama-13b,128,1,4297.0,1.0,1,H100,1697101675813,1697101680110.0,120,96.0,20.0,"[90, 477, 36, 1061, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98, 95, 71, 92, 91, 70, 90]","[1697101675903, 1697101676380, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679950, 1697101680020, 1697101680110]"
4169,4169,617,30,[],200,llama-13b,128,1,4758.0,1.0,1,H100,1697101675813,1697101680571.0,120,87.0,20.0,"[282, 1285, 97, 64, 51, 763, 89, 86, 65, 83, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101676095, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
4170,4170,238,31,[],200,llama-13b,128,1,1520.0,1.0,1,H100,1697101678248,1697101679768.0,120,563.0,6.0,"[6, 1034, 117, 98, 98, 95, 72]","[1697101678254, 1697101679288, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768]"
4171,4171,703,41,[],200,llama-13b,128,1,516.0,1.0,1,H100,1697101707798,1697101708314.0,120,12.0,1.0,"[12, 503]","[1697101707810, 1697101708313]"
4172,4172,221,42,[],200,llama-13b,128,1,4615.0,1.0,1,H100,1697101708315,1697101712930.0,120,364.0,36.0,"[24, 1035, 116, 89, 83, 82, 79, 882, 103, 97, 92, 89, 88, 86, 498, 99, 90, 85, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32, 31, 31, 25]","[1697101708339, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930]"
4173,4173,145,17,[],200,llama-13b,128,1,2672.0,1.0,1,H100,1697101659706,1697101662378.0,120,161.0,9.0,"[7, 934, 338, 102, 75, 98, 93, 87, 831, 107]","[1697101659713, 1697101660647, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378]"
4174,4174,502,18,[],200,llama-13b,128,1,958.0,1.0,1,H100,1697101662379,1697101663337.0,120,19.0,1.0,"[24, 934]","[1697101662403, 1697101663337]"
4175,4175,832,19,[],200,llama-13b,128,1,1108.0,1.0,1,H100,1697101663338,1697101664446.0,120,15.0,1.0,"[54, 1053]","[1697101663392, 1697101664445]"
4176,4176,453,39,[],200,llama-13b,128,1,582.0,1.0,1,H100,1697101693875,1697101694457.0,120,26.0,1.0,"[6, 576]","[1697101693881, 1697101694457]"
4177,4177,767,34,[],200,llama-13b,128,1,1449.0,1.0,1,H100,1697101694734,1697101696183.0,120,11.0,1.0,"[83, 1366]","[1697101694817, 1697101696183]"
4178,4178,200,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696184,1697101697589.0,120,,,"[13, 1198]","[1697101696197, 1697101697395]"
4179,4179,814,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694458,1697101697587.0,120,,,"[7, 871, 84, 79, 76, 831, 103, 100, 111, 93, 90]","[1697101694465, 1697101695336, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696813, 1697101696903]"
4180,4180,693,28,[],200,llama-13b,128,1,1964.0,1.0,1,H100,1697101667605,1697101669569.0,120,67.0,2.0,"[62, 1777, 125]","[1697101667667, 1697101669444, 1697101669569]"
4181,4181,123,29,[],200,llama-13b,128,1,915.0,1.0,1,H100,1697101669570,1697101670485.0,120,14.0,1.0,"[24, 891]","[1697101669594, 1697101670485]"
4182,4182,646,36,[],200,llama-13b,128,1,1873.0,1.0,1,H100,1697101697602,1697101699475.0,120,14.0,1.0,"[504, 1369]","[1697101698106, 1697101699475]"
4183,4183,455,30,[],200,llama-13b,128,1,4495.0,1.0,1,H100,1697101670486,1697101674981.0,120,91.0,20.0,"[12, 1061, 107, 97, 95, 94, 93, 90, 521, 114, 91, 68, 68, 87, 805, 97, 89, 88, 66, 65, 687]","[1697101670498, 1697101671559, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673084, 1697101673889, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981]"
4184,4184,71,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700635.0,120,,,"[42, 1033]","[1697101699519, 1697101700552]"
4185,4185,244,41,[],200,llama-13b,128,1,700.0,1.0,1,H100,1697101697595,1697101698295.0,120,9.0,1.0,"[89, 611]","[1697101697684, 1697101698295]"
4186,4186,576,42,[],200,llama-13b,128,1,2256.0,1.0,1,H100,1697101698296,1697101700552.0,120,14.0,1.0,"[42, 2214]","[1697101698338, 1697101700552]"
4187,4187,7,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700553,1697101703169.0,120,,,"[6, 535, 1188, 69]","[1697101700559, 1697101701094, 1697101702282, 1697101702351]"
4188,4188,361,44,[],200,llama-13b,128,1,2921.0,1.0,1,H100,1697101703174,1697101706095.0,120,67.0,7.0,"[426, 1300, 116, 97, 72, 728, 95, 87]","[1697101703600, 1697101704900, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095]"
4189,4189,49,31,[],200,llama-13b,128,1,1111.0,1.0,1,H100,1697101680581,1697101681692.0,120,109.0,3.0,"[25, 896, 106, 84]","[1697101680606, 1697101681502, 1697101681608, 1697101681692]"
4190,4190,410,32,[],200,llama-13b,128,1,2455.0,1.0,1,H100,1697101681693,1697101684148.0,120,364.0,12.0,"[6, 812, 193, 97, 92, 92, 69, 90, 90, 615, 102, 100, 97]","[1697101681699, 1697101682511, 1697101682704, 1697101682801, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148]"
4191,4191,769,33,[],200,llama-13b,128,1,4475.0,1.0,1,H100,1697101684149,1697101688624.0,120,47.0,20.0,"[6, 752, 107, 86, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106]","[1697101684155, 1697101684907, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
4192,4192,470,34,[],200,llama-13b,128,1,999.0,1.0,1,H100,1697101686238,1697101687237.0,120,39.0,2.0,"[7, 777, 215]","[1697101686245, 1697101687022, 1697101687237]"
4193,4193,715,45,[],200,llama-13b,128,1,949.0,1.0,1,H100,1697101706096,1697101707045.0,120,20.0,1.0,"[7, 941]","[1697101706103, 1697101707044]"
4194,4194,827,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687238,1697101689593.0,120,,,"[7, 1140, 133, 106, 97, 92, 92, 89]","[1697101687245, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4195,4195,816,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674982,1697101675810.0,120,,,"[7, 800]","[1697101674989, 1697101675789]"
4196,4196,246,32,[],200,llama-13b,128,1,9432.0,1.0,1,H100,1697101675813,1697101685245.0,120,58.0,47.0,"[108, 459, 39, 1058, 64, 50, 763, 90, 86, 64, 84, 80, 647, 98, 98, 95, 71, 93, 90, 70, 90, 461, 103, 57, 73, 803, 85, 82, 80, 78, 771, 97, 93, 91, 69, 91, 90, 614, 102, 101, 97, 91, 91, 90, 593, 86, 64, 82]","[1697101675921, 1697101676380, 1697101676419, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681607, 1697101681692, 1697101681774, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684239, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245]"
4197,4197,257,36,[],200,llama-13b,128,1,1502.0,1.0,1,H100,1697101689610,1697101691112.0,120,14.0,1.0,"[19, 1483]","[1697101689629, 1697101691112]"
4198,4198,612,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[192, 1801, 83, 82, 81, 81, 82]","[1697101697787, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4199,4199,615,37,[],200,llama-13b,128,1,3703.0,1.0,1,H100,1697101691113,1697101694816.0,120,93.0,20.0,"[7, 1154, 213, 212, 78, 111, 93, 94, 403, 102, 100, 99, 95, 91, 87, 68, 433, 78, 100, 85]","[1697101691120, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692888, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816]"
4200,4200,150,29,[],200,llama-13b,128,1,814.0,1.0,1,H100,1697101668755,1697101669569.0,120,216.0,2.0,"[7, 807]","[1697101668762, 1697101669569]"
4201,4201,598,30,[],200,llama-13b,128,1,2475.0,1.0,1,H100,1697101669570,1697101672045.0,120,345.0,12.0,"[18, 897, 226, 99, 71, 92, 91, 90, 511, 98, 95, 94, 93]","[1697101669588, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045]"
4202,4202,43,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[391, 1127, 122, 69]","[1697101701033, 1697101702160, 1697101702282, 1697101702351]"
4203,4203,23,31,[],200,llama-13b,128,1,500.0,1.0,1,H100,1697101672046,1697101672546.0,120,26.0,1.0,"[6, 494]","[1697101672052, 1697101672546]"
4204,4204,382,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672547,1697101675811.0,120,,,"[18, 1193, 130, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672565, 1697101673758, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4205,4205,404,42,[],200,llama-13b,128,1,5351.0,1.0,1,H100,1697101703175,1697101708526.0,120,87.0,20.0,"[328, 1396, 116, 98, 72, 728, 95, 87, 85, 83, 79, 944, 96, 92, 70, 91, 70, 91, 87, 545, 98]","[1697101703503, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
4206,4206,126,48,[],200,llama-13b,128,1,1723.0,1.0,1,H100,1697101703176,1697101704899.0,120,19.0,1.0,"[130, 1593]","[1697101703306, 1697101704899]"
4207,4207,203,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697596,1697101700635.0,120,,,"[377, 1502, 113, 83, 82, 81, 80, 83]","[1697101697973, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699914, 1697101699997]"
4208,4208,43,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[154, 1211, 223, 103, 101, 110, 92, 92]","[1697101694972, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696904]"
4209,4209,851,43,[],200,llama-13b,128,1,841.0,1.0,1,H100,1697101708533,1697101709374.0,120,23.0,1.0,"[143, 698]","[1697101708676, 1697101709374]"
4210,4210,572,49,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,16.0,1.0,"[78, 799]","[1697101704979, 1697101705778]"
4211,4211,164,34,[],200,llama-13b,128,1,945.0,1.0,1,H100,1697101688626,1697101689571.0,120,15.0,1.0,"[47, 897]","[1697101688673, 1697101689570]"
4212,4212,926,50,[],200,llama-13b,128,1,6335.0,1.0,1,H100,1697101705779,1697101712114.0,120,563.0,30.0,"[12, 1253, 241, 96, 93, 70, 91, 70, 91, 87, 544, 99, 65, 88, 812, 88, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 98, 91, 84, 83]","[1697101705791, 1697101707044, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114]"
4213,4213,282,44,[],200,llama-13b,128,1,3169.0,1.0,1,H100,1697101709375,1697101712544.0,120,87.0,20.0,"[30, 1023, 277, 103, 97, 92, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55]","[1697101709405, 1697101710428, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
4214,4214,519,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689572,1697101697587.0,120,,,"[18, 386, 1229, 1068, 214, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100, 84, 605, 78, 76, 832, 103, 100, 111, 92, 91]","[1697101689590, 1697101689976, 1697101691205, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694815, 1697101695420, 1697101695498, 1697101695574, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
4215,4215,373,39,[],200,llama-13b,128,1,1880.0,1.0,1,H100,1697101697595,1697101699475.0,120,15.0,1.0,"[274, 1606]","[1697101697869, 1697101699475]"
4216,4216,731,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700635.0,120,,,"[18, 1057]","[1697101699495, 1697101700552]"
4217,4217,900,24,[],200,llama-13b,128,1,3163.0,1.0,1,H100,1697101689615,1697101692778.0,120,67.0,6.0,"[201, 1296, 93, 1069, 213, 212, 79]","[1697101689816, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778]"
4218,4218,156,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[434, 1084, 122, 69]","[1697101701076, 1697101702160, 1697101702282, 1697101702351]"
4219,4219,362,36,[],200,llama-13b,128,1,390.0,1.0,1,H100,1697101692982,1697101693372.0,120,14.0,1.0,"[12, 378]","[1697101692994, 1697101693372]"
4220,4220,718,37,[],200,llama-13b,128,1,1084.0,1.0,1,H100,1697101693373,1697101694457.0,120,13.0,1.0,"[30, 1054]","[1697101693403, 1697101694457]"
4221,4221,150,38,[],200,llama-13b,128,1,962.0,1.0,1,H100,1697101694458,1697101695420.0,120,216.0,2.0,"[7, 955]","[1697101694465, 1697101695420]"
4222,4222,507,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695421,1697101697588.0,120,,,"[13, 749, 224, 102, 101, 111, 92, 91]","[1697101695434, 1697101696183, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696813, 1697101696904]"
4223,4223,841,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700637.0,120,,,"[204, 1789, 83, 82, 81, 81, 82]","[1697101697799, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4224,4224,731,22,[],200,llama-13b,128,1,4691.0,1.0,1,H100,1697101660096,1697101664787.0,120,89.0,20.0,"[12, 539, 338, 102, 75, 99, 92, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 89, 86, 845, 109]","[1697101660108, 1697101660647, 1697101660985, 1697101661087, 1697101661162, 1697101661261, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664787]"
4225,4225,820,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[124, 576, 43, 1250, 83, 82, 81, 81, 82]","[1697101697719, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4226,4226,178,28,[],200,llama-13b,128,1,617.0,1.0,1,H100,1697101656530,1697101657147.0,120,11.0,1.0,"[60, 557]","[1697101656590, 1697101657147]"
4227,4227,531,29,[],200,llama-13b,128,1,5227.0,1.0,1,H100,1697101657151,1697101662378.0,120,52.0,20.0,"[41, 968, 129, 96, 82, 82, 81, 1075, 101, 99, 97, 92, 86, 805, 102, 75, 104, 87, 87, 831, 107]","[1697101657192, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658630, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378]"
4228,4228,878,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697592,1697101700636.0,120,,,"[80, 623, 43, 1250, 83, 82, 81, 81, 82]","[1697101697672, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4229,4229,307,37,[],200,llama-13b,128,1,1517.0,1.0,1,H100,1697101700642,1697101702159.0,120,26.0,1.0,"[343, 1174]","[1697101700985, 1697101702159]"
4230,4230,672,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703171.0,120,,,[30],[1697101702191]
4231,4231,73,39,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703176,1697101704900.0,120,9.0,1.0,"[221, 1503]","[1697101703397, 1697101704900]"
4232,4232,430,40,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,15.0,1.0,"[68, 809]","[1697101704969, 1697101705778]"
4233,4233,785,41,[],200,llama-13b,128,1,1265.0,1.0,1,H100,1697101705780,1697101707045.0,120,10.0,1.0,"[65, 1200]","[1697101705845, 1697101707045]"
4234,4234,210,42,[],200,llama-13b,128,1,1382.0,1.0,1,H100,1697101707046,1697101708428.0,120,140.0,2.0,"[24, 1243, 115]","[1697101707070, 1697101708313, 1697101708428]"
4235,4235,568,43,[],200,llama-13b,128,1,945.0,1.0,1,H100,1697101708429,1697101709374.0,120,11.0,1.0,"[6, 939]","[1697101708435, 1697101709374]"
4236,4236,86,44,[],200,llama-13b,128,1,2997.0,1.0,1,H100,1697101709375,1697101712372.0,120,335.0,17.0,"[13, 1039, 278, 102, 98, 92, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51]","[1697101709388, 1697101710427, 1697101710705, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372]"
4237,4237,247,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101664789,1697101675811.0,120,,,"[41, 908, 130, 106, 100, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90, 724, 99, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 513, 97, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101664830, 1697101665738, 1697101665868, 1697101665974, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4238,4238,893,30,[],200,llama-13b,128,1,2607.0,1.0,1,H100,1697101662379,1697101664986.0,120,335.0,10.0,"[7, 951, 127, 101, 93, 88, 87, 845, 108, 103, 97]","[1697101662386, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664986]"
4239,4239,380,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689616,1697101697587.0,120,,,"[295, 1294, 1069, 213, 212, 79, 110, 92, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101, 83, 605, 79, 76, 831, 103, 100, 111, 92, 91]","[1697101689911, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692888, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732, 1697101694815, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
4240,4240,638,16,[],200,llama-13b,128,1,4779.0,1.0,1,H100,1697101628909,1697101633688.0,120,88.0,20.0,"[202, 1518, 105, 81, 68, 66, 864, 93, 91, 89, 89, 88, 84, 82, 742, 95, 72, 94, 93, 92, 71]","[1697101629111, 1697101630629, 1697101630734, 1697101630815, 1697101630883, 1697101630949, 1697101631813, 1697101631906, 1697101631997, 1697101632086, 1697101632175, 1697101632263, 1697101632347, 1697101632429, 1697101633171, 1697101633266, 1697101633338, 1697101633432, 1697101633525, 1697101633617, 1697101633688]"
4241,4241,242,21,[],200,llama-13b,128,1,2264.0,1.0,1,H100,1697101644421,1697101646685.0,120,345.0,9.0,"[48, 868, 103, 79, 77, 78, 75, 740, 98, 97]","[1697101644469, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645749, 1697101646489, 1697101646587, 1697101646684]"
4242,4242,294,31,[],200,llama-13b,128,1,883.0,1.0,1,H100,1697101664986,1697101665869.0,120,9.0,2.0,"[7, 876]","[1697101664993, 1697101665869]"
4243,4243,774,24,[],200,llama-13b,128,1,668.0,1.0,1,H100,1697101649243,1697101649911.0,120,8.0,1.0,"[30, 638]","[1697101649273, 1697101649911]"
4244,4244,202,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649913,1697101651604.0,120,,,"[30, 1365]","[1697101649943, 1697101651308]"
4245,4245,557,26,[],200,llama-13b,128,1,1863.0,1.0,1,H100,1697101651609,1697101653472.0,120,31.0,1.0,"[295, 1568]","[1697101651904, 1697101653472]"
4246,4246,911,27,[],200,llama-13b,128,1,2754.0,1.0,1,H100,1697101653478,1697101656232.0,120,335.0,11.0,"[7, 1146, 206, 108, 105, 101, 74, 100, 96, 92, 614, 105]","[1697101653485, 1697101654631, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656127, 1697101656232]"
4247,4247,652,39,[],200,llama-13b,128,1,1693.0,1.0,1,H100,1697101685329,1697101687022.0,120,14.0,1.0,"[36, 1657]","[1697101685365, 1697101687022]"
4248,4248,311,28,[],200,llama-13b,128,1,5027.0,1.0,1,H100,1697101656233,1697101661260.0,120,93.0,20.0,"[6, 995, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97, 93, 85, 805, 101, 76, 98]","[1697101656239, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260]"
4249,4249,52,40,[],200,llama-13b,128,1,1882.0,1.0,1,H100,1697101687023,1697101688905.0,120,58.0,6.0,"[7, 1355, 133, 106, 97, 92, 92]","[1697101687030, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905]"
4250,4250,651,32,[],200,llama-13b,128,1,1266.0,1.0,1,H100,1697101665870,1697101667136.0,120,457.0,2.0,"[17, 883, 366]","[1697101665887, 1697101666770, 1697101667136]"
4251,4251,80,33,[],200,llama-13b,128,1,1125.0,1.0,1,H100,1697101667138,1697101668263.0,120,13.0,1.0,"[54, 1071]","[1697101667192, 1697101668263]"
4252,4252,440,34,[],200,llama-13b,128,1,4393.0,1.0,1,H100,1697101668264,1697101672657.0,120,84.0,20.0,"[12, 1168, 125, 98, 94, 93, 93, 88, 676, 99, 70, 93, 90, 90, 513, 97, 95, 94, 93, 90, 522]","[1697101668276, 1697101669444, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657]"
4253,4253,402,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688905,1697101689610.0,120,,,"[7, 659]","[1697101688912, 1697101689571]"
4254,4254,761,42,[],200,llama-13b,128,1,5197.0,1.0,1,H100,1697101689619,1697101694816.0,120,85.0,20.0,"[484, 1927, 244, 213, 212, 78, 111, 93, 94, 403, 102, 100, 99, 94, 91, 88, 68, 434, 77, 100, 85]","[1697101690103, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692888, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
4255,4255,573,22,[],200,llama-13b,128,1,1398.0,1.0,1,H100,1697101646685,1697101648083.0,120,874.0,2.0,"[13, 1384]","[1697101646698, 1697101648082]"
4256,4256,1,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648094,1697101651604.0,120,,,"[6, 847, 102, 97, 96, 82, 942, 106, 103, 102, 90, 86]","[1697101648100, 1697101648947, 1697101649049, 1697101649146, 1697101649242, 1697101649324, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
4257,4257,626,23,[],200,llama-13b,128,1,1420.0,1.0,1,H100,1697101661917,1697101663337.0,120,10.0,1.0,"[12, 1408]","[1697101661929, 1697101663337]"
4258,4258,423,47,[],200,llama-13b,128,1,5352.0,1.0,1,H100,1697101703174,1697101708526.0,120,84.0,20.0,"[126, 1599, 117, 97, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 91, 70, 91, 86, 545, 99]","[1697101703300, 1697101704899, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
4259,4259,189,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[93, 1272, 223, 103, 101, 110, 92, 92]","[1697101694911, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696904]"
4260,4260,359,24,[],200,llama-13b,128,1,1863.0,1.0,1,H100,1697101651609,1697101653472.0,120,10.0,1.0,"[281, 1582]","[1697101651890, 1697101653472]"
4261,4261,555,44,[],200,llama-13b,128,1,1880.0,1.0,1,H100,1697101697596,1697101699476.0,120,11.0,1.0,"[378, 1501]","[1697101697974, 1697101699475]"
4262,4262,885,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700635.0,120,,,"[66, 1009]","[1697101699543, 1697101700552]"
4263,4263,382,21,[],200,llama-13b,128,1,4821.0,1.0,1,H100,1697101644421,1697101649242.0,120,47.0,20.0,"[43, 873, 103, 79, 77, 78, 75, 740, 98, 97, 84, 78, 1237, 95, 101, 100, 97, 88, 485, 96, 97]","[1697101644464, 1697101645337, 1697101645440, 1697101645519, 1697101645596, 1697101645674, 1697101645749, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648083, 1697101648178, 1697101648279, 1697101648379, 1697101648476, 1697101648564, 1697101649049, 1697101649145, 1697101649242]"
4264,4264,718,25,[],200,llama-13b,128,1,1152.0,1.0,1,H100,1697101653479,1697101654631.0,120,13.0,1.0,"[36, 1116]","[1697101653515, 1697101654631]"
4265,4265,54,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648381,1697101651604.0,120,,,"[6, 560, 103, 96, 96, 83, 941, 106, 103, 102, 90, 86]","[1697101648387, 1697101648947, 1697101649050, 1697101649146, 1697101649242, 1697101649325, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650667, 1697101650753]"
4266,4266,437,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[160, 1205, 223, 103, 101, 110, 92, 92]","[1697101694978, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696904]"
4267,4267,314,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[48, 1473, 123, 69]","[1697101700686, 1697101702159, 1697101702282, 1697101702351]"
4268,4268,851,23,[],200,llama-13b,128,1,567.0,1.0,1,H100,1697101675813,1697101676380.0,120,23.0,1.0,"[78, 489]","[1697101675891, 1697101676380]"
4269,4269,250,24,[],200,llama-13b,128,1,1866.0,1.0,1,H100,1697101676381,1697101678247.0,120,31.0,1.0,"[115, 1751]","[1697101676496, 1697101678247]"
4270,4270,673,47,[],200,llama-13b,128,1,5351.0,1.0,1,H100,1697101703175,1697101708526.0,120,93.0,20.0,"[346, 1494, 98, 72, 728, 95, 87, 85, 83, 79, 944, 96, 92, 70, 92, 69, 91, 87, 545, 98]","[1697101703521, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
4271,4271,612,25,[],200,llama-13b,128,1,4456.0,1.0,1,H100,1697101678248,1697101682704.0,120,93.0,20.0,"[30, 1127, 99, 98, 94, 72, 92, 90, 70, 91, 460, 103, 57, 73, 804, 84, 83, 79, 79, 771]","[1697101678278, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704]"
4272,4272,411,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101683235,1697101689595.0,120,,,"[6, 1772, 87, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 91, 90]","[1697101683241, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688994]"
4273,4273,726,30,[],200,llama-13b,128,1,10501.0,1.0,1,H100,1697101658162,1697101668663.0,120,67.0,47.0,"[47, 1164, 332, 101, 99, 97, 93, 85, 805, 101, 75, 99, 92, 88, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 100, 689, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101, 95, 71]","[1697101658209, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661161, 1697101661260, 1697101661352, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663]"
4274,4274,556,28,[],200,llama-13b,128,1,576.0,1.0,1,H100,1697101655422,1697101655998.0,120,9.0,1.0,"[7, 569]","[1697101655429, 1697101655998]"
4275,4275,703,39,[],200,llama-13b,128,1,851.0,1.0,1,H100,1697101710809,1697101711660.0,120,12.0,1.0,"[36, 815]","[1697101710845, 1697101711660]"
4276,4276,917,29,[],200,llama-13b,128,1,1235.0,1.0,1,H100,1697101655999,1697101657234.0,120,123.0,2.0,"[6, 1141, 88]","[1697101656005, 1697101657146, 1697101657234]"
4277,4277,417,35,[],200,llama-13b,128,1,905.0,1.0,1,H100,1697101686117,1697101687022.0,120,17.0,1.0,"[30, 875]","[1697101686147, 1697101687022]"
4278,4278,772,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687023,1697101689594.0,120,,,"[25, 1337, 133, 106, 97, 92, 92, 89]","[1697101687048, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4279,4279,87,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673892,1697101675812.0,120,,,"[29, 927, 133, 102, 99, 95, 70, 94]","[1697101673921, 1697101674848, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
4280,4280,265,20,[],200,llama-13b,128,1,4397.0,1.0,1,H100,1697101664447,1697101668844.0,120,86.0,20.0,"[6, 1285, 130, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90]","[1697101664453, 1697101665738, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844]"
4281,4281,592,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101679769,1697101689602.0,120,,,"[6, 704, 101, 95, 57, 72, 804, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90, 593, 86, 64, 83, 80, 911, 91, 89, 87, 86, 647, 101, 100, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101679775, 1697101680479, 1697101680580, 1697101680675, 1697101680732, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
4282,4282,414,25,[],200,llama-13b,128,1,4401.0,1.0,1,H100,1697101666311,1697101670712.0,120,87.0,20.0,"[6, 453, 366, 103, 99, 72, 99, 94, 793, 101, 95, 71, 91, 90, 725, 98, 95, 92, 93, 88, 676]","[1697101666317, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711]"
4283,4283,145,46,[],200,llama-13b,128,1,2698.0,1.0,1,H100,1697101707046,1697101709744.0,120,161.0,9.0,"[18, 1249, 115, 99, 64, 88, 812, 88, 83, 82]","[1697101707064, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744]"
4284,4284,740,34,[],200,llama-13b,128,1,3505.0,1.0,1,H100,1697101685014,1697101688519.0,120,563.0,14.0,"[7, 1095, 121, 91, 89, 87, 87, 646, 101, 100, 94, 92, 91, 88, 715]","[1697101685021, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518]"
4285,4285,619,21,[],200,llama-13b,128,1,1640.0,1.0,1,H100,1697101668845,1697101670485.0,120,10.0,1.0,"[25, 1615]","[1697101668870, 1697101670485]"
4286,4286,157,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101668664,1697101675811.0,120,,,"[6, 774, 125, 98, 95, 92, 93, 88, 676, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 91, 521, 113, 91, 68, 67, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101668670, 1697101669444, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4287,4287,745,26,[],200,llama-13b,128,1,834.0,1.0,1,H100,1697101670726,1697101671560.0,120,17.0,1.0,"[55, 779]","[1697101670781, 1697101671560]"
4288,4288,170,27,[],200,llama-13b,128,1,3523.0,1.0,1,H100,1697101671561,1697101675084.0,120,335.0,15.0,"[24, 961, 110, 114, 91, 68, 67, 88, 804, 98, 89, 88, 66, 65, 687, 102]","[1697101671585, 1697101672546, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083]"
4289,4289,590,47,[],200,llama-13b,128,1,2799.0,1.0,1,H100,1697101709745,1697101712544.0,120,88.0,20.0,"[12, 671, 277, 103, 97, 92, 89, 89, 85, 498, 99, 90, 85, 82, 79, 59, 69, 51, 60, 57, 55]","[1697101709757, 1697101710428, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
4290,4290,464,25,[],200,llama-13b,128,1,1211.0,1.0,1,H100,1697101658162,1697101659373.0,120,12.0,1.0,"[53, 1158]","[1697101658215, 1697101659373]"
4291,4291,823,26,[],200,llama-13b,128,1,5413.0,1.0,1,H100,1697101659374,1697101664787.0,120,90.0,20.0,"[7, 1266, 338, 101, 76, 98, 93, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108]","[1697101659381, 1697101660647, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786]"
4292,4292,530,28,[],200,llama-13b,128,1,704.0,1.0,1,H100,1697101675085,1697101675789.0,120,26.0,1.0,"[6, 698]","[1697101675091, 1697101675789]"
4293,4293,556,36,[],200,llama-13b,128,1,1521.0,1.0,1,H100,1697101700638,1697101702159.0,120,9.0,1.0,"[53, 1468]","[1697101700691, 1697101702159]"
4294,4294,736,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101675817,1697101689603.0,120,,,"[486, 1077, 97, 64, 51, 763, 89, 86, 64, 84, 80, 647, 99, 98, 94, 72, 92, 90, 70, 91, 460, 103, 57, 73, 804, 84, 83, 79, 78, 771, 97, 93, 91, 69, 91, 90, 614, 102, 101, 97, 91, 91, 90, 593, 86, 64, 82, 81, 911, 91, 88, 88, 86, 647, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 91, 89]","[1697101676303, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684239, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686416, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
4295,4295,916,37,[],200,llama-13b,128,1,874.0,1.0,1,H100,1697101702161,1697101703035.0,120,8.0,1.0,"[7, 867]","[1697101702168, 1697101703035]"
4296,4296,889,29,[],200,llama-13b,128,1,4320.0,1.0,1,H100,1697101675790,1697101680110.0,120,86.0,20.0,"[25, 565, 36, 1061, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98, 95, 71, 92, 91, 70, 90]","[1697101675815, 1697101676380, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679950, 1697101680020, 1697101680110]"
4297,4297,376,39,[],200,llama-13b,128,1,5117.0,1.0,1,H100,1697101689615,1697101694732.0,120,87.0,20.0,"[272, 1225, 93, 1069, 213, 212, 79, 110, 92, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101]","[1697101689887, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692888, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732]"
4298,4298,348,38,[],200,llama-13b,128,1,5490.0,1.0,1,H100,1697101703036,1697101708526.0,120,91.0,20.0,"[30, 622, 1320, 104, 72, 729, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 99]","[1697101703066, 1697101703688, 1697101705008, 1697101705112, 1697101705184, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
4299,4299,677,24,[],200,llama-13b,128,1,879.0,1.0,1,H100,1697101671667,1697101672546.0,120,9.0,1.0,"[13, 866]","[1697101671680, 1697101672546]"
4300,4300,708,40,[],200,llama-13b,128,1,602.0,1.0,1,H100,1697101694734,1697101695336.0,120,140.0,1.0,"[77, 525]","[1697101694811, 1697101695336]"
4301,4301,105,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672547,1697101675811.0,120,,,"[24, 1187, 130, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672571, 1697101673758, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4302,4302,139,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695337,1697101697588.0,120,,,"[30, 816, 224, 102, 101, 111, 92, 91]","[1697101695367, 1697101696183, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696813, 1697101696904]"
4303,4303,293,30,[],200,llama-13b,128,1,4308.0,1.0,1,H100,1697101680112,1697101684420.0,120,91.0,20.0,"[7, 1382, 107, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680119, 1697101681501, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
4304,4304,680,39,[],200,llama-13b,128,1,2554.0,1.0,1,H100,1697101708533,1697101711087.0,120,123.0,11.0,"[43, 798, 116, 89, 83, 82, 79, 881, 103, 98, 92, 89]","[1697101708576, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710905, 1697101710997, 1697101711086]"
4305,4305,604,33,[],200,llama-13b,128,1,1171.0,1.0,1,H100,1697101685246,1697101686417.0,120,161.0,4.0,"[7, 863, 121, 91, 89]","[1697101685253, 1697101686116, 1697101686237, 1697101686328, 1697101686417]"
4306,4306,25,34,[],200,llama-13b,128,1,605.0,1.0,1,H100,1697101686418,1697101687023.0,120,12.0,1.0,"[7, 598]","[1697101686425, 1697101687023]"
4307,4307,138,43,[],200,llama-13b,128,1,5351.0,1.0,1,H100,1697101703175,1697101708526.0,120,91.0,20.0,"[25, 1699, 116, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 99]","[1697101703200, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
4308,4308,519,42,[],200,llama-13b,128,1,9416.0,1.0,1,H100,1697101703173,1697101712589.0,120,58.0,47.0,"[422, 1305, 116, 97, 72, 728, 95, 87, 85, 83, 79, 944, 96, 92, 70, 92, 69, 91, 86, 546, 98, 65, 88, 811, 89, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 497, 99, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45]","[1697101703595, 1697101704900, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707882, 1697101708428, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589]"
4309,4309,93,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671154,1697101675812.0,120,,,"[7, 1385, 110, 114, 91, 68, 67, 88, 804, 99, 88, 88, 66, 66, 686, 102, 98, 95, 71, 94]","[1697101671161, 1697101672546, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673987, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675347, 1697101675441]"
4310,4310,213,32,[],200,llama-13b,128,1,1328.0,1.0,1,H100,1697101670717,1697101672045.0,120,123.0,6.0,"[12, 830, 107, 97, 95, 94, 93]","[1697101670729, 1697101671559, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045]"
4311,4311,702,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684426,1697101689612.0,120,,,"[42, 1648, 121, 91, 89, 87, 87, 646, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 92, 89]","[1697101684468, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4312,4312,39,17,[],200,llama-13b,128,1,1279.0,1.0,1,H100,1697101633690,1697101634969.0,120,8.0,1.0,"[100, 1179]","[1697101633790, 1697101634969]"
4313,4313,574,33,[],200,llama-13b,128,1,611.0,1.0,1,H100,1697101672046,1697101672657.0,120,364.0,2.0,"[6, 494, 111]","[1697101672052, 1697101672546, 1697101672657]"
4314,4314,396,18,[],200,llama-13b,128,1,4622.0,1.0,1,H100,1697101634970,1697101639592.0,120,89.0,20.0,"[37, 511, 65, 1314, 87, 56, 628, 91, 92, 87, 86, 82, 82, 767, 98, 93, 92, 93, 91, 91, 70]","[1697101635007, 1697101635518, 1697101635583, 1697101636897, 1697101636984, 1697101637040, 1697101637668, 1697101637759, 1697101637851, 1697101637938, 1697101638024, 1697101638106, 1697101638188, 1697101638955, 1697101639053, 1697101639146, 1697101639238, 1697101639331, 1697101639422, 1697101639513, 1697101639583]"
4315,4315,8,34,[],200,llama-13b,128,1,1329.0,1.0,1,H100,1697101672658,1697101673987.0,120,39.0,3.0,"[24, 1077, 129, 98]","[1697101672682, 1697101673759, 1697101673888, 1697101673986]"
4316,4316,131,31,[],200,llama-13b,128,1,1499.0,1.0,1,H100,1697101689613,1697101691112.0,120,8.0,1.0,"[93, 1406]","[1697101689706, 1697101691112]"
4317,4317,492,32,[],200,llama-13b,128,1,3703.0,1.0,1,H100,1697101691113,1697101694816.0,120,47.0,20.0,"[13, 904, 244, 213, 212, 78, 111, 93, 94, 403, 102, 100, 99, 95, 91, 87, 68, 433, 78, 100, 85]","[1697101691126, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692888, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816]"
4318,4318,763,33,[],200,llama-13b,128,1,856.0,1.0,1,H100,1697101662481,1697101663337.0,120,20.0,1.0,"[12, 844]","[1697101662493, 1697101663337]"
4319,4319,120,29,[],200,llama-13b,128,1,1101.0,1.0,1,H100,1697101672658,1697101673759.0,120,17.0,1.0,"[49, 1052]","[1697101672707, 1697101673759]"
4320,4320,7,27,[],200,llama-13b,128,1,3368.0,1.0,1,H100,1697101667605,1697101670973.0,120,345.0,11.0,"[24, 1815, 124, 99, 94, 93, 92, 89, 676, 99, 70, 93]","[1697101667629, 1697101669444, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973]"
4321,4321,6,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[159, 1429, 103, 101, 110, 92, 92]","[1697101694977, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696904]"
4322,4322,194,34,[],200,llama-13b,128,1,3901.0,1.0,1,H100,1697101663338,1697101667239.0,120,335.0,16.0,"[24, 1083, 233, 109, 102, 96, 94, 101, 688, 105, 101, 71, 72, 93, 92, 734, 103]","[1697101663362, 1697101664445, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239]"
4323,4323,747,19,[],200,llama-13b,128,1,8584.0,1.0,1,H100,1697101639594,1697101648178.0,120,140.0,36.0,"[13, 1395, 117, 85, 78, 78, 761, 101, 99, 95, 89, 82, 63, 800, 100, 95, 93, 95, 86, 86, 414, 87, 84, 84, 82, 684, 78, 78, 77, 75, 741, 98, 97, 84, 78, 1226, 106]","[1697101639607, 1697101641002, 1697101641119, 1697101641204, 1697101641282, 1697101641360, 1697101642121, 1697101642222, 1697101642321, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643450, 1697101643550, 1697101643645, 1697101643738, 1697101643833, 1697101643919, 1697101644005, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768, 1697101646846, 1697101648072, 1697101648178]"
4324,4324,118,26,[],200,llama-13b,128,1,5370.0,1.0,1,H100,1697101654632,1697101660002.0,120,85.0,20.0,"[24, 1342, 129, 105, 101, 98, 98, 83, 622, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97]","[1697101654656, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002]"
4325,4325,791,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[186, 1807, 83, 82, 81, 81, 82]","[1697101697781, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4326,4326,363,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697596,1697101700635.0,120,,,"[301, 1578, 112, 84, 82, 82, 80, 82]","[1697101697897, 1697101699475, 1697101699587, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
4327,4327,717,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[349, 1168, 123, 69]","[1697101700991, 1697101702159, 1697101702282, 1697101702351]"
4328,4328,94,48,[],200,llama-13b,128,1,3719.0,1.0,1,H100,1697101708533,1697101712252.0,120,86.0,20.0,"[96, 745, 117, 88, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 99, 91, 84, 83, 78, 59]","[1697101708629, 1697101709374, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712115, 1697101712193, 1697101712252]"
4329,4329,221,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[385, 1133, 122, 69]","[1697101701027, 1697101702160, 1697101702282, 1697101702351]"
4330,4330,565,40,[],200,llama-13b,128,1,5028.0,1.0,1,H100,1697101705779,1697101710807.0,120,91.0,20.0,"[48, 1218, 241, 96, 92, 70, 91, 70, 91, 87, 544, 99, 66, 87, 812, 88, 83, 82, 79, 881, 103]","[1697101705827, 1697101707045, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708592, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807]"
4331,4331,582,26,[],200,llama-13b,128,1,1726.0,1.0,1,H100,1697101703173,1697101704899.0,120,19.0,1.0,"[398, 1328]","[1697101703571, 1697101704899]"
4332,4332,912,27,[],200,llama-13b,128,1,4589.0,1.0,1,H100,1697101704901,1697101709490.0,120,92.0,20.0,"[6, 871, 135, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 546, 98, 65, 88, 811]","[1697101704907, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708428, 1697101708526, 1697101708591, 1697101708679, 1697101709490]"
4333,4333,22,38,[],200,llama-13b,128,1,921.0,1.0,1,H100,1697101680581,1697101681502.0,120,16.0,1.0,"[43, 878]","[1697101680624, 1697101681502]"
4334,4334,233,21,[],200,llama-13b,128,1,470.0,1.0,1,H100,1697101648477,1697101648947.0,120,6.0,1.0,"[6, 464]","[1697101648483, 1697101648947]"
4335,4335,377,39,[],200,llama-13b,128,1,1008.0,1.0,1,H100,1697101681503,1697101682511.0,120,13.0,1.0,"[42, 966]","[1697101681545, 1697101682511]"
4336,4336,560,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648949,1697101651605.0,120,,,"[59, 1258, 106, 104, 101, 91, 85]","[1697101649008, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
4337,4337,582,18,[],200,llama-13b,128,1,831.0,1.0,1,H100,1697101650477,1697101651308.0,120,19.0,1.0,"[12, 819]","[1697101650489, 1697101651308]"
4338,4338,11,19,[],200,llama-13b,128,1,4923.0,1.0,1,H100,1697101651309,1697101656232.0,120,732.0,17.0,"[7, 560, 842, 891, 86, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 91, 614, 106]","[1697101651316, 1697101651876, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655512, 1697101656126, 1697101656232]"
4339,4339,425,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[12, 1508, 124, 69]","[1697101700650, 1697101702158, 1697101702282, 1697101702351]"
4340,4340,605,27,[],200,llama-13b,128,1,1292.0,1.0,1,H100,1697101664447,1697101665739.0,120,8.0,1.0,"[18, 1273]","[1697101664465, 1697101665738]"
4341,4341,32,28,[],200,llama-13b,128,1,1769.0,1.0,1,H100,1697101665740,1697101667509.0,120,140.0,6.0,"[36, 994, 366, 103, 99, 72, 99]","[1697101665776, 1697101666770, 1697101667136, 1697101667239, 1697101667338, 1697101667410, 1697101667509]"
4342,4342,395,29,[],200,llama-13b,128,1,4156.0,1.0,1,H100,1697101667510,1697101671666.0,120,88.0,20.0,"[12, 741, 133, 101, 95, 71, 92, 89, 724, 99, 94, 93, 92, 89, 676, 99, 71, 92, 91, 89, 513]","[1697101667522, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668755, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666]"
4343,4343,784,39,[],200,llama-13b,128,1,5352.0,1.0,1,H100,1697101703174,1697101708526.0,120,89.0,20.0,"[214, 1512, 108, 105, 71, 729, 95, 87, 85, 83, 79, 943, 97, 92, 70, 91, 70, 91, 87, 544, 99]","[1697101703388, 1697101704900, 1697101705008, 1697101705113, 1697101705184, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526]"
4344,4344,706,30,[],200,llama-13b,128,1,5198.0,1.0,1,H100,1697101689618,1697101694816.0,120,86.0,20.0,"[470, 1942, 244, 213, 212, 78, 101, 103, 94, 403, 102, 100, 99, 94, 91, 88, 68, 434, 77, 100, 85]","[1697101690088, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
4345,4345,757,30,[],200,llama-13b,128,1,879.0,1.0,1,H100,1697101671667,1697101672546.0,120,20.0,1.0,"[42, 837]","[1697101671709, 1697101672546]"
4346,4346,156,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672547,1697101675810.0,120,,,"[6, 1205, 130, 98, 89, 88, 66, 66, 686, 102, 98, 95, 71, 94]","[1697101672553, 1697101673758, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675347, 1697101675441]"
4347,4347,397,30,[],200,llama-13b,128,1,1662.0,1.0,1,H100,1697101675815,1697101677477.0,120,67.0,2.0,"[366, 1199, 97]","[1697101676181, 1697101677380, 1697101677477]"
4348,4348,493,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700635.0,120,,,"[284, 1596, 113, 84, 81, 82, 80, 82]","[1697101697879, 1697101699475, 1697101699588, 1697101699672, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
4349,4349,711,23,[],200,llama-13b,128,1,1159.0,1.0,1,H100,1697101660003,1697101661162.0,120,457.0,4.0,"[7, 636, 339, 102, 75]","[1697101660010, 1697101660646, 1697101660985, 1697101661087, 1697101661162]"
4350,4350,22,33,[],200,llama-13b,128,1,2412.0,1.0,1,H100,1697101689618,1697101692030.0,120,16.0,1.0,"[379, 2033]","[1697101689997, 1697101692030]"
4351,4351,351,34,[],200,llama-13b,128,1,1843.0,1.0,1,H100,1697101692031,1697101693874.0,120,216.0,6.0,"[6, 1335, 107, 101, 100, 99, 95]","[1697101692037, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874]"
4352,4352,847,43,[],200,llama-13b,128,1,2390.0,1.0,1,H100,1697101700644,1697101703034.0,120,10.0,1.0,"[464, 1926]","[1697101701108, 1697101703034]"
4353,4353,110,24,[],200,llama-13b,128,1,1316.0,1.0,1,H100,1697101661163,1697101662479.0,120,96.0,4.0,"[6, 747, 356, 106, 101]","[1697101661169, 1697101661916, 1697101662272, 1697101662378, 1697101662479]"
4354,4354,277,44,[],200,llama-13b,128,1,652.0,1.0,1,H100,1697101703036,1697101703688.0,120,18.0,1.0,"[30, 622]","[1697101703066, 1697101703688]"
4355,4355,606,45,[],200,llama-13b,128,1,2089.0,1.0,1,H100,1697101703689,1697101705778.0,120,9.0,1.0,"[6, 2083]","[1697101703695, 1697101705778]"
4356,4356,515,32,[],200,llama-13b,128,1,1565.0,1.0,1,H100,1697101675815,1697101677380.0,120,11.0,1.0,"[262, 1303]","[1697101676077, 1697101677380]"
4357,4357,466,25,[],200,llama-13b,128,1,4656.0,1.0,1,H100,1697101662480,1697101667136.0,120,457.0,20.0,"[13, 844, 127, 101, 93, 89, 86, 845, 108, 103, 96, 94, 100, 689, 105, 101, 71, 72, 93, 92, 734]","[1697101662493, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136]"
4358,4358,866,33,[],200,llama-13b,128,1,3423.0,1.0,1,H100,1697101677381,1697101680804.0,120,93.0,20.0,"[37, 936, 91, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 103, 57, 73]","[1697101677418, 1697101678354, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804]"
4359,4359,39,46,[],200,llama-13b,128,1,1266.0,1.0,1,H100,1697101705779,1697101707045.0,120,8.0,1.0,"[18, 1248]","[1697101705797, 1697101707045]"
4360,4360,716,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693875,1697101697589.0,120,,,"[6, 576, 97, 77, 101, 84, 604, 78, 77, 832, 102, 101, 110, 93, 90]","[1697101693881, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696407, 1697101696509, 1697101696610, 1697101696720, 1697101696813, 1697101696903]"
4361,4361,424,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[282, 1235, 123, 69]","[1697101700924, 1697101702159, 1697101702282, 1697101702351]"
4362,4362,786,39,[],200,llama-13b,128,1,5352.0,1.0,1,H100,1697101703174,1697101708526.0,120,87.0,20.0,"[499, 1227, 113, 100, 72, 728, 95, 87, 85, 83, 79, 943, 96, 92, 71, 92, 69, 91, 86, 546, 98]","[1697101703673, 1697101704900, 1697101705013, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707882, 1697101708428, 1697101708526]"
4363,4363,485,21,[],200,llama-13b,128,1,844.0,1.0,1,H100,1697101644675,1697101645519.0,120,67.0,3.0,"[6, 656, 103, 79]","[1697101644681, 1697101645337, 1697101645440, 1697101645519]"
4364,4364,400,47,[],200,llama-13b,128,1,2534.0,1.0,1,H100,1697101707045,1697101709579.0,120,123.0,7.0,"[7, 1261, 115, 99, 64, 88, 812, 88]","[1697101707052, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709491, 1697101709579]"
4365,4365,846,22,[],200,llama-13b,128,1,1327.0,1.0,1,H100,1697101645520,1697101646847.0,120,140.0,6.0,"[6, 773, 191, 97, 98, 83, 79]","[1697101645526, 1697101646299, 1697101646490, 1697101646587, 1697101646685, 1697101646768, 1697101646847]"
4366,4366,827,26,[],200,llama-13b,128,1,4529.0,1.0,1,H100,1697101667137,1697101671666.0,120,96.0,20.0,"[37, 1089, 133, 101, 95, 71, 91, 90, 724, 99, 94, 93, 92, 89, 677, 98, 71, 92, 91, 89, 513]","[1697101667174, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670712, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666]"
4367,4367,758,48,[],200,llama-13b,128,1,2964.0,1.0,1,H100,1697101709580,1697101712544.0,120,84.0,20.0,"[6, 842, 277, 103, 97, 92, 89, 89, 85, 498, 98, 91, 85, 82, 79, 59, 69, 51, 60, 57, 55]","[1697101709586, 1697101710428, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
4368,4368,16,30,[],200,llama-13b,128,1,807.0,1.0,1,H100,1697101674982,1697101675789.0,120,9.0,1.0,"[30, 777]","[1697101675012, 1697101675789]"
4369,4369,250,23,[],200,llama-13b,128,1,2099.0,1.0,1,H100,1697101646848,1697101648947.0,120,31.0,1.0,"[12, 2087]","[1697101646860, 1697101648947]"
4370,4370,373,31,[],200,llama-13b,128,1,589.0,1.0,1,H100,1697101675791,1697101676380.0,120,15.0,1.0,"[42, 547]","[1697101675833, 1697101676380]"
4371,4371,608,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651604.0,120,,,"[7, 956, 355, 106, 104, 101, 91, 85]","[1697101648955, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
4372,4372,216,40,[],200,llama-13b,128,1,3719.0,1.0,1,H100,1697101708533,1697101712252.0,120,91.0,20.0,"[79, 762, 117, 88, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 99, 91, 84, 83, 78, 59]","[1697101708612, 1697101709374, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712115, 1697101712193, 1697101712252]"
4373,4373,256,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671667,1697101675810.0,120,,,"[36, 843, 111, 113, 91, 68, 68, 88, 803, 98, 89, 88, 66, 66, 686, 103, 98, 95, 70, 94]","[1697101671703, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675084, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
4374,4374,38,25,[],200,llama-13b,128,1,5002.0,1.0,1,H100,1697101651609,1697101656611.0,120,88.0,20.0,"[359, 1504, 138, 85, 84, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 97, 83]","[1697101651968, 1697101653472, 1697101653610, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611]"
4375,4375,24,23,[],200,llama-13b,128,1,2863.0,1.0,1,H100,1697101675815,1697101678678.0,120,79.0,9.0,"[384, 1278, 64, 50, 764, 90, 85, 65, 83]","[1697101676199, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678]"
4376,4376,738,32,[],200,llama-13b,128,1,2297.0,1.0,1,H100,1697101676381,1697101678678.0,120,79.0,6.0,"[91, 1882, 90, 86, 64, 84]","[1697101676472, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678]"
4377,4377,425,28,[],200,llama-13b,128,1,4764.0,1.0,1,H100,1697101675816,1697101680580.0,120,88.0,20.0,"[401, 1162, 98, 64, 50, 764, 90, 85, 65, 82, 82, 646, 99, 97, 95, 72, 92, 91, 69, 91, 468]","[1697101676217, 1697101677379, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678677, 1697101678759, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680111, 1697101680579]"
4378,4378,389,24,[],200,llama-13b,128,1,609.0,1.0,1,H100,1697101678679,1697101679288.0,120,8.0,1.0,"[6, 603]","[1697101678685, 1697101679288]"
4379,4379,719,25,[],200,llama-13b,128,1,2319.0,1.0,1,H100,1697101679289,1697101681608.0,120,182.0,6.0,"[18, 1264, 103, 58, 72, 804]","[1697101679307, 1697101680571, 1697101680674, 1697101680732, 1697101680804, 1697101681608]"
4380,4380,739,33,[],200,llama-13b,128,1,701.0,1.0,1,H100,1697101697594,1697101698295.0,120,216.0,1.0,"[32, 669]","[1697101697626, 1697101698295]"
4381,4381,352,31,[],200,llama-13b,128,1,1111.0,1.0,1,H100,1697101680581,1697101681692.0,120,11.0,3.0,"[9, 1018, 84]","[1697101680590, 1697101681608, 1697101681692]"
4382,4382,337,35,[],200,llama-13b,128,1,860.0,1.0,1,H100,1697101673988,1697101674848.0,120,12.0,1.0,"[6, 854]","[1697101673994, 1697101674848]"
4383,4383,710,32,[],200,llama-13b,128,1,818.0,1.0,1,H100,1697101681693,1697101682511.0,120,14.0,1.0,"[7, 811]","[1697101681700, 1697101682511]"
4384,4384,367,28,[],200,llama-13b,128,1,1071.0,1.0,1,H100,1697101670974,1697101672045.0,120,92.0,6.0,"[7, 579, 106, 97, 95, 94, 93]","[1697101670981, 1697101671560, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045]"
4385,4385,135,33,[],200,llama-13b,128,1,1337.0,1.0,1,H100,1697101682512,1697101683849.0,120,52.0,2.0,"[6, 1123, 208]","[1697101682518, 1697101683641, 1697101683849]"
4386,4386,495,34,[],200,llama-13b,128,1,1057.0,1.0,1,H100,1697101683850,1697101684907.0,120,13.0,1.0,"[12, 1045]","[1697101683862, 1697101684907]"
4387,4387,512,38,[],200,llama-13b,128,1,674.0,1.0,1,H100,1697101696722,1697101697396.0,120,11.0,1.0,"[24, 650]","[1697101696746, 1697101697396]"
4388,4388,147,26,[],200,llama-13b,128,1,902.0,1.0,1,H100,1697101681609,1697101682511.0,120,182.0,1.0,"[12, 890]","[1697101681621, 1697101682511]"
4389,4389,827,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684908,1697101689611.0,120,,,"[18, 1190, 121, 91, 89, 87, 87, 646, 101, 100, 94, 92, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101684926, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4390,4390,868,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697397,1697101700636.0,120,,,"[6, 369, 566, 1249, 84, 82, 81, 80, 83]","[1697101697403, 1697101697772, 1697101698338, 1697101699587, 1697101699671, 1697101699753, 1697101699834, 1697101699914, 1697101699997]"
4391,4391,637,35,[],200,llama-13b,128,1,4426.0,1.0,1,H100,1697101667240,1697101671666.0,120,96.0,20.0,"[7, 1016, 133, 101, 95, 71, 91, 90, 724, 99, 94, 93, 92, 89, 676, 99, 71, 92, 91, 89, 513]","[1697101667247, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666]"
4392,4392,410,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702162,1697101703171.0,120,,,"[71, 802]","[1697101702233, 1697101703035]"
4393,4393,740,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101649243,1697101651603.0,120,,,"[18, 650, 355, 106, 104, 101, 91, 86]","[1697101649261, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650754]"
4394,4394,151,36,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703175,1697101704899.0,120,39.0,1.0,"[400, 1324]","[1697101703575, 1697101704899]"
4395,4395,768,43,[],200,llama-13b,128,1,2832.0,1.0,1,H100,1697101703176,1697101706008.0,120,47.0,6.0,"[453, 1271, 117, 96, 72, 728, 95]","[1697101703629, 1697101704900, 1697101705017, 1697101705113, 1697101705185, 1697101705913, 1697101706008]"
4396,4396,509,37,[],200,llama-13b,128,1,1107.0,1.0,1,H100,1697101704901,1697101706008.0,120,286.0,3.0,"[18, 859, 135, 95]","[1697101704919, 1697101705778, 1697101705913, 1697101706008]"
4397,4397,287,28,[],200,llama-13b,128,1,795.0,1.0,1,H100,1697101665975,1697101666770.0,120,10.0,1.0,"[30, 765]","[1697101666005, 1697101666770]"
4398,4398,883,33,[],200,llama-13b,128,1,181.0,1.0,1,H100,1697101675790,1697101675971.0,120,563.0,1.0,"[12, 169]","[1697101675802, 1697101675971]"
4399,4399,313,34,[],200,llama-13b,128,1,1408.0,1.0,1,H100,1697101675972,1697101677380.0,120,20.0,1.0,"[434, 974]","[1697101676406, 1697101677380]"
4400,4400,839,38,[],200,llama-13b,128,1,1535.0,1.0,1,H100,1697101706009,1697101707544.0,120,58.0,5.0,"[12, 1023, 242, 96, 92, 70]","[1697101706021, 1697101707044, 1697101707286, 1697101707382, 1697101707474, 1697101707544]"
4401,4401,615,29,[],200,llama-13b,128,1,4895.0,1.0,1,H100,1697101666771,1697101671666.0,120,93.0,20.0,"[12, 1613, 101, 95, 71, 91, 90, 724, 99, 95, 92, 92, 89, 676, 99, 71, 92, 91, 89, 513]","[1697101666783, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669762, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666]"
4402,4402,37,26,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101682705,1697101683641.0,120,20.0,1.0,"[42, 894]","[1697101682747, 1697101683641]"
4403,4403,395,27,[],200,llama-13b,128,1,4982.0,1.0,1,H100,1697101683642,1697101688624.0,120,88.0,20.0,"[18, 1247, 106, 87, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 94, 92, 91, 88, 715, 106]","[1697101683660, 1697101684907, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
4404,4404,344,28,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101709492,1697101710428.0,120,13.0,1.0,"[30, 905]","[1697101709522, 1697101710427]"
4405,4405,271,39,[],200,llama-13b,128,1,4402.0,1.0,1,H100,1697101707545,1697101711947.0,120,87.0,20.0,"[6, 762, 115, 99, 64, 88, 811, 89, 83, 82, 79, 882, 103, 97, 92, 89, 88, 86, 498, 98, 91]","[1697101707551, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947]"
4406,4406,637,35,[],200,llama-13b,128,1,3424.0,1.0,1,H100,1697101677381,1697101680805.0,120,96.0,20.0,"[54, 812, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 104, 57, 73]","[1697101677435, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680675, 1697101680732, 1697101680805]"
4407,4407,701,29,[],200,llama-13b,128,1,2917.0,1.0,1,H100,1697101710429,1697101713346.0,120,58.0,43.0,"[18, 1213, 98, 99, 90, 85, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26, 24, 20, 20, 21, 19, 18, 19, 20, 18, 19, 17, 17]","[1697101710447, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114, 1697101713138, 1697101713158, 1697101713178, 1697101713199, 1697101713218, 1697101713236, 1697101713255, 1697101713275, 1697101713293, 1697101713312, 1697101713329, 1697101713346]"
4408,4408,136,32,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,31.0,1.0,"[255, 1309]","[1697101676071, 1697101677380]"
4409,4409,754,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688625,1697101689595.0,120,,,[37],[1697101688662]
4410,4410,493,33,[],200,llama-13b,128,1,3424.0,1.0,1,H100,1697101677381,1697101680805.0,120,83.0,20.0,"[54, 812, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 104, 56, 73]","[1697101677435, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680675, 1697101680731, 1697101680804]"
4411,4411,194,44,[],200,llama-13b,128,1,3653.0,1.0,1,H100,1697101706009,1697101709662.0,120,335.0,16.0,"[6, 1029, 242, 96, 92, 70, 92, 69, 91, 87, 545, 98, 65, 88, 812, 88, 83]","[1697101706015, 1697101707044, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662]"
4412,4412,62,36,[],200,llama-13b,128,1,4521.0,1.0,1,H100,1697101680806,1697101685327.0,120,91.0,20.0,"[65, 1640, 193, 96, 93, 92, 69, 90, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 81]","[1697101680871, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327]"
4413,4413,555,45,[],200,llama-13b,128,1,765.0,1.0,1,H100,1697101709663,1697101710428.0,120,11.0,1.0,"[7, 758]","[1697101709670, 1697101710428]"
4414,4414,278,34,[],200,llama-13b,128,1,666.0,1.0,1,H100,1697101684241,1697101684907.0,120,13.0,1.0,"[7, 659]","[1697101684248, 1697101684907]"
4415,4415,154,29,[],200,llama-13b,128,1,1495.0,1.0,1,H100,1697101689617,1697101691112.0,120,13.0,1.0,"[356, 1139]","[1697101689973, 1697101691112]"
4416,4416,513,30,[],200,llama-13b,128,1,3703.0,1.0,1,H100,1697101691113,1697101694816.0,120,83.0,20.0,"[66, 851, 244, 214, 211, 79, 109, 93, 95, 404, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 85]","[1697101691179, 1697101692030, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692887, 1697101692980, 1697101693075, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816]"
4417,4417,635,35,[],200,llama-13b,128,1,1208.0,1.0,1,H100,1697101684908,1697101686116.0,120,23.0,1.0,"[25, 1183]","[1697101684933, 1697101686116]"
4418,4418,32,36,[],200,llama-13b,128,1,1506.0,1.0,1,H100,1697101686118,1697101687624.0,120,140.0,6.0,"[41, 863, 215, 102, 99, 95, 91]","[1697101686159, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624]"
4419,4419,58,37,[],200,llama-13b,128,1,1518.0,1.0,1,H100,1697101700642,1697101702160.0,120,15.0,1.0,"[440, 1078]","[1697101701082, 1697101702160]"
4420,4420,386,37,[],200,llama-13b,128,1,1280.0,1.0,1,H100,1697101687625,1697101688905.0,120,140.0,6.0,"[6, 754, 133, 106, 97, 92, 92]","[1697101687631, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905]"
4421,4421,419,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702162,1697101703171.0,120,,,"[117, 756]","[1697101702279, 1697101703035]"
4422,4422,422,37,[],200,llama-13b,128,1,1694.0,1.0,1,H100,1697101685329,1697101687023.0,120,26.0,1.0,"[60, 1633]","[1697101685389, 1697101687022]"
4423,4423,782,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687023,1697101689594.0,120,,,"[19, 1343, 133, 106, 97, 92, 92, 89]","[1697101687042, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4424,4424,884,46,[],200,llama-13b,128,1,2382.0,1.0,1,H100,1697101710429,1697101712811.0,120,90.0,20.0,"[30, 1201, 98, 99, 90, 85, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 41, 31, 32]","[1697101710459, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712748, 1697101712779, 1697101712811]"
4425,4425,826,34,[],200,llama-13b,128,1,4520.0,1.0,1,H100,1697101680806,1697101685326.0,120,87.0,20.0,"[13, 1692, 193, 96, 93, 92, 68, 91, 90, 614, 102, 101, 97, 92, 90, 90, 594, 86, 63, 83, 80]","[1697101680819, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685326]"
4426,4426,707,40,[],200,llama-13b,128,1,1129.0,1.0,1,H100,1697101682512,1697101683641.0,120,8.0,1.0,"[36, 1093]","[1697101682548, 1697101683641]"
4427,4427,429,30,[],200,llama-13b,128,1,11262.0,1.0,1,H100,1697101657235,1697101668497.0,120,244.0,50.0,"[12, 913, 129, 96, 82, 82, 81, 1075, 101, 99, 97, 92, 86, 805, 102, 75, 104, 87, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 100, 689, 105, 100, 72, 72, 93, 92, 733, 104, 98, 73, 98, 95, 793, 101]","[1697101657247, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658630, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497]"
4428,4428,369,45,[],200,llama-13b,128,1,3973.0,1.0,1,H100,1697101707884,1697101711857.0,120,216.0,15.0,"[6, 1484, 116, 89, 83, 82, 79, 882, 103, 97, 92, 89, 88, 86, 498, 99]","[1697101707890, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711857]"
4429,4429,280,26,[],200,llama-13b,128,1,4264.0,1.0,1,H100,1697101670717,1697101674981.0,120,91.0,20.0,"[16, 826, 107, 97, 95, 94, 93, 91, 520, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687]","[1697101670733, 1697101671559, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981]"
4430,4430,215,39,[],200,llama-13b,128,1,1497.0,1.0,1,H100,1697101689615,1697101691112.0,120,12.0,1.0,"[192, 1305]","[1697101689807, 1697101691112]"
4431,4431,169,35,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101688520,1697101689571.0,120,10.0,1.0,"[11, 1040]","[1697101688531, 1697101689571]"
4432,4432,501,36,[],200,llama-13b,128,1,1540.0,1.0,1,H100,1697101689572,1697101691112.0,120,19.0,1.0,"[34, 1506]","[1697101689606, 1697101691112]"
4433,4433,439,26,[],200,llama-13b,128,1,1778.0,1.0,1,H100,1697101675814,1697101677592.0,120,13.0,4.0,"[194, 1371, 98, 64, 51]","[1697101676008, 1697101677379, 1697101677477, 1697101677541, 1697101677592]"
4434,4434,236,29,[],200,llama-13b,128,1,955.0,1.0,1,H100,1697101673893,1697101674848.0,120,8.0,1.0,"[60, 895]","[1697101673953, 1697101674848]"
4435,4435,597,30,[],200,llama-13b,128,1,940.0,1.0,1,H100,1697101674849,1697101675789.0,120,39.0,1.0,"[48, 892]","[1697101674897, 1697101675789]"
4436,4436,587,28,[],200,llama-13b,128,1,565.0,1.0,1,H100,1697101675815,1697101676380.0,120,13.0,1.0,"[107, 458]","[1697101675922, 1697101676380]"
4437,4437,18,29,[],200,llama-13b,128,1,1866.0,1.0,1,H100,1697101676381,1697101678247.0,120,15.0,1.0,"[103, 1763]","[1697101676484, 1697101678247]"
4438,4438,378,30,[],200,llama-13b,128,1,9284.0,1.0,1,H100,1697101678248,1697101687532.0,120,93.0,47.0,"[24, 1133, 99, 97, 95, 72, 92, 90, 70, 91, 460, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97, 92, 90, 90, 594, 85, 64, 82, 81, 911, 91, 89, 87, 86, 647, 101, 99, 95]","[1697101678272, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532]"
4439,4439,801,27,[],200,llama-13b,128,1,5111.0,1.0,1,H100,1697101677593,1697101682704.0,120,47.0,20.0,"[6, 1689, 117, 98, 98, 95, 72, 92, 90, 70, 90, 461, 103, 57, 73, 804, 84, 83, 79, 79, 771]","[1697101677599, 1697101679288, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704]"
4440,4440,2,31,[],200,llama-13b,128,1,2564.0,1.0,1,H100,1697101675790,1697101678354.0,120,58.0,6.0,"[25, 565, 36, 1061, 64, 50, 763]","[1697101675815, 1697101676380, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354]"
4441,4441,359,32,[],200,llama-13b,128,1,932.0,1.0,1,H100,1697101678356,1697101679288.0,120,10.0,1.0,"[12, 920]","[1697101678368, 1697101679288]"
4442,4442,52,23,[],200,llama-13b,128,1,1275.0,1.0,1,H100,1697101684052,1697101685327.0,120,58.0,6.0,"[24, 831, 107, 86, 63, 83, 81]","[1697101684076, 1697101684907, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327]"
4443,4443,474,16,[],200,llama-13b,128,1,6557.0,1.0,1,H100,1697101640211,1697101646768.0,120,109.0,33.0,"[89, 820, 84, 79, 77, 761, 100, 99, 96, 89, 82, 63, 801, 98, 97, 92, 96, 86, 85, 414, 87, 84, 84, 82, 684, 78, 78, 77, 75, 741, 98, 97, 84]","[1697101640300, 1697101641120, 1697101641204, 1697101641283, 1697101641360, 1697101642121, 1697101642221, 1697101642320, 1697101642416, 1697101642505, 1697101642587, 1697101642650, 1697101643451, 1697101643549, 1697101643646, 1697101643738, 1697101643834, 1697101643920, 1697101644005, 1697101644419, 1697101644506, 1697101644590, 1697101644674, 1697101644756, 1697101645440, 1697101645518, 1697101645596, 1697101645673, 1697101645748, 1697101646489, 1697101646587, 1697101646684, 1697101646768]"
4444,4444,245,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[258, 1260, 122, 69]","[1697101700900, 1697101702160, 1697101702282, 1697101702351]"
4445,4445,670,29,[],200,llama-13b,128,1,3913.0,1.0,1,H100,1697101661267,1697101665180.0,120,67.0,18.0,"[25, 624, 356, 106, 101, 95, 94, 90, 706, 101, 93, 88, 87, 845, 109, 102, 96, 94, 101]","[1697101661292, 1697101661916, 1697101662272, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662758, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180]"
4446,4446,106,30,[],200,llama-13b,128,1,5529.0,1.0,1,H100,1697101665182,1697101670711.0,120,161.0,20.0,"[12, 1576, 366, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 726, 98, 94, 93, 92, 89, 676]","[1697101665194, 1697101666770, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711]"
4447,4447,317,31,[],200,llama-13b,128,1,9619.0,1.0,1,H100,1697101663465,1697101673084.0,120,244.0,50.0,"[31, 950, 232, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 733, 104, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88]","[1697101663496, 1697101664446, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084]"
4448,4448,797,35,[],200,llama-13b,128,1,1101.0,1.0,1,H100,1697101672658,1697101673759.0,120,26.0,1.0,"[66, 1035]","[1697101672724, 1697101673759]"
4449,4449,229,28,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101682705,1697101683641.0,120,15.0,1.0,"[18, 918]","[1697101682723, 1697101683641]"
4450,4450,588,29,[],200,llama-13b,128,1,1265.0,1.0,1,H100,1697101683642,1697101684907.0,120,11.0,1.0,"[24, 1241]","[1697101683666, 1697101684907]"
4451,4451,199,36,[],200,llama-13b,128,1,1088.0,1.0,1,H100,1697101673760,1697101674848.0,120,13.0,1.0,"[36, 1052]","[1697101673796, 1697101674848]"
4452,4452,10,30,[],200,llama-13b,128,1,2530.0,1.0,1,H100,1697101684908,1697101687438.0,120,563.0,9.0,"[24, 1184, 121, 91, 89, 87, 87, 646, 101, 100]","[1697101684932, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687338, 1697101687438]"
4453,4453,556,37,[],200,llama-13b,128,1,940.0,1.0,1,H100,1697101674849,1697101675789.0,120,9.0,1.0,"[36, 904]","[1697101674885, 1697101675789]"
4454,4454,913,38,[],200,llama-13b,128,1,4320.0,1.0,1,H100,1697101675790,1697101680110.0,120,88.0,20.0,"[19, 162, 445, 1061, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98, 95, 71, 92, 91, 70, 90]","[1697101675809, 1697101675971, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679950, 1697101680020, 1697101680110]"
4455,4455,456,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687439,1697101689594.0,120,,,"[6, 940, 133, 106, 97, 92, 92, 89]","[1697101687445, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4456,4456,56,24,[],200,llama-13b,128,1,4265.0,1.0,1,H100,1697101663338,1697101667603.0,120,86.0,20.0,"[12, 1095, 233, 108, 103, 96, 94, 101, 688, 105, 101, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95]","[1697101663350, 1697101664445, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603]"
4457,4457,343,39,[],200,llama-13b,128,1,4309.0,1.0,1,H100,1697101680111,1697101684420.0,120,84.0,20.0,"[8, 1383, 106, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680119, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
4458,4458,796,24,[],200,llama-13b,128,1,3993.0,1.0,1,H100,1697101668664,1697101672657.0,120,86.0,20.0,"[12, 768, 125, 98, 95, 92, 93, 88, 676, 99, 70, 93, 90, 91, 511, 98, 95, 94, 93, 91, 521]","[1697101668676, 1697101669444, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672136, 1697101672657]"
4459,4459,292,40,[],200,llama-13b,128,1,1521.0,1.0,1,H100,1697101700638,1697101702159.0,120,286.0,1.0,"[17, 1504]","[1697101700655, 1697101702159]"
4460,4460,650,41,[],200,llama-13b,128,1,874.0,1.0,1,H100,1697101702161,1697101703035.0,120,13.0,1.0,"[66, 808]","[1697101702227, 1697101703035]"
4461,4461,224,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672658,1697101675812.0,120,,,"[37, 1064, 129, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672695, 1697101673759, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4462,4462,414,25,[],200,llama-13b,128,1,5052.0,1.0,1,H100,1697101667605,1697101672657.0,120,87.0,20.0,"[12, 1827, 124, 99, 94, 93, 92, 89, 676, 99, 71, 92, 91, 89, 513, 97, 95, 94, 93, 90, 522]","[1697101667617, 1697101669444, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657]"
4463,4463,707,30,[],200,llama-13b,128,1,841.0,1.0,1,H100,1697101708533,1697101709374.0,120,8.0,1.0,"[142, 699]","[1697101708675, 1697101709374]"
4464,4464,80,42,[],200,llama-13b,128,1,652.0,1.0,1,H100,1697101703036,1697101703688.0,120,13.0,1.0,"[54, 598]","[1697101703090, 1697101703688]"
4465,4465,417,43,[],200,llama-13b,128,1,2089.0,1.0,1,H100,1697101703689,1697101705778.0,120,17.0,1.0,"[18, 2071]","[1697101703707, 1697101705778]"
4466,4466,135,31,[],200,llama-13b,128,1,1330.0,1.0,1,H100,1697101709375,1697101710705.0,120,52.0,2.0,"[25, 1028, 277]","[1697101709400, 1697101710428, 1697101710705]"
4467,4467,776,44,[],200,llama-13b,128,1,1506.0,1.0,1,H100,1697101705779,1697101707285.0,120,67.0,2.0,"[6, 1259, 241]","[1697101705785, 1697101707044, 1697101707285]"
4468,4468,782,39,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,90.0,20.0,"[430, 1294, 116, 97, 72, 728, 95, 87, 85, 83, 79, 944, 96, 92, 70, 92, 69, 92, 85, 546, 98]","[1697101703606, 1697101704900, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707797, 1697101707882, 1697101708428, 1697101708526]"
4469,4469,926,41,[],200,llama-13b,128,1,2279.0,1.0,1,H100,1697101710809,1697101713088.0,120,563.0,30.0,"[12, 839, 98, 99, 90, 85, 83, 78, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27]","[1697101710821, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088]"
4470,4470,582,26,[],200,llama-13b,128,1,1563.0,1.0,1,H100,1697101675817,1697101677380.0,120,19.0,1.0,"[560, 1003]","[1697101676377, 1697101677380]"
4471,4471,912,27,[],200,llama-13b,128,1,3424.0,1.0,1,H100,1697101677381,1697101680805.0,120,92.0,20.0,"[90, 776, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 104, 57, 73]","[1697101677471, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680675, 1697101680732, 1697101680805]"
4472,4472,441,36,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,6.0,1.0,"[377, 1187]","[1697101676193, 1697101677380]"
4473,4473,204,45,[],200,llama-13b,128,1,2204.0,1.0,1,H100,1697101707286,1697101709490.0,120,67.0,6.0,"[7, 1020, 115, 99, 64, 88, 811]","[1697101707293, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709490]"
4474,4474,800,37,[],200,llama-13b,128,1,3424.0,1.0,1,H100,1697101677381,1697101680805.0,120,140.0,20.0,"[42, 932, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 104, 56, 73]","[1697101677423, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680675, 1697101680731, 1697101680804]"
4475,4475,558,46,[],200,llama-13b,128,1,3052.0,1.0,1,H100,1697101709492,1697101712544.0,120,58.0,20.0,"[6, 929, 278, 103, 97, 92, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55]","[1697101709498, 1697101710427, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
4476,4476,922,23,[],200,llama-13b,128,1,5001.0,1.0,1,H100,1697101651610,1697101656611.0,120,91.0,20.0,"[276, 1586, 138, 85, 84, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 98, 82]","[1697101651886, 1697101653472, 1697101653610, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656611]"
4477,4477,210,40,[],200,llama-13b,128,1,2171.0,1.0,1,H100,1697101708534,1697101710705.0,120,140.0,2.0,"[171, 1722, 277]","[1697101708705, 1697101710427, 1697101710704]"
4478,4478,565,41,[],200,llama-13b,128,1,2106.0,1.0,1,H100,1697101710705,1697101712811.0,120,91.0,20.0,"[7, 948, 98, 99, 90, 85, 83, 78, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 41, 31, 32]","[1697101710712, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712748, 1697101712779, 1697101712811]"
4479,4479,212,40,[],200,llama-13b,128,1,841.0,1.0,1,H100,1697101708533,1697101709374.0,120,31.0,1.0,"[136, 705]","[1697101708669, 1697101709374]"
4480,4480,549,41,[],200,llama-13b,128,1,3169.0,1.0,1,H100,1697101709375,1697101712544.0,120,93.0,20.0,"[19, 1311, 102, 98, 92, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55]","[1697101709394, 1697101710705, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
4481,4481,864,37,[],200,llama-13b,128,1,3703.0,1.0,1,H100,1697101691113,1697101694816.0,120,83.0,20.0,"[25, 892, 244, 213, 212, 79, 111, 92, 94, 403, 102, 100, 99, 95, 91, 87, 68, 433, 78, 100, 85]","[1697101691138, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692889, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816]"
4482,4482,752,31,[],200,llama-13b,128,1,967.0,1.0,1,H100,1697101677478,1697101678445.0,120,39.0,3.0,"[19, 750, 108, 90]","[1697101677497, 1697101678247, 1697101678355, 1697101678445]"
4483,4483,704,31,[],200,llama-13b,128,1,842.0,1.0,1,H100,1697101670717,1697101671559.0,120,14.0,1.0,"[32, 810]","[1697101670749, 1697101671559]"
4484,4484,186,32,[],200,llama-13b,128,1,4447.0,1.0,1,H100,1697101678446,1697101682893.0,120,123.0,22.0,"[24, 818, 117, 99, 98, 94, 72, 92, 91, 69, 91, 460, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93]","[1697101678470, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893]"
4485,4485,140,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671560,1697101675812.0,120,,,"[7, 979, 110, 113, 92, 68, 67, 88, 804, 99, 88, 88, 66, 66, 686, 102, 98, 95, 71, 94]","[1697101671567, 1697101672546, 1697101672656, 1697101672769, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673987, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675347, 1697101675441]"
4486,4486,293,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[70, 1295, 223, 103, 101, 110, 92, 91]","[1697101694888, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4487,4487,647,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697596,1697101700635.0,120,,,"[277, 1602, 113, 84, 81, 82, 80, 82]","[1697101697873, 1697101699475, 1697101699588, 1697101699672, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
4488,4488,77,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[239, 1279, 122, 69]","[1697101700881, 1697101702160, 1697101702282, 1697101702351]"
4489,4489,378,30,[],200,llama-13b,128,1,9512.0,1.0,1,H100,1697101675814,1697101685326.0,120,93.0,47.0,"[367, 1296, 64, 50, 764, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 91, 69, 90, 461, 103, 57, 73, 803, 85, 83, 79, 78, 771, 97, 93, 91, 69, 91, 90, 614, 102, 101, 97, 91, 91, 90, 593, 86, 64, 82, 81]","[1697101676181, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681607, 1697101681692, 1697101681775, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684239, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326]"
4490,4490,355,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687024,1697101689595.0,120,,,"[36, 1325, 133, 106, 97, 92, 92, 89]","[1697101687060, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4491,4491,402,41,[],200,llama-13b,128,1,2832.0,1.0,1,H100,1697101703176,1697101706008.0,120,457.0,6.0,"[429, 1295, 116, 97, 72, 728, 95]","[1697101703605, 1697101704900, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008]"
4492,4492,266,41,[],200,llama-13b,128,1,2392.0,1.0,1,H100,1697101700642,1697101703034.0,120,9.0,1.0,"[465, 1927]","[1697101701107, 1697101703034]"
4493,4493,894,28,[],200,llama-13b,128,1,835.0,1.0,1,H100,1697101670725,1697101671560.0,120,14.0,1.0,"[38, 796]","[1697101670763, 1697101671559]"
4494,4494,385,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700635.0,120,,,"[319, 1561, 112, 84, 82, 81, 81, 82]","[1697101697914, 1697101699475, 1697101699587, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4495,4495,316,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671560,1697101675812.0,120,,,"[19, 967, 110, 114, 91, 68, 67, 88, 804, 98, 89, 88, 66, 65, 687, 102, 99, 94, 71, 94]","[1697101671579, 1697101672546, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675182, 1697101675276, 1697101675347, 1697101675441]"
4496,4496,497,34,[],200,llama-13b,128,1,687.0,1.0,1,H100,1697101694733,1697101695420.0,120,67.0,2.0,"[6, 597, 84]","[1697101694739, 1697101695336, 1697101695420]"
4497,4497,853,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695421,1697101697588.0,120,,,"[13, 749, 224, 102, 101, 111, 92, 91]","[1697101695434, 1697101696183, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696813, 1697101696904]"
4498,4498,743,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[360, 1157, 123, 69]","[1697101701002, 1697101702159, 1697101702282, 1697101702351]"
4499,4499,281,36,[],200,llama-13b,128,1,1879.0,1.0,1,H100,1697101697596,1697101699475.0,120,23.0,1.0,"[372, 1507]","[1697101697968, 1697101699475]"
4500,4500,492,44,[],200,llama-13b,128,1,4010.0,1.0,1,H100,1697101708534,1697101712544.0,120,47.0,20.0,"[165, 1728, 277, 103, 97, 93, 89, 88, 86, 497, 100, 91, 84, 82, 79, 59, 69, 51, 60, 57, 55]","[1697101708699, 1697101710427, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711857, 1697101711948, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
4501,4501,635,37,[],200,llama-13b,128,1,1075.0,1.0,1,H100,1697101699477,1697101700552.0,120,23.0,1.0,"[54, 1021]","[1697101699531, 1697101700552]"
4502,4502,713,33,[],200,llama-13b,128,1,2486.0,1.0,1,H100,1697101679289,1697101681775.0,120,874.0,8.0,"[6, 1276, 103, 57, 73, 804, 84, 83]","[1697101679295, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775]"
4503,4503,66,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700553,1697101703170.0,120,,,"[24, 517, 1188, 69]","[1697101700577, 1697101701094, 1697101702282, 1697101702351]"
4504,4504,397,39,[],200,llama-13b,128,1,1843.0,1.0,1,H100,1697101703174,1697101705017.0,120,67.0,2.0,"[438, 1288, 117]","[1697101703612, 1697101704900, 1697101705017]"
4505,4505,601,43,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,83.0,20.0,"[245, 1479, 108, 105, 72, 728, 95, 87, 85, 83, 80, 942, 97, 92, 70, 91, 70, 91, 87, 545, 98]","[1697101703421, 1697101704900, 1697101705008, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
4506,4506,759,40,[],200,llama-13b,128,1,4467.0,1.0,1,H100,1697101705024,1697101709491.0,120,92.0,20.0,"[49, 705, 135, 95, 87, 85, 84, 79, 942, 96, 93, 70, 91, 70, 91, 87, 544, 99, 65, 88, 812]","[1697101705073, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706264, 1697101706343, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709491]"
4507,4507,494,33,[],200,llama-13b,128,1,3108.0,1.0,1,H100,1697101684425,1697101687533.0,120,6.0,10.0,"[7, 1684, 121, 91, 89, 87, 86, 647, 102, 99, 95]","[1697101684432, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533]"
4508,4508,140,34,[],200,llama-13b,128,1,3550.0,1.0,1,H100,1697101681776,1697101685326.0,120,96.0,20.0,"[6, 729, 193, 97, 92, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90, 593, 86, 64, 82, 81]","[1697101681782, 1697101682511, 1697101682704, 1697101682801, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326]"
4509,4509,189,41,[],200,llama-13b,128,1,3052.0,1.0,1,H100,1697101709492,1697101712544.0,120,88.0,20.0,"[24, 911, 278, 103, 97, 92, 89, 89, 85, 498, 98, 91, 85, 82, 79, 59, 69, 51, 60, 57, 55]","[1697101709516, 1697101710427, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
4510,4510,118,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673892,1697101675812.0,120,,,"[25, 931, 133, 102, 99, 95, 70, 94]","[1697101673917, 1697101674848, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
4511,4511,691,36,[],200,llama-13b,128,1,940.0,1.0,1,H100,1697101674849,1697101675789.0,120,47.0,1.0,"[54, 886]","[1697101674903, 1697101675789]"
4512,4512,497,35,[],200,llama-13b,128,1,1909.0,1.0,1,H100,1697101685328,1697101687237.0,120,67.0,2.0,"[25, 1669, 215]","[1697101685353, 1697101687022, 1697101687237]"
4513,4513,829,36,[],200,llama-13b,128,1,1147.0,1.0,1,H100,1697101687238,1697101688385.0,120,20.0,1.0,"[7, 1140]","[1697101687245, 1697101688385]"
4514,4514,121,37,[],200,llama-13b,128,1,590.0,1.0,1,H100,1697101675790,1697101676380.0,120,13.0,1.0,"[36, 554]","[1697101675826, 1697101676380]"
4515,4515,260,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688386,1697101689594.0,120,,,"[19, 1165]","[1697101688405, 1697101689570]"
4516,4516,476,26,[],200,llama-13b,128,1,10601.0,1.0,1,H100,1697101675816,1697101686417.0,120,6.0,50.0,"[493, 1071, 97, 64, 51, 762, 90, 86, 64, 84, 80, 647, 99, 98, 94, 72, 92, 91, 69, 91, 468, 95, 57, 73, 804, 84, 83, 79, 78, 771, 97, 93, 91, 69, 91, 90, 614, 102, 101, 97, 91, 91, 90, 593, 86, 64, 82, 81, 911, 91, 89]","[1697101676309, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680111, 1697101680579, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684239, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417]"
4517,4517,621,38,[],200,llama-13b,128,1,5119.0,1.0,1,H100,1697101689612,1697101694731.0,120,88.0,20.0,"[77, 1423, 93, 1069, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100]","[1697101689689, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
4518,4518,474,38,[],200,llama-13b,128,1,7468.0,1.0,1,H100,1697101676381,1697101683849.0,120,109.0,33.0,"[51, 1922, 90, 86, 64, 84, 81, 646, 98, 98, 95, 71, 92, 92, 69, 90, 470, 94, 57, 73, 804, 84, 83, 79, 79, 770, 97, 93, 91, 69, 91, 90, 615]","[1697101676432, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679951, 1697101680020, 1697101680110, 1697101680580, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849]"
4519,4519,855,27,[],200,llama-13b,128,1,4034.0,1.0,1,H100,1697101669855,1697101673889.0,120,83.0,20.0,"[6, 624, 226, 99, 71, 92, 91, 90, 512, 97, 95, 94, 93, 90, 521, 114, 91, 68, 68, 88, 803]","[1697101669861, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888]"
4520,4520,170,23,[],200,llama-13b,128,1,3907.0,1.0,1,H100,1697101651606,1697101655513.0,120,335.0,15.0,"[9, 1045, 58, 891, 86, 83, 82, 81, 896, 107, 106, 100, 75, 99, 97, 92]","[1697101651615, 1697101652660, 1697101652718, 1697101653609, 1697101653695, 1697101653778, 1697101653860, 1697101653941, 1697101654837, 1697101654944, 1697101655050, 1697101655150, 1697101655225, 1697101655324, 1697101655421, 1697101655513]"
4521,4521,475,27,[],200,llama-13b,128,1,4784.0,1.0,1,H100,1697101660003,1697101664787.0,120,89.0,20.0,"[13, 630, 339, 102, 75, 98, 93, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 109]","[1697101660016, 1697101660646, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664787]"
4522,4522,830,27,[],200,llama-13b,128,1,2100.0,1.0,1,H100,1697101686418,1697101688518.0,120,140.0,9.0,"[6, 598, 215, 102, 99, 95, 91, 91, 88, 715]","[1697101686424, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518]"
4523,4523,256,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688519,1697101689594.0,120,,,"[12, 1039]","[1697101688531, 1697101689570]"
4524,4524,558,26,[],200,llama-13b,128,1,5043.0,1.0,1,H100,1697101668845,1697101673888.0,120,58.0,20.0,"[13, 1627, 226, 99, 70, 93, 90, 91, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 67, 88, 804]","[1697101668858, 1697101670485, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888]"
4525,4525,614,29,[],200,llama-13b,128,1,2412.0,1.0,1,H100,1697101689618,1697101692030.0,120,15.0,1.0,"[453, 1959]","[1697101690071, 1697101692030]"
4526,4526,746,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672658,1697101675811.0,120,,,"[12, 1089, 129, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672670, 1697101673759, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4527,4527,18,30,[],200,llama-13b,128,1,1341.0,1.0,1,H100,1697101692031,1697101693372.0,120,15.0,1.0,"[18, 1323]","[1697101692049, 1697101693372]"
4528,4528,377,31,[],200,llama-13b,128,1,1083.0,1.0,1,H100,1697101693374,1697101694457.0,120,13.0,1.0,"[35, 1048]","[1697101693409, 1697101694457]"
4529,4529,738,32,[],200,llama-13b,128,1,2051.0,1.0,1,H100,1697101694458,1697101696509.0,120,79.0,6.0,"[13, 949, 78, 77, 831, 103]","[1697101694471, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509]"
4530,4530,834,28,[],200,llama-13b,128,1,4056.0,1.0,1,H100,1697101664788,1697101668844.0,120,85.0,20.0,"[18, 933, 129, 106, 99, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90]","[1697101664806, 1697101665739, 1697101665868, 1697101665974, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844]"
4531,4531,521,24,[],200,llama-13b,128,1,1632.0,1.0,1,H100,1697101655514,1697101657146.0,120,18.0,1.0,"[6, 1626]","[1697101655520, 1697101657146]"
4532,4532,168,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696512,1697101697589.0,120,,,"[6, 878]","[1697101696518, 1697101697396]"
4533,4533,880,25,[],200,llama-13b,128,1,1139.0,1.0,1,H100,1697101657150,1697101658289.0,120,84.0,2.0,"[12, 998, 129]","[1697101657162, 1697101658160, 1697101658289]"
4534,4534,466,32,[],200,llama-13b,128,1,2105.0,1.0,1,H100,1697101710706,1697101712811.0,120,457.0,20.0,"[6, 948, 98, 99, 90, 85, 83, 78, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32]","[1697101710712, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811]"
4535,4535,772,34,[],200,llama-13b,128,1,5119.0,1.0,1,H100,1697101689612,1697101694731.0,120,83.0,20.0,"[76, 1424, 93, 1069, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100]","[1697101689688, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
4536,4536,280,26,[],200,llama-13b,128,1,5174.0,1.0,1,H100,1697101658291,1697101663465.0,120,91.0,20.0,"[48, 1034, 332, 102, 98, 97, 93, 85, 805, 101, 76, 98, 93, 87, 831, 107, 102, 95, 93, 89, 707]","[1697101658339, 1697101659373, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662480, 1697101662575, 1697101662668, 1697101662757, 1697101663464]"
4537,4537,421,27,[],200,llama-13b,128,1,4774.0,1.0,1,H100,1697101683850,1697101688624.0,120,85.0,20.0,"[6, 1051, 106, 87, 63, 83, 81, 910, 91, 89, 87, 87, 646, 102, 99, 94, 92, 91, 88, 715, 106]","[1697101683856, 1697101684907, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
4538,4538,886,27,[],200,llama-13b,128,1,957.0,1.0,1,H100,1697101673891,1697101674848.0,120,17.0,1.0,"[18, 939]","[1697101673909, 1697101674848]"
4539,4539,311,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674849,1697101675810.0,120,,,[42],[1697101674891]
4540,4540,522,34,[],200,llama-13b,128,1,1876.0,1.0,1,H100,1697101697599,1697101699475.0,120,20.0,1.0,"[512, 1364]","[1697101698111, 1697101699475]"
4541,4541,34,35,[],200,llama-13b,128,1,1075.0,1.0,1,H100,1697101699477,1697101700552.0,120,12.0,1.0,"[60, 1015]","[1697101699537, 1697101700552]"
4542,4542,671,29,[],200,llama-13b,128,1,1567.0,1.0,1,H100,1697101675813,1697101677380.0,120,12.0,1.0,"[260, 1307]","[1697101676073, 1697101677380]"
4543,4543,393,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700553,1697101703170.0,120,,,"[30, 511, 1188, 69]","[1697101700583, 1697101701094, 1697101702282, 1697101702351]"
4544,4544,136,41,[],200,llama-13b,128,1,1265.0,1.0,1,H100,1697101683642,1697101684907.0,120,31.0,1.0,"[24, 1241]","[1697101683666, 1697101684907]"
4545,4545,493,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684908,1697101689605.0,120,,,"[12, 1196, 121, 91, 89, 87, 87, 646, 101, 99, 95, 92, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101684920, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4546,4546,197,35,[],200,llama-13b,128,1,1987.0,1.0,1,H100,1697101694733,1697101696720.0,120,6.0,8.0,"[18, 585, 84, 79, 76, 831, 103, 100, 111]","[1697101694751, 1697101695336, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720]"
4547,4547,871,31,[],200,llama-13b,128,1,1994.0,1.0,1,H100,1697101694818,1697101696812.0,120,123.0,6.0,"[12, 1353, 223, 103, 101, 110, 92]","[1697101694830, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812]"
4548,4548,746,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688906,1697101689604.0,120,,,"[12, 653]","[1697101688918, 1697101689571]"
4549,4549,545,40,[],200,llama-13b,128,1,1665.0,1.0,1,H100,1697101691113,1697101692778.0,120,216.0,5.0,"[61, 856, 244, 214, 211, 79]","[1697101691174, 1697101692030, 1697101692274, 1697101692488, 1697101692699, 1697101692778]"
4550,4550,815,31,[],200,llama-13b,128,1,2110.0,1.0,1,H100,1697101685328,1697101687438.0,120,52.0,4.0,"[19, 1675, 215, 102, 99]","[1697101685347, 1697101687022, 1697101687237, 1697101687339, 1697101687438]"
4551,4551,903,41,[],200,llama-13b,128,1,1186.0,1.0,1,H100,1697101692779,1697101693965.0,120,244.0,7.0,"[12, 581, 107, 101, 100, 99, 95, 91]","[1697101692791, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965]"
4552,4552,240,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687439,1697101689594.0,120,,,"[6, 940, 133, 106, 97, 92, 92, 89]","[1697101687445, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4553,4553,324,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693966,1697101697587.0,120,,,"[12, 479, 97, 77, 101, 84, 604, 79, 76, 832, 102, 101, 110, 93, 90]","[1697101693978, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696407, 1697101696509, 1697101696610, 1697101696720, 1697101696813, 1697101696903]"
4554,4554,335,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101692779,1697101697588.0,120,,,"[24, 569, 107, 101, 100, 99, 95, 91, 88, 67, 434, 77, 100, 85, 604, 78, 77, 831, 103, 101, 110, 92, 91]","[1697101692803, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4555,4555,612,26,[],200,llama-13b,128,1,5116.0,1.0,1,H100,1697101689616,1697101694732.0,120,93.0,20.0,"[266, 1323, 1069, 213, 212, 79, 110, 92, 94, 405, 101, 100, 99, 95, 91, 88, 67, 433, 78, 101]","[1697101689882, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692888, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694732]"
4556,4556,730,28,[],200,llama-13b,128,1,2576.0,1.0,1,H100,1697101672771,1697101675347.0,120,364.0,12.0,"[6, 982, 129, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70]","[1697101672777, 1697101673759, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346]"
4557,4557,220,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101675182,1697101675811.0,120,,,"[7, 600]","[1697101675189, 1697101675789]"
4558,4558,579,25,[],200,llama-13b,128,1,1565.0,1.0,1,H100,1697101675815,1697101677380.0,120,19.0,1.0,"[482, 1083]","[1697101676297, 1697101677380]"
4559,4559,469,33,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,17.0,1.0,"[598, 966]","[1697101676414, 1697101677380]"
4560,4560,907,26,[],200,llama-13b,128,1,866.0,1.0,1,H100,1697101677381,1697101678247.0,120,10.0,1.0,"[30, 836]","[1697101677411, 1697101678247]"
4561,4561,630,33,[],200,llama-13b,128,1,747.0,1.0,1,H100,1697101682894,1697101683641.0,120,6.0,1.0,"[6, 741]","[1697101682900, 1697101683641]"
4562,4562,336,27,[],200,llama-13b,128,1,1612.0,1.0,1,H100,1697101678248,1697101679860.0,120,58.0,7.0,"[18, 1022, 117, 99, 97, 95, 72, 92]","[1697101678266, 1697101679288, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768, 1697101679860]"
4563,4563,63,34,[],200,llama-13b,128,1,1264.0,1.0,1,H100,1697101683643,1697101684907.0,120,39.0,1.0,"[35, 1229]","[1697101683678, 1697101684907]"
4564,4564,42,27,[],200,llama-13b,128,1,1450.0,1.0,1,H100,1697101694733,1697101696183.0,120,10.0,1.0,"[85, 1365]","[1697101694818, 1697101696183]"
4565,4565,422,35,[],200,llama-13b,128,1,1208.0,1.0,1,H100,1697101684908,1697101686116.0,120,26.0,1.0,"[36, 1172]","[1697101684944, 1697101686116]"
4566,4566,695,28,[],200,llama-13b,128,1,4190.0,1.0,1,H100,1697101679861,1697101684051.0,120,92.0,20.0,"[6, 613, 99, 96, 57, 72, 804, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 102, 100]","[1697101679867, 1697101680480, 1697101680579, 1697101680675, 1697101680732, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051]"
4567,4567,776,36,[],200,llama-13b,128,1,1120.0,1.0,1,H100,1697101686117,1697101687237.0,120,67.0,2.0,"[18, 888, 214]","[1697101686135, 1697101687023, 1697101687237]"
4568,4568,172,35,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703176,1697101704900.0,120,19.0,1.0,"[251, 1473]","[1697101703427, 1697101704900]"
4569,4569,177,37,[],200,llama-13b,128,1,1146.0,1.0,1,H100,1697101687239,1697101688385.0,120,14.0,1.0,"[12, 1134]","[1697101687251, 1697101688385]"
4570,4570,501,36,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,19.0,1.0,"[102, 775]","[1697101705003, 1697101705778]"
4571,4571,531,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688386,1697101689594.0,120,,,"[30, 1154]","[1697101688416, 1697101689570]"
4572,4572,892,39,[],200,llama-13b,128,1,5121.0,1.0,1,H100,1697101689610,1697101694731.0,120,87.0,20.0,"[18, 1484, 93, 1069, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100]","[1697101689628, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
4573,4573,151,29,[],200,llama-13b,128,1,441.0,1.0,1,H100,1697101675348,1697101675789.0,120,39.0,1.0,"[6, 435]","[1697101675354, 1697101675789]"
4574,4574,864,37,[],200,llama-13b,128,1,5028.0,1.0,1,H100,1697101705779,1697101710807.0,120,83.0,20.0,"[18, 1247, 241, 97, 92, 70, 91, 70, 91, 87, 544, 99, 65, 88, 812, 88, 83, 82, 79, 881, 103]","[1697101705797, 1697101707044, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807]"
4575,4575,510,30,[],200,llama-13b,128,1,626.0,1.0,1,H100,1697101675790,1697101676416.0,120,79.0,2.0,"[13, 168, 445]","[1697101675803, 1697101675971, 1697101676416]"
4576,4576,498,27,[],200,llama-13b,128,1,1129.0,1.0,1,H100,1697101682512,1697101683641.0,120,9.0,1.0,"[18, 1111]","[1697101682530, 1697101683641]"
4577,4577,857,28,[],200,llama-13b,128,1,1265.0,1.0,1,H100,1697101683642,1697101684907.0,120,18.0,1.0,"[18, 1247]","[1697101683660, 1697101684907]"
4578,4578,389,26,[],200,llama-13b,128,1,1547.0,1.0,1,H100,1697101656613,1697101658160.0,120,8.0,1.0,"[13, 1534]","[1697101656626, 1697101658160]"
4579,4579,286,29,[],200,llama-13b,128,1,2807.0,1.0,1,H100,1697101684908,1697101687715.0,120,161.0,12.0,"[30, 1299, 91, 89, 87, 87, 646, 101, 100, 94, 92, 91]","[1697101684938, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715]"
4580,4580,746,27,[],200,llama-13b,128,1,4507.0,1.0,1,H100,1697101658161,1697101662668.0,120,345.0,18.0,"[7, 1205, 332, 101, 99, 97, 92, 86, 805, 101, 76, 98, 93, 87, 831, 107, 101, 95, 94]","[1697101658168, 1697101659373, 1697101659705, 1697101659806, 1697101659905, 1697101660002, 1697101660094, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668]"
4581,4581,618,30,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101687716,1697101688385.0,120,9.0,1.0,"[6, 663]","[1697101687722, 1697101688385]"
4582,4582,52,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688386,1697101689594.0,120,,,"[13, 1171]","[1697101688399, 1697101689570]"
4583,4583,410,32,[],200,llama-13b,128,1,4160.0,1.0,1,H100,1697101689619,1697101693779.0,120,364.0,12.0,"[552, 1859, 244, 213, 212, 78, 110, 94, 94, 403, 102, 100, 99]","[1697101690171, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779]"
4584,4584,764,33,[],200,llama-13b,128,1,677.0,1.0,1,H100,1697101693780,1697101694457.0,120,39.0,1.0,"[7, 670]","[1697101693787, 1697101694457]"
4585,4585,289,38,[],200,llama-13b,128,1,2002.0,1.0,1,H100,1697101710809,1697101712811.0,120,89.0,20.0,"[18, 833, 98, 99, 90, 85, 83, 78, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32]","[1697101710827, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811]"
4586,4586,190,34,[],200,llama-13b,128,1,2445.0,1.0,1,H100,1697101694458,1697101696903.0,120,335.0,10.0,"[13, 865, 84, 78, 77, 831, 103, 100, 111, 93, 90]","[1697101694471, 1697101695336, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696813, 1697101696903]"
4587,4587,868,31,[],200,llama-13b,128,1,4383.0,1.0,1,H100,1697101676421,1697101680804.0,120,85.0,20.0,"[64, 1762, 107, 90, 86, 64, 84, 81, 646, 98, 98, 95, 71, 93, 90, 70, 90, 470, 94, 57, 73]","[1697101676485, 1697101678247, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680580, 1697101680674, 1697101680731, 1697101680804]"
4588,4588,148,28,[],200,llama-13b,128,1,668.0,1.0,1,H100,1697101662669,1697101663337.0,120,16.0,1.0,"[12, 656]","[1697101662681, 1697101663337]"
4589,4589,647,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673086,1697101675812.0,120,,,"[6, 1756, 133, 102, 98, 95, 70, 95]","[1697101673092, 1697101674848, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4590,4590,509,29,[],200,llama-13b,128,1,1449.0,1.0,1,H100,1697101663338,1697101664787.0,120,286.0,3.0,"[18, 1089, 233, 109]","[1697101663356, 1697101664445, 1697101664678, 1697101664787]"
4591,4591,870,30,[],200,llama-13b,128,1,4056.0,1.0,1,H100,1697101664788,1697101668844.0,120,88.0,20.0,"[30, 920, 130, 106, 99, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90]","[1697101664818, 1697101665738, 1697101665868, 1697101665974, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844]"
4592,4592,259,36,[],200,llama-13b,128,1,5116.0,1.0,1,H100,1697101689616,1697101694732.0,120,87.0,20.0,"[278, 1218, 93, 1069, 213, 212, 79, 111, 91, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101]","[1697101689894, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692889, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732]"
4593,4593,104,31,[],200,llama-13b,128,1,5119.0,1.0,1,H100,1697101689612,1697101694731.0,120,93.0,20.0,"[82, 1511, 1069, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100]","[1697101689694, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
4594,4594,732,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700635.0,120,,,"[212, 1668, 113, 83, 82, 81, 81, 82]","[1697101697807, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4595,4595,322,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694733,1697101697587.0,120,,,"[12, 675, 78, 77, 831, 103, 100, 111, 93, 90]","[1697101694745, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696813, 1697101696903]"
4596,4596,83,33,[],200,llama-13b,128,1,3952.0,1.0,1,H100,1697101675816,1697101679768.0,120,123.0,15.0,"[579, 985, 97, 64, 51, 762, 90, 86, 64, 84, 80, 647, 99, 98, 94, 72]","[1697101676395, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768]"
4597,4597,618,37,[],200,llama-13b,128,1,1449.0,1.0,1,H100,1697101694734,1697101696183.0,120,9.0,1.0,"[89, 1360]","[1697101694823, 1697101696183]"
4598,4598,47,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696184,1697101697588.0,120,,,"[7, 1204]","[1697101696191, 1697101697395]"
4599,4599,397,39,[],200,llama-13b,128,1,1993.0,1.0,1,H100,1697101697595,1697101699588.0,120,67.0,2.0,"[308, 1572, 112]","[1697101697903, 1697101699475, 1697101699587]"
4600,4600,727,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699589,1697101700635.0,120,,,"[6, 957]","[1697101699595, 1697101700552]"
4601,4601,156,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[78, 1443, 123, 69]","[1697101700716, 1697101702159, 1697101702282, 1697101702351]"
4602,4602,519,42,[],200,llama-13b,128,1,9414.0,1.0,1,H100,1697101703175,1697101712589.0,120,58.0,47.0,"[43, 1681, 116, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 91, 70, 91, 86, 545, 99, 65, 88, 811, 88, 84, 82, 79, 881, 103, 97, 93, 89, 88, 86, 497, 99, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45]","[1697101703218, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709578, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589]"
4603,4603,160,24,[],200,llama-13b,128,1,1519.0,1.0,1,H100,1697101700641,1697101702160.0,120,13.0,1.0,"[229, 1290]","[1697101700870, 1697101702160]"
4604,4604,177,20,[],200,llama-13b,128,1,769.0,1.0,1,H100,1697101648179,1697101648948.0,120,14.0,1.0,"[13, 755]","[1697101648192, 1697101648947]"
4605,4605,535,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648948,1697101651605.0,120,,,"[37, 926, 355, 106, 104, 101, 91, 85]","[1697101648985, 1697101649911, 1697101650266, 1697101650372, 1697101650476, 1697101650577, 1697101650668, 1697101650753]"
4606,4606,442,34,[],200,llama-13b,128,1,4471.0,1.0,1,H100,1697101679769,1697101684240.0,120,39.0,22.0,"[12, 698, 100, 96, 57, 72, 803, 85, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 102, 100, 97, 92]","[1697101679781, 1697101680479, 1697101680579, 1697101680675, 1697101680732, 1697101680804, 1697101681607, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240]"
4607,4607,865,22,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101651610,1697101652661.0,120,9.0,1.0,"[162, 889]","[1697101651772, 1697101652661]"
4608,4608,300,23,[],200,llama-13b,128,1,816.0,1.0,1,H100,1697101652662,1697101653478.0,120,9.0,1.0,"[54, 762]","[1697101652716, 1697101653478]"
4609,4609,658,24,[],200,llama-13b,128,1,1151.0,1.0,1,H100,1697101653480,1697101654631.0,120,11.0,1.0,"[103, 1048]","[1697101653583, 1697101654631]"
4610,4610,516,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703171.0,120,,,"[12, 862]","[1697101702173, 1697101703035]"
4611,4611,878,26,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,83.0,20.0,"[337, 1386, 116, 98, 72, 728, 95, 87, 85, 83, 79, 944, 96, 92, 70, 91, 70, 91, 87, 545, 98]","[1697101703513, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
4612,4612,204,37,[],200,llama-13b,128,1,3270.0,1.0,1,H100,1697101689618,1697101692888.0,120,67.0,6.0,"[367, 2044, 244, 214, 212, 78, 111]","[1697101689985, 1697101692029, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692888]"
4613,4613,279,27,[],200,llama-13b,128,1,3582.0,1.0,1,H100,1697101708533,1697101712115.0,120,67.0,18.0,"[55, 786, 116, 89, 83, 82, 79, 881, 103, 98, 92, 89, 89, 85, 498, 99, 91, 84, 83]","[1697101708588, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712115]"
4614,4614,88,25,[],200,llama-13b,128,1,5370.0,1.0,1,H100,1697101654632,1697101660002.0,120,58.0,20.0,"[18, 1348, 129, 105, 101, 98, 98, 83, 622, 81, 79, 60, 835, 96, 82, 82, 80, 1076, 101, 99, 97]","[1697101654650, 1697101655998, 1697101656127, 1697101656232, 1697101656333, 1697101656431, 1697101656529, 1697101656612, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659806, 1697101659905, 1697101660002]"
4615,4615,534,38,[],200,llama-13b,128,1,3823.0,1.0,1,H100,1697101692898,1697101696721.0,120,96.0,20.0,"[12, 462, 107, 101, 100, 100, 94, 91, 88, 67, 434, 77, 100, 85, 604, 79, 76, 831, 103, 101, 111]","[1697101692910, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693780, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696721]"
4616,4616,853,43,[],200,llama-13b,128,1,5881.0,1.0,1,H100,1697101689618,1697101695499.0,120,364.0,22.0,"[409, 2003, 244, 213, 212, 78, 101, 103, 94, 403, 102, 100, 100, 94, 90, 88, 68, 434, 77, 100, 85, 604, 78]","[1697101690027, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693780, 1697101693874, 1697101693964, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695498]"
4617,4617,438,26,[],200,llama-13b,128,1,643.0,1.0,1,H100,1697101660004,1697101660647.0,120,9.0,1.0,"[24, 619]","[1697101660028, 1697101660647]"
4618,4618,610,27,[],200,llama-13b,128,1,4295.0,1.0,1,H100,1697101675815,1697101680110.0,120,89.0,20.0,"[101, 464, 37, 1060, 64, 50, 763, 90, 86, 64, 84, 80, 647, 98, 98, 95, 71, 93, 90, 70, 90]","[1697101675916, 1697101676380, 1697101676417, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679860, 1697101679950, 1697101680020, 1697101680110]"
4619,4619,800,35,[],200,llama-13b,128,1,4383.0,1.0,1,H100,1697101684241,1697101688624.0,120,140.0,20.0,"[6, 767, 86, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106]","[1697101684247, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
4620,4620,892,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696722,1697101697589.0,120,,,"[24, 650]","[1697101696746, 1697101697396]"
4621,4621,767,27,[],200,llama-13b,128,1,1268.0,1.0,1,H100,1697101660648,1697101661916.0,120,11.0,1.0,"[36, 1232]","[1697101660684, 1697101661916]"
4622,4622,320,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697597,1697101700635.0,120,,,"[406, 1585, 83, 82, 81, 81, 82]","[1697101698003, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4623,4623,679,41,[],200,llama-13b,128,1,1521.0,1.0,1,H100,1697101700638,1697101702159.0,120,15.0,1.0,"[167, 1354]","[1697101700805, 1697101702159]"
4624,4624,107,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703171.0,120,,,[18],[1697101702179]
4625,4625,197,28,[],200,llama-13b,128,1,2869.0,1.0,1,H100,1697101661917,1697101664786.0,120,6.0,8.0,"[6, 1414, 127, 101, 93, 88, 87, 845, 108]","[1697101661923, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786]"
4626,4626,552,43,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,87.0,20.0,"[441, 1283, 116, 97, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 92, 69, 91, 86, 546, 98]","[1697101703617, 1697101704900, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707882, 1697101708428, 1697101708526]"
4627,4627,352,24,[],200,llama-13b,128,1,1772.0,1.0,1,H100,1697101656613,1697101658385.0,120,11.0,3.0,"[82, 1594, 96]","[1697101656695, 1697101658289, 1697101658385]"
4628,4628,709,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101658387,1697101675810.0,120,,,"[7, 979, 333, 101, 98, 97, 93, 85, 805, 101, 76, 98, 93, 87, 831, 107, 101, 95, 94, 89, 707, 101, 93, 88, 87, 845, 108, 103, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 88, 677, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101658394, 1697101659373, 1697101659706, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668, 1697101662757, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670034, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4629,4629,912,44,[],200,llama-13b,128,1,3719.0,1.0,1,H100,1697101708533,1697101712252.0,120,92.0,20.0,"[25, 816, 116, 89, 83, 82, 79, 882, 102, 98, 92, 89, 89, 85, 498, 99, 91, 84, 82, 79, 59]","[1697101708558, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712114, 1697101712193, 1697101712252]"
4630,4630,225,36,[],200,llama-13b,128,1,946.0,1.0,1,H100,1697101688625,1697101689571.0,120,23.0,1.0,"[42, 903]","[1697101688667, 1697101689570]"
4631,4631,668,37,[],200,llama-13b,128,1,3206.0,1.0,1,H100,1697101689571,1697101692777.0,120,109.0,6.0,"[7, 398, 1229, 1068, 214, 212, 78]","[1697101689578, 1697101689976, 1697101691205, 1697101692273, 1697101692487, 1697101692699, 1697101692777]"
4632,4632,526,28,[],200,llama-13b,128,1,4308.0,1.0,1,H100,1697101680112,1697101684420.0,120,89.0,20.0,"[31, 1359, 106, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680143, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
4633,4633,97,38,[],200,llama-13b,128,1,3942.0,1.0,1,H100,1697101692778,1697101696720.0,120,6.0,20.0,"[7, 587, 107, 101, 100, 99, 95, 91, 88, 67, 434, 77, 100, 85, 604, 78, 77, 831, 103, 101, 110]","[1697101692785, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720]"
4634,4634,638,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674982,1697101675810.0,120,,,"[18, 789]","[1697101675000, 1697101675789]"
4635,4635,71,28,[],200,llama-13b,128,1,3591.0,1.0,1,H100,1697101675814,1697101679405.0,120,364.0,11.0,"[283, 1283, 97, 64, 51, 763, 89, 86, 65, 83, 80, 647]","[1697101676097, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678758, 1697101679405]"
4636,4636,884,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684425,1697101689610.0,120,,,"[25, 1666, 121, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101684450, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4637,4637,761,42,[],200,llama-13b,128,1,4799.0,1.0,1,H100,1697101706009,1697101710808.0,120,85.0,20.0,"[6, 1029, 242, 96, 92, 70, 92, 69, 91, 87, 545, 98, 65, 88, 812, 88, 83, 82, 79, 881, 103]","[1697101706015, 1697101707044, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807]"
4638,4638,432,29,[],200,llama-13b,128,1,1074.0,1.0,1,H100,1697101679406,1697101680480.0,120,13.0,1.0,"[7, 1067]","[1697101679413, 1697101680480]"
4639,4639,763,30,[],200,llama-13b,128,1,1021.0,1.0,1,H100,1697101680481,1697101681502.0,120,20.0,1.0,"[18, 1003]","[1697101680499, 1697101681502]"
4640,4640,683,43,[],200,llama-13b,128,1,744.0,1.0,1,H100,1697101697594,1697101698338.0,120,874.0,2.0,"[113, 631]","[1697101697707, 1697101698338]"
4641,4641,364,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101666220,1697101675812.0,120,,,"[16, 2027, 133, 101, 95, 71, 91, 90, 725, 98, 95, 92, 93, 88, 676, 99, 71, 92, 90, 90, 513, 97, 95, 94, 93, 90, 521, 114, 91, 68, 67, 88, 804, 99, 88, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101666236, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669569, 1697101669667, 1697101669762, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673987, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4642,4642,296,34,[],200,llama-13b,128,1,1705.0,1.0,1,H100,1697101680806,1697101682511.0,120,6.0,1.0,"[13, 1692]","[1697101680819, 1697101682511]"
4643,4643,654,35,[],200,llama-13b,128,1,1539.0,1.0,1,H100,1697101682512,1697101684051.0,120,47.0,4.0,"[6, 1123, 208, 102, 100]","[1697101682518, 1697101683641, 1697101683849, 1697101683951, 1697101684051]"
4644,4644,703,30,[],200,llama-13b,128,1,1101.0,1.0,1,H100,1697101672658,1697101673759.0,120,12.0,1.0,"[61, 1040]","[1697101672719, 1697101673759]"
4645,4645,673,30,[],200,llama-13b,128,1,4755.0,1.0,1,H100,1697101675816,1697101680571.0,120,93.0,20.0,"[291, 1370, 64, 51, 763, 89, 86, 65, 83, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101676107, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
4646,4646,176,36,[],200,llama-13b,128,1,962.0,1.0,1,H100,1697101684052,1697101685014.0,120,216.0,2.0,"[24, 938]","[1697101684076, 1697101685014]"
4647,4647,536,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685015,1697101689613.0,120,,,"[18, 1083, 121, 91, 89, 87, 87, 646, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101685033, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4648,4648,828,34,[],200,llama-13b,128,1,1296.0,1.0,1,H100,1697101677382,1697101678678.0,120,182.0,6.0,"[85, 888, 90, 85, 65, 83]","[1697101677467, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678]"
4649,4649,893,38,[],200,llama-13b,128,1,3865.0,1.0,1,H100,1697101689614,1697101693479.0,120,335.0,10.0,"[116, 1382, 93, 1069, 213, 212, 78, 110, 93, 94, 404]","[1697101689730, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692980, 1697101693074, 1697101693478]"
4650,4650,253,35,[],200,llama-13b,128,1,4025.0,1.0,1,H100,1697101678679,1697101682704.0,120,67.0,20.0,"[12, 597, 117, 99, 98, 94, 72, 92, 91, 69, 91, 460, 103, 57, 73, 804, 84, 83, 79, 79, 771]","[1697101678691, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704]"
4651,4651,408,24,[],200,llama-13b,128,1,1694.0,1.0,1,H100,1697101685329,1697101687023.0,120,16.0,1.0,"[60, 1634]","[1697101685389, 1697101687023]"
4652,4652,736,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687025,1697101689595.0,120,,,"[41, 1319, 133, 106, 97, 92, 92, 89]","[1697101687066, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4653,4653,318,39,[],200,llama-13b,128,1,1940.0,1.0,1,H100,1697101693480,1697101695420.0,120,6.0,6.0,"[6, 971, 97, 77, 101, 84, 604]","[1697101693486, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420]"
4654,4654,478,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673760,1697101675812.0,120,,,"[30, 1058, 133, 102, 98, 95, 71, 94]","[1697101673790, 1697101674848, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675347, 1697101675441]"
4655,4655,606,36,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101682705,1697101683641.0,120,9.0,1.0,"[36, 900]","[1697101682741, 1697101683641]"
4656,4656,37,37,[],200,llama-13b,128,1,1265.0,1.0,1,H100,1697101683642,1697101684907.0,120,20.0,1.0,"[30, 1235]","[1697101683672, 1697101684907]"
4657,4657,369,38,[],200,llama-13b,128,1,3717.0,1.0,1,H100,1697101684908,1697101688625.0,120,216.0,15.0,"[18, 1190, 121, 91, 89, 87, 87, 646, 101, 100, 94, 92, 91, 88, 715, 106]","[1697101684926, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
4658,4658,171,34,[],200,llama-13b,128,1,1180.0,1.0,1,H100,1697101698296,1697101699476.0,120,6.0,1.0,"[36, 1144]","[1697101698332, 1697101699476]"
4659,4659,501,35,[],200,llama-13b,128,1,1076.0,1.0,1,H100,1697101699477,1697101700553.0,120,19.0,1.0,"[108, 967]","[1697101699585, 1697101700552]"
4660,4660,725,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672046,1697101675810.0,120,,,"[13, 487, 111, 113, 91, 68, 68, 88, 803, 98, 89, 88, 66, 66, 686, 103, 97, 96, 70, 94]","[1697101672059, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675084, 1697101675181, 1697101675277, 1697101675347, 1697101675441]"
4661,4661,855,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700553,1697101703170.0,120,,,"[48, 494, 1187, 69]","[1697101700601, 1697101701095, 1697101702282, 1697101702351]"
4662,4662,284,37,[],200,llama-13b,128,1,7822.0,1.0,1,H100,1697101703175,1697101710997.0,120,90.0,31.0,"[241, 1592, 105, 72, 728, 95, 87, 85, 83, 80, 942, 97, 92, 70, 91, 70, 91, 87, 545, 98, 65, 88, 811, 88, 84, 82, 79, 881, 103, 97, 93]","[1697101703416, 1697101705008, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709578, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997]"
4663,4663,30,44,[],200,llama-13b,128,1,4010.0,1.0,1,H100,1697101708534,1697101712544.0,120,93.0,20.0,"[159, 2011, 103, 97, 93, 89, 88, 86, 497, 100, 91, 84, 83, 78, 59, 69, 51, 60, 57, 55]","[1697101708693, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711857, 1697101711948, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
4664,4664,834,39,[],200,llama-13b,128,1,4774.0,1.0,1,H100,1697101683850,1697101688624.0,120,85.0,20.0,"[6, 1051, 106, 87, 63, 83, 81, 910, 91, 89, 87, 87, 646, 102, 99, 95, 91, 91, 88, 715, 106]","[1697101683856, 1697101684907, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
4665,4665,301,32,[],200,llama-13b,128,1,6909.0,1.0,1,H100,1697101680806,1697101687715.0,120,109.0,31.0,"[24, 1681, 193, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 80, 911, 91, 89, 87, 86, 647, 101, 100, 94, 92, 91]","[1697101680830, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715]"
4666,4666,754,37,[],200,llama-13b,128,1,2921.0,1.0,1,H100,1697101703174,1697101706095.0,120,88.0,7.0,"[96, 1745, 98, 72, 728, 95, 87]","[1697101703270, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095]"
4667,4667,183,38,[],200,llama-13b,128,1,6891.0,1.0,1,H100,1697101706096,1697101712987.0,120,17.0,50.0,"[7, 941, 242, 96, 92, 70, 92, 69, 91, 87, 545, 98, 65, 88, 812, 88, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45, 44, 41, 33, 40, 32, 32, 32, 31, 31, 25, 29, 28]","[1697101706103, 1697101707044, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712674, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987]"
4668,4668,683,41,[],200,llama-13b,128,1,743.0,1.0,1,H100,1697101697595,1697101698338.0,120,874.0,2.0,"[88, 655]","[1697101697683, 1697101698338]"
4669,4669,750,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688625,1697101689594.0,120,,,"[30, 916]","[1697101688655, 1697101689571]"
4670,4670,184,29,[],200,llama-13b,128,1,5198.0,1.0,1,H100,1697101689618,1697101694816.0,120,87.0,20.0,"[379, 2033, 244, 213, 212, 78, 101, 102, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101, 84]","[1697101689997, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732, 1697101694816]"
4671,4671,237,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688625,1697101689595.0,120,,,"[19, 927]","[1697101688644, 1697101689571]"
4672,4672,47,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671667,1697101675810.0,120,,,"[30, 849, 111, 113, 91, 68, 68, 88, 803, 98, 89, 88, 66, 66, 686, 102, 99, 95, 70, 94]","[1697101671697, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
4673,4673,175,39,[],200,llama-13b,128,1,3457.0,1.0,1,H100,1697101689618,1697101693075.0,120,140.0,8.0,"[476, 2180, 213, 212, 78, 110, 94, 94]","[1697101690094, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692981, 1697101693075]"
4674,4674,598,41,[],200,llama-13b,128,1,4066.0,1.0,1,H100,1697101689614,1697101693680.0,120,345.0,12.0,"[163, 1335, 93, 1069, 213, 212, 78, 111, 92, 94, 404, 102, 100]","[1697101689777, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692888, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680]"
4675,4675,837,40,[],200,llama-13b,128,1,2799.0,1.0,1,H100,1697101709745,1697101712544.0,120,85.0,20.0,"[6, 677, 277, 103, 97, 92, 89, 89, 85, 498, 99, 90, 85, 82, 79, 59, 69, 51, 60, 57, 55]","[1697101709751, 1697101710428, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
4676,4676,49,22,[],200,llama-13b,128,1,1277.0,1.0,1,H100,1697101670486,1697101671763.0,120,109.0,3.0,"[6, 1067, 107, 97]","[1697101670492, 1697101671559, 1697101671666, 1697101671763]"
4677,4677,403,23,[],200,llama-13b,128,1,893.0,1.0,1,H100,1697101671764,1697101672657.0,120,874.0,2.0,"[6, 776, 111]","[1697101671770, 1697101672546, 1697101672657]"
4678,4678,736,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672660,1697101675812.0,120,,,"[70, 1029, 129, 99, 88, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672730, 1697101673759, 1697101673888, 1697101673987, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4679,4679,599,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689615,1697101697587.0,120,,,"[177, 1320, 93, 1069, 213, 212, 78, 110, 93, 94, 404, 102, 100, 99, 95, 91, 87, 68, 433, 78, 100, 84, 605, 78, 77, 831, 103, 100, 111, 92, 91]","[1697101689792, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694815, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
4680,4680,713,36,[],200,llama-13b,128,1,3367.0,1.0,1,H100,1697101689613,1697101692980.0,120,874.0,8.0,"[88, 1504, 1069, 213, 212, 78, 101, 102]","[1697101689701, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980]"
4681,4681,623,42,[],200,llama-13b,128,1,2078.0,1.0,1,H100,1697101703035,1697101705113.0,120,140.0,3.0,"[7, 1966, 105]","[1697101703042, 1697101705008, 1697101705113]"
4682,4682,146,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697599,1697101700636.0,120,,,"[476, 1400, 113, 83, 82, 81, 81, 82]","[1697101698075, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4683,4683,503,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[12, 1509, 123, 69]","[1697101700650, 1697101702159, 1697101702282, 1697101702351]"
4684,4684,17,38,[],200,llama-13b,128,1,1725.0,1.0,1,H100,1697101703175,1697101704900.0,120,23.0,1.0,"[292, 1433]","[1697101703467, 1697101704900]"
4685,4685,370,39,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,31.0,1.0,"[90, 787]","[1697101704991, 1697101705778]"
4686,4686,53,43,[],200,llama-13b,128,1,7845.0,1.0,1,H100,1697101705114,1697101712959.0,120,216.0,55.0,"[6, 658, 135, 95, 88, 84, 84, 79, 942, 96, 93, 70, 91, 70, 91, 87, 544, 99, 65, 88, 812, 88, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45, 44, 41, 33, 40, 32, 32, 32, 31, 31, 25, 29]","[1697101705120, 1697101705778, 1697101705913, 1697101706008, 1697101706096, 1697101706180, 1697101706264, 1697101706343, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712674, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959]"
4687,4687,728,40,[],200,llama-13b,128,1,1266.0,1.0,1,H100,1697101705779,1697101707045.0,120,20.0,1.0,"[30, 1236]","[1697101705809, 1697101707045]"
4688,4688,193,43,[],200,llama-13b,128,1,2002.0,1.0,1,H100,1697101710809,1697101712811.0,120,79.0,20.0,"[18, 931, 99, 90, 85, 83, 78, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32]","[1697101710827, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811]"
4689,4689,400,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696184,1697101697588.0,120,,,"[7, 1204]","[1697101696191, 1697101697395]"
4690,4690,647,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684425,1697101689606.0,120,,,"[7, 1684, 121, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101684432, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4691,4691,725,26,[],200,llama-13b,128,1,4763.0,1.0,1,H100,1697101675816,1697101680579.0,120,90.0,20.0,"[568, 996, 97, 64, 51, 762, 90, 86, 64, 84, 80, 647, 99, 98, 94, 72, 92, 91, 69, 91, 461]","[1697101676384, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680111, 1697101680572]"
4692,4692,104,31,[],200,llama-13b,128,1,3840.0,1.0,1,H100,1697101680581,1697101684421.0,120,93.0,20.0,"[61, 966, 84, 83, 79, 79, 771, 96, 93, 91, 69, 91, 90, 615, 102, 100, 97, 92, 90, 91]","[1697101680642, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684421]"
4693,4693,77,32,[],200,llama-13b,128,1,5197.0,1.0,1,H100,1697101689619,1697101694816.0,120,92.0,20.0,"[558, 1852, 245, 213, 212, 78, 101, 103, 94, 403, 102, 100, 99, 94, 91, 88, 68, 434, 77, 100, 85]","[1697101690177, 1697101692029, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
4694,4694,813,32,[],200,llama-13b,128,1,5117.0,1.0,1,H100,1697101689614,1697101694731.0,120,85.0,20.0,"[157, 1341, 93, 1069, 213, 212, 78, 104, 99, 94, 404, 102, 100, 99, 95, 91, 87, 68, 433, 78, 100]","[1697101689771, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692881, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
4695,4695,757,29,[],200,llama-13b,128,1,1880.0,1.0,1,H100,1697101697595,1697101699475.0,120,20.0,1.0,"[198, 1682]","[1697101697793, 1697101699475]"
4696,4696,159,30,[],200,llama-13b,128,1,1075.0,1.0,1,H100,1697101699477,1697101700552.0,120,31.0,1.0,"[48, 1027]","[1697101699525, 1697101700552]"
4697,4697,518,35,[],200,llama-13b,128,1,867.0,1.0,1,H100,1697101696905,1697101697772.0,120,23.0,1.0,"[6, 861]","[1697101696911, 1697101697772]"
4698,4698,877,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697773,1697101700636.0,120,,,"[405, 1297, 113, 83, 82, 81, 81, 82]","[1697101698178, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4699,4699,928,31,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,20.0,1.0,"[383, 1181]","[1697101676199, 1697101677380]"
4700,4700,732,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688625,1697101689598.0,120,,,"[43, 902]","[1697101688668, 1697101689570]"
4701,4701,263,29,[],200,llama-13b,128,1,1640.0,1.0,1,H100,1697101668845,1697101670485.0,120,15.0,1.0,"[36, 1604]","[1697101668881, 1697101670485]"
4702,4702,249,33,[],200,llama-13b,128,1,1673.0,1.0,1,H100,1697101694733,1697101696406.0,120,874.0,5.0,"[36, 567, 84, 78, 77, 831]","[1697101694769, 1697101695336, 1697101695420, 1697101695498, 1697101695575, 1697101696406]"
4703,4703,622,30,[],200,llama-13b,128,1,1073.0,1.0,1,H100,1697101670486,1697101671559.0,120,20.0,1.0,"[30, 1043]","[1697101670516, 1697101671559]"
4704,4704,22,31,[],200,llama-13b,128,1,986.0,1.0,1,H100,1697101671560,1697101672546.0,120,16.0,1.0,"[25, 961]","[1697101671585, 1697101672546]"
4705,4705,408,22,[],200,llama-13b,128,1,1053.0,1.0,1,H100,1697101651608,1697101652661.0,120,16.0,1.0,"[25, 1028]","[1697101651633, 1697101652661]"
4706,4706,608,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696407,1697101697589.0,120,,,"[7, 982]","[1697101696414, 1697101697396]"
4707,4707,379,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672547,1697101675810.0,120,,,"[12, 1329, 98, 89, 88, 66, 66, 686, 102, 98, 95, 71, 94]","[1697101672559, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675347, 1697101675441]"
4708,4708,763,23,[],200,llama-13b,128,1,815.0,1.0,1,H100,1697101652662,1697101653477.0,120,20.0,1.0,"[24, 786]","[1697101652686, 1697101653472]"
4709,4709,359,32,[],200,llama-13b,128,1,865.0,1.0,1,H100,1697101677382,1697101678247.0,120,10.0,1.0,"[109, 756]","[1697101677491, 1697101678247]"
4710,4710,192,24,[],200,llama-13b,128,1,4810.0,1.0,1,H100,1697101653479,1697101658289.0,120,93.0,20.0,"[35, 1323, 108, 105, 101, 74, 100, 96, 92, 613, 106, 100, 99, 97, 83, 623, 81, 79, 60, 835]","[1697101653514, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656332, 1697101656431, 1697101656528, 1697101656611, 1697101657234, 1697101657315, 1697101657394, 1697101657454, 1697101658289]"
4711,4711,153,30,[],200,llama-13b,128,1,1778.0,1.0,1,H100,1697101675814,1697101677592.0,120,335.0,4.0,"[191, 1374, 98, 64, 51]","[1697101676005, 1697101677379, 1697101677477, 1697101677541, 1697101677592]"
4712,4712,719,33,[],200,llama-13b,128,1,1520.0,1.0,1,H100,1697101678248,1697101679768.0,120,182.0,6.0,"[18, 1139, 99, 97, 95, 72]","[1697101678266, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768]"
4713,4713,296,32,[],200,llama-13b,128,1,583.0,1.0,1,H100,1697101696813,1697101697396.0,120,6.0,1.0,"[7, 576]","[1697101696820, 1697101697396]"
4714,4714,484,31,[],200,llama-13b,128,1,7570.0,1.0,1,H100,1697101677593,1697101685163.0,120,86.0,36.0,"[6, 1689, 117, 98, 98, 95, 72, 92, 90, 70, 90, 461, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97, 92, 90, 90, 593, 86, 64]","[1697101677599, 1697101679288, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163]"
4715,4715,144,34,[],200,llama-13b,128,1,4282.0,1.0,1,H100,1697101679769,1697101684051.0,120,96.0,20.0,"[6, 704, 92, 103, 58, 72, 803, 85, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 102, 100]","[1697101679775, 1697101680479, 1697101680571, 1697101680674, 1697101680732, 1697101680804, 1697101681607, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051]"
4716,4716,83,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101698339,1697101700637.0,120,,,"[7, 2206]","[1697101698346, 1697101700552]"
4717,4717,658,33,[],200,llama-13b,128,1,376.0,1.0,1,H100,1697101697397,1697101697773.0,120,11.0,1.0,"[36, 339]","[1697101697433, 1697101697772]"
4718,4718,58,34,[],200,llama-13b,128,1,1702.0,1.0,1,H100,1697101697774,1697101699476.0,120,15.0,1.0,"[428, 1274]","[1697101698202, 1697101699476]"
4719,4719,497,35,[],200,llama-13b,128,1,962.0,1.0,1,H100,1697101684052,1697101685014.0,120,67.0,2.0,"[12, 843, 107]","[1697101684064, 1697101684907, 1697101685014]"
4720,4720,828,36,[],200,llama-13b,128,1,1576.0,1.0,1,H100,1697101685015,1697101686591.0,120,182.0,6.0,"[12, 1210, 91, 89, 87, 87]","[1697101685027, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591]"
4721,4721,419,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700635.0,120,,,"[102, 973]","[1697101699579, 1697101700552]"
4722,4722,775,36,[],200,llama-13b,128,1,1521.0,1.0,1,H100,1697101700638,1697101702159.0,120,17.0,1.0,"[65, 1456]","[1697101700703, 1697101702159]"
4723,4723,131,31,[],200,llama-13b,128,1,1365.0,1.0,1,H100,1697101694818,1697101696183.0,120,8.0,1.0,"[99, 1266]","[1697101694917, 1697101696183]"
4724,4724,203,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703172.0,120,,,"[66, 808]","[1697101702227, 1697101703035]"
4725,4725,491,32,[],200,llama-13b,128,1,1211.0,1.0,1,H100,1697101696185,1697101697396.0,120,14.0,1.0,"[48, 1163]","[1697101696233, 1697101697396]"
4726,4726,816,33,[],200,llama-13b,128,1,2274.0,1.0,1,H100,1697101697397,1697101699671.0,120,182.0,4.0,"[18, 357, 566, 1249, 84]","[1697101697415, 1697101697772, 1697101698338, 1697101699587, 1697101699671]"
4727,4727,557,38,[],200,llama-13b,128,1,1723.0,1.0,1,H100,1697101703176,1697101704899.0,120,31.0,1.0,"[357, 1366]","[1697101703533, 1697101704899]"
4728,4728,75,39,[],200,llama-13b,128,1,3691.0,1.0,1,H100,1697101704901,1697101708592.0,120,345.0,18.0,"[42, 835, 135, 95, 87, 85, 83, 80, 942, 96, 93, 70, 91, 70, 91, 87, 544, 99, 65]","[1697101704943, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591]"
4729,4729,256,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685328,1697101689603.0,120,,,"[13, 1681, 215, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101685341, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4730,4730,436,40,[],200,llama-13b,128,1,3952.0,1.0,1,H100,1697101708592,1697101712544.0,120,86.0,20.0,"[119, 1716, 277, 103, 97, 93, 89, 88, 86, 498, 98, 91, 85, 82, 79, 59, 69, 51, 60, 57, 55]","[1697101708711, 1697101710427, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
4731,4731,831,37,[],200,llama-13b,128,1,1075.0,1.0,1,H100,1697101699477,1697101700552.0,120,11.0,1.0,"[18, 1057]","[1697101699495, 1697101700552]"
4732,4732,263,38,[],200,llama-13b,128,1,541.0,1.0,1,H100,1697101700553,1697101701094.0,120,15.0,1.0,"[18, 523]","[1697101700571, 1697101701094]"
4733,4733,520,34,[],200,llama-13b,128,1,603.0,1.0,1,H100,1697101694733,1697101695336.0,120,11.0,1.0,"[60, 543]","[1697101694793, 1697101695336]"
4734,4734,610,36,[],200,llama-13b,128,1,5198.0,1.0,1,H100,1697101689618,1697101694816.0,120,89.0,20.0,"[455, 1957, 244, 213, 212, 78, 101, 103, 94, 403, 102, 100, 100, 94, 90, 88, 68, 434, 77, 100, 85]","[1697101690073, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693780, 1697101693874, 1697101693964, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
4735,4735,877,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695337,1697101697588.0,120,,,"[13, 833, 223, 103, 101, 110, 92, 92]","[1697101695350, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696904]"
4736,4736,165,25,[],200,llama-13b,128,1,4755.0,1.0,1,H100,1697101675816,1697101680571.0,120,83.0,20.0,"[273, 1291, 97, 64, 51, 763, 89, 86, 65, 83, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101676089, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
4737,4737,602,32,[],200,llama-13b,128,1,1564.0,1.0,1,H100,1697101675816,1697101677380.0,120,15.0,1.0,"[505, 1059]","[1697101676321, 1697101677380]"
4738,4738,526,26,[],200,llama-13b,128,1,3840.0,1.0,1,H100,1697101680581,1697101684421.0,120,89.0,20.0,"[49, 872, 106, 84, 83, 79, 79, 770, 97, 93, 91, 69, 91, 90, 615, 102, 100, 97, 92, 90, 91]","[1697101680630, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684421]"
4739,4739,886,27,[],200,llama-13b,128,1,1691.0,1.0,1,H100,1697101684425,1697101686116.0,120,17.0,1.0,"[25, 1666]","[1697101684450, 1697101686116]"
4740,4740,282,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101686117,1697101689594.0,120,,,"[12, 894, 214, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 91, 90]","[1697101686129, 1697101687023, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688994]"
4741,4741,636,29,[],200,llama-13b,128,1,1495.0,1.0,1,H100,1697101689617,1697101691112.0,120,31.0,1.0,"[355, 1140]","[1697101689972, 1697101691112]"
4742,4742,30,33,[],200,llama-13b,128,1,3423.0,1.0,1,H100,1697101677381,1697101680804.0,120,93.0,20.0,"[19, 954, 90, 86, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 469, 95, 57, 73]","[1697101677400, 1697101678354, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680579, 1697101680674, 1697101680731, 1697101680804]"
4743,4743,464,31,[],200,llama-13b,128,1,835.0,1.0,1,H100,1697101670725,1697101671560.0,120,12.0,1.0,"[42, 792]","[1697101670767, 1697101671559]"
4744,4744,66,30,[],200,llama-13b,128,1,3703.0,1.0,1,H100,1697101691113,1697101694816.0,120,84.0,20.0,"[55, 862, 244, 214, 211, 79, 109, 93, 95, 404, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 85]","[1697101691168, 1697101692030, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692887, 1697101692980, 1697101693075, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816]"
4745,4745,794,32,[],200,llama-13b,128,1,985.0,1.0,1,H100,1697101671561,1697101672546.0,120,11.0,1.0,"[36, 949]","[1697101671597, 1697101672546]"
4746,4746,219,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672547,1697101675810.0,120,,,"[12, 1199, 130, 98, 89, 88, 66, 66, 686, 102, 98, 95, 71, 94]","[1697101672559, 1697101673758, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675347, 1697101675441]"
4747,4747,168,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101678681,1697101689604.0,120,,,"[10, 597, 117, 99, 98, 94, 72, 92, 91, 69, 91, 460, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97, 92, 90, 90, 593, 86, 64, 83, 80, 911, 91, 89, 87, 86, 647, 101, 99, 95, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101678691, 1697101679288, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679951, 1697101680020, 1697101680111, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
4748,4748,168,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689614,1697101697587.0,120,,,"[175, 1323, 93, 1069, 213, 212, 78, 110, 93, 94, 404, 102, 100, 99, 95, 91, 87, 68, 433, 78, 100, 84, 605, 78, 77, 831, 103, 100, 111, 92, 91]","[1697101689789, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694815, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
4749,4749,674,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695422,1697101697588.0,120,,,"[18, 743, 224, 102, 101, 111, 92, 91]","[1697101695440, 1697101696183, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696813, 1697101696904]"
4750,4750,828,17,[],200,llama-13b,128,1,1707.0,1.0,1,H100,1697101646769,1697101648476.0,120,182.0,6.0,"[6, 1308, 96, 100, 101, 96]","[1697101646775, 1697101648083, 1697101648179, 1697101648279, 1697101648380, 1697101648476]"
4751,4751,465,32,[],200,llama-13b,128,1,1902.0,1.0,1,H100,1697101684426,1697101686328.0,120,364.0,3.0,"[48, 1642, 121, 91]","[1697101684474, 1697101686116, 1697101686237, 1697101686328]"
4752,4752,732,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687533,1697101689594.0,120,,,"[7, 845, 133, 106, 97, 92, 92, 89]","[1697101687540, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4753,4753,124,29,[],200,llama-13b,128,1,962.0,1.0,1,H100,1697101684052,1697101685014.0,120,83.0,2.0,"[6, 956]","[1697101684058, 1697101685014]"
4754,4754,454,30,[],200,llama-13b,128,1,1577.0,1.0,1,H100,1697101685014,1697101686591.0,120,182.0,6.0,"[13, 1089, 121, 91, 89, 87, 87]","[1697101685027, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591]"
4755,4755,227,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.75 GiB. GPU 0 has a total capacty of 79.11 GiB of which 17.16 GiB is free. Process 1607256 has 61.94 GiB memory in use. Of the allocated memory 46.86 GiB is allocated by PyTorch, and 14.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101648477,1697101651604.0,120,,,"[13, 457, 103, 96, 96, 83, 941, 106, 103, 102, 91, 85]","[1697101648490, 1697101648947, 1697101649050, 1697101649146, 1697101649242, 1697101649325, 1697101650266, 1697101650372, 1697101650475, 1697101650577, 1697101650668, 1697101650753]"
4756,4756,300,31,[],200,llama-13b,128,1,1640.0,1.0,1,H100,1697101668845,1697101670485.0,120,9.0,1.0,"[31, 1609]","[1697101668876, 1697101670485]"
4757,4757,654,32,[],200,llama-13b,128,1,1372.0,1.0,1,H100,1697101670486,1697101671858.0,120,47.0,4.0,"[6, 1067, 107, 97, 95]","[1697101670492, 1697101671559, 1697101671666, 1697101671763, 1697101671858]"
4758,4758,812,31,[],200,llama-13b,128,1,1793.0,1.0,1,H100,1697101686592,1697101688385.0,120,16.0,1.0,"[12, 1781]","[1697101686604, 1697101688385]"
4759,4759,51,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694733,1697101697587.0,120,,,"[48, 555, 85, 78, 76, 831, 103, 101, 110, 92, 91]","[1697101694781, 1697101695336, 1697101695421, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4760,4760,51,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671859,1697101675810.0,120,,,"[6, 681, 111, 113, 91, 68, 68, 88, 803, 98, 89, 88, 66, 66, 686, 103, 98, 95, 70, 94]","[1697101671865, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675084, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
4761,4761,310,37,[],200,llama-13b,128,1,1518.0,1.0,1,H100,1697101700642,1697101702160.0,120,26.0,1.0,"[429, 1089]","[1697101701071, 1697101702160]"
4762,4762,405,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700637.0,120,,,"[194, 1686, 113, 83, 82, 81, 81, 82]","[1697101697789, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4763,4763,671,38,[],200,llama-13b,128,1,873.0,1.0,1,H100,1697101702162,1697101703035.0,120,12.0,1.0,"[127, 746]","[1697101702289, 1697101703035]"
4764,4764,189,39,[],200,llama-13b,128,1,5490.0,1.0,1,H100,1697101703036,1697101708526.0,120,88.0,20.0,"[36, 616, 1320, 105, 71, 729, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 99]","[1697101703072, 1697101703688, 1697101705008, 1697101705113, 1697101705184, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
4765,4765,730,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700643,1697101703170.0,120,,,"[451, 1066, 122, 69]","[1697101701094, 1697101702160, 1697101702282, 1697101702351]"
4766,4766,583,19,[],200,llama-13b,128,1,5002.0,1.0,1,H100,1697101651609,1697101656611.0,120,96.0,20.0,"[358, 1505, 138, 85, 84, 81, 81, 896, 108, 105, 101, 74, 100, 96, 92, 613, 106, 101, 98, 97, 83]","[1697101651967, 1697101653472, 1697101653610, 1697101653695, 1697101653779, 1697101653860, 1697101653941, 1697101654837, 1697101654945, 1697101655050, 1697101655151, 1697101655225, 1697101655325, 1697101655421, 1697101655513, 1697101656126, 1697101656232, 1697101656333, 1697101656431, 1697101656528, 1697101656611]"
4767,4767,784,37,[],200,llama-13b,128,1,5351.0,1.0,1,H100,1697101703175,1697101708526.0,120,89.0,20.0,"[13, 1711, 116, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 99]","[1697101703188, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
4768,4768,160,42,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703175,1697101704899.0,120,13.0,1.0,"[394, 1330]","[1697101703569, 1697101704899]"
4769,4769,409,34,[],200,llama-13b,128,1,6889.0,1.0,1,H100,1697101675814,1697101682703.0,120,109.0,30.0,"[96, 470, 37, 1060, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98, 95, 71, 92, 91, 70, 90, 461, 103, 57, 73, 803, 85, 82, 80, 78, 771]","[1697101675910, 1697101676380, 1697101676417, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681607, 1697101681692, 1697101681774, 1697101681854, 1697101681932, 1697101682703]"
4770,4770,240,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688386,1697101689594.0,120,,,"[6, 1178]","[1697101688392, 1697101689570]"
4771,4771,521,43,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,18.0,1.0,"[74, 803]","[1697101704975, 1697101705778]"
4772,4772,879,44,[],200,llama-13b,128,1,7334.0,1.0,1,H100,1697101705780,1697101713114.0,120,39.0,55.0,"[89, 1176, 241, 96, 92, 70, 92, 69, 91, 87, 545, 99, 65, 87, 812, 88, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45, 44, 41, 33, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26]","[1697101705869, 1697101707045, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708527, 1697101708592, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712674, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114]"
4773,4773,173,27,[],200,llama-13b,128,1,4756.0,1.0,1,H100,1697101675815,1697101680571.0,120,96.0,20.0,"[175, 1389, 98, 64, 50, 763, 90, 86, 64, 84, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101675990, 1697101677379, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
4774,4774,103,30,[],200,llama-13b,128,1,866.0,1.0,1,H100,1697101677381,1697101678247.0,120,15.0,1.0,"[36, 830]","[1697101677417, 1697101678247]"
4775,4775,462,31,[],200,llama-13b,128,1,1040.0,1.0,1,H100,1697101678248,1697101679288.0,120,52.0,1.0,"[42, 998]","[1697101678290, 1697101679288]"
4776,4776,15,20,[],200,llama-13b,128,1,5765.0,1.0,1,H100,1697101656613,1697101662378.0,120,100.0,20.0,"[7, 1540, 129, 96, 82, 82, 80, 1076, 102, 97, 99, 92, 85, 805, 102, 75, 104, 87, 87, 831, 107]","[1697101656620, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549, 1697101658629, 1697101659705, 1697101659807, 1697101659904, 1697101660003, 1697101660095, 1697101660180, 1697101660985, 1697101661087, 1697101661162, 1697101661266, 1697101661353, 1697101661440, 1697101662271, 1697101662378]"
4777,4777,794,32,[],200,llama-13b,128,1,1191.0,1.0,1,H100,1697101679289,1697101680480.0,120,11.0,1.0,"[24, 1167]","[1697101679313, 1697101680480]"
4778,4778,770,35,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101682705,1697101683641.0,120,13.0,1.0,"[6, 930]","[1697101682711, 1697101683641]"
4779,4779,531,28,[],200,llama-13b,128,1,3839.0,1.0,1,H100,1697101680581,1697101684420.0,120,52.0,20.0,"[9, 912, 106, 84, 83, 79, 79, 771, 97, 92, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680590, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682801, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
4780,4780,201,36,[],200,llama-13b,128,1,4982.0,1.0,1,H100,1697101683642,1697101688624.0,120,67.0,20.0,"[13, 1252, 106, 87, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 94, 92, 91, 88, 715, 106]","[1697101683655, 1697101684907, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
4781,4781,372,21,[],200,llama-13b,128,1,5224.0,1.0,1,H100,1697101662379,1697101667603.0,120,874.0,25.0,"[30, 928, 127, 101, 93, 88, 87, 845, 108, 103, 97, 93, 92, 696, 106, 101, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95]","[1697101662409, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663746, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664986, 1697101665079, 1697101665171, 1697101665867, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603]"
4782,4782,739,33,[],200,llama-13b,128,1,1567.0,1.0,1,H100,1697101675813,1697101677380.0,120,216.0,1.0,"[270, 1297]","[1697101676083, 1697101677380]"
4783,4783,539,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693076,1697101697588.0,120,,,"[6, 1375, 97, 77, 100, 85, 604, 79, 76, 831, 103, 101, 111, 91, 91]","[1697101693082, 1697101694457, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
4784,4784,706,22,[],200,llama-13b,128,1,5052.0,1.0,1,H100,1697101667605,1697101672657.0,120,86.0,20.0,"[18, 1821, 124, 99, 94, 93, 92, 89, 676, 98, 71, 93, 90, 90, 513, 97, 95, 94, 93, 90, 522]","[1697101667623, 1697101669444, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657]"
4785,4785,889,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684426,1697101689611.0,120,,,"[36, 1654, 121, 91, 89, 87, 86, 647, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 92, 89]","[1697101684462, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4786,4786,29,42,[],200,llama-13b,128,1,1739.0,1.0,1,H100,1697101693681,1697101695420.0,120,161.0,6.0,"[6, 770, 97, 77, 101, 84, 604]","[1697101693687, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420]"
4787,4787,164,34,[],200,llama-13b,128,1,866.0,1.0,1,H100,1697101677381,1697101678247.0,120,15.0,1.0,"[61, 805]","[1697101677442, 1697101678247]"
4788,4788,524,35,[],200,llama-13b,128,1,5900.0,1.0,1,H100,1697101678248,1697101684148.0,120,100.0,30.0,"[12, 1028, 117, 99, 97, 95, 72, 92, 90, 70, 90, 461, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97]","[1697101678260, 1697101679288, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148]"
4789,4789,248,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699672,1697101700635.0,120,,,"[7, 873]","[1697101699679, 1697101700552]"
4790,4790,607,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[234, 1284, 122, 69]","[1697101700876, 1697101702160, 1697101702282, 1697101702351]"
4791,4791,38,36,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,88.0,20.0,"[423, 1301, 116, 97, 72, 728, 95, 87, 85, 83, 79, 944, 96, 92, 70, 92, 69, 91, 86, 546, 98]","[1697101703599, 1697101704900, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707882, 1697101708428, 1697101708526]"
4792,4792,318,30,[],200,llama-13b,128,1,3269.0,1.0,1,H100,1697101689619,1697101692888.0,120,6.0,6.0,"[559, 1852, 244, 213, 212, 78, 110]","[1697101690178, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887]"
4793,4793,622,39,[],200,llama-13b,128,1,1939.0,1.0,1,H100,1697101701096,1697101703035.0,120,20.0,1.0,"[105, 1834]","[1697101701201, 1697101703035]"
4794,4794,51,40,[],200,llama-13b,128,1,8820.0,1.0,1,H100,1697101703036,1697101711856.0,120,364.0,36.0,"[48, 604, 1320, 105, 72, 728, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 99, 65, 88, 811, 88, 84, 82, 79, 881, 103, 97, 93, 89, 88, 86, 497, 99]","[1697101703084, 1697101703688, 1697101705008, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709578, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711856]"
4795,4795,252,27,[],200,llama-13b,128,1,4879.0,1.0,1,H100,1697101664788,1697101669667.0,120,182.0,22.0,"[18, 933, 129, 106, 99, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90, 724, 99]","[1697101664806, 1697101665739, 1697101665868, 1697101665974, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667]"
4796,4796,145,37,[],200,llama-13b,128,1,1139.0,1.0,1,H100,1697101692981,1697101694120.0,120,161.0,9.0,"[7, 384, 107, 101, 100, 100, 94, 91, 88, 67]","[1697101692988, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693780, 1697101693874, 1697101693965, 1697101694053, 1697101694120]"
4797,4797,112,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101698339,1697101700637.0,120,,,"[7, 2206]","[1697101698346, 1697101700552]"
4798,4798,308,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700635.0,120,,,"[384, 1496, 113, 83, 82, 81, 81, 82]","[1697101697979, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4799,4799,446,45,[],200,llama-13b,128,1,2391.0,1.0,1,H100,1697101700644,1697101703035.0,120,26.0,1.0,"[470, 1920]","[1697101701114, 1697101703034]"
4800,4800,508,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694121,1697101697587.0,120,,,"[7, 1208, 84, 78, 77, 831, 103, 100, 111, 93, 90]","[1697101694128, 1697101695336, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696813, 1697101696903]"
4801,4801,807,46,[],200,llama-13b,128,1,5489.0,1.0,1,H100,1697101703036,1697101708525.0,120,90.0,20.0,"[12, 640, 1320, 105, 71, 729, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 98]","[1697101703048, 1697101703688, 1697101705008, 1697101705113, 1697101705184, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708525]"
4802,4802,665,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[30, 1491, 123, 69]","[1697101700668, 1697101702159, 1697101702282, 1697101702351]"
4803,4803,68,38,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703175,1697101704899.0,120,12.0,1.0,"[344, 1380]","[1697101703519, 1697101704899]"
4804,4804,423,39,[],200,llama-13b,128,1,4589.0,1.0,1,H100,1697101704901,1697101709490.0,120,84.0,20.0,"[18, 859, 135, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 87, 544, 99, 65, 88, 811]","[1697101704919, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490]"
4805,4805,836,39,[],200,llama-13b,128,1,702.0,1.0,1,H100,1697101697593,1697101698295.0,120,11.0,1.0,"[32, 670]","[1697101697625, 1697101698295]"
4806,4806,841,32,[],200,llama-13b,128,1,3461.0,1.0,1,H100,1697101685164,1697101688625.0,120,123.0,15.0,"[6, 1067, 91, 89, 87, 87, 646, 102, 99, 94, 92, 91, 88, 715, 106]","[1697101685170, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
4807,4807,781,40,[],200,llama-13b,128,1,2365.0,1.0,1,H100,1697101709492,1697101711857.0,120,335.0,10.0,"[18, 917, 278, 103, 97, 92, 89, 88, 86, 498, 98]","[1697101709510, 1697101710427, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856]"
4808,4808,797,33,[],200,llama-13b,128,1,694.0,1.0,1,H100,1697101686329,1697101687023.0,120,26.0,1.0,"[7, 686]","[1697101686336, 1697101687022]"
4809,4809,227,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687023,1697101689594.0,120,,,"[25, 1337, 133, 106, 97, 92, 92, 89]","[1697101687048, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4810,4810,66,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671667,1697101675810.0,120,,,"[30, 849, 111, 113, 91, 68, 68, 88, 803, 98, 89, 88, 66, 66, 686, 102, 99, 95, 70, 94]","[1697101671697, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
4811,4811,701,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684425,1697101689595.0,120,,,"[13, 1678, 121, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101684438, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4812,4812,581,35,[],200,llama-13b,128,1,5117.0,1.0,1,H100,1697101689615,1697101694732.0,120,47.0,20.0,"[215, 1282, 93, 1069, 213, 212, 79, 110, 92, 94, 405, 101, 100, 99, 95, 91, 87, 68, 433, 78, 101]","[1697101689830, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692888, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694732]"
4813,4813,101,41,[],200,llama-13b,128,1,1498.0,1.0,1,H100,1697101689614,1697101691112.0,120,13.0,1.0,"[154, 1344]","[1697101689768, 1697101691112]"
4814,4814,157,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689611,1697101697587.0,120,,,"[472, 1947, 244, 213, 212, 78, 101, 103, 94, 403, 102, 100, 99, 94, 91, 88, 68, 434, 77, 100, 85, 604, 79, 76, 831, 103, 101, 110, 92, 91]","[1697101690083, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4815,4815,460,42,[],200,llama-13b,128,1,3703.0,1.0,1,H100,1697101691113,1697101694816.0,120,87.0,20.0,"[43, 874, 244, 214, 211, 79, 101, 101, 95, 404, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 85]","[1697101691156, 1697101692030, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692879, 1697101692980, 1697101693075, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816]"
4816,4816,543,40,[],200,llama-13b,128,1,9822.0,1.0,1,H100,1697101708536,1697101718358.0,120,244.0,381.0,"[175, 1716, 277, 103, 97, 93, 89, 88, 86, 498, 98, 92, 84, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26, 24, 20, 20, 21, 19, 18, 19, 20, 18, 19, 17, 17, 18, 16, 16, 16, 16, 16, 17, 16, 15, 16, 15, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 16, 15, 16, 15, 16, 15, 16, 16, 15, 15, 14, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 15, 15, 17, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 15, 15, 15, 15, 16, 15, 15, 15, 15, 15, 16, 15, 15, 15, 15, 16, 15, 15, 15, 15, 16, 15, 15, 15, 16, 15, 15, 15, 16, 15, 15, 15, 15, 16, 15, 15, 16, 15, 15, 15, 16, 15, 15, 15, 15, 16, 15, 15, 16, 15, 15, 15, 16, 15, 15, 15, 16, 15, 15, 16, 15, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 13, 14, 13, 13, 14, 13, 14, 13, 14, 13, 14, 13, 13, 14, 13, 14, 13, 14, 13, 13, 14, 13]","[1697101708711, 1697101710427, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711948, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114, 1697101713138, 1697101713158, 1697101713178, 1697101713199, 1697101713218, 1697101713236, 1697101713255, 1697101713275, 1697101713293, 1697101713312, 1697101713329, 1697101713346, 1697101713364, 1697101713380, 1697101713396, 1697101713412, 1697101713428, 1697101713444, 1697101713461, 1697101713477, 1697101713492, 1697101713508, 1697101713523, 1697101713538, 1697101713554, 1697101713569, 1697101713584, 1697101713600, 1697101713615, 1697101713631, 1697101713646, 1697101713661, 1697101713677, 1697101713692, 1697101713707, 1697101713723, 1697101713738, 1697101713754, 1697101713769, 1697101713784, 1697101713800, 1697101713815, 1697101713830, 1697101713846, 1697101713861, 1697101713877, 1697101713892, 1697101713907, 1697101713923, 1697101713938, 1697101713954, 1697101713969, 1697101713984, 1697101714000, 1697101714015, 1697101714031, 1697101714046, 1697101714062, 1697101714077, 1697101714093, 1697101714108, 1697101714124, 1697101714139, 1697101714155, 1697101714170, 1697101714186, 1697101714201, 1697101714217, 1697101714232, 1697101714248, 1697101714263, 1697101714279, 1697101714294, 1697101714310, 1697101714326, 1697101714341, 1697101714357, 1697101714372, 1697101714388, 1697101714403, 1697101714419, 1697101714435, 1697101714450, 1697101714465, 1697101714479, 1697101714494, 1697101714509, 1697101714524, 1697101714539, 1697101714554, 1697101714568, 1697101714583, 1697101714598, 1697101714613, 1697101714628, 1697101714643, 1697101714657, 1697101714672, 1697101714687, 1697101714702, 1697101714717, 1697101714732, 1697101714746, 1697101714761, 1697101714776, 1697101714791, 1697101714806, 1697101714821, 1697101714836, 1697101714850, 1697101714865, 1697101714880, 1697101714895, 1697101714910, 1697101714925, 1697101714940, 1697101714955, 1697101714969, 1697101714984, 1697101714999, 1697101715014, 1697101715029, 1697101715044, 1697101715059, 1697101715074, 1697101715089, 1697101715104, 1697101715119, 1697101715134, 1697101715149, 1697101715164, 1697101715179, 1697101715194, 1697101715209, 1697101715224, 1697101715239, 1697101715253, 1697101715268, 1697101715283, 1697101715298, 1697101715313, 1697101715328, 1697101715343, 1697101715358, 1697101715375, 1697101715391, 1697101715406, 1697101715421, 1697101715436, 1697101715451, 1697101715466, 1697101715481, 1697101715496, 1697101715511, 1697101715526, 1697101715541, 1697101715556, 1697101715571, 1697101715586, 1697101715601, 1697101715616, 1697101715631, 1697101715646, 1697101715661, 1697101715676, 1697101715691, 1697101715707, 1697101715722, 1697101715737, 1697101715752, 1697101715767, 1697101715782, 1697101715797, 1697101715812, 1697101715827, 1697101715842, 1697101715857, 1697101715872, 1697101715887, 1697101715902, 1697101715918, 1697101715933, 1697101715948, 1697101715963, 1697101715978, 1697101715993, 1697101716008, 1697101716023, 1697101716038, 1697101716053, 1697101716069, 1697101716084, 1697101716099, 1697101716114, 1697101716129, 1697101716145, 1697101716160, 1697101716175, 1697101716190, 1697101716205, 1697101716220, 1697101716236, 1697101716251, 1697101716266, 1697101716281, 1697101716296, 1697101716312, 1697101716327, 1697101716342, 1697101716357, 1697101716372, 1697101716388, 1697101716403, 1697101716418, 1697101716433, 1697101716449, 1697101716464, 1697101716479, 1697101716494, 1697101716510, 1697101716525, 1697101716540, 1697101716555, 1697101716570, 1697101716586, 1697101716601, 1697101716616, 1697101716632, 1697101716647, 1697101716662, 1697101716677, 1697101716693, 1697101716708, 1697101716723, 1697101716738, 1697101716753, 1697101716769, 1697101716784, 1697101716799, 1697101716815, 1697101716830, 1697101716845, 1697101716860, 1697101716876, 1697101716891, 1697101716906, 1697101716921, 1697101716937, 1697101716952, 1697101716967, 1697101716983, 1697101716998, 1697101717013, 1697101717029, 1697101717044, 1697101717059, 1697101717075, 1697101717090, 1697101717106, 1697101717121, 1697101717136, 1697101717152, 1697101717167, 1697101717182, 1697101717198, 1697101717213, 1697101717229, 1697101717244, 1697101717259, 1697101717275, 1697101717290, 1697101717306, 1697101717321, 1697101717337, 1697101717352, 1697101717367, 1697101717383, 1697101717398, 1697101717414, 1697101717429, 1697101717444, 1697101717460, 1697101717475, 1697101717491, 1697101717506, 1697101717522, 1697101717537, 1697101717552, 1697101717568, 1697101717583, 1697101717599, 1697101717614, 1697101717630, 1697101717645, 1697101717661, 1697101717676, 1697101717692, 1697101717707, 1697101717723, 1697101717738, 1697101717753, 1697101717769, 1697101717784, 1697101717800, 1697101717815, 1697101717831, 1697101717846, 1697101717862, 1697101717877, 1697101717893, 1697101717908, 1697101717924, 1697101717939, 1697101717955, 1697101717970, 1697101717986, 1697101718001, 1697101718017, 1697101718032, 1697101718048, 1697101718063, 1697101718076, 1697101718090, 1697101718103, 1697101718116, 1697101718130, 1697101718143, 1697101718157, 1697101718170, 1697101718184, 1697101718197, 1697101718211, 1697101718224, 1697101718237, 1697101718251, 1697101718264, 1697101718278, 1697101718291, 1697101718305, 1697101718318, 1697101718331, 1697101718345, 1697101718358]"
4817,4817,176,36,[],200,llama-13b,128,1,1812.0,1.0,1,H100,1697101684425,1697101686237.0,120,216.0,2.0,"[19, 1793]","[1697101684444, 1697101686237]"
4818,4818,8,35,[],200,llama-13b,128,1,2077.0,1.0,1,H100,1697101697595,1697101699672.0,120,39.0,3.0,"[290, 1590, 113, 84]","[1697101697885, 1697101699475, 1697101699588, 1697101699672]"
4819,4819,362,36,[],200,llama-13b,128,1,879.0,1.0,1,H100,1697101699673,1697101700552.0,120,14.0,1.0,"[12, 867]","[1697101699685, 1697101700552]"
4820,4820,373,20,[],200,llama-13b,128,1,914.0,1.0,1,H100,1697101656233,1697101657147.0,120,15.0,1.0,"[12, 902]","[1697101656245, 1697101657147]"
4821,4821,727,21,[],200,llama-13b,128,1,1399.0,1.0,1,H100,1697101657150,1697101658549.0,120,58.0,5.0,"[24, 986, 129, 96, 82, 82]","[1697101657174, 1697101658160, 1697101658289, 1697101658385, 1697101658467, 1697101658549]"
4822,4822,718,37,[],200,llama-13b,128,1,542.0,1.0,1,H100,1697101700553,1697101701095.0,120,13.0,1.0,"[30, 511]","[1697101700583, 1697101701094]"
4823,4823,538,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697587.0,120,,,"[6, 1359, 223, 103, 101, 110, 92, 91]","[1697101694824, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4824,4824,341,28,[],200,llama-13b,128,1,4521.0,1.0,1,H100,1697101680806,1697101685327.0,120,87.0,20.0,"[30, 1675, 193, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 81]","[1697101680836, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327]"
4825,4825,897,31,[],200,llama-13b,128,1,1886.0,1.0,1,H100,1697101697590,1697101699476.0,120,9.0,1.0,"[414, 1472]","[1697101698004, 1697101699476]"
4826,4826,326,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700637.0,120,,,"[12, 1063]","[1697101699489, 1697101700552]"
4827,4827,653,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[281, 1236, 123, 69]","[1697101700923, 1697101702159, 1697101702282, 1697101702351]"
4828,4828,84,34,[],200,llama-13b,128,1,1725.0,1.0,1,H100,1697101703174,1697101704899.0,120,26.0,1.0,"[132, 1593]","[1697101703306, 1697101704899]"
4829,4829,441,35,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,6.0,1.0,"[92, 785]","[1697101704993, 1697101705778]"
4830,4830,147,38,[],200,llama-13b,128,1,1940.0,1.0,1,H100,1697101701095,1697101703035.0,120,182.0,1.0,"[100, 1840]","[1697101701195, 1697101703035]"
4831,4831,799,36,[],200,llama-13b,128,1,5028.0,1.0,1,H100,1697101705779,1697101710807.0,120,84.0,20.0,"[24, 1242, 240, 97, 92, 70, 91, 70, 91, 87, 544, 99, 65, 88, 812, 88, 83, 82, 79, 881, 103]","[1697101705803, 1697101707045, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807]"
4832,4832,257,37,[],200,llama-13b,128,1,1793.0,1.0,1,H100,1697101686592,1697101688385.0,120,14.0,1.0,"[12, 1781]","[1697101686604, 1697101688385]"
4833,4833,507,39,[],200,llama-13b,128,1,5490.0,1.0,1,H100,1697101703036,1697101708526.0,120,83.0,20.0,"[24, 628, 1320, 104, 72, 729, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 99]","[1697101703060, 1697101703688, 1697101705008, 1697101705112, 1697101705184, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
4834,4834,229,38,[],200,llama-13b,128,1,1705.0,1.0,1,H100,1697101680806,1697101682511.0,120,15.0,1.0,"[68, 1637]","[1697101680874, 1697101682511]"
4835,4835,679,39,[],200,llama-13b,128,1,1129.0,1.0,1,H100,1697101682512,1697101683641.0,120,15.0,1.0,"[30, 1099]","[1697101682542, 1697101683641]"
4836,4836,111,40,[],200,llama-13b,128,1,1604.0,1.0,1,H100,1697101683642,1697101685246.0,120,79.0,5.0,"[6, 1259, 106, 87, 63, 83]","[1697101683648, 1697101684907, 1697101685013, 1697101685100, 1697101685163, 1697101685246]"
4837,4837,642,28,[],200,llama-13b,128,1,5736.0,1.0,1,H100,1697101688995,1697101694731.0,120,89.0,20.0,"[6, 975, 1229, 1068, 214, 212, 78, 101, 101, 95, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100]","[1697101689001, 1697101689976, 1697101691205, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692979, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
4838,4838,622,38,[],200,llama-13b,128,1,1185.0,1.0,1,H100,1697101688386,1697101689571.0,120,20.0,1.0,"[24, 1160]","[1697101688410, 1697101689570]"
4839,4839,468,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685247,1697101689615.0,120,,,"[12, 857, 121, 91, 89, 87, 87, 646, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101685259, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4840,4840,52,39,[],200,llama-13b,128,1,3206.0,1.0,1,H100,1697101689571,1697101692777.0,120,58.0,6.0,"[7, 398, 1229, 1068, 214, 212, 78]","[1697101689578, 1697101689976, 1697101691205, 1697101692273, 1697101692487, 1697101692699, 1697101692777]"
4841,4841,74,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694733,1697101697587.0,120,,,"[24, 579, 84, 79, 76, 831, 103, 100, 111, 92, 91]","[1697101694757, 1697101695336, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
4842,4842,393,37,[],200,llama-13b,128,1,3838.0,1.0,1,H100,1697101708534,1697101712372.0,120,182.0,22.0,"[30, 810, 116, 89, 83, 82, 79, 881, 103, 98, 92, 89, 89, 85, 498, 99, 91, 84, 82, 79, 59, 69, 51]","[1697101708564, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372]"
4843,4843,380,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101692779,1697101697588.0,120,,,"[18, 682, 101, 100, 99, 95, 91, 88, 67, 434, 77, 100, 85, 604, 78, 77, 831, 103, 101, 110, 92, 91]","[1697101692797, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4844,4844,317,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689616,1697101697587.0,120,,,"[351, 1145, 93, 1068, 214, 212, 78, 110, 93, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101, 83, 605, 78, 77, 831, 103, 101, 110, 92, 91]","[1697101689967, 1697101691112, 1697101691205, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732, 1697101694815, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4845,4845,267,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101698296,1697101700636.0,120,,,"[6, 1174, 112, 83, 82, 82, 80, 82]","[1697101698302, 1697101699476, 1697101699588, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
4846,4846,136,31,[],200,llama-13b,128,1,1088.0,1.0,1,H100,1697101673760,1697101674848.0,120,31.0,1.0,"[18, 1070]","[1697101673778, 1697101674848]"
4847,4847,437,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700635.0,120,,,"[302, 1578, 113, 83, 82, 82, 80, 82]","[1697101697897, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
4848,4848,784,29,[],200,llama-13b,128,1,3840.0,1.0,1,H100,1697101680581,1697101684421.0,120,89.0,20.0,"[61, 860, 106, 84, 83, 79, 79, 770, 97, 93, 91, 69, 91, 90, 615, 102, 100, 97, 92, 90, 91]","[1697101680642, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684421]"
4849,4849,466,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101674849,1697101675810.0,120,,,"[6, 934]","[1697101674855, 1697101675789]"
4850,4850,820,33,[],200,llama-13b,128,1,2782.0,1.0,1,H100,1697101675813,1697101678595.0,120,161.0,9.0,"[114, 453, 36, 1061, 64, 50, 763, 90, 86, 64]","[1697101675927, 1697101676380, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594]"
4851,4851,765,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[458, 1934]","[1697101701100, 1697101703034]"
4852,4852,617,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[535, 1858]","[1697101701177, 1697101703035]"
4853,4853,74,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[130, 570, 43, 1250, 83, 82, 81, 81, 82]","[1697101697725, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4854,4854,608,24,[],200,llama-13b,128,1,4757.0,1.0,1,H100,1697101675814,1697101680571.0,120,96.0,20.0,"[355, 1211, 97, 64, 50, 764, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101676169, 1697101677380, 1697101677477, 1697101677541, 1697101677591, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
4855,4855,190,32,[],200,llama-13b,128,1,3167.0,1.0,1,H100,1697101703175,1697101706342.0,120,335.0,10.0,"[107, 1617, 117, 97, 72, 728, 95, 87, 85, 83, 79]","[1697101703282, 1697101704899, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342]"
4856,4856,288,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673893,1697101675810.0,120,,,"[58, 1030, 103, 98, 95, 70, 95]","[1697101673951, 1697101674981, 1697101675084, 1697101675182, 1697101675277, 1697101675347, 1697101675442]"
4857,4857,435,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[535, 1857]","[1697101701177, 1697101703034]"
4858,4858,250,34,[],200,llama-13b,128,1,692.0,1.0,1,H100,1697101678596,1697101679288.0,120,31.0,1.0,"[6, 686]","[1697101678602, 1697101679288]"
4859,4859,794,43,[],200,llama-13b,128,1,1723.0,1.0,1,H100,1697101703176,1697101704899.0,120,11.0,1.0,"[325, 1398]","[1697101703501, 1697101704899]"
4860,4860,462,32,[],200,llama-13b,128,1,603.0,1.0,1,H100,1697101694733,1697101695336.0,120,52.0,1.0,"[42, 561]","[1697101694775, 1697101695336]"
4861,4861,604,35,[],200,llama-13b,128,1,1443.0,1.0,1,H100,1697101679289,1697101680732.0,120,161.0,4.0,"[12, 1178, 92, 103, 58]","[1697101679301, 1697101680479, 1697101680571, 1697101680674, 1697101680732]"
4862,4862,641,27,[],200,llama-13b,128,1,9618.0,1.0,1,H100,1697101663466,1697101673084.0,120,16.0,50.0,"[36, 1176, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 733, 104, 99, 72, 99, 94, 793, 101, 95, 71, 91, 89, 725, 99, 94, 93, 92, 89, 676, 99, 70, 93, 90, 90, 512, 98, 95, 94, 93, 90, 521, 113, 92, 67, 68, 88]","[1697101663502, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668843, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672769, 1697101672861, 1697101672928, 1697101672996, 1697101673084]"
4863,4863,226,44,[],200,llama-13b,128,1,7806.0,1.0,1,H100,1697101704901,1697101712707.0,120,216.0,47.0,"[24, 853, 135, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 87, 544, 99, 65, 88, 811, 89, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 497, 99, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45, 44, 41, 33]","[1697101704925, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712674, 1697101712707]"
4864,4864,209,38,[],200,llama-13b,128,1,845.0,1.0,1,H100,1697101708529,1697101709374.0,120,20.0,1.0,"[17, 828]","[1697101708546, 1697101709374]"
4865,4865,534,39,[],200,llama-13b,128,1,3169.0,1.0,1,H100,1697101709375,1697101712544.0,120,96.0,20.0,"[30, 1023, 277, 103, 97, 92, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55]","[1697101709405, 1697101710428, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
4866,4866,439,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700643,1697101703170.0,120,,,"[427, 1090, 122, 69]","[1697101701070, 1697101702160, 1697101702282, 1697101702351]"
4867,4867,407,31,[],200,llama-13b,128,1,1567.0,1.0,1,H100,1697101675813,1697101677380.0,120,16.0,1.0,"[267, 1300]","[1697101676080, 1697101677380]"
4868,4868,229,37,[],200,llama-13b,128,1,851.0,1.0,1,H100,1697101710809,1697101711660.0,120,15.0,1.0,"[30, 821]","[1697101710839, 1697101711660]"
4869,4869,760,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101677381,1697101689614.0,120,,,"[42, 824, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93, 91, 69, 91, 90, 615, 101, 101, 97, 92, 90, 90, 593, 86, 64, 82, 81, 911, 91, 89, 87, 86, 647, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 91, 89]","[1697101677423, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
4870,4870,382,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695421,1697101697588.0,120,,,"[8, 754, 224, 102, 101, 111, 92, 91]","[1697101695429, 1697101696183, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696813, 1697101696904]"
4871,4871,244,22,[],200,llama-13b,128,1,824.0,1.0,1,H100,1697101658550,1697101659374.0,120,9.0,1.0,"[6, 818]","[1697101658556, 1697101659374]"
4872,4872,649,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688625,1697101689595.0,120,,,"[7, 939]","[1697101688632, 1697101689571]"
4873,4873,598,23,[],200,llama-13b,128,1,3294.0,1.0,1,H100,1697101659374,1697101662668.0,120,345.0,12.0,"[19, 1253, 339, 102, 75, 98, 93, 87, 831, 107, 101, 95, 94]","[1697101659393, 1697101660646, 1697101660985, 1697101661087, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662479, 1697101662574, 1697101662668]"
4874,4874,56,25,[],200,llama-13b,128,1,4037.0,1.0,1,H100,1697101663566,1697101667603.0,120,86.0,20.0,"[6, 874, 232, 109, 102, 96, 94, 101, 688, 105, 100, 72, 72, 93, 92, 733, 104, 99, 72, 99, 94]","[1697101663572, 1697101664446, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603]"
4875,4875,74,38,[],200,llama-13b,128,1,5120.0,1.0,1,H100,1697101689612,1697101694732.0,120,88.0,20.0,"[281, 1219, 93, 1069, 213, 212, 79, 111, 91, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101]","[1697101689893, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692889, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732]"
4876,4876,740,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697596,1697101700635.0,120,,,"[389, 1490, 113, 83, 82, 81, 81, 82]","[1697101697985, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4877,4877,130,28,[],200,llama-13b,128,1,1390.0,1.0,1,H100,1697101680112,1697101681502.0,120,14.0,1.0,"[37, 1353]","[1697101680149, 1697101681502]"
4878,4878,254,45,[],200,llama-13b,128,1,1521.0,1.0,1,H100,1697101700638,1697101702159.0,120,58.0,1.0,"[150, 1371]","[1697101700788, 1697101702159]"
4879,4879,31,24,[],200,llama-13b,128,1,4467.0,1.0,1,H100,1697101662669,1697101667136.0,120,84.0,20.0,"[6, 662, 127, 101, 93, 89, 86, 845, 108, 103, 96, 94, 101, 688, 105, 101, 71, 72, 93, 92, 734]","[1697101662675, 1697101663337, 1697101663464, 1697101663565, 1697101663658, 1697101663747, 1697101663833, 1697101664678, 1697101664786, 1697101664889, 1697101664985, 1697101665079, 1697101665180, 1697101665868, 1697101665973, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136]"
4880,4880,419,26,[],200,llama-13b,128,1,5052.0,1.0,1,H100,1697101667605,1697101672657.0,120,88.0,20.0,"[12, 1827, 124, 99, 94, 93, 92, 89, 676, 99, 71, 92, 91, 89, 513, 97, 95, 94, 93, 90, 522]","[1697101667617, 1697101669444, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657]"
4881,4881,484,29,[],200,llama-13b,128,1,7310.0,1.0,1,H100,1697101681503,1697101688813.0,120,86.0,36.0,"[36, 972, 193, 96, 93, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90, 593, 86, 64, 83, 80, 911, 91, 89, 87, 86, 647, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92]","[1697101681539, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813]"
4882,4882,614,46,[],200,llama-13b,128,1,874.0,1.0,1,H100,1697101702161,1697101703035.0,120,15.0,1.0,"[18, 856]","[1697101702179, 1697101703035]"
4883,4883,45,47,[],200,llama-13b,128,1,652.0,1.0,1,H100,1697101703036,1697101703688.0,120,19.0,1.0,"[60, 592]","[1697101703096, 1697101703688]"
4884,4884,406,48,[],200,llama-13b,128,1,2406.0,1.0,1,H100,1697101703689,1697101706095.0,120,244.0,4.0,"[12, 2077, 135, 95, 87]","[1697101703701, 1697101705778, 1697101705913, 1697101706008, 1697101706095]"
4885,4885,760,49,[],200,llama-13b,128,1,7197.0,1.0,1,H100,1697101706096,1697101713293.0,120,335.0,64.0,"[13, 936, 241, 96, 92, 70, 92, 69, 91, 87, 545, 99, 64, 88, 812, 88, 83, 82, 79, 881, 104, 96, 93, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45, 44, 41, 33, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26, 24, 20, 20, 21, 19, 18, 19, 20, 18]","[1697101706109, 1697101707045, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710808, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712674, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114, 1697101713138, 1697101713158, 1697101713178, 1697101713199, 1697101713218, 1697101713236, 1697101713255, 1697101713275, 1697101713293]"
4886,4886,587,28,[],200,llama-13b,128,1,567.0,1.0,1,H100,1697101675813,1697101676380.0,120,13.0,1.0,"[85, 482]","[1697101675898, 1697101676380]"
4887,4887,15,29,[],200,llama-13b,128,1,4423.0,1.0,1,H100,1697101676381,1697101680804.0,120,100.0,20.0,"[38, 1828, 107, 90, 86, 64, 84, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 470, 94, 57, 73]","[1697101676419, 1697101678247, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680580, 1697101680674, 1697101680731, 1697101680804]"
4888,4888,235,47,[],200,llama-13b,128,1,2642.0,1.0,1,H100,1697101708533,1697101711175.0,120,161.0,12.0,"[61, 897, 88, 83, 82, 79, 881, 103, 98, 92, 89, 89]","[1697101708594, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711175]"
4889,4889,828,42,[],200,llama-13b,128,1,3162.0,1.0,1,H100,1697101689616,1697101692778.0,120,182.0,6.0,"[272, 1317, 1069, 213, 212, 79]","[1697101689888, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778]"
4890,4890,248,43,[],200,llama-13b,128,1,3627.0,1.0,1,H100,1697101692779,1697101696406.0,120,182.0,17.0,"[6, 587, 107, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 85, 604, 79, 76, 831]","[1697101692785, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696406]"
4891,4891,374,30,[],200,llama-13b,128,1,4520.0,1.0,1,H100,1697101680806,1697101685326.0,120,85.0,20.0,"[7, 1698, 193, 96, 93, 92, 68, 91, 90, 614, 102, 101, 97, 91, 91, 90, 594, 86, 63, 83, 80]","[1697101680813, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684239, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685326]"
4892,4892,736,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685330,1697101689594.0,120,,,"[65, 1628, 214, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101685395, 1697101687023, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
4893,4893,134,32,[],200,llama-13b,128,1,5120.0,1.0,1,H100,1697101689611,1697101694731.0,120,86.0,20.0,"[59, 1442, 93, 1069, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100]","[1697101689670, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
4894,4894,424,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[52, 1313, 223, 103, 101, 110, 92, 91]","[1697101694870, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4895,4895,123,27,[],200,llama-13b,128,1,921.0,1.0,1,H100,1697101680581,1697101681502.0,120,14.0,1.0,"[93, 828]","[1697101680674, 1697101681502]"
4896,4896,824,34,[],200,llama-13b,128,1,1188.0,1.0,1,H100,1697101687534,1697101688722.0,120,58.0,4.0,"[12, 839, 133, 106, 97]","[1697101687546, 1697101688385, 1697101688518, 1697101688624, 1697101688721]"
4897,4897,161,40,[],200,llama-13b,128,1,3272.0,1.0,1,H100,1697101689616,1697101692888.0,120,109.0,7.0,"[283, 1306, 1069, 213, 212, 79, 109]","[1697101689899, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692887]"
4898,4898,796,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695337,1697101697588.0,120,,,"[7, 839, 223, 103, 101, 110, 92, 92]","[1697101695344, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696904]"
4899,4899,480,28,[],200,llama-13b,128,1,1008.0,1.0,1,H100,1697101681503,1697101682511.0,120,26.0,1.0,"[48, 960]","[1697101681551, 1697101682511]"
4900,4900,834,29,[],200,llama-13b,128,1,4827.0,1.0,1,H100,1697101682512,1697101687339.0,120,85.0,20.0,"[12, 1117, 208, 102, 100, 97, 92, 90, 90, 593, 86, 64, 82, 81, 911, 91, 89, 87, 86, 647, 102]","[1697101682524, 1697101683641, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339]"
4901,4901,734,29,[],200,llama-13b,128,1,2631.0,1.0,1,H100,1697101675813,1697101678444.0,120,100.0,6.0,"[162, 1404, 98, 64, 50, 763, 90]","[1697101675975, 1697101677379, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444]"
4902,4902,435,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[58, 1307, 223, 103, 101, 110, 92, 91]","[1697101694876, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4903,4903,530,37,[],200,llama-13b,128,1,784.0,1.0,1,H100,1697101686238,1697101687022.0,120,26.0,1.0,"[6, 778]","[1697101686244, 1697101687022]"
4904,4904,531,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697591,1697101700636.0,120,,,"[22, 682, 43, 1250, 83, 82, 81, 81, 82]","[1697101697613, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4905,4905,889,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687023,1697101689594.0,120,,,"[13, 1349, 133, 106, 97, 92, 92, 89]","[1697101687036, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
4906,4906,36,36,[],200,llama-13b,128,1,3687.0,1.0,1,H100,1697101680733,1697101684420.0,120,457.0,20.0,"[6, 763, 106, 84, 83, 79, 79, 771, 96, 93, 91, 69, 91, 90, 614, 102, 101, 97, 92, 90, 90]","[1697101680739, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
4907,4907,793,44,[],200,llama-13b,128,1,7822.0,1.0,1,H100,1697101703175,1697101710997.0,120,92.0,31.0,"[19, 1821, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 99, 65, 88, 811, 88, 84, 82, 79, 881, 103, 97, 93]","[1697101703194, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709578, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997]"
4908,4908,188,31,[],200,llama-13b,128,1,3824.0,1.0,1,H100,1697101681503,1697101685327.0,120,85.0,20.0,"[24, 984, 193, 96, 93, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90, 593, 87, 63, 83, 81]","[1697101681527, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685327]"
4909,4909,252,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695500,1697101697589.0,120,,,"[6, 677, 224, 102, 101, 111, 92, 91]","[1697101695506, 1697101696183, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696813, 1697101696904]"
4910,4910,790,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697590,1697101700636.0,120,,,"[17, 688, 43, 1250, 83, 82, 81, 81, 82]","[1697101697607, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4911,4911,432,39,[],200,llama-13b,128,1,603.0,1.0,1,H100,1697101694733,1697101695336.0,120,13.0,1.0,"[78, 525]","[1697101694811, 1697101695336]"
4912,4912,787,40,[],200,llama-13b,128,1,1476.0,1.0,1,H100,1697101695337,1697101696813.0,120,123.0,6.0,"[13, 833, 223, 103, 101, 110, 92]","[1697101695350, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812]"
4913,4913,219,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696813,1697101697589.0,120,,,"[7, 576]","[1697101696820, 1697101697396]"
4914,4914,549,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697597,1697101700635.0,120,,,"[401, 1590, 83, 82, 81, 81, 82]","[1697101697998, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4915,4915,159,41,[],200,llama-13b,128,1,1267.0,1.0,1,H100,1697101707046,1697101708313.0,120,31.0,1.0,"[36, 1231]","[1697101707082, 1697101708313]"
4916,4916,523,42,[],200,llama-13b,128,1,2946.0,1.0,1,H100,1697101708314,1697101711260.0,120,345.0,13.0,"[13, 1047, 116, 89, 83, 82, 79, 882, 103, 97, 92, 89, 88, 86]","[1697101708327, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260]"
4917,4917,606,45,[],200,llama-13b,128,1,1873.0,1.0,1,H100,1697101697603,1697101699476.0,120,9.0,1.0,"[569, 1303]","[1697101698172, 1697101699475]"
4918,4918,907,43,[],200,llama-13b,128,1,1521.0,1.0,1,H100,1697101700638,1697101702159.0,120,10.0,1.0,"[42, 1479]","[1697101700680, 1697101702159]"
4919,4919,341,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703171.0,120,,,"[24, 850]","[1697101702185, 1697101703035]"
4920,4920,39,46,[],200,llama-13b,128,1,1075.0,1.0,1,H100,1697101699477,1697101700552.0,120,8.0,1.0,"[30, 1045]","[1697101699507, 1697101700552]"
4921,4921,397,47,[],200,llama-13b,128,1,1729.0,1.0,1,H100,1697101700553,1697101702282.0,120,67.0,2.0,"[6, 535, 1188]","[1697101700559, 1697101701094, 1697101702282]"
4922,4922,695,45,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,92.0,20.0,"[297, 1426, 112, 102, 72, 728, 95, 87, 85, 83, 79, 943, 97, 92, 70, 91, 70, 91, 87, 545, 98]","[1697101703473, 1697101704899, 1697101705011, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
4923,4923,35,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697587.0,120,,,"[51, 1314, 223, 103, 101, 110, 92, 91]","[1697101694869, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
4924,4924,758,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702283,1697101703172.0,120,,,"[6, 746]","[1697101702289, 1697101703035]"
4925,4925,156,49,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,86.0,20.0,"[192, 1532, 115, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 91, 70, 91, 87, 544, 99]","[1697101703368, 1697101704900, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526]"
4926,4926,170,34,[],200,llama-13b,128,1,4351.0,1.0,1,H100,1697101689614,1697101693965.0,120,335.0,15.0,"[172, 1326, 93, 1069, 213, 212, 78, 110, 93, 94, 404, 102, 100, 99, 95, 91]","[1697101689786, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965]"
4927,4927,514,50,[],200,llama-13b,128,1,3719.0,1.0,1,H100,1697101708533,1697101712252.0,120,85.0,20.0,"[25, 816, 116, 89, 83, 82, 79, 882, 102, 98, 92, 89, 89, 85, 498, 99, 91, 84, 82, 79, 59]","[1697101708558, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712114, 1697101712193, 1697101712252]"
4928,4928,498,35,[],200,llama-13b,128,1,492.0,1.0,1,H100,1697101693966,1697101694458.0,120,9.0,1.0,"[6, 485]","[1697101693972, 1697101694457]"
4929,4929,859,36,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101694459,1697101695336.0,120,23.0,1.0,"[18, 859]","[1697101694477, 1697101695336]"
4930,4930,290,37,[],200,llama-13b,128,1,846.0,1.0,1,H100,1697101695337,1697101696183.0,120,14.0,1.0,"[18, 828]","[1697101695355, 1697101696183]"
4931,4931,644,38,[],200,llama-13b,128,1,1211.0,1.0,1,H100,1697101696185,1697101697396.0,120,19.0,1.0,"[48, 1163]","[1697101696233, 1697101697396]"
4932,4932,69,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697397,1697101700635.0,120,,,"[12, 363, 566, 1249, 84, 82, 81, 80, 83]","[1697101697409, 1697101697772, 1697101698338, 1697101699587, 1697101699671, 1697101699753, 1697101699834, 1697101699914, 1697101699997]"
4933,4933,731,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685328,1697101689602.0,120,,,"[37, 1657, 215, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 92, 88]","[1697101685365, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688993]"
4934,4934,520,31,[],200,llama-13b,128,1,541.0,1.0,1,H100,1697101700554,1697101701095.0,120,11.0,1.0,"[53, 487]","[1697101700607, 1697101701094]"
4935,4935,876,32,[],200,llama-13b,128,1,1939.0,1.0,1,H100,1697101701096,1697101703035.0,120,11.0,1.0,"[93, 1846]","[1697101701189, 1697101703035]"
4936,4936,304,33,[],200,llama-13b,128,1,5490.0,1.0,1,H100,1697101703036,1697101708526.0,120,86.0,20.0,"[24, 628, 1320, 105, 71, 729, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 99]","[1697101703060, 1697101703688, 1697101705008, 1697101705113, 1697101705184, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
4937,4937,267,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688625,1697101689595.0,120,,,"[13, 933]","[1697101688638, 1697101689571]"
4938,4938,160,32,[],200,llama-13b,128,1,2412.0,1.0,1,H100,1697101689618,1697101692030.0,120,13.0,1.0,"[403, 2009]","[1697101690021, 1697101692030]"
4939,4939,398,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703171.0,120,,,"[156, 1365, 123, 69]","[1697101700794, 1697101702159, 1697101702282, 1697101702351]"
4940,4940,46,42,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,79.0,20.0,"[253, 1579, 105, 72, 728, 95, 87, 85, 83, 80, 942, 97, 92, 70, 91, 70, 91, 87, 545, 97]","[1697101703429, 1697101705008, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708525]"
4941,4941,760,41,[],200,llama-13b,128,1,9938.0,1.0,1,H100,1697101703176,1697101713114.0,120,335.0,64.0,"[118, 1605, 117, 97, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 91, 70, 91, 86, 545, 99, 65, 88, 811, 88, 84, 82, 79, 881, 103, 97, 93, 89, 88, 86, 497, 99, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45, 44, 41, 33, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26]","[1697101703294, 1697101704899, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709578, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712674, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114]"
4942,4942,490,33,[],200,llama-13b,128,1,1748.0,1.0,1,H100,1697101692031,1697101693779.0,120,11.0,5.0,"[6, 1442, 101, 100, 99]","[1697101692037, 1697101693479, 1697101693580, 1697101693680, 1697101693779]"
4943,4943,848,34,[],200,llama-13b,128,1,676.0,1.0,1,H100,1697101693781,1697101694457.0,120,47.0,1.0,"[12, 664]","[1697101693793, 1697101694457]"
4944,4944,273,35,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101694459,1697101695336.0,120,19.0,1.0,"[18, 859]","[1697101694477, 1697101695336]"
4945,4945,628,34,[],200,llama-13b,128,1,3863.0,1.0,1,H100,1697101689616,1697101693479.0,120,732.0,10.0,"[289, 1207, 93, 1069, 213, 212, 79, 106, 96, 94, 405]","[1697101689905, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692884, 1697101692980, 1697101693074, 1697101693479]"
4946,4946,636,36,[],200,llama-13b,128,1,845.0,1.0,1,H100,1697101695338,1697101696183.0,120,31.0,1.0,"[48, 797]","[1697101695386, 1697101696183]"
4947,4947,65,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696184,1697101697589.0,120,,,"[31, 1180]","[1697101696215, 1697101697395]"
4948,4948,487,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700637.0,120,,,"[210, 1783, 83, 82, 81, 81, 82]","[1697101697805, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4949,4949,512,38,[],200,llama-13b,128,1,1880.0,1.0,1,H100,1697101697595,1697101699475.0,120,11.0,1.0,"[182, 1698]","[1697101697777, 1697101699475]"
4950,4950,849,34,[],200,llama-13b,128,1,1518.0,1.0,1,H100,1697101700642,1697101702160.0,120,10.0,1.0,"[251, 1267]","[1697101700893, 1697101702160]"
4951,4951,868,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700635.0,120,,,"[30, 1045]","[1697101699507, 1697101700552]"
4952,4952,279,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702162,1697101703171.0,120,,,"[71, 802]","[1697101702233, 1697101703035]"
4953,4953,640,36,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703176,1697101704900.0,120,15.0,1.0,"[459, 1265]","[1697101703635, 1697101704900]"
4954,4954,64,37,[],200,llama-13b,128,1,4589.0,1.0,1,H100,1697101704901,1697101709490.0,120,89.0,20.0,"[30, 847, 135, 95, 87, 85, 83, 80, 942, 96, 92, 71, 91, 70, 91, 87, 544, 99, 65, 88, 811]","[1697101704931, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490]"
4955,4955,405,43,[],200,llama-13b,128,1,3719.0,1.0,1,H100,1697101708533,1697101712252.0,120,87.0,20.0,"[19, 822, 116, 89, 83, 82, 79, 882, 103, 97, 92, 89, 89, 85, 498, 99, 90, 85, 82, 79, 59]","[1697101708552, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252]"
4956,4956,634,33,[],200,llama-13b,128,1,669.0,1.0,1,H100,1697101687716,1697101688385.0,120,13.0,1.0,"[12, 657]","[1697101687728, 1697101688385]"
4957,4957,38,25,[],200,llama-13b,128,1,3840.0,1.0,1,H100,1697101680581,1697101684421.0,120,88.0,20.0,"[37, 884, 106, 84, 83, 79, 79, 770, 97, 93, 91, 70, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680618, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
4958,4958,526,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696721,1697101697589.0,120,,,"[7, 668]","[1697101696728, 1697101697396]"
4959,4959,553,25,[],200,llama-13b,128,1,5174.0,1.0,1,H100,1697101658291,1697101663465.0,120,88.0,20.0,"[42, 1040, 332, 102, 98, 97, 93, 85, 805, 101, 76, 98, 93, 87, 831, 107, 102, 94, 94, 89, 707]","[1697101658333, 1697101659373, 1697101659705, 1697101659807, 1697101659905, 1697101660002, 1697101660095, 1697101660180, 1697101660985, 1697101661086, 1697101661162, 1697101661260, 1697101661353, 1697101661440, 1697101662271, 1697101662378, 1697101662480, 1697101662574, 1697101662668, 1697101662757, 1697101663464]"
4960,4960,881,37,[],200,llama-13b,128,1,2312.0,1.0,1,H100,1697101697603,1697101699915.0,120,58.0,6.0,"[531, 1341, 113, 83, 82, 81, 81]","[1697101698134, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915]"
4961,4961,311,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699916,1697101700635.0,120,,,[6],[1697101699922]
4962,4962,672,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[70, 1574, 69]","[1697101700708, 1697101702282, 1697101702351]"
4963,4963,72,40,[],200,llama-13b,128,1,5351.0,1.0,1,H100,1697101703175,1697101708526.0,120,84.0,20.0,"[13, 1711, 116, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 99]","[1697101703188, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
4964,4964,395,38,[],200,llama-13b,128,1,3052.0,1.0,1,H100,1697101709492,1697101712544.0,120,88.0,20.0,"[24, 911, 278, 103, 97, 92, 89, 89, 85, 498, 98, 91, 85, 82, 79, 59, 69, 51, 60, 57, 55]","[1697101709516, 1697101710427, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
4965,4965,297,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703171.0,120,,,"[162, 1359, 123, 69]","[1697101700800, 1697101702159, 1697101702282, 1697101702351]"
4966,4966,659,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101668846,1697101675812.0,120,,,"[42, 1597, 226, 99, 71, 92, 90, 91, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 67, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101668888, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671063, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4967,4967,366,37,[],200,llama-13b,128,1,2166.0,1.0,1,H100,1697101684425,1697101686591.0,120,85.0,6.0,"[19, 1672, 121, 91, 89, 87, 86]","[1697101684444, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590]"
4968,4968,727,38,[],200,llama-13b,128,1,2222.0,1.0,1,H100,1697101686591,1697101688813.0,120,58.0,5.0,"[7, 1787, 133, 106, 97, 92]","[1697101686598, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813]"
4969,4969,656,41,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703176,1697101704900.0,120,26.0,1.0,"[412, 1312]","[1697101703588, 1697101704900]"
4970,4970,81,42,[],200,llama-13b,128,1,2804.0,1.0,1,H100,1697101704901,1697101707705.0,120,732.0,13.0,"[98, 779, 135, 95, 87, 85, 83, 80, 942, 96, 93, 70, 91, 70]","[1697101704999, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705]"
4971,4971,841,40,[],200,llama-13b,128,1,3324.0,1.0,1,H100,1697101708533,1697101711857.0,120,123.0,15.0,"[55, 903, 88, 83, 82, 79, 881, 103, 98, 92, 89, 89, 85, 498, 99]","[1697101708588, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857]"
4972,4972,541,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685329,1697101689594.0,120,,,"[54, 1639, 215, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101685383, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
4973,4973,884,26,[],200,llama-13b,128,1,4138.0,1.0,1,H100,1697101663465,1697101667603.0,120,90.0,20.0,"[31, 950, 232, 109, 102, 96, 94, 100, 689, 105, 100, 72, 72, 93, 92, 733, 104, 99, 72, 99, 94]","[1697101663496, 1697101664446, 1697101664678, 1697101664787, 1697101664889, 1697101664985, 1697101665079, 1697101665179, 1697101665868, 1697101665973, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667135, 1697101667239, 1697101667338, 1697101667410, 1697101667509, 1697101667603]"
4974,4974,156,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688814,1697101689607.0,120,,,"[7, 750]","[1697101688821, 1697101689571]"
4975,4975,657,24,[],200,llama-13b,128,1,1839.0,1.0,1,H100,1697101667605,1697101669444.0,120,10.0,1.0,"[24, 1815]","[1697101667629, 1697101669444]"
4976,4976,85,25,[],200,llama-13b,128,1,4443.0,1.0,1,H100,1697101669445,1697101673888.0,120,88.0,20.0,"[6, 1034, 226, 99, 71, 92, 90, 91, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 68, 87, 804]","[1697101669451, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671063, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673084, 1697101673888]"
4977,4977,512,40,[],200,llama-13b,128,1,1499.0,1.0,1,H100,1697101689613,1697101691112.0,120,11.0,1.0,"[106, 1393]","[1697101689719, 1697101691112]"
4978,4978,559,29,[],200,llama-13b,128,1,4056.0,1.0,1,H100,1697101664788,1697101668844.0,120,86.0,20.0,"[6, 944, 130, 106, 99, 72, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90]","[1697101664794, 1697101665738, 1697101665868, 1697101665974, 1697101666073, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844]"
4979,4979,866,41,[],200,llama-13b,128,1,3703.0,1.0,1,H100,1697101691113,1697101694816.0,120,93.0,20.0,"[42, 1119, 214, 211, 79, 111, 92, 94, 404, 101, 100, 99, 95, 91, 88, 67, 433, 78, 100, 85]","[1697101691155, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692889, 1697101692981, 1697101693075, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816]"
4980,4980,114,26,[],200,llama-13b,128,1,3954.0,1.0,1,H100,1697101664890,1697101668844.0,120,88.0,20.0,"[13, 835, 130, 106, 100, 71, 72, 93, 92, 734, 103, 98, 73, 98, 95, 793, 101, 95, 71, 91, 90]","[1697101664903, 1697101665738, 1697101665868, 1697101665974, 1697101666074, 1697101666145, 1697101666217, 1697101666310, 1697101666402, 1697101667136, 1697101667239, 1697101667337, 1697101667410, 1697101667508, 1697101667603, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844]"
4981,4981,194,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[450, 1072, 122, 69]","[1697101701088, 1697101702160, 1697101702282, 1697101702351]"
4982,4982,266,42,[],200,llama-13b,128,1,1365.0,1.0,1,H100,1697101694818,1697101696183.0,120,9.0,1.0,"[87, 1278]","[1697101694905, 1697101696183]"
4983,4983,628,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696185,1697101697589.0,120,,,"[41, 1169]","[1697101696226, 1697101697395]"
4984,4984,477,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101668845,1697101675812.0,120,,,"[42, 1598, 226, 99, 71, 92, 90, 91, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 67, 88, 804, 98, 89, 88, 66, 65, 687, 102, 98, 95, 70, 95]","[1697101668887, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671063, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
4985,4985,60,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697598,1697101700636.0,120,,,"[502, 1488, 83, 82, 81, 81, 82]","[1697101698100, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
4986,4986,730,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700635.0,120,,,"[290, 1590, 113, 84, 81, 82, 80, 82]","[1697101697885, 1697101699475, 1697101699588, 1697101699672, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
4987,4987,160,42,[],200,llama-13b,128,1,1517.0,1.0,1,H100,1697101700642,1697101702159.0,120,13.0,1.0,"[348, 1169]","[1697101700990, 1697101702159]"
4988,4988,574,34,[],200,llama-13b,128,1,606.0,1.0,1,H100,1697101675813,1697101676419.0,120,364.0,2.0,"[61, 506, 36]","[1697101675874, 1697101676380, 1697101676416]"
4989,4989,518,43,[],200,llama-13b,128,1,874.0,1.0,1,H100,1697101702161,1697101703035.0,120,23.0,1.0,"[24, 850]","[1697101702185, 1697101703035]"
4990,4990,419,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700644,1697101703171.0,120,,,"[458, 1932]","[1697101701102, 1697101703034]"
4991,4991,488,33,[],200,llama-13b,128,1,603.0,1.0,1,H100,1697101694733,1697101695336.0,120,6.0,1.0,"[30, 573]","[1697101694763, 1697101695336]"
4992,4992,877,44,[],200,llama-13b,128,1,5490.0,1.0,1,H100,1697101703036,1697101708526.0,120,85.0,20.0,"[42, 610, 1326, 98, 72, 729, 95, 87, 85, 83, 79, 943, 96, 92, 70, 92, 70, 91, 86, 545, 99]","[1697101703078, 1697101703688, 1697101705014, 1697101705112, 1697101705184, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707543, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
4993,4993,550,36,[],200,llama-13b,128,1,5355.0,1.0,1,H100,1697101703172,1697101708527.0,120,91.0,20.0,"[239, 1489, 108, 105, 72, 728, 95, 87, 85, 83, 80, 942, 97, 92, 70, 91, 70, 91, 87, 545, 99]","[1697101703411, 1697101704900, 1697101705008, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708527]"
4994,4994,3,35,[],200,llama-13b,128,1,4383.0,1.0,1,H100,1697101676421,1697101680804.0,120,89.0,20.0,"[70, 1756, 107, 90, 86, 64, 84, 81, 646, 98, 98, 95, 71, 93, 90, 70, 90, 469, 95, 57, 73]","[1697101676491, 1697101678247, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680579, 1697101680674, 1697101680731, 1697101680804]"
4995,4995,599,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689614,1697101697587.0,120,,,"[111, 1387, 93, 1069, 213, 212, 78, 110, 93, 94, 404, 102, 100, 99, 95, 91, 87, 68, 433, 78, 100, 84, 605, 78, 77, 831, 103, 100, 111, 92, 91]","[1697101689725, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694815, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
4996,4996,752,48,[],200,llama-13b,128,1,2275.0,1.0,1,H100,1697101708533,1697101710808.0,120,39.0,3.0,"[155, 1739, 277, 103]","[1697101708688, 1697101710427, 1697101710704, 1697101710807]"
4997,4997,766,31,[],200,llama-13b,128,1,1886.0,1.0,1,H100,1697101697590,1697101699476.0,120,11.0,1.0,"[390, 1495]","[1697101697980, 1697101699475]"
4998,4998,196,32,[],200,llama-13b,128,1,1076.0,1.0,1,H100,1697101699477,1697101700553.0,120,13.0,1.0,"[102, 973]","[1697101699579, 1697101700552]"
4999,4999,180,49,[],200,llama-13b,128,1,1680.0,1.0,1,H100,1697101710809,1697101712489.0,120,123.0,12.0,"[12, 839, 98, 99, 90, 85, 83, 78, 59, 69, 51, 60, 57]","[1697101710821, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489]"
5000,5000,554,33,[],200,llama-13b,128,1,541.0,1.0,1,H100,1697101700554,1697101701095.0,120,26.0,1.0,"[47, 494]","[1697101700601, 1697101701095]"
5001,5001,515,41,[],200,llama-13b,128,1,481.0,1.0,1,H100,1697101692891,1697101693372.0,120,11.0,1.0,"[13, 468]","[1697101692904, 1697101693372]"
5002,5002,159,30,[],200,llama-13b,128,1,842.0,1.0,1,H100,1697101678446,1697101679288.0,120,31.0,1.0,"[24, 818]","[1697101678470, 1697101679288]"
5003,5003,517,31,[],200,llama-13b,128,1,1191.0,1.0,1,H100,1697101679289,1697101680480.0,120,15.0,1.0,"[30, 1161]","[1697101679319, 1697101680480]"
5004,5004,873,32,[],200,llama-13b,128,1,1021.0,1.0,1,H100,1697101680481,1697101681502.0,120,6.0,1.0,"[18, 1003]","[1697101680499, 1697101681502]"
5005,5005,268,30,[],200,llama-13b,128,1,1045.0,1.0,1,H100,1697101687340,1697101688385.0,120,19.0,1.0,"[18, 1027]","[1697101687358, 1697101688385]"
5006,5006,625,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688386,1697101689594.0,120,,,"[12, 1172]","[1697101688398, 1697101689570]"
5007,5007,27,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689615,1697101697587.0,120,,,"[260, 1237, 93, 1069, 213, 212, 79, 110, 92, 94, 405, 101, 100, 99, 95, 91, 88, 67, 433, 78, 101, 83, 605, 79, 76, 831, 103, 100, 111, 92, 91]","[1697101689875, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692888, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631, 1697101694732, 1697101694815, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
5008,5008,821,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[81, 1284, 223, 103, 101, 110, 92, 91]","[1697101694899, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5009,5009,304,33,[],200,llama-13b,128,1,3823.0,1.0,1,H100,1697101681503,1697101685326.0,120,86.0,20.0,"[36, 972, 193, 96, 93, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90, 593, 86, 64, 83, 80]","[1697101681539, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685246, 1697101685326]"
5010,5010,702,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685328,1697101689607.0,120,,,"[31, 1663, 215, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 92, 88]","[1697101685359, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688993]"
5011,5011,633,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685328,1697101689615.0,120,,,"[19, 1675, 215, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 92, 88]","[1697101685347, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688993]"
5012,5012,25,34,[],200,llama-13b,128,1,700.0,1.0,1,H100,1697101697595,1697101698295.0,120,12.0,1.0,"[83, 617]","[1697101697678, 1697101698295]"
5013,5013,901,33,[],200,llama-13b,128,1,1498.0,1.0,1,H100,1697101689614,1697101691112.0,120,17.0,1.0,"[181, 1317]","[1697101689795, 1697101691112]"
5014,5014,129,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689615,1697101697587.0,120,,,"[198, 1299, 93, 1069, 213, 212, 78, 110, 93, 94, 405, 101, 100, 99, 95, 91, 87, 68, 433, 78, 100, 84, 605, 78, 77, 831, 103, 100, 111, 92, 91]","[1697101689813, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694815, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696812, 1697101696903]"
5015,5015,330,34,[],200,llama-13b,128,1,2852.0,1.0,1,H100,1697101691113,1697101693965.0,120,345.0,14.0,"[60, 857, 244, 214, 211, 79, 110, 92, 95, 404, 101, 100, 99, 95, 91]","[1697101691173, 1697101692030, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692888, 1697101692980, 1697101693075, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965]"
5016,5016,355,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101698296,1697101700636.0,120,,,"[18, 1162, 112, 83, 82, 82, 80, 82]","[1697101698314, 1697101699476, 1697101699588, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
5017,5017,248,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[100, 600, 43, 1250, 83, 82, 81, 81, 82]","[1697101697695, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5018,5018,88,27,[],200,llama-13b,128,1,4764.0,1.0,1,H100,1697101675815,1697101680579.0,120,58.0,20.0,"[460, 1105, 97, 64, 51, 763, 90, 85, 65, 82, 82, 646, 99, 97, 95, 72, 92, 90, 70, 91, 460]","[1697101676275, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678677, 1697101678759, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680571]"
5019,5019,692,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[175, 1342, 123, 69]","[1697101700817, 1697101702159, 1697101702282, 1697101702351]"
5020,5020,716,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700643,1697101703170.0,120,,,"[354, 1162, 123, 69]","[1697101700997, 1697101702159, 1697101702282, 1697101702351]"
5021,5021,414,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673891,1697101675811.0,120,,,"[9, 948, 133, 102, 99, 94, 71, 94]","[1697101673900, 1697101674848, 1697101674981, 1697101675083, 1697101675182, 1697101675276, 1697101675347, 1697101675441]"
5022,5022,117,46,[],200,llama-13b,128,1,1843.0,1.0,1,H100,1697101703173,1697101705016.0,120,364.0,2.0,"[420, 1307, 116]","[1697101703593, 1697101704900, 1697101705016]"
5023,5023,449,28,[],200,llama-13b,128,1,3840.0,1.0,1,H100,1697101680581,1697101684421.0,120,86.0,20.0,"[31, 890, 106, 84, 83, 79, 79, 770, 98, 92, 91, 70, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680612, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682703, 1697101682801, 1697101682893, 1697101682984, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
5024,5024,145,37,[],200,llama-13b,128,1,3088.0,1.0,1,H100,1697101703175,1697101706263.0,120,161.0,9.0,"[334, 1390, 116, 98, 72, 728, 95, 87, 85, 83]","[1697101703509, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263]"
5025,5025,331,33,[],200,llama-13b,128,1,582.0,1.0,1,H100,1697101693875,1697101694457.0,120,26.0,1.0,"[12, 570]","[1697101693887, 1697101694457]"
5026,5026,658,34,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101694459,1697101695336.0,120,11.0,1.0,"[24, 853]","[1697101694483, 1697101695336]"
5027,5027,87,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695337,1697101697588.0,120,,,"[25, 821, 223, 103, 101, 111, 92, 91]","[1697101695362, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696721, 1697101696813, 1697101696904]"
5028,5028,209,30,[],200,llama-13b,128,1,1690.0,1.0,1,H100,1697101684426,1697101686116.0,120,20.0,1.0,"[54, 1636]","[1697101684480, 1697101686116]"
5029,5029,567,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101686117,1697101689594.0,120,,,"[6, 900, 214, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 91, 90]","[1697101686123, 1697101687023, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688994]"
5030,5030,480,47,[],200,llama-13b,128,1,760.0,1.0,1,H100,1697101705018,1697101705778.0,120,26.0,1.0,"[10, 750]","[1697101705028, 1697101705778]"
5031,5031,508,38,[],200,llama-13b,128,1,4544.0,1.0,1,H100,1697101706264,1697101710808.0,120,86.0,20.0,"[7, 774, 241, 96, 92, 70, 92, 69, 91, 87, 545, 99, 64, 88, 812, 88, 83, 82, 79, 881, 104]","[1697101706271, 1697101707045, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710808]"
5032,5032,577,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696407,1697101697589.0,120,,,[7],[1697101696414]
5033,5033,837,48,[],200,llama-13b,128,1,5028.0,1.0,1,H100,1697101705779,1697101710807.0,120,85.0,20.0,"[60, 1206, 241, 96, 92, 70, 91, 70, 91, 87, 544, 99, 66, 87, 812, 88, 83, 82, 79, 881, 103]","[1697101705839, 1697101707045, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708592, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807]"
5034,5034,7,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[76, 624, 43, 1250, 83, 82, 81, 81, 82]","[1697101697671, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5035,5035,390,34,[],200,llama-13b,128,1,4521.0,1.0,1,H100,1697101680806,1697101685327.0,120,84.0,20.0,"[37, 1668, 193, 96, 93, 92, 69, 90, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 81]","[1697101680843, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327]"
5036,5036,446,36,[],200,llama-13b,128,1,700.0,1.0,1,H100,1697101697595,1697101698295.0,120,26.0,1.0,"[112, 588]","[1697101697707, 1697101698295]"
5037,5037,270,49,[],200,llama-13b,128,1,1680.0,1.0,1,H100,1697101710809,1697101712489.0,120,364.0,12.0,"[24, 827, 98, 99, 90, 85, 83, 78, 59, 69, 51, 60, 57]","[1697101710833, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489]"
5038,5038,804,37,[],200,llama-13b,128,1,1180.0,1.0,1,H100,1697101698296,1697101699476.0,120,20.0,1.0,"[36, 1144]","[1697101698332, 1697101699476]"
5039,5039,782,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684426,1697101689606.0,120,,,"[30, 1660, 121, 91, 89, 87, 86, 647, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 92, 89]","[1697101684456, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
5040,5040,229,38,[],200,llama-13b,128,1,1074.0,1.0,1,H100,1697101699478,1697101700552.0,120,15.0,1.0,"[107, 967]","[1697101699585, 1697101700552]"
5041,5041,563,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700553,1697101703169.0,120,,,"[12, 529, 1188, 69]","[1697101700565, 1697101701094, 1697101702282, 1697101702351]"
5042,5042,845,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695337,1697101697588.0,120,,,"[36, 810, 224, 102, 101, 111, 92, 91]","[1697101695373, 1697101696183, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696813, 1697101696904]"
5043,5043,137,26,[],200,llama-13b,128,1,4297.0,1.0,1,H100,1697101675813,1697101680110.0,120,86.0,20.0,"[26, 541, 36, 1061, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98, 95, 71, 92, 91, 70, 90]","[1697101675839, 1697101676380, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679767, 1697101679859, 1697101679950, 1697101680020, 1697101680110]"
5044,5044,212,30,[],200,llama-13b,128,1,2411.0,1.0,1,H100,1697101689619,1697101692030.0,120,31.0,1.0,"[553, 1858]","[1697101690172, 1697101692030]"
5045,5045,569,31,[],200,llama-13b,128,1,1341.0,1.0,1,H100,1697101692031,1697101693372.0,120,16.0,1.0,"[12, 1329]","[1697101692043, 1697101693372]"
5046,5046,276,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697599,1697101700635.0,120,,,"[410, 1579, 83, 82, 81, 81, 82]","[1697101698009, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5047,5047,584,27,[],200,llama-13b,128,1,1390.0,1.0,1,H100,1697101680112,1697101681502.0,120,10.0,1.0,"[24, 1366]","[1697101680136, 1697101681502]"
5048,5048,227,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697596,1697101700635.0,120,,,"[319, 1560, 112, 84, 82, 81, 81, 82]","[1697101697915, 1697101699475, 1697101699587, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5049,5049,12,28,[],200,llama-13b,128,1,1008.0,1.0,1,H100,1697101681503,1697101682511.0,120,11.0,1.0,"[30, 978]","[1697101681533, 1697101682511]"
5050,5050,920,32,[],200,llama-13b,128,1,1359.0,1.0,1,H100,1697101693373,1697101694732.0,120,96.0,4.0,"[7, 1077, 97, 77, 101]","[1697101693380, 1697101694457, 1697101694554, 1697101694631, 1697101694732]"
5051,5051,350,33,[],200,llama-13b,128,1,603.0,1.0,1,H100,1697101694733,1697101695336.0,120,216.0,1.0,"[13, 590]","[1697101694746, 1697101695336]"
5052,5052,795,34,[],200,llama-13b,128,1,846.0,1.0,1,H100,1697101695337,1697101696183.0,120,12.0,1.0,"[37, 809]","[1697101695374, 1697101696183]"
5053,5053,226,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696184,1697101697589.0,120,,,"[31, 1180]","[1697101696215, 1697101697395]"
5054,5054,914,37,[],200,llama-13b,128,1,3718.0,1.0,1,H100,1697101708534,1697101712252.0,120,84.0,20.0,"[72, 768, 117, 88, 83, 82, 79, 881, 103, 97, 93, 89, 89, 85, 498, 99, 91, 84, 83, 78, 59]","[1697101708606, 1697101709374, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712115, 1697101712193, 1697101712252]"
5055,5055,370,29,[],200,llama-13b,128,1,1129.0,1.0,1,H100,1697101682512,1697101683641.0,120,31.0,1.0,"[36, 1093]","[1697101682548, 1697101683641]"
5056,5056,590,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697597,1697101700635.0,120,,,"[311, 1567, 112, 84, 82, 82, 80, 82]","[1697101697908, 1697101699475, 1697101699587, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
5057,5057,18,37,[],200,llama-13b,128,1,1521.0,1.0,1,H100,1697101700638,1697101702159.0,120,15.0,1.0,"[41, 1480]","[1697101700679, 1697101702159]"
5058,5058,349,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703171.0,120,,,"[48, 825]","[1697101702209, 1697101703034]"
5059,5059,730,30,[],200,llama-13b,128,1,3595.0,1.0,1,H100,1697101683642,1697101687237.0,120,364.0,12.0,"[6, 1259, 106, 86, 64, 83, 81, 910, 91, 89, 87, 86, 647]","[1697101683648, 1697101684907, 1697101685013, 1697101685099, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237]"
5060,5060,699,39,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703176,1697101704900.0,120,39.0,1.0,"[217, 1507]","[1697101703393, 1697101704900]"
5061,5061,128,40,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,9.0,1.0,"[72, 805]","[1697101704973, 1697101705778]"
5062,5062,62,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688386,1697101689594.0,120,,,"[25, 1159]","[1697101688411, 1697101689570]"
5063,5063,129,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687240,1697101689594.0,120,,,"[11, 1134, 133, 106, 97, 92, 92, 89]","[1697101687251, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
5064,5064,487,41,[],200,llama-13b,128,1,3965.0,1.0,1,H100,1697101705779,1697101709744.0,120,123.0,17.0,"[48, 1459, 96, 92, 70, 92, 69, 91, 87, 544, 99, 66, 87, 812, 88, 83, 82]","[1697101705827, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708592, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744]"
5065,5065,422,35,[],200,llama-13b,128,1,1497.0,1.0,1,H100,1697101689615,1697101691112.0,120,26.0,1.0,"[195, 1302]","[1697101689810, 1697101691112]"
5066,5066,888,28,[],200,llama-13b,128,1,1517.0,1.0,1,H100,1697101700642,1697101702159.0,120,19.0,1.0,"[342, 1175]","[1697101700984, 1697101702159]"
5067,5067,483,32,[],200,llama-13b,128,1,5129.0,1.0,1,H100,1697101689602,1697101694731.0,120,84.0,20.0,"[8, 1502, 93, 1068, 214, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100]","[1697101689610, 1697101691112, 1697101691205, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
5068,5068,772,36,[],200,llama-13b,128,1,3703.0,1.0,1,H100,1697101691113,1697101694816.0,120,83.0,20.0,"[13, 904, 244, 213, 212, 79, 110, 93, 94, 403, 102, 100, 99, 95, 91, 87, 68, 433, 78, 100, 85]","[1697101691126, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692888, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816]"
5069,5069,313,29,[],200,llama-13b,128,1,874.0,1.0,1,H100,1697101702161,1697101703035.0,120,20.0,1.0,"[48, 825]","[1697101702209, 1697101703034]"
5070,5070,846,33,[],200,llama-13b,128,1,1776.0,1.0,1,H100,1697101694733,1697101696509.0,120,140.0,6.0,"[25, 578, 84, 78, 77, 831, 103]","[1697101694758, 1697101695336, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509]"
5071,5071,644,30,[],200,llama-13b,128,1,652.0,1.0,1,H100,1697101703036,1697101703688.0,120,19.0,1.0,"[42, 610]","[1697101703078, 1697101703688]"
5072,5072,69,31,[],200,llama-13b,128,1,5801.0,1.0,1,H100,1697101703689,1697101709490.0,120,85.0,20.0,"[6, 2083, 135, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 546, 98, 65, 88, 811]","[1697101703695, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708428, 1697101708526, 1697101708591, 1697101708679, 1697101709490]"
5073,5073,848,42,[],200,llama-13b,128,1,683.0,1.0,1,H100,1697101709745,1697101710428.0,120,47.0,1.0,"[12, 671]","[1697101709757, 1697101710428]"
5074,5074,457,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696721,1697101697589.0,120,,,[19],[1697101696740]
5075,5075,689,26,[],200,llama-13b,128,1,1880.0,1.0,1,H100,1697101697595,1697101699475.0,120,15.0,1.0,"[283, 1597]","[1697101697878, 1697101699475]"
5076,5076,117,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700635.0,120,,,"[24, 1051]","[1697101699501, 1697101700552]"
5077,5077,390,25,[],200,llama-13b,128,1,4529.0,1.0,1,H100,1697101667137,1697101671666.0,120,84.0,20.0,"[19, 1107, 133, 101, 95, 71, 91, 90, 724, 99, 94, 93, 92, 89, 677, 98, 71, 92, 91, 89, 513]","[1697101667156, 1697101668263, 1697101668396, 1697101668497, 1697101668592, 1697101668663, 1697101668754, 1697101668844, 1697101669568, 1697101669667, 1697101669761, 1697101669854, 1697101669946, 1697101670035, 1697101670712, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671153, 1697101671666]"
5078,5078,443,28,[],200,llama-13b,128,1,1518.0,1.0,1,H100,1697101700642,1697101702160.0,120,19.0,1.0,"[246, 1272]","[1697101700888, 1697101702160]"
5079,5079,804,29,[],200,llama-13b,128,1,873.0,1.0,1,H100,1697101702162,1697101703035.0,120,20.0,1.0,"[115, 758]","[1697101702277, 1697101703035]"
5080,5080,235,30,[],200,llama-13b,128,1,4345.0,1.0,1,H100,1697101703036,1697101707381.0,120,161.0,12.0,"[60, 1919, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96]","[1697101703096, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381]"
5081,5081,388,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[176, 524, 43, 1250, 83, 82, 81, 81, 82]","[1697101697771, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5082,5082,42,36,[],200,llama-13b,128,1,758.0,1.0,1,H100,1697101684149,1697101684907.0,120,10.0,1.0,"[12, 746]","[1697101684161, 1697101684907]"
5083,5083,124,46,[],200,llama-13b,128,1,958.0,1.0,1,H100,1697101708533,1697101709491.0,120,83.0,2.0,"[78, 880]","[1697101708611, 1697101709491]"
5084,5084,747,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672658,1697101675811.0,120,,,"[6, 1095, 129, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672664, 1697101673759, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
5085,5085,450,47,[],200,llama-13b,128,1,3052.0,1.0,1,H100,1697101709492,1697101712544.0,120,91.0,20.0,"[36, 899, 278, 103, 97, 92, 89, 89, 85, 498, 98, 91, 85, 82, 79, 59, 69, 51, 60, 57, 55]","[1697101709528, 1697101710427, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
5086,5086,393,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[24, 676, 43, 1250, 83, 82, 81, 81, 82]","[1697101697619, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5087,5087,719,26,[],200,llama-13b,128,1,1330.0,1.0,1,H100,1697101671667,1697101672997.0,120,182.0,6.0,"[12, 978, 113, 91, 68, 68]","[1697101671679, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997]"
5088,5088,788,32,[],200,llama-13b,128,1,701.0,1.0,1,H100,1697101697594,1697101698295.0,120,31.0,1.0,"[44, 657]","[1697101697638, 1697101698295]"
5089,5089,305,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101698296,1697101700636.0,120,,,"[12, 1168, 112, 83, 82, 82, 80, 82]","[1697101698308, 1697101699476, 1697101699588, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
5090,5090,180,28,[],200,llama-13b,128,1,3690.0,1.0,1,H100,1697101675814,1697101679504.0,120,123.0,12.0,"[409, 1156, 98, 64, 51, 763, 90, 85, 65, 82, 82, 646, 99]","[1697101676223, 1697101677379, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678677, 1697101678759, 1697101679405, 1697101679504]"
5091,5091,147,27,[],200,llama-13b,128,1,762.0,1.0,1,H100,1697101672997,1697101673759.0,120,182.0,1.0,"[7, 755]","[1697101673004, 1697101673759]"
5092,5092,506,28,[],200,llama-13b,128,1,1088.0,1.0,1,H100,1697101673760,1697101674848.0,120,16.0,1.0,"[42, 1046]","[1697101673802, 1697101674848]"
5093,5093,539,29,[],200,llama-13b,128,1,4546.0,1.0,1,H100,1697101679505,1697101684051.0,120,83.0,20.0,"[6, 969, 91, 104, 57, 72, 804, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 101, 101]","[1697101679511, 1697101680480, 1697101680571, 1697101680675, 1697101680732, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051]"
5094,5094,863,29,[],200,llama-13b,128,1,940.0,1.0,1,H100,1697101674849,1697101675789.0,120,10.0,1.0,"[48, 892]","[1697101674897, 1697101675789]"
5095,5095,292,30,[],200,llama-13b,128,1,181.0,1.0,1,H100,1697101675790,1697101675971.0,120,286.0,1.0,"[7, 174]","[1697101675797, 1697101675971]"
5096,5096,842,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[233, 1285, 122, 69]","[1697101700875, 1697101702160, 1697101702282, 1697101702351]"
5097,5097,272,40,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,86.0,20.0,"[204, 1520, 108, 105, 72, 728, 95, 87, 85, 83, 79, 943, 97, 92, 70, 91, 70, 91, 87, 544, 99]","[1697101703380, 1697101704900, 1697101705008, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526]"
5098,5098,622,31,[],200,llama-13b,128,1,2275.0,1.0,1,H100,1697101675972,1697101678247.0,120,20.0,1.0,"[449, 1826]","[1697101676421, 1697101678247]"
5099,5099,51,32,[],200,llama-13b,128,1,6915.0,1.0,1,H100,1697101678248,1697101685163.0,120,364.0,36.0,"[12, 1028, 117, 99, 97, 95, 72, 92, 90, 70, 90, 461, 103, 57, 73, 804, 84, 83, 79, 79, 771, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97, 92, 90, 90, 593, 86, 64]","[1697101678260, 1697101679288, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163]"
5100,5100,892,30,[],200,llama-13b,128,1,4572.0,1.0,1,H100,1697101684052,1697101688624.0,120,87.0,20.0,"[18, 837, 107, 86, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106]","[1697101684070, 1697101684907, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
5101,5101,294,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688625,1697101689595.0,120,,,[7],[1697101688632]
5102,5102,648,32,[],200,llama-13b,128,1,5118.0,1.0,1,H100,1697101689613,1697101694731.0,120,84.0,20.0,"[99, 1400, 93, 1069, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 92, 87, 68, 433, 78, 100]","[1697101689712, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
5103,5103,658,34,[],200,llama-13b,128,1,842.0,1.0,1,H100,1697101708532,1697101709374.0,120,11.0,1.0,"[8, 834]","[1697101708540, 1697101709374]"
5104,5104,633,41,[],200,llama-13b,128,1,3719.0,1.0,1,H100,1697101708533,1697101712252.0,120,90.0,20.0,"[49, 792, 116, 89, 83, 82, 79, 881, 103, 98, 92, 89, 89, 85, 498, 99, 91, 84, 82, 79, 59]","[1697101708582, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712114, 1697101712193, 1697101712252]"
5105,5105,549,33,[],200,llama-13b,128,1,5603.0,1.0,1,H100,1697101706344,1697101711947.0,120,93.0,20.0,"[6, 2078, 99, 64, 88, 812, 88, 83, 82, 79, 881, 104, 96, 93, 89, 88, 86, 498, 98, 91]","[1697101706350, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710808, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947]"
5106,5106,63,35,[],200,llama-13b,128,1,1051.0,1.0,1,H100,1697101709376,1697101710427.0,120,39.0,1.0,"[36, 1015]","[1697101709412, 1697101710427]"
5107,5107,420,36,[],200,llama-13b,128,1,2383.0,1.0,1,H100,1697101710428,1697101712811.0,120,52.0,20.0,"[7, 1225, 98, 99, 90, 85, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32]","[1697101710435, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811]"
5108,5108,585,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[35, 1486, 123, 69]","[1697101700673, 1697101702159, 1697101702282, 1697101702351]"
5109,5109,80,33,[],200,llama-13b,128,1,602.0,1.0,1,H100,1697101694734,1697101695336.0,120,13.0,1.0,"[71, 531]","[1697101694805, 1697101695336]"
5110,5110,439,34,[],200,llama-13b,128,1,1273.0,1.0,1,H100,1697101695337,1697101696610.0,120,13.0,4.0,"[7, 839, 223, 103, 101]","[1697101695344, 1697101696183, 1697101696406, 1697101696509, 1697101696610]"
5111,5111,640,36,[],200,llama-13b,128,1,1521.0,1.0,1,H100,1697101700638,1697101702159.0,120,15.0,1.0,"[173, 1348]","[1697101700811, 1697101702159]"
5112,5112,28,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693480,1697101697589.0,120,,,"[12, 965, 97, 77, 101, 84, 604, 78, 77, 832, 102, 101, 111, 92, 90]","[1697101693492, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696813, 1697101696903]"
5113,5113,800,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696611,1697101697589.0,120,,,[6],[1697101696617]
5114,5114,200,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[95, 605, 43, 1250, 83, 82, 81, 81, 82]","[1697101697690, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5115,5115,39,37,[],200,llama-13b,128,1,874.0,1.0,1,H100,1697101702161,1697101703035.0,120,8.0,1.0,"[60, 814]","[1697101702221, 1697101703035]"
5116,5116,219,33,[],200,llama-13b,128,1,3939.0,1.0,1,H100,1697101680481,1697101684420.0,120,90.0,20.0,"[6, 1015, 106, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680487, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
5117,5117,398,38,[],200,llama-13b,128,1,5490.0,1.0,1,H100,1697101703036,1697101708526.0,120,87.0,20.0,"[12, 640, 1320, 105, 71, 729, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 98]","[1697101703048, 1697101703688, 1697101705008, 1697101705113, 1697101705184, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708525]"
5118,5118,556,37,[],200,llama-13b,128,1,1518.0,1.0,1,H100,1697101700642,1697101702160.0,120,9.0,1.0,"[257, 1261]","[1697101700899, 1697101702160]"
5119,5119,909,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702162,1697101703171.0,120,,,"[53, 819]","[1697101702215, 1697101703034]"
5120,5120,339,39,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,87.0,20.0,"[355, 1368, 116, 98, 72, 728, 95, 87, 85, 83, 79, 944, 96, 92, 70, 92, 69, 91, 87, 545, 98]","[1697101703531, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
5121,5121,318,39,[],200,llama-13b,128,1,3163.0,1.0,1,H100,1697101689614,1697101692777.0,120,6.0,6.0,"[169, 1329, 93, 1069, 213, 212, 78]","[1697101689783, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777]"
5122,5122,700,40,[],200,llama-13b,128,1,4310.0,1.0,1,H100,1697101708533,1697101712843.0,120,140.0,33.0,"[13, 944, 89, 83, 82, 79, 882, 103, 97, 92, 89, 89, 85, 498, 99, 91, 84, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32]","[1697101708546, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843]"
5123,5123,749,39,[],200,llama-13b,128,1,3719.0,1.0,1,H100,1697101708533,1697101712252.0,120,47.0,20.0,"[8, 833, 116, 89, 83, 82, 79, 882, 103, 97, 92, 89, 88, 86, 498, 99, 90, 85, 82, 79, 59]","[1697101708541, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252]"
5124,5124,674,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101692779,1697101697588.0,120,,,"[24, 569, 107, 101, 100, 99, 95, 91, 88, 67, 434, 77, 100, 85, 604, 79, 76, 831, 103, 101, 110, 92, 91]","[1697101692803, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5125,5125,435,41,[],200,llama-13b,128,1,4100.0,1.0,1,H100,1697101708533,1697101712633.0,120,563.0,27.0,"[49, 792, 116, 89, 83, 82, 79, 881, 103, 98, 92, 89, 89, 85, 498, 99, 91, 84, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44]","[1697101708582, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633]"
5126,5126,274,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696510,1697101697589.0,120,,,"[6, 880]","[1697101696516, 1697101697396]"
5127,5127,633,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697603,1697101700636.0,120,,,"[587, 1286, 112, 83, 82, 82, 80, 82]","[1697101698190, 1697101699476, 1697101699588, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
5128,5128,578,34,[],200,llama-13b,128,1,1690.0,1.0,1,H100,1697101684426,1697101686116.0,120,31.0,1.0,"[60, 1630]","[1697101684486, 1697101686116]"
5129,5129,3,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101686117,1697101689594.0,120,,,"[6, 900, 214, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 91, 90]","[1697101686123, 1697101687023, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688994]"
5130,5130,788,31,[],200,llama-13b,128,1,946.0,1.0,1,H100,1697101668498,1697101669444.0,120,31.0,1.0,"[6, 940]","[1697101668504, 1697101669444]"
5131,5131,213,32,[],200,llama-13b,128,1,1619.0,1.0,1,H100,1697101669445,1697101671064.0,120,123.0,6.0,"[12, 1028, 226, 99, 71, 92, 91]","[1697101669457, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064]"
5132,5132,33,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700643,1697101703170.0,120,,,"[439, 1078, 122, 69]","[1697101701082, 1697101702160, 1697101702282, 1697101702351]"
5133,5133,390,37,[],200,llama-13b,128,1,5353.0,1.0,1,H100,1697101703173,1697101708526.0,120,84.0,20.0,"[322, 1404, 116, 98, 72, 728, 95, 87, 85, 83, 79, 944, 96, 92, 70, 91, 70, 91, 87, 545, 98]","[1697101703495, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
5134,5134,361,36,[],200,llama-13b,128,1,3284.0,1.0,1,H100,1697101689603,1697101692887.0,120,67.0,7.0,"[68, 1441, 93, 1069, 213, 212, 78, 101]","[1697101689671, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878]"
5135,5135,665,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693966,1697101697589.0,120,,,"[6, 485, 97, 77, 101, 84, 604, 79, 76, 832, 102, 101, 110, 93, 90]","[1697101693972, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696407, 1697101696509, 1697101696610, 1697101696720, 1697101696813, 1697101696903]"
5136,5136,820,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697603,1697101700637.0,120,,,"[575, 1297, 113, 83, 82, 81, 81, 82]","[1697101698178, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5137,5137,403,37,[],200,llama-13b,128,1,1329.0,1.0,1,H100,1697101684908,1697101686237.0,120,874.0,2.0,"[6, 1202, 121]","[1697101684914, 1697101686116, 1697101686237]"
5138,5138,759,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101686238,1697101689594.0,120,,,"[13, 771, 215, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 91, 90]","[1697101686251, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688994]"
5139,5139,742,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[276, 1241, 123, 69]","[1697101700918, 1697101702159, 1697101702282, 1697101702351]"
5140,5140,922,30,[],200,llama-13b,128,1,5043.0,1.0,1,H100,1697101668845,1697101673888.0,120,91.0,20.0,"[19, 1621, 226, 99, 70, 93, 90, 91, 511, 98, 95, 94, 93, 90, 522, 113, 91, 68, 67, 88, 804]","[1697101668864, 1697101670485, 1697101670711, 1697101670810, 1697101670880, 1697101670973, 1697101671063, 1697101671154, 1697101671665, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672996, 1697101673084, 1697101673888]"
5141,5141,695,37,[],200,llama-13b,128,1,3832.0,1.0,1,H100,1697101692889,1697101696721.0,120,92.0,20.0,"[10, 473, 107, 101, 100, 100, 94, 91, 88, 67, 434, 77, 100, 85, 604, 79, 76, 831, 103, 101, 110]","[1697101692899, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693780, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720]"
5142,5142,922,32,[],200,llama-13b,128,1,5132.0,1.0,1,H100,1697101689599,1697101694731.0,120,91.0,20.0,"[18, 1495, 93, 1068, 214, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100]","[1697101689617, 1697101691112, 1697101691205, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
5143,5143,372,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700640,1697101703170.0,120,,,"[34, 1485, 123, 69]","[1697101700674, 1697101702159, 1697101702282, 1697101702351]"
5144,5144,326,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694733,1697101697587.0,120,,,"[54, 549, 85, 77, 77, 831, 103, 101, 110, 92, 91]","[1697101694787, 1697101695336, 1697101695421, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5145,5145,437,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673891,1697101675812.0,120,,,"[16, 941, 133, 102, 99, 94, 71, 94]","[1697101673907, 1697101674848, 1697101674981, 1697101675083, 1697101675182, 1697101675276, 1697101675347, 1697101675441]"
5146,5146,874,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693373,1697101697589.0,120,,,"[24, 1060, 97, 77, 101, 84, 604, 78, 77, 831, 103, 101, 111, 91, 91]","[1697101693397, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
5147,5147,364,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101680807,1697101689594.0,120,,,"[73, 1631, 193, 96, 93, 92, 69, 90, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101680880, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
5148,5148,428,32,[],200,llama-13b,128,1,2266.0,1.0,1,H100,1697101709492,1697101711758.0,120,31.0,9.0,"[12, 923, 278, 103, 97, 92, 89, 88, 86, 498]","[1697101709504, 1697101710427, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758]"
5149,5149,131,23,[],200,llama-13b,128,1,1101.0,1.0,1,H100,1697101672658,1697101673759.0,120,8.0,1.0,"[36, 1065]","[1697101672694, 1697101673759]"
5150,5150,491,24,[],200,llama-13b,128,1,1088.0,1.0,1,H100,1697101673760,1697101674848.0,120,14.0,1.0,"[30, 1058]","[1697101673790, 1697101674848]"
5151,5151,848,25,[],200,llama-13b,128,1,940.0,1.0,1,H100,1697101674849,1697101675789.0,120,47.0,1.0,"[24, 916]","[1697101674873, 1697101675789]"
5152,5152,277,26,[],200,llama-13b,128,1,589.0,1.0,1,H100,1697101675791,1697101676380.0,120,18.0,1.0,"[41, 548]","[1697101675832, 1697101676380]"
5153,5153,413,43,[],200,llama-13b,128,1,5472.0,1.0,1,H100,1697101707706,1697101713178.0,120,244.0,50.0,"[6, 601, 115, 99, 64, 88, 811, 88, 84, 82, 79, 882, 103, 97, 92, 89, 88, 86, 498, 98, 91, 85, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 41, 33, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26, 24, 20, 20]","[1697101707712, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709490, 1697101709578, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712674, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114, 1697101713138, 1697101713158, 1697101713178]"
5154,5154,724,27,[],200,llama-13b,128,1,1866.0,1.0,1,H100,1697101676381,1697101678247.0,120,11.0,1.0,"[110, 1756]","[1697101676491, 1697101678247]"
5155,5155,152,28,[],200,llama-13b,128,1,4456.0,1.0,1,H100,1697101678248,1697101682704.0,120,87.0,20.0,"[6, 1034, 117, 98, 98, 95, 72, 92, 90, 70, 90, 461, 103, 57, 73, 804, 84, 83, 79, 79, 771]","[1697101678254, 1697101679288, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680674, 1697101680731, 1697101680804, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704]"
5156,5156,316,27,[],200,llama-13b,128,1,5052.0,1.0,1,H100,1697101667605,1697101672657.0,120,86.0,20.0,"[48, 1791, 125, 98, 94, 93, 93, 88, 676, 98, 71, 93, 90, 90, 513, 97, 95, 94, 93, 90, 522]","[1697101667653, 1697101669444, 1697101669569, 1697101669667, 1697101669761, 1697101669854, 1697101669947, 1697101670035, 1697101670711, 1697101670809, 1697101670880, 1697101670973, 1697101671063, 1697101671153, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672657]"
5157,5157,750,38,[],200,llama-13b,128,1,3719.0,1.0,1,H100,1697101708533,1697101712252.0,120,88.0,20.0,"[19, 822, 116, 89, 83, 82, 79, 882, 103, 97, 92, 89, 89, 85, 498, 99, 90, 85, 82, 79, 59]","[1697101708552, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252]"
5158,5158,670,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101672658,1697101675811.0,120,,,"[43, 1058, 129, 98, 89, 88, 66, 66, 686, 102, 98, 95, 70, 95]","[1697101672701, 1697101673759, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674295, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
5159,5159,864,46,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,83.0,20.0,"[30, 1693, 116, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 91, 70, 91, 86, 545, 99]","[1697101703206, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
5160,5160,921,40,[],200,llama-13b,128,1,1725.0,1.0,1,H100,1697101703174,1697101704899.0,120,31.0,1.0,"[114, 1611]","[1697101703288, 1697101704899]"
5161,5161,353,41,[],200,llama-13b,128,1,1194.0,1.0,1,H100,1697101704901,1697101706095.0,120,52.0,4.0,"[30, 847, 135, 95, 87]","[1697101704931, 1697101705778, 1697101705913, 1697101706008, 1697101706095]"
5162,5162,458,31,[],200,llama-13b,128,1,700.0,1.0,1,H100,1697101697595,1697101698295.0,120,11.0,1.0,"[106, 594]","[1697101697701, 1697101698295]"
5163,5163,817,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101698296,1697101700637.0,120,,,"[30, 1150, 112, 84, 81, 82, 80, 82]","[1697101698326, 1697101699476, 1697101699588, 1697101699672, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
5164,5164,253,43,[],200,llama-13b,128,1,2382.0,1.0,1,H100,1697101710429,1697101712811.0,120,67.0,20.0,"[24, 1207, 98, 99, 90, 85, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 41, 31, 32]","[1697101710453, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712748, 1697101712779, 1697101712811]"
5165,5165,270,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697603,1697101700636.0,120,,,"[525, 1347, 113, 83, 82, 81, 81, 82]","[1697101698128, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5166,5166,410,33,[],200,llama-13b,128,1,2551.0,1.0,1,H100,1697101685164,1697101687715.0,120,364.0,12.0,"[6, 946, 121, 91, 89, 87, 87, 646, 102, 99, 94, 92, 91]","[1697101685170, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715]"
5167,5167,67,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689618,1697101697587.0,120,,,"[390, 2022, 244, 213, 212, 78, 101, 102, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101, 83, 605, 79, 76, 831, 103, 101, 110, 92, 91]","[1697101690008, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732, 1697101694815, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5168,5168,768,34,[],200,llama-13b,128,1,1189.0,1.0,1,H100,1697101687716,1697101688905.0,120,47.0,6.0,"[6, 663, 134, 105, 97, 92, 92]","[1697101687722, 1697101688385, 1697101688519, 1697101688624, 1697101688721, 1697101688813, 1697101688905]"
5169,5169,201,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697588.0,120,,,"[69, 1296, 223, 103, 101, 110, 92, 91]","[1697101694887, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5170,5170,531,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697590,1697101700635.0,120,,,"[402, 1483, 113, 83, 82, 81, 81, 82]","[1697101697992, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5171,5171,172,35,[],200,llama-13b,128,1,1723.0,1.0,1,H100,1697101703176,1697101704899.0,120,19.0,1.0,"[339, 1384]","[1697101703515, 1697101704899]"
5172,5172,526,36,[],200,llama-13b,128,1,4590.0,1.0,1,H100,1697101704901,1697101709491.0,120,89.0,20.0,"[48, 829, 135, 95, 87, 85, 83, 80, 942, 96, 93, 70, 91, 70, 91, 87, 544, 99, 65, 88, 811]","[1697101704949, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490]"
5173,5173,858,37,[],200,llama-13b,128,1,2540.0,1.0,1,H100,1697101709492,1697101712032.0,120,182.0,12.0,"[42, 1171, 103, 97, 92, 89, 89, 85, 498, 98, 91, 85]","[1697101709534, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712032]"
5174,5174,197,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688906,1697101689611.0,120,,,"[12, 653]","[1697101688918, 1697101689571]"
5175,5175,895,39,[],200,llama-13b,128,1,1521.0,1.0,1,H100,1697101700638,1697101702159.0,120,15.0,1.0,"[150, 1371]","[1697101700788, 1697101702159]"
5176,5176,527,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689616,1697101697587.0,120,,,"[307, 1189, 93, 1069, 214, 211, 79, 110, 92, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101, 83, 605, 79, 76, 831, 103, 101, 110, 92, 91]","[1697101689923, 1697101691112, 1697101691205, 1697101692274, 1697101692488, 1697101692699, 1697101692778, 1697101692888, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732, 1697101694815, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5177,5177,325,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703172.0,120,,,"[36, 837]","[1697101702197, 1697101703034]"
5178,5178,593,31,[],200,llama-13b,128,1,2361.0,1.0,1,H100,1697101707383,1697101709744.0,120,335.0,9.0,"[6, 924, 115, 99, 65, 87, 811, 89, 83, 82]","[1697101707389, 1697101708313, 1697101708428, 1697101708527, 1697101708592, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744]"
5179,5179,683,41,[],200,llama-13b,128,1,1840.0,1.0,1,H100,1697101703176,1697101705016.0,120,874.0,2.0,"[331, 1509]","[1697101703507, 1697101705016]"
5180,5180,11,36,[],200,llama-13b,128,1,4621.0,1.0,1,H100,1697101703175,1697101707796.0,120,732.0,17.0,"[101, 1623, 117, 97, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 91, 70, 91]","[1697101703276, 1697101704899, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796]"
5181,5181,7,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694733,1697101697587.0,120,,,"[42, 561, 85, 78, 76, 831, 103, 101, 110, 92, 91]","[1697101694775, 1697101695336, 1697101695421, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5182,5182,904,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101701096,1697101703172.0,120,,,"[87, 1852]","[1697101701183, 1697101703035]"
5183,5183,335,35,[],200,llama-13b,128,1,9885.0,1.0,1,H100,1697101703176,1697101713061.0,120,58.0,62.0,"[498, 1226, 116, 97, 72, 728, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 546, 98, 65, 88, 811, 89, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 497, 99, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45, 44, 41, 33, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23]","[1697101703674, 1697101704900, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708428, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712674, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061]"
5184,5184,747,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101685329,1697101689593.0,120,,,"[48, 1645, 215, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101685377, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
5185,5185,365,37,[],200,llama-13b,128,1,517.0,1.0,1,H100,1697101707797,1697101708314.0,120,23.0,1.0,"[7, 510]","[1697101707804, 1697101708314]"
5186,5186,694,38,[],200,llama-13b,128,1,2945.0,1.0,1,H100,1697101708315,1697101711260.0,120,161.0,13.0,"[29, 1146, 89, 83, 82, 79, 882, 103, 97, 92, 89, 88, 86]","[1697101708344, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260]"
5187,5187,365,37,[],200,llama-13b,128,1,701.0,1.0,1,H100,1697101697594,1697101698295.0,120,23.0,1.0,"[37, 664]","[1697101697631, 1697101698295]"
5188,5188,697,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101698296,1697101700636.0,120,,,"[24, 1156, 112, 83, 82, 82, 80, 82]","[1697101698320, 1697101699476, 1697101699588, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
5189,5189,28,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[37, 663, 43, 1250, 83, 82, 81, 81, 82]","[1697101697632, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5190,5190,128,39,[],200,llama-13b,128,1,1518.0,1.0,1,H100,1697101700642,1697101702160.0,120,9.0,1.0,"[269, 1249]","[1697101700911, 1697101702160]"
5191,5191,489,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702162,1697101703171.0,120,,,"[109, 764]","[1697101702271, 1697101703035]"
5192,5192,573,33,[],200,llama-13b,128,1,602.0,1.0,1,H100,1697101671064,1697101671666.0,120,874.0,2.0,"[7, 595]","[1697101671071, 1697101671666]"
5193,5193,5,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101671667,1697101675812.0,120,,,"[6, 873, 111, 113, 91, 68, 68, 88, 803, 98, 89, 88, 66, 65, 687, 102, 99, 94, 71, 94]","[1697101671673, 1697101672546, 1697101672657, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888, 1697101673986, 1697101674075, 1697101674163, 1697101674229, 1697101674294, 1697101674981, 1697101675083, 1697101675182, 1697101675276, 1697101675347, 1697101675441]"
5194,5194,473,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703171.0,120,,,"[180, 1341, 123, 69]","[1697101700818, 1697101702159, 1697101702282, 1697101702351]"
5195,5195,77,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697596,1697101700635.0,120,,,"[313, 1566, 112, 84, 82, 81, 81, 82]","[1697101697909, 1697101699475, 1697101699587, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5196,5196,337,35,[],200,llama-13b,128,1,1563.0,1.0,1,H100,1697101675816,1697101677379.0,120,12.0,1.0,"[401, 1162]","[1697101676217, 1697101677379]"
5197,5197,832,36,[],200,llama-13b,128,1,1723.0,1.0,1,H100,1697101703176,1697101704899.0,120,15.0,1.0,"[118, 1605]","[1697101703294, 1697101704899]"
5198,5198,773,27,[],200,llama-13b,128,1,4766.0,1.0,1,H100,1697101675814,1697101680580.0,120,90.0,20.0,"[459, 1107, 97, 64, 51, 763, 90, 85, 65, 82, 82, 646, 99, 97, 95, 72, 92, 90, 70, 91, 468]","[1697101676273, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678677, 1697101678759, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680579]"
5199,5199,257,37,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,14.0,1.0,"[115, 762]","[1697101705016, 1697101705778]"
5200,5200,620,38,[],200,llama-13b,128,1,2017.0,1.0,1,H100,1697101705779,1697101707796.0,120,100.0,8.0,"[24, 1241, 241, 97, 92, 70, 91, 70, 91]","[1697101705803, 1697101707044, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796]"
5201,5201,696,36,[],200,llama-13b,128,1,3423.0,1.0,1,H100,1697101677381,1697101680804.0,120,83.0,20.0,"[13, 853, 107, 90, 86, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 469, 95, 57, 73]","[1697101677394, 1697101678247, 1697101678354, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680579, 1697101680674, 1697101680731, 1697101680804]"
5202,5202,434,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[354, 1163, 123, 69]","[1697101700996, 1697101702159, 1697101702282, 1697101702351]"
5203,5203,49,39,[],200,llama-13b,128,1,730.0,1.0,1,H100,1697101707797,1697101708527.0,120,109.0,3.0,"[7, 509, 115, 99]","[1697101707804, 1697101708313, 1697101708428, 1697101708527]"
5204,5204,380,40,[],200,llama-13b,128,1,4812.0,1.0,1,H100,1697101708534,1697101713346.0,120,216.0,50.0,"[171, 1999, 103, 97, 93, 89, 88, 86, 497, 100, 91, 84, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26, 24, 20, 20, 21, 19, 18, 19, 20, 18, 19, 17, 17]","[1697101708705, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711857, 1697101711948, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114, 1697101713138, 1697101713158, 1697101713178, 1697101713199, 1697101713218, 1697101713236, 1697101713255, 1697101713275, 1697101713293, 1697101713312, 1697101713329, 1697101713346]"
5205,5205,202,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101680583,1697101689606.0,120,,,"[103, 816, 106, 84, 83, 79, 79, 771, 96, 93, 91, 69, 91, 90, 614, 102, 101, 97, 92, 90, 90, 593, 87, 63, 83, 80, 911, 91, 89, 87, 86, 647, 101, 100, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101680686, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
5206,5206,120,37,[],200,llama-13b,128,1,1705.0,1.0,1,H100,1697101680806,1697101682511.0,120,17.0,1.0,"[62, 1643]","[1697101680868, 1697101682511]"
5207,5207,475,38,[],200,llama-13b,128,1,4827.0,1.0,1,H100,1697101682512,1697101687339.0,120,89.0,20.0,"[24, 1105, 208, 102, 100, 97, 92, 90, 90, 593, 86, 64, 82, 81, 911, 91, 89, 87, 86, 647, 102]","[1697101682536, 1697101683641, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339]"
5208,5208,185,33,[],200,llama-13b,128,1,5117.0,1.0,1,H100,1697101689615,1697101694732.0,120,93.0,20.0,"[261, 1329, 1069, 213, 212, 79, 110, 92, 94, 405, 101, 100, 99, 95, 91, 87, 68, 433, 78, 101]","[1697101689876, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692888, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694732]"
5209,5209,220,41,[],200,llama-13b,128,1,1640.0,1.0,1,H100,1697101700642,1697101702282.0,120,67.0,2.0,"[336, 1181, 123]","[1697101700978, 1697101702159, 1697101702282]"
5210,5210,574,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702283,1697101703172.0,120,,,"[12, 740]","[1697101702295, 1697101703035]"
5211,5211,4,43,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,89.0,20.0,"[198, 1526, 115, 98, 72, 728, 95, 87, 85, 83, 79, 943, 97, 92, 70, 91, 70, 91, 87, 544, 99]","[1697101703374, 1697101704900, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526]"
5212,5212,358,44,[],200,llama-13b,128,1,1046.0,1.0,1,H100,1697101708533,1697101709579.0,120,216.0,3.0,"[97, 744, 117, 88]","[1697101708630, 1697101709374, 1697101709491, 1697101709579]"
5213,5213,717,45,[],200,llama-13b,128,1,2964.0,1.0,1,H100,1697101709580,1697101712544.0,120,89.0,20.0,"[12, 836, 277, 103, 97, 92, 89, 89, 85, 498, 98, 91, 85, 82, 79, 59, 69, 51, 60, 57, 55]","[1697101709592, 1697101710428, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
5214,5214,496,34,[],200,llama-13b,128,1,3964.0,1.0,1,H100,1697101689616,1697101693580.0,120,335.0,11.0,"[313, 1183, 93, 1068, 214, 212, 78, 111, 92, 94, 405, 101]","[1697101689929, 1697101691112, 1697101691205, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692888, 1697101692980, 1697101693074, 1697101693479, 1697101693580]"
5215,5215,421,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[118, 582, 43, 1250, 83, 82, 81, 81, 82]","[1697101697713, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5216,5216,732,47,[],200,llama-13b,128,1,4207.0,1.0,1,H100,1697101703175,1697101707382.0,120,345.0,12.0,"[304, 1420, 115, 99, 72, 728, 95, 87, 85, 83, 79, 943, 97]","[1697101703479, 1697101704899, 1697101705014, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707382]"
5217,5217,515,34,[],200,llama-13b,128,1,602.0,1.0,1,H100,1697101694734,1697101695336.0,120,11.0,1.0,"[65, 537]","[1697101694799, 1697101695336]"
5218,5218,861,39,[],200,llama-13b,128,1,851.0,1.0,1,H100,1697101710809,1697101711660.0,120,10.0,1.0,"[24, 827]","[1697101710833, 1697101711660]"
5219,5219,57,41,[],200,llama-13b,128,1,1876.0,1.0,1,H100,1697101697599,1697101699475.0,120,13.0,1.0,"[477, 1399]","[1697101698076, 1697101699475]"
5220,5220,415,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700635.0,120,,,[42],[1697101699519]
5221,5221,764,43,[],200,llama-13b,128,1,1517.0,1.0,1,H100,1697101700642,1697101702159.0,120,39.0,1.0,"[287, 1230]","[1697101700929, 1697101702159]"
5222,5222,194,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703172.0,120,,,"[30, 843]","[1697101702191, 1697101703034]"
5223,5223,552,45,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,87.0,20.0,"[309, 1414, 116, 98, 72, 728, 95, 87, 85, 83, 79, 943, 97, 92, 70, 91, 70, 91, 87, 545, 98]","[1697101703485, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
5224,5224,387,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700635.0,120,,,"[224, 1656, 113, 83, 82, 82, 80, 82]","[1697101697819, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
5225,5225,393,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684426,1697101689605.0,120,,,"[48, 1642, 121, 91, 89, 87, 87, 646, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 92, 89]","[1697101684474, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
5226,5226,745,37,[],200,llama-13b,128,1,1517.0,1.0,1,H100,1697101700642,1697101702159.0,120,17.0,1.0,"[361, 1156]","[1697101701003, 1697101702159]"
5227,5227,174,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703171.0,120,,,"[42, 831]","[1697101702203, 1697101703034]"
5228,5228,285,45,[],200,llama-13b,128,1,4100.0,1.0,1,H100,1697101708533,1697101712633.0,120,100.0,27.0,"[90, 751, 117, 88, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 99, 91, 84, 83, 78, 59, 69, 51, 60, 57, 55, 45, 44]","[1697101708623, 1697101709374, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633]"
5229,5229,108,42,[],200,llama-13b,128,1,895.0,1.0,1,H100,1697101705019,1697101705914.0,120,182.0,2.0,"[16, 743, 135]","[1697101705035, 1697101705778, 1697101705913]"
5230,5230,509,29,[],200,llama-13b,128,1,1246.0,1.0,1,H100,1697101682705,1697101683951.0,120,286.0,3.0,"[6, 930, 208, 102]","[1697101682711, 1697101683641, 1697101683849, 1697101683951]"
5231,5231,435,43,[],200,llama-13b,128,1,5942.0,1.0,1,H100,1697101705914,1697101711856.0,120,563.0,27.0,"[7, 1123, 242, 96, 92, 70, 92, 69, 91, 87, 544, 99, 66, 87, 812, 88, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 98]","[1697101705921, 1697101707044, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708592, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856]"
5232,5232,794,43,[],200,llama-13b,128,1,1726.0,1.0,1,H100,1697101703173,1697101704899.0,120,11.0,1.0,"[324, 1402]","[1697101703497, 1697101704899]"
5233,5233,868,30,[],200,llama-13b,128,1,4672.0,1.0,1,H100,1697101683952,1697101688624.0,120,85.0,20.0,"[6, 949, 106, 87, 63, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 95, 91, 91, 88, 715, 106]","[1697101683958, 1697101684907, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624]"
5234,5234,222,44,[],200,llama-13b,128,1,4590.0,1.0,1,H100,1697101704901,1697101709491.0,120,96.0,20.0,"[80, 797, 135, 95, 87, 85, 83, 80, 942, 96, 93, 70, 91, 70, 91, 87, 544, 99, 65, 88, 811]","[1697101704981, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490]"
5235,5235,834,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101687340,1697101689594.0,120,,,"[6, 1039, 133, 106, 97, 92, 92, 89]","[1697101687346, 1697101688385, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
5236,5236,556,29,[],200,llama-13b,128,1,2411.0,1.0,1,H100,1697101689619,1697101692030.0,120,9.0,1.0,"[490, 1921]","[1697101690109, 1697101692030]"
5237,5237,425,37,[],200,llama-13b,128,1,4758.0,1.0,1,H100,1697101675813,1697101680571.0,120,88.0,20.0,"[255, 1312, 97, 64, 51, 763, 89, 86, 65, 83, 80, 647, 98, 98, 95, 72, 92, 90, 70, 90, 461]","[1697101676068, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678444, 1697101678530, 1697101678595, 1697101678678, 1697101678758, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571]"
5238,5238,918,30,[],200,llama-13b,128,1,1341.0,1.0,1,H100,1697101692031,1697101693372.0,120,23.0,1.0,"[30, 1311]","[1697101692061, 1697101693372]"
5239,5239,318,31,[],200,llama-13b,128,1,2047.0,1.0,1,H100,1697101693373,1697101695420.0,120,6.0,6.0,"[18, 1066, 97, 77, 101, 84, 604]","[1697101693391, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420]"
5240,5240,783,38,[],200,llama-13b,128,1,921.0,1.0,1,H100,1697101680581,1697101681502.0,120,286.0,1.0,"[87, 834]","[1697101680668, 1697101681502]"
5241,5241,75,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673086,1697101675812.0,120,,,"[12, 1750, 133, 102, 98, 95, 70, 95]","[1697101673098, 1697101674848, 1697101674981, 1697101675083, 1697101675181, 1697101675276, 1697101675346, 1697101675441]"
5242,5242,646,31,[],200,llama-13b,128,1,481.0,1.0,1,H100,1697101692891,1697101693372.0,120,14.0,1.0,"[14, 467]","[1697101692905, 1697101693372]"
5243,5243,77,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101693373,1697101697588.0,120,,,"[13, 1071, 97, 77, 101, 84, 604, 78, 77, 831, 103, 101, 111, 91, 91]","[1697101693386, 1697101694457, 1697101694554, 1697101694631, 1697101694732, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
5244,5244,843,41,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703176,1697101704900.0,120,14.0,1.0,"[503, 1221]","[1697101703679, 1697101704900]"
5245,5245,432,29,[],200,llama-13b,128,1,1565.0,1.0,1,H100,1697101675815,1697101677380.0,120,13.0,1.0,"[390, 1175]","[1697101676205, 1697101677380]"
5246,5246,786,30,[],200,llama-13b,128,1,3423.0,1.0,1,H100,1697101677382,1697101680805.0,120,87.0,20.0,"[91, 774, 108, 90, 85, 65, 83, 81, 646, 98, 98, 95, 72, 92, 90, 70, 90, 461, 104, 57, 73]","[1697101677473, 1697101678247, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678678, 1697101678759, 1697101679405, 1697101679503, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680110, 1697101680571, 1697101680675, 1697101680732, 1697101680805]"
5247,5247,438,33,[],200,llama-13b,128,1,1876.0,1.0,1,H100,1697101697599,1697101699475.0,120,9.0,1.0,"[470, 1406]","[1697101698069, 1697101699475]"
5248,5248,791,32,[],200,llama-13b,128,1,13088.0,1.0,1,H100,1697101675816,1697101688904.0,120,182.0,64.0,"[464, 1197, 64, 51, 763, 90, 86, 64, 82, 82, 646, 99, 97, 95, 72, 92, 90, 70, 91, 468, 95, 57, 73, 803, 85, 83, 79, 78, 771, 97, 93, 91, 69, 91, 90, 614, 102, 100, 97, 92, 91, 90, 593, 86, 64, 82, 81, 911, 91, 88, 88, 86, 647, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 91]","[1697101676280, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678445, 1697101678531, 1697101678595, 1697101678677, 1697101678759, 1697101679405, 1697101679504, 1697101679601, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680579, 1697101680674, 1697101680731, 1697101680804, 1697101681607, 1697101681692, 1697101681775, 1697101681854, 1697101681932, 1697101682703, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684050, 1697101684147, 1697101684239, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685245, 1697101685326, 1697101686237, 1697101686328, 1697101686416, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904]"
5249,5249,792,34,[],200,llama-13b,128,1,1075.0,1.0,1,H100,1697101699477,1697101700552.0,120,11.0,1.0,"[36, 1039]","[1697101699513, 1697101700552]"
5250,5250,309,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700553,1697101703170.0,120,,,"[18, 523, 1188, 69]","[1697101700571, 1697101701094, 1697101702282, 1697101702351]"
5251,5251,100,29,[],200,llama-13b,128,1,3788.0,1.0,1,H100,1697101675813,1697101679601.0,120,732.0,14.0,"[66, 501, 36, 1061, 64, 50, 763, 90, 86, 64, 83, 81, 647, 98, 98]","[1697101675879, 1697101676380, 1697101676416, 1697101677477, 1697101677541, 1697101677591, 1697101678354, 1697101678444, 1697101678530, 1697101678594, 1697101678677, 1697101678758, 1697101679405, 1697101679503, 1697101679601]"
5252,5252,663,36,[],200,llama-13b,128,1,5352.0,1.0,1,H100,1697101703174,1697101708526.0,120,79.0,20.0,"[196, 1529, 116, 98, 72, 728, 95, 87, 85, 83, 79, 943, 97, 92, 70, 91, 70, 91, 87, 544, 99]","[1697101703370, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526]"
5253,5253,297,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688625,1697101689595.0,120,,,"[13, 933]","[1697101688638, 1697101689571]"
5254,5254,629,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689619,1697101697588.0,120,,,"[565, 1846, 244, 213, 212, 78, 111, 93, 94, 403, 102, 100, 99, 94, 92, 87, 68, 434, 77, 100, 85, 604, 78, 77, 831, 103, 101, 110, 92, 91]","[1697101690184, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692888, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693965, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5255,5255,131,48,[],200,llama-13b,128,1,930.0,1.0,1,H100,1697101707383,1697101708313.0,120,8.0,1.0,"[12, 918]","[1697101707395, 1697101708313]"
5256,5256,485,49,[],200,llama-13b,128,1,1265.0,1.0,1,H100,1697101708314,1697101709579.0,120,67.0,3.0,"[13, 1047, 116, 89]","[1697101708327, 1697101709374, 1697101709490, 1697101709579]"
5257,5257,841,50,[],200,llama-13b,128,1,2672.0,1.0,1,H100,1697101709580,1697101712252.0,120,123.0,15.0,"[6, 1119, 103, 97, 92, 89, 89, 85, 498, 98, 91, 85, 82, 79, 59]","[1697101709586, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252]"
5258,5258,286,47,[],200,llama-13b,128,1,2642.0,1.0,1,H100,1697101708533,1697101711175.0,120,161.0,12.0,"[61, 897, 88, 83, 82, 79, 881, 103, 98, 92, 89, 89]","[1697101708594, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711175]"
5259,5259,245,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[227, 1291, 122, 69]","[1697101700869, 1697101702160, 1697101702282, 1697101702351]"
5260,5260,602,34,[],200,llama-13b,128,1,1723.0,1.0,1,H100,1697101703176,1697101704899.0,120,15.0,1.0,"[136, 1587]","[1697101703312, 1697101704899]"
5261,5261,233,40,[],200,llama-13b,128,1,1497.0,1.0,1,H100,1697101689615,1697101691112.0,120,6.0,1.0,"[254, 1243]","[1697101689869, 1697101691112]"
5262,5262,33,35,[],200,llama-13b,128,1,1442.0,1.0,1,H100,1697101704901,1697101706343.0,120,140.0,7.0,"[54, 823, 135, 95, 87, 85, 83, 80]","[1697101704955, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343]"
5263,5263,217,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688905,1697101689611.0,120,,,"[7, 659]","[1697101688912, 1697101689571]"
5264,5264,95,36,[],200,llama-13b,128,1,1879.0,1.0,1,H100,1697101697596,1697101699475.0,120,12.0,1.0,"[295, 1584]","[1697101697891, 1697101699475]"
5265,5265,453,37,[],200,llama-13b,128,1,1075.0,1.0,1,H100,1697101699477,1697101700552.0,120,26.0,1.0,"[36, 1039]","[1697101699513, 1697101700552]"
5266,5266,803,38,[],200,llama-13b,128,1,541.0,1.0,1,H100,1697101700554,1697101701095.0,120,20.0,1.0,"[41, 500]","[1697101700595, 1697101701095]"
5267,5267,575,34,[],200,llama-13b,128,1,5120.0,1.0,1,H100,1697101689612,1697101694732.0,120,86.0,20.0,"[210, 1290, 93, 1069, 213, 212, 78, 101, 102, 94, 405, 101, 100, 99, 95, 91, 87, 68, 433, 78, 101]","[1697101689822, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694732]"
5268,5268,533,39,[],200,llama-13b,128,1,1840.0,1.0,1,H100,1697101703176,1697101705016.0,120,216.0,2.0,"[100, 1740]","[1697101703276, 1697101705016]"
5269,5269,233,39,[],200,llama-13b,128,1,1939.0,1.0,1,H100,1697101701096,1697101703035.0,120,6.0,1.0,"[111, 1828]","[1697101701207, 1697101703035]"
5270,5270,780,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[526, 1866]","[1697101701168, 1697101703034]"
5271,5271,209,38,[],200,llama-13b,128,1,1724.0,1.0,1,H100,1697101703175,1697101704899.0,120,20.0,1.0,"[7, 1717]","[1697101703182, 1697101704899]"
5272,5272,52,40,[],200,llama-13b,128,1,1246.0,1.0,1,H100,1697101705018,1697101706264.0,120,58.0,6.0,"[51, 709, 135, 95, 87, 85, 84]","[1697101705069, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706264]"
5273,5273,562,40,[],200,llama-13b,128,1,9078.0,1.0,1,H100,1697101703036,1697101712114.0,120,67.0,39.0,"[48, 604, 1327, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 99, 65, 88, 811, 88, 84, 82, 79, 881, 103, 97, 93, 89, 88, 86, 497, 99, 91, 84, 83]","[1697101703084, 1697101703688, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709578, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711856, 1697101711947, 1697101712031, 1697101712114]"
5274,5274,537,39,[],200,llama-13b,128,1,4589.0,1.0,1,H100,1697101704901,1697101709490.0,120,83.0,20.0,"[6, 871, 135, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 546, 98, 65, 88, 811]","[1697101704907, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708428, 1697101708526, 1697101708591, 1697101708679, 1697101709490]"
5275,5275,21,32,[],200,llama-13b,128,1,683.0,1.0,1,H100,1697101709745,1697101710428.0,120,15.0,1.0,"[6, 677]","[1697101709751, 1697101710428]"
5276,5276,407,41,[],200,llama-13b,128,1,780.0,1.0,1,H100,1697101706265,1697101707045.0,120,16.0,1.0,"[17, 763]","[1697101706282, 1697101707045]"
5277,5277,188,39,[],200,llama-13b,128,1,5120.0,1.0,1,H100,1697101689611,1697101694731.0,120,85.0,20.0,"[66, 1435, 93, 1069, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100]","[1697101689677, 1697101691112, 1697101691205, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
5278,5278,847,35,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101693581,1697101694458.0,120,10.0,1.0,"[7, 869]","[1697101693588, 1697101694457]"
5279,5279,277,36,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101694459,1697101695336.0,120,18.0,1.0,"[24, 853]","[1697101694483, 1697101695336]"
5280,5280,599,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101691113,1697101697588.0,120,,,"[18, 899, 244, 213, 212, 79, 110, 93, 94, 403, 102, 100, 99, 95, 91, 87, 68, 433, 78, 100, 85, 604, 79, 76, 831, 103, 101, 110, 92, 91]","[1697101691131, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692778, 1697101692888, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5281,5281,635,37,[],200,llama-13b,128,1,846.0,1.0,1,H100,1697101695337,1697101696183.0,120,23.0,1.0,"[25, 821]","[1697101695362, 1697101696183]"
5282,5282,66,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696184,1697101697589.0,120,,,"[19, 1192]","[1697101696203, 1697101697395]"
5283,5283,684,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[42, 658, 43, 1250, 83, 82, 81, 81, 82]","[1697101697637, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5284,5284,117,35,[],200,llama-13b,128,1,1640.0,1.0,1,H100,1697101700642,1697101702282.0,120,364.0,2.0,"[390, 1128, 122]","[1697101701032, 1697101702160, 1697101702282]"
5285,5285,471,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702283,1697101703172.0,120,,,"[18, 734]","[1697101702301, 1697101703035]"
5286,5286,175,36,[],200,llama-13b,128,1,3377.0,1.0,1,H100,1697101689603,1697101692980.0,120,140.0,8.0,"[14, 1588, 1068, 214, 212, 78, 101, 102]","[1697101689617, 1697101691205, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980]"
5287,5287,253,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688723,1697101689606.0,120,,,"[12, 836]","[1697101688735, 1697101689571]"
5288,5288,765,42,[],200,llama-13b,128,1,1383.0,1.0,1,H100,1697101707045,1697101708428.0,120,84.0,2.0,"[13, 1255, 115]","[1697101707058, 1697101708313, 1697101708428]"
5289,5289,747,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689617,1697101697587.0,120,,,"[367, 2045, 245, 213, 212, 78, 111, 92, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101, 83, 605, 78, 77, 831, 103, 101, 110, 92, 91]","[1697101689984, 1697101692029, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692888, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732, 1697101694815, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5290,5290,830,37,[],200,llama-13b,128,1,3087.0,1.0,1,H100,1697101703176,1697101706263.0,120,140.0,9.0,"[307, 1416, 116, 98, 72, 728, 95, 87, 85, 83]","[1697101703483, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263]"
5291,5291,230,38,[],200,llama-13b,128,1,1280.0,1.0,1,H100,1697101706264,1697101707544.0,120,86.0,5.0,"[6, 775, 241, 96, 92, 70]","[1697101706270, 1697101707045, 1697101707286, 1697101707382, 1697101707474, 1697101707544]"
5292,5292,877,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695337,1697101697589.0,120,,,"[19, 827, 223, 103, 101, 110, 92, 92]","[1697101695356, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696904]"
5293,5293,587,39,[],200,llama-13b,128,1,767.0,1.0,1,H100,1697101707546,1697101708313.0,120,13.0,1.0,"[6, 761]","[1697101707552, 1697101708313]"
5294,5294,29,42,[],200,llama-13b,128,1,2244.0,1.0,1,H100,1697101697590,1697101699834.0,120,161.0,6.0,"[12, 693, 43, 1250, 83, 82, 81]","[1697101697602, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834]"
5295,5295,18,40,[],200,llama-13b,128,1,1059.0,1.0,1,H100,1697101708315,1697101709374.0,120,15.0,1.0,"[24, 1035]","[1697101708339, 1697101709374]"
5296,5296,375,41,[],200,llama-13b,128,1,2997.0,1.0,1,H100,1697101709375,1697101712372.0,120,874.0,17.0,"[13, 1317, 102, 98, 92, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51]","[1697101709388, 1697101710705, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372]"
5297,5297,387,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699835,1697101700635.0,120,,,"[7, 710]","[1697101699842, 1697101700552]"
5298,5298,518,39,[],200,llama-13b,128,1,1876.0,1.0,1,H100,1697101697599,1697101699475.0,120,23.0,1.0,"[495, 1381]","[1697101698094, 1697101699475]"
5299,5299,874,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700635.0,120,,,"[24, 1051]","[1697101699501, 1697101700552]"
5300,5300,305,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[263, 1255, 122, 69]","[1697101700905, 1697101702160, 1697101702282, 1697101702351]"
5301,5301,655,42,[],200,llama-13b,128,1,4109.0,1.0,1,H100,1697101703176,1697101707285.0,120,335.0,11.0,"[148, 1575, 118, 96, 72, 728, 95, 87, 85, 83, 79, 943]","[1697101703324, 1697101704899, 1697101705017, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285]"
5302,5302,84,43,[],200,llama-13b,128,1,1026.0,1.0,1,H100,1697101707287,1697101708313.0,120,26.0,1.0,"[12, 1014]","[1697101707299, 1697101708313]"
5303,5303,414,44,[],200,llama-13b,128,1,3938.0,1.0,1,H100,1697101708314,1697101712252.0,120,87.0,20.0,"[19, 1041, 116, 89, 83, 82, 79, 882, 103, 97, 92, 89, 88, 86, 498, 99, 90, 85, 82, 79, 59]","[1697101708333, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252]"
5304,5304,842,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101688814,1697101689611.0,120,,,"[6, 751]","[1697101688820, 1697101689571]"
5305,5305,267,31,[],200,llama-13b,128,1,5116.0,1.0,1,H100,1697101689616,1697101694732.0,120,83.0,20.0,"[312, 1184, 93, 1068, 215, 211, 78, 111, 92, 95, 404, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101]","[1697101689928, 1697101691112, 1697101691205, 1697101692273, 1697101692488, 1697101692699, 1697101692777, 1697101692888, 1697101692980, 1697101693075, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732]"
5306,5306,183,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101680806,1697101689606.0,120,,,"[25, 1680, 193, 96, 93, 92, 68, 91, 90, 615, 101, 101, 97, 92, 90, 90, 594, 86, 63, 83, 80, 911, 91, 89, 87, 86, 647, 101, 100, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101680831, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683053, 1697101683144, 1697101683234, 1697101683849, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685014, 1697101685100, 1697101685163, 1697101685246, 1697101685326, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
5307,5307,884,46,[],200,llama-13b,128,1,3719.0,1.0,1,H100,1697101708533,1697101712252.0,120,90.0,20.0,"[84, 757, 117, 88, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 99, 91, 84, 83, 78, 59]","[1697101708617, 1697101709374, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712115, 1697101712193, 1697101712252]"
5308,5308,505,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101692981,1697101697588.0,120,,,"[7, 491, 101, 100, 100, 94, 91, 88, 67, 434, 77, 100, 85, 604, 78, 77, 831, 103, 101, 111, 91, 91]","[1697101692988, 1697101693479, 1697101693580, 1697101693680, 1697101693780, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696721, 1697101696812, 1697101696903]"
5309,5309,190,43,[],200,llama-13b,128,1,2568.0,1.0,1,H100,1697101708429,1697101710997.0,120,335.0,10.0,"[6, 939, 116, 89, 83, 82, 79, 882, 103, 97, 92]","[1697101708435, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997]"
5310,5310,897,40,[],200,llama-13b,128,1,935.0,1.0,1,H100,1697101709492,1697101710427.0,120,9.0,1.0,"[6, 929]","[1697101709498, 1697101710427]"
5311,5311,325,41,[],200,llama-13b,128,1,2383.0,1.0,1,H100,1697101710428,1697101712811.0,120,85.0,20.0,"[7, 1225, 98, 99, 90, 85, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32]","[1697101710435, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811]"
5312,5312,884,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697596,1697101700635.0,120,,,"[390, 1489, 113, 83, 82, 81, 81, 82]","[1697101697986, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5313,5313,663,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703170.0,120,,,"[367, 1150, 123, 69]","[1697101701009, 1697101702159, 1697101702282, 1697101702351]"
5314,5314,312,38,[],200,llama-13b,128,1,1521.0,1.0,1,H100,1697101700638,1697101702159.0,120,23.0,1.0,"[132, 1389]","[1697101700770, 1697101702159]"
5315,5315,672,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702161,1697101703171.0,120,,,[13],[1697101702174]
5316,5316,213,39,[],200,llama-13b,128,1,1551.0,1.0,1,H100,1697101681503,1697101683054.0,120,123.0,6.0,"[24, 984, 193, 96, 93, 92, 69]","[1697101681527, 1697101682511, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054]"
5317,5317,97,40,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,6.0,20.0,"[42, 1681, 116, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 91, 70, 91, 86, 545, 99]","[1697101703218, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
5318,5318,677,32,[],200,llama-13b,128,1,762.0,1.0,1,H100,1697101695421,1697101696183.0,120,9.0,1.0,"[7, 755]","[1697101695428, 1697101696183]"
5319,5319,106,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696184,1697101697589.0,120,,,"[25, 1186]","[1697101696209, 1697101697395]"
5320,5320,463,34,[],200,llama-13b,128,1,1873.0,1.0,1,H100,1697101697602,1697101699475.0,120,39.0,1.0,"[491, 1382]","[1697101698093, 1697101699475]"
5321,5321,97,37,[],200,llama-13b,128,1,3720.0,1.0,1,H100,1697101708533,1697101712253.0,120,6.0,20.0,"[136, 705, 117, 88, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 497, 100, 91, 84, 83, 78, 59]","[1697101708669, 1697101709374, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711757, 1697101711857, 1697101711948, 1697101712032, 1697101712115, 1697101712193, 1697101712252]"
5322,5322,794,35,[],200,llama-13b,128,1,1075.0,1.0,1,H100,1697101699477,1697101700552.0,120,11.0,1.0,"[48, 1027]","[1697101699525, 1697101700552]"
5323,5323,219,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700553,1697101703170.0,120,,,"[24, 517, 1188, 69]","[1697101700577, 1697101701094, 1697101702282, 1697101702351]"
5324,5324,365,36,[],200,llama-13b,128,1,1969.0,1.0,1,H100,1697101706344,1697101708313.0,120,23.0,1.0,"[12, 1957]","[1697101706356, 1697101708313]"
5325,5325,273,42,[],200,llama-13b,128,1,876.0,1.0,1,H100,1697101704902,1697101705778.0,120,19.0,1.0,"[119, 757]","[1697101705021, 1697101705778]"
5326,5326,719,37,[],200,llama-13b,128,1,1509.0,1.0,1,H100,1697101708314,1697101709823.0,120,182.0,6.0,"[7, 1169, 88, 84, 82, 79]","[1697101708321, 1697101709490, 1697101709578, 1697101709662, 1697101709744, 1697101709823]"
5327,5327,149,38,[],200,llama-13b,128,1,2548.0,1.0,1,H100,1697101709824,1697101712372.0,120,563.0,10.0,"[7, 1829, 98, 99, 90, 85, 82, 79, 59, 69, 51]","[1697101709831, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372]"
5328,5328,579,45,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101709492,1697101710428.0,120,19.0,1.0,"[42, 894]","[1697101709534, 1697101710428]"
5329,5329,89,35,[],200,llama-13b,128,1,5351.0,1.0,1,H100,1697101703175,1697101708526.0,120,52.0,20.0,"[316, 1408, 115, 99, 72, 728, 95, 87, 85, 83, 79, 944, 96, 92, 70, 91, 70, 91, 87, 545, 98]","[1697101703491, 1697101704899, 1697101705014, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526]"
5330,5330,148,28,[],200,llama-13b,128,1,1880.0,1.0,1,H100,1697101697595,1697101699475.0,120,16.0,1.0,"[188, 1692]","[1697101697783, 1697101699475]"
5331,5331,506,29,[],200,llama-13b,128,1,1075.0,1.0,1,H100,1697101699477,1697101700552.0,120,16.0,1.0,"[6, 1069]","[1697101699483, 1697101700552]"
5332,5332,868,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700553,1697101703170.0,120,,,"[12, 529, 1188, 69]","[1697101700565, 1697101701094, 1697101702282, 1697101702351]"
5333,5333,444,36,[],200,llama-13b,128,1,1291.0,1.0,1,H100,1697101708533,1697101709824.0,120,457.0,6.0,"[67, 774, 117, 88, 83, 82, 79]","[1697101708600, 1697101709374, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823]"
5334,5334,547,41,[],200,llama-13b,128,1,840.0,1.0,1,H100,1697101708534,1697101709374.0,120,12.0,1.0,"[101, 739]","[1697101708635, 1697101709374]"
5335,5335,905,42,[],200,llama-13b,128,1,1053.0,1.0,1,H100,1697101709375,1697101710428.0,120,11.0,1.0,"[25, 1028]","[1697101709400, 1697101710428]"
5336,5336,337,43,[],200,llama-13b,128,1,1231.0,1.0,1,H100,1697101710429,1697101711660.0,120,12.0,1.0,"[30, 1201]","[1697101710459, 1697101711660]"
5337,5337,300,31,[],200,llama-13b,128,1,1726.0,1.0,1,H100,1697101703174,1697101704900.0,120,9.0,1.0,"[450, 1276]","[1697101703624, 1697101704900]"
5338,5338,629,32,[],200,llama-13b,128,1,13145.0,1.0,1,H100,1697101704903,1697101718048.0,120,457.0,381.0,"[122, 753, 135, 95, 87, 85, 83, 80, 942, 96, 93, 70, 91, 70, 91, 87, 544, 99, 65, 88, 811, 89, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26, 24, 20, 20, 21, 19, 18, 19, 20, 18, 19, 17, 17, 18, 16, 16, 16, 16, 16, 17, 16, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 15, 16, 15, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 16, 15, 16, 15, 16, 15, 16, 16, 15, 15, 14, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 14, 15, 15, 15, 15, 15, 15, 15, 17, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 15, 15, 15, 15, 16, 15, 15, 15, 15, 15, 16, 15, 15, 15, 15, 16, 15, 15, 15, 16, 15, 15, 15, 15, 16, 15, 15, 15, 16, 15, 15, 15, 16, 15, 15, 15, 15, 16, 15, 15, 16, 15, 15, 15, 15, 16, 15, 15, 16, 15, 15, 15, 16, 15, 15, 15, 16, 15, 15, 16, 15, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 15, 16, 16, 15]","[1697101705025, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706343, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114, 1697101713138, 1697101713158, 1697101713178, 1697101713199, 1697101713218, 1697101713236, 1697101713255, 1697101713275, 1697101713293, 1697101713312, 1697101713329, 1697101713346, 1697101713364, 1697101713380, 1697101713396, 1697101713412, 1697101713428, 1697101713444, 1697101713461, 1697101713477, 1697101713492, 1697101713508, 1697101713523, 1697101713538, 1697101713554, 1697101713569, 1697101713585, 1697101713600, 1697101713615, 1697101713631, 1697101713646, 1697101713661, 1697101713677, 1697101713692, 1697101713707, 1697101713723, 1697101713738, 1697101713753, 1697101713769, 1697101713784, 1697101713800, 1697101713815, 1697101713830, 1697101713846, 1697101713861, 1697101713877, 1697101713892, 1697101713907, 1697101713923, 1697101713938, 1697101713954, 1697101713969, 1697101713985, 1697101714000, 1697101714015, 1697101714031, 1697101714046, 1697101714062, 1697101714077, 1697101714093, 1697101714108, 1697101714124, 1697101714139, 1697101714155, 1697101714170, 1697101714186, 1697101714201, 1697101714217, 1697101714232, 1697101714248, 1697101714263, 1697101714279, 1697101714294, 1697101714310, 1697101714326, 1697101714341, 1697101714357, 1697101714372, 1697101714388, 1697101714403, 1697101714419, 1697101714435, 1697101714450, 1697101714465, 1697101714479, 1697101714494, 1697101714509, 1697101714524, 1697101714539, 1697101714554, 1697101714568, 1697101714583, 1697101714598, 1697101714613, 1697101714628, 1697101714643, 1697101714657, 1697101714672, 1697101714687, 1697101714702, 1697101714717, 1697101714732, 1697101714746, 1697101714761, 1697101714776, 1697101714791, 1697101714806, 1697101714821, 1697101714836, 1697101714850, 1697101714865, 1697101714880, 1697101714895, 1697101714910, 1697101714925, 1697101714940, 1697101714955, 1697101714969, 1697101714984, 1697101714999, 1697101715014, 1697101715029, 1697101715044, 1697101715059, 1697101715074, 1697101715089, 1697101715104, 1697101715119, 1697101715134, 1697101715149, 1697101715164, 1697101715179, 1697101715194, 1697101715209, 1697101715224, 1697101715239, 1697101715253, 1697101715268, 1697101715283, 1697101715298, 1697101715313, 1697101715328, 1697101715343, 1697101715358, 1697101715375, 1697101715391, 1697101715406, 1697101715421, 1697101715436, 1697101715451, 1697101715466, 1697101715481, 1697101715496, 1697101715511, 1697101715526, 1697101715541, 1697101715556, 1697101715571, 1697101715586, 1697101715601, 1697101715616, 1697101715631, 1697101715646, 1697101715661, 1697101715676, 1697101715691, 1697101715707, 1697101715722, 1697101715737, 1697101715752, 1697101715767, 1697101715782, 1697101715797, 1697101715812, 1697101715827, 1697101715842, 1697101715857, 1697101715872, 1697101715887, 1697101715902, 1697101715918, 1697101715933, 1697101715948, 1697101715963, 1697101715978, 1697101715993, 1697101716008, 1697101716023, 1697101716038, 1697101716053, 1697101716069, 1697101716084, 1697101716099, 1697101716114, 1697101716129, 1697101716145, 1697101716160, 1697101716175, 1697101716190, 1697101716205, 1697101716220, 1697101716236, 1697101716251, 1697101716266, 1697101716281, 1697101716296, 1697101716312, 1697101716327, 1697101716342, 1697101716357, 1697101716373, 1697101716388, 1697101716403, 1697101716418, 1697101716433, 1697101716449, 1697101716464, 1697101716479, 1697101716494, 1697101716510, 1697101716525, 1697101716540, 1697101716555, 1697101716571, 1697101716586, 1697101716601, 1697101716616, 1697101716631, 1697101716647, 1697101716662, 1697101716677, 1697101716693, 1697101716708, 1697101716723, 1697101716738, 1697101716753, 1697101716769, 1697101716784, 1697101716799, 1697101716815, 1697101716830, 1697101716845, 1697101716860, 1697101716876, 1697101716891, 1697101716906, 1697101716921, 1697101716937, 1697101716952, 1697101716967, 1697101716983, 1697101716998, 1697101717013, 1697101717029, 1697101717044, 1697101717059, 1697101717075, 1697101717090, 1697101717106, 1697101717121, 1697101717136, 1697101717152, 1697101717167, 1697101717182, 1697101717198, 1697101717213, 1697101717229, 1697101717244, 1697101717259, 1697101717275, 1697101717290, 1697101717306, 1697101717321, 1697101717337, 1697101717352, 1697101717367, 1697101717383, 1697101717398, 1697101717414, 1697101717429, 1697101717444, 1697101717460, 1697101717475, 1697101717491, 1697101717506, 1697101717522, 1697101717537, 1697101717552, 1697101717568, 1697101717583, 1697101717599, 1697101717614, 1697101717630, 1697101717645, 1697101717661, 1697101717676, 1697101717692, 1697101717707, 1697101717723, 1697101717738, 1697101717753, 1697101717769, 1697101717784, 1697101717800, 1697101717815, 1697101717831, 1697101717846, 1697101717862, 1697101717877, 1697101717893, 1697101717908, 1697101717924, 1697101717939, 1697101717955, 1697101717970, 1697101717986, 1697101718001, 1697101718017, 1697101718033, 1697101718048]"
5339,5339,801,37,[],200,llama-13b,128,1,2987.0,1.0,1,H100,1697101709824,1697101712811.0,120,47.0,20.0,"[7, 1829, 98, 99, 90, 85, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32]","[1697101709831, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811]"
5340,5340,580,37,[],200,llama-13b,128,1,5352.0,1.0,1,H100,1697101703174,1697101708526.0,120,88.0,20.0,"[456, 1270, 113, 100, 72, 728, 95, 87, 85, 83, 79, 943, 96, 92, 71, 92, 69, 91, 86, 546, 98]","[1697101703630, 1697101704900, 1697101705013, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707882, 1697101708428, 1697101708526]"
5341,5341,613,28,[],200,llama-13b,128,1,4221.0,1.0,1,H100,1697101669668,1697101673889.0,120,90.0,20.0,"[13, 804, 226, 99, 71, 92, 91, 90, 512, 97, 95, 94, 93, 90, 521, 114, 91, 68, 68, 88, 803]","[1697101669681, 1697101670485, 1697101670711, 1697101670810, 1697101670881, 1697101670973, 1697101671064, 1697101671154, 1697101671666, 1697101671763, 1697101671858, 1697101671952, 1697101672045, 1697101672135, 1697101672656, 1697101672770, 1697101672861, 1697101672929, 1697101672997, 1697101673085, 1697101673888]"
5342,5342,351,33,[],200,llama-13b,128,1,1686.0,1.0,1,H100,1697101710429,1697101712115.0,120,216.0,6.0,"[12, 1219, 98, 99, 90, 85, 82]","[1697101710441, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712114]"
5343,5343,863,38,[],200,llama-13b,128,1,700.0,1.0,1,H100,1697101697595,1697101698295.0,120,10.0,1.0,"[136, 564]","[1697101697731, 1697101698295]"
5344,5344,293,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101698296,1697101700636.0,120,,,"[18, 1162, 112, 83, 82, 82, 80, 82]","[1697101698314, 1697101699476, 1697101699588, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
5345,5345,717,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[132, 1389, 123, 69]","[1697101700770, 1697101702159, 1697101702282, 1697101702351]"
5346,5346,138,45,[],200,llama-13b,128,1,5351.0,1.0,1,H100,1697101703175,1697101708526.0,120,91.0,20.0,"[19, 1705, 116, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 99]","[1697101703194, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
5347,5347,542,40,[],200,llama-13b,128,1,586.0,1.0,1,H100,1697101683055,1697101683641.0,120,11.0,1.0,"[12, 574]","[1697101683067, 1697101683641]"
5348,5348,903,41,[],200,llama-13b,128,1,2595.0,1.0,1,H100,1697101683642,1697101686237.0,120,244.0,7.0,"[12, 1253, 106, 87, 63, 83, 81, 910]","[1697101683654, 1697101684907, 1697101685013, 1697101685100, 1697101685163, 1697101685246, 1697101685327, 1697101686237]"
5349,5349,330,42,[],200,llama-13b,128,1,2756.0,1.0,1,H100,1697101686238,1697101688994.0,120,345.0,14.0,"[12, 772, 215, 102, 99, 95, 91, 91, 88, 715, 106, 97, 92, 91, 90]","[1697101686250, 1697101687022, 1697101687237, 1697101687339, 1697101687438, 1697101687533, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688994]"
5350,5350,687,43,[],200,llama-13b,128,1,5736.0,1.0,1,H100,1697101688995,1697101694731.0,120,96.0,20.0,"[12, 969, 1229, 1068, 214, 212, 78, 101, 102, 94, 404, 102, 100, 99, 94, 91, 88, 68, 433, 78, 100]","[1697101689007, 1697101689976, 1697101691205, 1697101692273, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694553, 1697101694631, 1697101694731]"
5351,5351,112,44,[],200,llama-13b,128,1,688.0,1.0,1,H100,1697101694733,1697101695421.0,120,16.0,2.0,"[30, 573, 85]","[1697101694763, 1697101695336, 1697101695421]"
5352,5352,708,42,[],200,llama-13b,128,1,948.0,1.0,1,H100,1697101706097,1697101707045.0,120,140.0,1.0,"[12, 936]","[1697101706109, 1697101707045]"
5353,5353,134,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.33 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.91 GiB is free. Process 1607256 has 75.18 GiB memory in use. Of the allocated memory 46.85 GiB is allocated by PyTorch, and 27.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101673892,1697101675812.0,120,,,"[31, 925, 133, 102, 99, 95, 70, 94]","[1697101673923, 1697101674848, 1697101674981, 1697101675083, 1697101675182, 1697101675277, 1697101675347, 1697101675441]"
5354,5354,458,30,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101679603,1697101680480.0,120,11.0,1.0,"[6, 871]","[1697101679609, 1697101680480]"
5355,5355,786,31,[],200,llama-13b,128,1,3939.0,1.0,1,H100,1697101680481,1697101684420.0,120,87.0,20.0,"[12, 1009, 106, 84, 83, 79, 79, 771, 96, 93, 92, 69, 90, 90, 615, 102, 100, 97, 92, 90, 90]","[1697101680493, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682985, 1697101683054, 1697101683144, 1697101683234, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420]"
5356,5356,446,45,[],200,llama-13b,128,1,760.0,1.0,1,H100,1697101695423,1697101696183.0,120,26.0,1.0,"[17, 743]","[1697101695440, 1697101696183]"
5357,5357,224,43,[],200,llama-13b,128,1,4901.0,1.0,1,H100,1697101707046,1697101711947.0,120,85.0,20.0,"[12, 1255, 115, 99, 64, 88, 812, 88, 83, 82, 79, 882, 103, 96, 93, 89, 88, 86, 498, 98, 91]","[1697101707058, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947]"
5358,5358,803,46,[],200,llama-13b,128,1,1211.0,1.0,1,H100,1697101696185,1697101697396.0,120,20.0,1.0,"[53, 1158]","[1697101696238, 1697101697396]"
5359,5359,814,37,[],200,llama-13b,128,1,5197.0,1.0,1,H100,1697101689619,1697101694816.0,120,89.0,20.0,"[493, 1918, 244, 213, 212, 78, 110, 94, 94, 403, 102, 100, 99, 94, 91, 88, 68, 434, 77, 100, 85]","[1697101690112, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
5360,5360,495,46,[],200,llama-13b,128,1,840.0,1.0,1,H100,1697101708534,1697101709374.0,120,13.0,1.0,"[84, 756]","[1697101708618, 1697101709374]"
5361,5361,855,47,[],200,llama-13b,128,1,3169.0,1.0,1,H100,1697101709375,1697101712544.0,120,83.0,20.0,"[7, 1046, 276, 103, 97, 93, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55]","[1697101709382, 1697101710428, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
5362,5362,236,47,[],200,llama-13b,128,1,376.0,1.0,1,H100,1697101697397,1697101697773.0,120,8.0,1.0,"[30, 345]","[1697101697427, 1697101697772]"
5363,5363,127,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696721,1697101697589.0,120,,,"[13, 661]","[1697101696734, 1697101697395]"
5364,5364,595,48,[],200,llama-13b,128,1,1702.0,1.0,1,H100,1697101697774,1697101699476.0,120,8.0,1.0,"[410, 1292]","[1697101698184, 1697101699476]"
5365,5365,920,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699477,1697101700635.0,120,,,"[54, 1021]","[1697101699531, 1697101700552]"
5366,5366,486,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697600,1697101700636.0,120,,,"[518, 1357, 113, 83, 82, 81, 81, 82]","[1697101698118, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5367,5367,351,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700643,1697101703170.0,120,,,"[329, 1187, 123, 69]","[1697101700972, 1697101702159, 1697101702282, 1697101702351]"
5368,5368,844,40,[],200,llama-13b,128,1,1518.0,1.0,1,H100,1697101700642,1697101702160.0,120,10.0,1.0,"[434, 1084]","[1697101701076, 1697101702160]"
5369,5369,704,51,[],200,llama-13b,128,1,1726.0,1.0,1,H100,1697101703174,1697101704900.0,120,14.0,1.0,"[225, 1501]","[1697101703399, 1697101704900]"
5370,5370,136,52,[],200,llama-13b,128,1,877.0,1.0,1,H100,1697101704901,1697101705778.0,120,31.0,1.0,"[54, 823]","[1697101704955, 1697101705778]"
5371,5371,496,53,[],200,llama-13b,128,1,2748.0,1.0,1,H100,1697101705779,1697101708527.0,120,335.0,11.0,"[36, 1230, 240, 97, 92, 70, 91, 70, 91, 87, 544, 99]","[1697101705815, 1697101707045, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526]"
5372,5372,610,36,[],200,llama-13b,128,1,5198.0,1.0,1,H100,1697101689618,1697101694816.0,120,89.0,20.0,"[402, 2010, 244, 213, 212, 78, 101, 102, 94, 404, 102, 100, 99, 95, 90, 89, 67, 434, 77, 100, 85]","[1697101690020, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693964, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
5373,5373,264,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702162,1697101703172.0,120,,,"[121, 752]","[1697101702283, 1697101703035]"
5374,5374,711,42,[],200,llama-13b,128,1,2009.0,1.0,1,H100,1697101703176,1697101705185.0,120,457.0,4.0,"[447, 1277, 116, 97, 72]","[1697101703623, 1697101704900, 1697101705016, 1697101705113, 1697101705185]"
5375,5375,15,54,[],200,llama-13b,128,1,3718.0,1.0,1,H100,1697101708534,1697101712252.0,120,100.0,20.0,"[72, 768, 117, 88, 83, 82, 79, 881, 103, 97, 93, 89, 89, 85, 498, 99, 91, 84, 83, 78, 59]","[1697101708606, 1697101709374, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712115, 1697101712193, 1697101712252]"
5376,5376,139,43,[],200,llama-13b,128,1,5718.0,1.0,1,H100,1697101705186,1697101710904.0,120,39.0,21.0,"[6, 1852, 241, 96, 93, 70, 91, 70, 91, 87, 544, 99, 65, 88, 812, 88, 83, 82, 79, 881, 103, 97]","[1697101705192, 1697101707044, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904]"
5377,5377,309,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[118, 582, 43, 1250, 83, 82, 81, 81, 82]","[1697101697713, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5378,5378,668,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703171.0,120,,,"[264, 1254, 122, 69]","[1697101700906, 1697101702160, 1697101702282, 1697101702351]"
5379,5379,184,38,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,87.0,20.0,"[216, 1508, 108, 105, 72, 728, 95, 87, 85, 83, 79, 943, 97, 92, 70, 91, 70, 91, 87, 544, 99]","[1697101703392, 1697101704900, 1697101705008, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526]"
5380,5380,714,43,[],200,llama-13b,128,1,5028.0,1.0,1,H100,1697101705779,1697101710807.0,120,83.0,20.0,"[42, 1224, 241, 96, 92, 70, 91, 70, 91, 87, 544, 99, 65, 88, 812, 88, 83, 82, 79, 881, 103]","[1697101705821, 1697101707045, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807]"
5381,5381,243,38,[],200,llama-13b,128,1,1792.0,1.0,1,H100,1697101694818,1697101696610.0,120,67.0,4.0,"[88, 1277, 223, 103, 101]","[1697101694906, 1697101696183, 1697101696406, 1697101696509, 1697101696610]"
5382,5382,597,39,[],200,llama-13b,128,1,785.0,1.0,1,H100,1697101696611,1697101697396.0,120,39.0,1.0,"[12, 773]","[1697101696623, 1697101697396]"
5383,5383,504,44,[],200,llama-13b,128,1,1906.0,1.0,1,H100,1697101710905,1697101712811.0,120,58.0,20.0,"[7, 748, 98, 99, 91, 84, 83, 78, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32]","[1697101710912, 1697101711660, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811]"
5384,5384,27,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697397,1697101700636.0,120,,,"[24, 351, 566, 1249, 84, 82, 81, 81, 82]","[1697101697421, 1697101697772, 1697101698338, 1697101699587, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5385,5385,541,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694733,1697101697587.0,120,,,"[7, 596, 84, 79, 76, 831, 103, 100, 111, 93, 90]","[1697101694740, 1697101695336, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696609, 1697101696720, 1697101696813, 1697101696903]"
5386,5386,839,28,[],200,llama-13b,128,1,2539.0,1.0,1,H100,1697101675816,1697101678355.0,120,58.0,5.0,"[407, 1157, 97, 64, 50, 764]","[1697101676223, 1697101677380, 1697101677477, 1697101677541, 1697101677591, 1697101678355]"
5387,5387,40,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694818,1697101697587.0,120,,,"[11, 1354, 223, 103, 101, 110, 92, 91]","[1697101694829, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5388,5388,872,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[82, 618, 43, 1250, 83, 82, 81, 81, 82]","[1697101697677, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5389,5389,304,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700643,1697101703171.0,120,,,"[476, 1915]","[1697101701119, 1697101703034]"
5390,5390,664,43,[],200,llama-13b,128,1,3087.0,1.0,1,H100,1697101703176,1697101706263.0,120,364.0,9.0,"[205, 1519, 108, 105, 72, 728, 95, 87, 85, 83]","[1697101703381, 1697101704900, 1697101705008, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263]"
5391,5391,95,44,[],200,llama-13b,128,1,780.0,1.0,1,H100,1697101706265,1697101707045.0,120,12.0,1.0,"[17, 763]","[1697101706282, 1697101707045]"
5392,5392,420,45,[],200,llama-13b,128,1,4901.0,1.0,1,H100,1697101707046,1697101711947.0,120,52.0,20.0,"[24, 1243, 115, 99, 64, 88, 811, 89, 83, 82, 79, 882, 103, 96, 93, 89, 88, 86, 498, 98, 91]","[1697101707070, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947]"
5393,5393,8,35,[],200,llama-13b,128,1,766.0,1.0,1,H100,1697101694733,1697101695499.0,120,39.0,3.0,"[36, 567, 84, 78]","[1697101694769, 1697101695336, 1697101695420, 1697101695498]"
5394,5394,338,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101695500,1697101697588.0,120,,,"[6, 677, 224, 102, 101, 111, 92, 91]","[1697101695506, 1697101696183, 1697101696407, 1697101696509, 1697101696610, 1697101696721, 1697101696813, 1697101696904]"
5395,5395,651,40,[],200,llama-13b,128,1,1640.0,1.0,1,H100,1697101700642,1697101702282.0,120,457.0,2.0,"[366, 1151, 123]","[1697101701008, 1697101702159, 1697101702282]"
5396,5396,630,44,[],200,llama-13b,128,1,2393.0,1.0,1,H100,1697101700642,1697101703035.0,120,6.0,1.0,"[547, 1846]","[1697101701189, 1697101703035]"
5397,5397,80,41,[],200,llama-13b,128,1,751.0,1.0,1,H100,1697101702284,1697101703035.0,120,13.0,1.0,"[23, 728]","[1697101702307, 1697101703035]"
5398,5398,61,45,[],200,llama-13b,128,1,652.0,1.0,1,H100,1697101703036,1697101703688.0,120,9.0,1.0,"[18, 634]","[1697101703054, 1697101703688]"
5399,5399,410,42,[],200,llama-13b,128,1,4345.0,1.0,1,H100,1697101703036,1697101707381.0,120,364.0,12.0,"[36, 616, 1320, 104, 72, 729, 95, 87, 85, 83, 79, 943, 96]","[1697101703072, 1697101703688, 1697101705008, 1697101705112, 1697101705184, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381]"
5400,5400,216,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684426,1697101689611.0,120,,,"[36, 1654, 121, 91, 89, 87, 86, 647, 101, 99, 95, 92, 90, 89, 715, 106, 97, 92, 92, 89]","[1697101684462, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687338, 1697101687437, 1697101687532, 1697101687624, 1697101687714, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
5401,5401,540,39,[],200,llama-13b,128,1,2463.0,1.0,1,H100,1697101708534,1697101710997.0,120,140.0,5.0,"[147, 1746, 277, 103, 97, 93]","[1697101708681, 1697101710427, 1697101710704, 1697101710807, 1697101710904, 1697101710997]"
5402,5402,422,46,[],200,llama-13b,128,1,2089.0,1.0,1,H100,1697101703689,1697101705778.0,120,26.0,1.0,"[12, 2077]","[1697101703701, 1697101705778]"
5403,5403,699,37,[],200,llama-13b,128,1,700.0,1.0,1,H100,1697101697595,1697101698295.0,120,39.0,1.0,"[101, 599]","[1697101697696, 1697101698295]"
5404,5404,771,43,[],200,llama-13b,128,1,4564.0,1.0,1,H100,1697101707383,1697101711947.0,120,47.0,20.0,"[12, 918, 115, 99, 64, 88, 811, 89, 83, 82, 79, 882, 103, 97, 92, 89, 88, 86, 498, 98, 91]","[1697101707395, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947]"
5405,5405,124,38,[],200,llama-13b,128,1,1292.0,1.0,1,H100,1697101698296,1697101699588.0,120,83.0,2.0,"[6, 1286]","[1697101698302, 1697101699588]"
5406,5406,483,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101699589,1697101700635.0,120,,,"[12, 951]","[1697101699601, 1697101700552]"
5407,5407,776,47,[],200,llama-13b,128,1,1507.0,1.0,1,H100,1697101705779,1697101707286.0,120,67.0,2.0,"[6, 1259, 241]","[1697101705785, 1697101707044, 1697101707285]"
5408,5408,10,38,[],200,llama-13b,128,1,2371.0,1.0,1,H100,1697101708533,1697101710904.0,120,563.0,9.0,"[102, 739, 117, 88, 83, 82, 79, 881, 103, 97]","[1697101708635, 1697101709374, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904]"
5409,5409,294,48,[],200,llama-13b,128,1,1142.0,1.0,1,H100,1697101707286,1697101708428.0,120,9.0,2.0,"[7, 1135]","[1697101707293, 1697101708428]"
5410,5410,651,49,[],200,llama-13b,128,1,1061.0,1.0,1,H100,1697101708429,1697101709490.0,120,457.0,2.0,"[12, 933, 116]","[1697101708441, 1697101709374, 1697101709490]"
5411,5411,837,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703170.0,120,,,"[72, 1449, 123, 69]","[1697101700710, 1697101702159, 1697101702282, 1697101702351]"
5412,5412,627,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101694733,1697101697587.0,120,,,"[72, 616, 78, 76, 831, 103, 101, 110, 92, 91]","[1697101694805, 1697101695421, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5413,5413,238,41,[],200,llama-13b,128,1,2833.0,1.0,1,H100,1697101703175,1697101706008.0,120,563.0,6.0,"[406, 1318, 116, 98, 72, 728, 95]","[1697101703581, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008]"
5414,5414,78,50,[],200,llama-13b,128,1,3052.0,1.0,1,H100,1697101709492,1697101712544.0,120,84.0,20.0,"[18, 917, 278, 103, 97, 92, 89, 89, 85, 498, 98, 91, 85, 82, 79, 59, 69, 51, 60, 57, 55]","[1697101709510, 1697101710427, 1697101710705, 1697101710808, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544]"
5415,5415,269,29,[],200,llama-13b,128,1,932.0,1.0,1,H100,1697101678356,1697101679288.0,120,11.0,1.0,"[18, 914]","[1697101678374, 1697101679288]"
5416,5416,598,30,[],200,llama-13b,128,1,3511.0,1.0,1,H100,1697101679289,1697101682800.0,120,345.0,12.0,"[6, 1184, 92, 103, 58, 72, 804, 84, 82, 80, 79, 771, 96]","[1697101679295, 1697101680479, 1697101680571, 1697101680674, 1697101680732, 1697101680804, 1697101681608, 1697101681692, 1697101681774, 1697101681854, 1697101681933, 1697101682704, 1697101682800]"
5417,5417,30,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700635.0,120,,,"[222, 1771, 83, 82, 82, 80, 82]","[1697101697817, 1697101699588, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
5418,5418,541,32,[],200,llama-13b,128,1,5197.0,1.0,1,H100,1697101689619,1697101694816.0,120,90.0,20.0,"[481, 1930, 244, 213, 212, 78, 110, 94, 94, 403, 102, 100, 99, 94, 91, 88, 68, 434, 77, 100, 85]","[1697101690100, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692887, 1697101692981, 1697101693075, 1697101693478, 1697101693580, 1697101693680, 1697101693779, 1697101693873, 1697101693964, 1697101694052, 1697101694120, 1697101694554, 1697101694631, 1697101694731, 1697101694816]"
5419,5419,371,39,[],200,llama-13b,128,1,754.0,1.0,1,H100,1697101710906,1697101711660.0,120,13.0,1.0,"[12, 742]","[1697101710918, 1697101711660]"
5420,5420,910,46,[],200,llama-13b,128,1,1231.0,1.0,1,H100,1697101710429,1697101711660.0,120,8.0,1.0,"[24, 1207]","[1697101710453, 1697101711660]"
5421,5421,599,42,[],200,llama-13b,128,1,7105.0,1.0,1,H100,1697101706009,1697101713114.0,120,58.0,55.0,"[12, 1023, 242, 96, 92, 70, 92, 69, 91, 87, 545, 98, 65, 88, 812, 88, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 98, 91, 84, 83, 79, 59, 69, 51, 60, 57, 55, 45, 44, 41, 33, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26]","[1697101706021, 1697101707044, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707636, 1697101707705, 1697101707796, 1697101707883, 1697101708428, 1697101708526, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947, 1697101712031, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712674, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114]"
5422,5422,903,33,[],200,llama-13b,128,1,2086.0,1.0,1,H100,1697101694818,1697101696904.0,120,244.0,7.0,"[82, 1283, 223, 103, 101, 110, 92, 92]","[1697101694900, 1697101696183, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696904]"
5423,5423,146,44,[],200,llama-13b,128,1,2002.0,1.0,1,H100,1697101710809,1697101712811.0,120,96.0,20.0,"[6, 845, 98, 99, 90, 85, 83, 78, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32]","[1697101710815, 1697101711660, 1697101711758, 1697101711857, 1697101711947, 1697101712032, 1697101712115, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811]"
5424,5424,389,34,[],200,llama-13b,128,1,1518.0,1.0,1,H100,1697101700642,1697101702160.0,120,8.0,1.0,"[245, 1273]","[1697101700887, 1697101702160]"
5425,5425,751,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702162,1697101703171.0,120,,,[111],[1697101702273]
5426,5426,179,36,[],200,llama-13b,128,1,2009.0,1.0,1,H100,1697101703176,1697101705185.0,120,161.0,4.0,"[124, 1599, 117, 97, 72]","[1697101703300, 1697101704899, 1697101705016, 1697101705113, 1697101705185]"
5427,5427,56,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697596,1697101700635.0,120,,,"[227, 1652, 113, 83, 82, 82, 80, 82]","[1697101697823, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699835, 1697101699915, 1697101699997]"
5428,5428,492,30,[],200,llama-13b,128,1,4765.0,1.0,1,H100,1697101675815,1697101680580.0,120,47.0,20.0,"[472, 1093, 97, 64, 51, 763, 90, 85, 65, 82, 81, 647, 99, 98, 94, 72, 92, 90, 70, 91, 468]","[1697101676287, 1697101677380, 1697101677477, 1697101677541, 1697101677592, 1697101678355, 1697101678445, 1697101678530, 1697101678595, 1697101678677, 1697101678758, 1697101679405, 1697101679504, 1697101679602, 1697101679696, 1697101679768, 1697101679860, 1697101679950, 1697101680020, 1697101680111, 1697101680579]"
5429,5429,505,37,[],200,llama-13b,128,1,6670.0,1.0,1,H100,1697101705186,1697101711856.0,120,100.0,27.0,"[6, 2093, 96, 93, 70, 91, 70, 91, 87, 544, 99, 65, 88, 812, 88, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86, 498, 98]","[1697101705192, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856]"
5430,5430,414,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700641,1697101703171.0,120,,,"[182, 1337, 122, 69]","[1697101700823, 1697101702160, 1697101702282, 1697101702351]"
5431,5431,773,35,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,90.0,20.0,"[48, 1675, 116, 98, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 91, 70, 91, 86, 545, 99]","[1697101703224, 1697101704899, 1697101705015, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
5432,5432,850,31,[],200,llama-13b,128,1,3839.0,1.0,1,H100,1697101680582,1697101684421.0,120,109.0,20.0,"[98, 822, 106, 84, 83, 79, 79, 771, 96, 93, 91, 69, 91, 90, 614, 102, 101, 97, 92, 90, 91]","[1697101680680, 1697101681502, 1697101681608, 1697101681692, 1697101681775, 1697101681854, 1697101681933, 1697101682704, 1697101682800, 1697101682893, 1697101682984, 1697101683053, 1697101683144, 1697101683234, 1697101683848, 1697101683950, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684421]"
5433,5433,381,41,[],200,llama-13b,128,1,1644.0,1.0,1,H100,1697101700638,1697101702282.0,120,140.0,2.0,"[168, 1353, 123]","[1697101700806, 1697101702159, 1697101702282]"
5434,5434,368,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[180, 1700, 113, 83, 82, 81, 81, 82]","[1697101697775, 1697101699475, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5435,5435,713,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101702283,1697101703172.0,120,,,[18],[1697101702301]
5436,5436,272,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101684426,1697101689612.0,120,,,"[54, 1636, 121, 91, 89, 87, 87, 646, 101, 100, 94, 92, 91, 88, 715, 106, 97, 92, 92, 89]","[1697101684480, 1697101686116, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686591, 1697101687237, 1697101687338, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688905, 1697101688994]"
5437,5437,333,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101696904,1697101700636.0,120,,,"[7, 861, 566, 1249, 84, 82, 81, 80, 83]","[1697101696911, 1697101697772, 1697101698338, 1697101699587, 1697101699671, 1697101699753, 1697101699834, 1697101699914, 1697101699997]"
5438,5438,573,33,[],200,llama-13b,128,1,1592.0,1.0,1,H100,1697101689613,1697101691205.0,120,874.0,2.0,"[105, 1487]","[1697101689718, 1697101691205]"
5439,5439,7,34,[],200,llama-13b,128,1,3425.0,1.0,1,H100,1697101691206,1697101694631.0,120,345.0,11.0,"[6, 2160, 107, 101, 100, 99, 95, 91, 88, 67, 433, 78]","[1697101691212, 1697101693372, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694553, 1697101694631]"
5440,5440,362,35,[],200,llama-13b,128,1,704.0,1.0,1,H100,1697101694632,1697101695336.0,120,14.0,1.0,"[7, 697]","[1697101694639, 1697101695336]"
5441,5441,690,36,[],200,llama-13b,128,1,845.0,1.0,1,H100,1697101695338,1697101696183.0,120,39.0,1.0,"[41, 804]","[1697101695379, 1697101696183]"
5442,5442,19,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.43 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.05 GiB is free. Process 1607256 has 75.04 GiB memory in use. Of the allocated memory 46.26 GiB is allocated by PyTorch, and 27.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101682802,1697101689595.0,120,,,"[6, 833, 208, 102, 100, 97, 92, 90, 90, 593, 86, 64, 83, 81, 910, 91, 89, 87, 86, 647, 102, 99, 94, 92, 91, 88, 715, 106, 97, 92, 91, 89]","[1697101682808, 1697101683641, 1697101683849, 1697101683951, 1697101684051, 1697101684148, 1697101684240, 1697101684330, 1697101684420, 1697101685013, 1697101685099, 1697101685163, 1697101685246, 1697101685327, 1697101686237, 1697101686328, 1697101686417, 1697101686504, 1697101686590, 1697101687237, 1697101687339, 1697101687438, 1697101687532, 1697101687624, 1697101687715, 1697101687803, 1697101688518, 1697101688624, 1697101688721, 1697101688813, 1697101688904, 1697101688993]"
5443,5443,198,36,[],200,llama-13b,128,1,3719.0,1.0,1,H100,1697101708533,1697101712252.0,120,96.0,20.0,"[66, 775, 117, 88, 83, 82, 79, 881, 103, 98, 92, 89, 89, 85, 498, 99, 91, 84, 83, 78, 59]","[1697101708599, 1697101709374, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712115, 1697101712193, 1697101712252]"
5444,5444,121,37,[],200,llama-13b,128,1,1212.0,1.0,1,H100,1697101696184,1697101697396.0,120,13.0,1.0,"[43, 1169]","[1697101696227, 1697101697396]"
5445,5445,726,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700642,1697101703172.0,120,,,"[270, 1248, 122, 69]","[1697101700912, 1697101702160, 1697101702282, 1697101702351]"
5446,5446,629,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689618,1697101697587.0,120,,,"[385, 2027, 244, 213, 212, 78, 101, 102, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101, 83, 605, 79, 76, 831, 103, 101, 110, 92, 91]","[1697101690003, 1697101692030, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732, 1697101694815, 1697101695420, 1697101695499, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5447,5447,143,43,[],200,llama-13b,128,1,4206.0,1.0,1,H100,1697101703176,1697101707382.0,120,6.0,12.0,"[435, 1289, 116, 97, 72, 728, 95, 87, 85, 83, 79, 944, 96]","[1697101703611, 1697101704900, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707286, 1697101707382]"
5448,5448,665,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700640,1697101703171.0,120,,,"[159, 1360, 123, 69]","[1697101700799, 1697101702159, 1697101702282, 1697101702351]"
5449,5449,95,36,[],200,llama-13b,128,1,1723.0,1.0,1,H100,1697101703176,1697101704899.0,120,12.0,1.0,"[313, 1410]","[1697101703489, 1697101704899]"
5450,5450,449,37,[],200,llama-13b,128,1,4589.0,1.0,1,H100,1697101704901,1697101709490.0,120,86.0,20.0,"[24, 853, 135, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 87, 544, 99, 65, 88, 811]","[1697101704925, 1697101705778, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708591, 1697101708679, 1697101709490]"
5451,5451,156,40,[],200,llama-13b,128,1,5350.0,1.0,1,H100,1697101703176,1697101708526.0,120,86.0,20.0,"[106, 1617, 117, 97, 72, 728, 95, 87, 85, 83, 79, 943, 96, 93, 70, 91, 70, 91, 86, 545, 99]","[1697101703282, 1697101704899, 1697101705016, 1697101705113, 1697101705185, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708526]"
5452,5452,504,44,[],200,llama-13b,128,1,4564.0,1.0,1,H100,1697101707383,1697101711947.0,120,58.0,20.0,"[18, 912, 115, 99, 64, 88, 811, 88, 84, 82, 79, 882, 103, 96, 93, 89, 88, 86, 498, 98, 91]","[1697101707401, 1697101708313, 1697101708428, 1697101708527, 1697101708591, 1697101708679, 1697101709490, 1697101709578, 1697101709662, 1697101709744, 1697101709823, 1697101710705, 1697101710808, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260, 1697101711758, 1697101711856, 1697101711947]"
5453,5453,378,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 17.05 GiB. GPU 0 has a total capacty of 79.11 GiB of which 3.80 GiB is free. Process 1607256 has 75.30 GiB memory in use. Of the allocated memory 45.52 GiB is allocated by PyTorch, and 28.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101689618,1697101697587.0,120,,,"[385, 2271, 213, 212, 78, 101, 102, 94, 405, 101, 100, 99, 95, 91, 88, 67, 434, 77, 101, 83, 605, 78, 77, 831, 103, 101, 110, 92, 91]","[1697101690003, 1697101692274, 1697101692487, 1697101692699, 1697101692777, 1697101692878, 1697101692980, 1697101693074, 1697101693479, 1697101693580, 1697101693680, 1697101693779, 1697101693874, 1697101693965, 1697101694053, 1697101694120, 1697101694554, 1697101694631, 1697101694732, 1697101694815, 1697101695420, 1697101695498, 1697101695575, 1697101696406, 1697101696509, 1697101696610, 1697101696720, 1697101696812, 1697101696903]"
5454,5454,478,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697397,1697101700636.0,120,,,"[24, 351, 566, 1249, 84, 82, 81, 80, 83]","[1697101697421, 1697101697772, 1697101698338, 1697101699587, 1697101699671, 1697101699753, 1697101699834, 1697101699914, 1697101699997]"
5455,5455,835,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 14.95 GiB. GPU 0 has a total capacty of 79.11 GiB of which 11.17 GiB is free. Process 1607256 has 67.93 GiB memory in use. Of the allocated memory 42.98 GiB is allocated by PyTorch, and 24.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101700638,1697101703171.0,120,,,"[174, 1347, 123, 69]","[1697101700812, 1697101702159, 1697101702282, 1697101702351]"
5456,5456,352,40,[],200,llama-13b,128,1,1936.0,1.0,1,H100,1697101703177,1697101705113.0,120,11.0,3.0,"[135, 1705, 96]","[1697101703312, 1697101705017, 1697101705113]"
5457,5457,803,38,[],200,llama-13b,128,1,936.0,1.0,1,H100,1697101709492,1697101710428.0,120,20.0,1.0,"[30, 905]","[1697101709522, 1697101710427]"
5458,5458,514,41,[],200,llama-13b,128,1,3719.0,1.0,1,H100,1697101708533,1697101712252.0,120,85.0,20.0,"[31, 810, 116, 89, 83, 82, 79, 881, 103, 98, 92, 89, 89, 85, 498, 99, 91, 84, 82, 79, 59]","[1697101708564, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712114, 1697101712193, 1697101712252]"
5459,5459,233,39,[],200,llama-13b,128,1,1231.0,1.0,1,H100,1697101710429,1697101711660.0,120,6.0,1.0,"[18, 1213]","[1697101710447, 1697101711660]"
5460,5460,710,41,[],200,llama-13b,128,1,664.0,1.0,1,H100,1697101705114,1697101705778.0,120,14.0,1.0,"[7, 657]","[1697101705121, 1697101705778]"
5461,5461,140,42,[],200,llama-13b,128,1,5028.0,1.0,1,H100,1697101705779,1697101710807.0,120,96.0,20.0,"[42, 1224, 241, 96, 92, 70, 91, 70, 91, 87, 544, 99, 66, 87, 812, 88, 83, 82, 79, 881, 103]","[1697101705821, 1697101707045, 1697101707286, 1697101707382, 1697101707474, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707883, 1697101708427, 1697101708526, 1697101708592, 1697101708679, 1697101709491, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807]"
5462,5462,735,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697595,1697101700636.0,120,,,"[94, 606, 43, 1250, 83, 82, 81, 81, 82]","[1697101697689, 1697101698295, 1697101698338, 1697101699588, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5463,5463,498,43,[],200,llama-13b,128,1,851.0,1.0,1,H100,1697101710809,1697101711660.0,120,9.0,1.0,"[6, 845]","[1697101710815, 1697101711660]"
5464,5464,31,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.44 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.19 GiB is free. Process 1607256 has 70.90 GiB memory in use. Of the allocated memory 41.43 GiB is allocated by PyTorch, and 28.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-13b,128,1,,,1,H100,1697101697590,1697101700636.0,120,,,"[12, 693, 43, 1249, 84, 82, 81, 81, 82]","[1697101697602, 1697101698295, 1697101698338, 1697101699587, 1697101699671, 1697101699753, 1697101699834, 1697101699915, 1697101699997]"
5465,5465,171,34,[],200,llama-13b,128,1,1518.0,1.0,1,H100,1697101700642,1697101702160.0,120,6.0,1.0,"[384, 1134]","[1697101701026, 1697101702160]"
5466,5466,501,35,[],200,llama-13b,128,1,874.0,1.0,1,H100,1697101702161,1697101703035.0,120,19.0,1.0,"[6, 868]","[1697101702167, 1697101703035]"
5467,5467,859,36,[],200,llama-13b,128,1,652.0,1.0,1,H100,1697101703036,1697101703688.0,120,23.0,1.0,"[54, 598]","[1697101703090, 1697101703688]"
5468,5468,284,37,[],200,llama-13b,128,1,7571.0,1.0,1,H100,1697101703689,1697101711260.0,120,90.0,31.0,"[18, 2206, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 546, 98, 65, 88, 811, 89, 83, 82, 79, 881, 103, 97, 93, 89, 88, 86]","[1697101703707, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708428, 1697101708526, 1697101708591, 1697101708679, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710904, 1697101710997, 1697101711086, 1697101711174, 1697101711260]"
5469,5469,392,35,[],200,llama-13b,128,1,2393.0,1.0,1,H100,1697101700642,1697101703035.0,120,20.0,1.0,"[471, 1921]","[1697101701113, 1697101703034]"
5470,5470,753,36,[],200,llama-13b,128,1,5490.0,1.0,1,H100,1697101703035,1697101708525.0,120,83.0,20.0,"[7, 646, 1320, 105, 71, 729, 95, 87, 85, 83, 79, 943, 96, 92, 71, 91, 70, 91, 86, 545, 98]","[1697101703042, 1697101703688, 1697101705008, 1697101705113, 1697101705184, 1697101705913, 1697101706008, 1697101706095, 1697101706180, 1697101706263, 1697101706342, 1697101707285, 1697101707381, 1697101707473, 1697101707544, 1697101707635, 1697101707705, 1697101707796, 1697101707882, 1697101708427, 1697101708525]"
5471,5471,183,37,[],200,llama-13b,128,1,4722.0,1.0,1,H100,1697101708533,1697101713255.0,120,17.0,50.0,"[37, 804, 116, 89, 83, 82, 79, 881, 103, 98, 92, 89, 89, 85, 498, 99, 91, 84, 82, 79, 59, 69, 51, 60, 57, 55, 45, 44, 42, 32, 40, 32, 32, 32, 31, 31, 25, 29, 28, 28, 23, 23, 27, 26, 24, 20, 20, 21, 19, 18, 19]","[1697101708570, 1697101709374, 1697101709490, 1697101709579, 1697101709662, 1697101709744, 1697101709823, 1697101710704, 1697101710807, 1697101710905, 1697101710997, 1697101711086, 1697101711175, 1697101711260, 1697101711758, 1697101711857, 1697101711948, 1697101712032, 1697101712114, 1697101712193, 1697101712252, 1697101712321, 1697101712372, 1697101712432, 1697101712489, 1697101712544, 1697101712589, 1697101712633, 1697101712675, 1697101712707, 1697101712747, 1697101712779, 1697101712811, 1697101712843, 1697101712874, 1697101712905, 1697101712930, 1697101712959, 1697101712987, 1697101713015, 1697101713038, 1697101713061, 1697101713088, 1697101713114, 1697101713138, 1697101713158, 1697101713178, 1697101713199, 1697101713218, 1697101713236, 1697101713255]"
