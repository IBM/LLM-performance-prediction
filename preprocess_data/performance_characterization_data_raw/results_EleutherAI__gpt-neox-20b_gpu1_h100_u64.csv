,smpnum,reqnum,errors,status,model,num_users,requests,latency_ms,records,n_gpus,gpu_type,start_timestamp,end_timestamp,experiment_duration_s,n_input_tokens,n_output_tokens,latency_ms_per_token,timestamps_per_token
0,693,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013554,120,,,[13],[1697105012563]
1,258,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012555,1697105013555,120,,,[122],[1697105012677]
2,13,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013555,120,,,[42],[1697105012592]
3,158,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013555,120,,,[49],[1697105012599]
4,673,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013554,120,,,[26],[1697105012576]
5,231,0,[],200,EleutherAI/gpt-neox-20b,64,1,975.0,1.0,1,H100,1697105012552,1697105013527,120,13.0,1.0,"[73, 902]","[1697105012625, 1697105013527]"
6,718,0,[],200,EleutherAI/gpt-neox-20b,64,1,924.0,1.0,1,H100,1697105012603,1697105013527,120,13.0,1.0,"[113, 811]","[1697105012716, 1697105013527]"
7,858,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012552,1697105013555,120,,,[60],[1697105012612]
8,343,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012603,1697105013555,120,,,[109],[1697105012712]
9,553,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012549,1697105013554,120,,,[26],[1697105012575]
10,13,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012603,1697105013555,120,,,[99],[1697105012702]
11,424,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013554,120,,,[34],[1697105012584]
12,861,0,[],200,EleutherAI/gpt-neox-20b,64,1,360.0,1.0,1,H100,1697105012549,1697105012909,120,10.0,1.0,"[6, 354]","[1697105012555, 1697105012909]"
13,854,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012551,1697105013554,120,,,[78],[1697105012629]
14,262,0,[],200,EleutherAI/gpt-neox-20b,64,1,972.0,1.0,1,H100,1697105012555,1697105013527,120,39.0,1.0,"[78, 894]","[1697105012633, 1697105013527]"
15,837,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012551,1697105013555,120,,,[69],[1697105012620]
16,371,0,[],200,EleutherAI/gpt-neox-20b,64,1,976.0,1.0,1,H100,1697105012551,1697105013527,120,13.0,1.0,"[77, 899]","[1697105012628, 1697105013527]"
17,609,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013555,120,,,[67],[1697105012617]
18,380,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013560,1697105015236,120,,,"[127, 849, 46, 169, 58, 56]","[1697105013687, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
19,778,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012552,1697105013555,120,,,[64],[1697105012616]
20,41,2,[],200,EleutherAI/gpt-neox-20b,64,1,6071.0,1.0,1,H100,1697105015240,1697105021311,120,39.0,43.0,"[50, 833, 48, 526, 62, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48, 61]","[1697105015290, 1697105016123, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311]"
21,533,0,[],200,EleutherAI/gpt-neox-20b,64,1,58.0,1.0,1,H100,1697105012549,1697105012607,120,216.0,2.0,"[6, 51]","[1697105012555, 1697105012606]"
22,280,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012608,1697105013555,120,,,[160],[1697105012768]
23,864,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013560,1697105015236,120,,,"[132, 844, 46, 169, 58, 56]","[1697105013692, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
24,430,1,[],200,EleutherAI/gpt-neox-20b,64,1,900.0,1.0,1,H100,1697105013560,1697105014460,120,15.0,1.0,"[139, 761]","[1697105013699, 1697105014460]"
25,635,3,[],200,EleutherAI/gpt-neox-20b,64,1,820.0,1.0,1,H100,1697105015240,1697105016060,120,23.0,1.0,"[14, 806]","[1697105015254, 1697105016060]"
26,295,4,[],200,EleutherAI/gpt-neox-20b,64,1,636.0,1.0,1,H100,1697105016062,1697105016698,120,52.0,2.0,"[16, 620]","[1697105016078, 1697105016698]"
27,141,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013528,1697105015236,120,,,"[13, 995, 46, 169, 58, 56]","[1697105013541, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
28,207,2,[],200,EleutherAI/gpt-neox-20b,64,1,753.0,1.0,1,H100,1697105014461,1697105015214,120,10.0,1.0,"[28, 725]","[1697105014489, 1697105015214]"
29,69,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012552,1697105013555,120,,,[129],[1697105012681]
30,659,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013560,1697105015236,120,,,"[123, 853, 46, 169, 58, 56]","[1697105013683, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
31,627,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013556,1697105015235,120,,,"[63, 917, 46, 170, 57, 56]","[1697105013619, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
32,724,2,[],200,EleutherAI/gpt-neox-20b,64,1,1254.0,1.0,1,H100,1697105015242,1697105016496,120,11.0,1.0,"[136, 1117]","[1697105015378, 1697105016495]"
33,879,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013555,120,,,[54],[1697105012604]
34,473,3,[],200,EleutherAI/gpt-neox-20b,64,1,6766.0,1.0,1,H100,1697105016497,1697105023263,120,244.0,50.0,"[14, 687, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63, 53, 572, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 54, 53, 545, 60, 56, 44, 586, 73]","[1697105016511, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022504, 1697105022560, 1697105022604, 1697105023190, 1697105023263]"
35,630,1,[],200,EleutherAI/gpt-neox-20b,64,1,902.0,1.0,1,H100,1697105013557,1697105014459,120,6.0,1.0,"[110, 792]","[1697105013667, 1697105014459]"
36,738,1,[],200,EleutherAI/gpt-neox-20b,64,1,1308.0,1.0,1,H100,1697105013557,1697105014865,120,79.0,6.0,"[54, 925, 46, 170, 57, 56]","[1697105013611, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
37,93,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012551,1697105013555,120,,,[36],[1697105012587]
38,427,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013555,120,,,[46],[1697105012596]
39,515,2,[],200,EleutherAI/gpt-neox-20b,64,1,502.0,1.0,1,H100,1697105014866,1697105015368,120,11.0,1.0,"[16, 486]","[1697105014882, 1697105015368]"
40,328,1,[],200,EleutherAI/gpt-neox-20b,64,1,1307.0,1.0,1,H100,1697105013558,1697105014865,120,109.0,6.0,"[121, 857, 46, 169, 58, 56]","[1697105013679, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
41,398,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014460,1697105015235,120,,,[25],[1697105014485]
42,98,2,[],200,EleutherAI/gpt-neox-20b,64,1,502.0,1.0,1,H100,1697105014866,1697105015368,120,14.0,1.0,"[13, 489]","[1697105014879, 1697105015368]"
43,778,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012603,1697105013555,120,,,[95],[1697105012698]
44,687,3,[],200,EleutherAI/gpt-neox-20b,64,1,3742.0,1.0,1,H100,1697105015369,1697105019111,120,96.0,20.0,"[13, 1316, 61, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 76, 69, 70, 54, 63, 53, 572]","[1697105015382, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111]"
45,60,3,[],200,EleutherAI/gpt-neox-20b,64,1,5503.0,1.0,1,H100,1697105015239,1697105020742,120,93.0,36.0,"[83, 801, 48, 527, 61, 57, 56, 54, 272, 63, 59, 56, 57, 54, 667, 75, 70, 70, 54, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67]","[1697105015322, 1697105016123, 1697105016171, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017376, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742]"
46,466,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013528,1697105015236,120,,,"[17, 991, 46, 169, 58, 56]","[1697105013545, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
47,550,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013557,1697105015235,120,,,"[54, 925, 46, 170, 57, 56]","[1697105013611, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
48,199,1,[],200,EleutherAI/gpt-neox-20b,64,1,903.0,1.0,1,H100,1697105013557,1697105014460,120,13.0,1.0,"[62, 841]","[1697105013619, 1697105014460]"
49,777,2,[],200,EleutherAI/gpt-neox-20b,64,1,753.0,1.0,1,H100,1697105014461,1697105015214,120,9.0,1.0,"[16, 737]","[1697105014477, 1697105015214]"
50,554,3,[],200,EleutherAI/gpt-neox-20b,64,1,153.0,1.0,1,H100,1697105015215,1697105015368,120,26.0,1.0,"[9, 144]","[1697105015224, 1697105015368]"
51,233,2,[],200,EleutherAI/gpt-neox-20b,64,1,820.0,1.0,1,H100,1697105015240,1697105016060,120,6.0,1.0,"[75, 745]","[1697105015315, 1697105016060]"
52,753,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012603,1697105013555,120,,,[111],[1697105012714]
53,823,3,[],200,EleutherAI/gpt-neox-20b,64,1,3048.0,1.0,1,H100,1697105016063,1697105019111,120,90.0,20.0,"[12, 623, 61, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 76, 69, 70, 54, 63, 53, 572]","[1697105016075, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111]"
54,413,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013560,1697105015236,120,,,"[135, 841, 46, 170, 57, 56]","[1697105013695, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
55,231,0,[],200,EleutherAI/gpt-neox-20b,64,1,924.0,1.0,1,H100,1697105012603,1697105013527,120,13.0,1.0,"[103, 821]","[1697105012706, 1697105013527]"
56,507,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012549,1697105013554,120,,,[12],[1697105012561]
57,206,4,[],200,EleutherAI/gpt-neox-20b,64,1,1126.0,1.0,1,H100,1697105015370,1697105016496,120,16.0,1.0,"[20, 1105]","[1697105015390, 1697105016495]"
58,912,5,[],200,EleutherAI/gpt-neox-20b,64,1,2886.0,1.0,1,H100,1697105016497,1697105019383,120,92.0,20.0,"[18, 683, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63, 53, 572, 59, 47, 57, 55, 54]","[1697105016515, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383]"
59,805,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012555,1697105013554,120,,,[113],[1697105012668]
60,575,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013558,1697105015235,120,,,"[37, 941, 46, 170, 57, 56]","[1697105013595, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
61,822,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013528,1697105015236,120,,,"[9, 999, 46, 169, 58, 56]","[1697105013537, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
62,161,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013558,1697105015236,120,,,"[117, 861, 46, 169, 58, 56]","[1697105013675, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
63,592,2,[],200,EleutherAI/gpt-neox-20b,64,1,6606.0,1.0,1,H100,1697105015240,1697105021846,120,15.0,50.0,"[62, 821, 48, 526, 62, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 54]","[1697105015302, 1697105016123, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846]"
64,234,2,[],200,EleutherAI/gpt-neox-20b,64,1,4035.0,1.0,1,H100,1697105015239,1697105019274,120,457.0,25.0,"[47, 774, 63, 48, 526, 62, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63, 53, 571, 60, 47, 57]","[1697105015286, 1697105016060, 1697105016123, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274]"
65,593,4,[],200,EleutherAI/gpt-neox-20b,64,1,1563.0,1.0,1,H100,1697105019112,1697105020675,120,335.0,9.0,"[8, 798, 123, 70, 66, 62, 60, 307, 69]","[1697105019120, 1697105019918, 1697105020041, 1697105020111, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675]"
66,462,1,[],200,EleutherAI/gpt-neox-20b,64,1,901.0,1.0,1,H100,1697105013558,1697105014459,120,52.0,1.0,"[32, 869]","[1697105013590, 1697105014459]"
67,118,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014460,1697105015235,120,,,[17],[1697105014477]
68,823,3,[],200,EleutherAI/gpt-neox-20b,64,1,3247.0,1.0,1,H100,1697105015239,1697105018486,120,90.0,20.0,"[43, 841, 49, 526, 61, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63]","[1697105015282, 1697105016123, 1697105016172, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
69,39,0,[],200,EleutherAI/gpt-neox-20b,64,1,974.0,1.0,1,H100,1697105012553,1697105013527,120,8.0,1.0,"[132, 842]","[1697105012685, 1697105013527]"
70,744,1,[],200,EleutherAI/gpt-neox-20b,64,1,1337.0,1.0,1,H100,1697105013528,1697105014865,120,161.0,6.0,"[5, 1003, 46, 169, 58, 56]","[1697105013533, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
71,837,2,[],200,EleutherAI/gpt-neox-20b,64,1,3246.0,1.0,1,H100,1697105015240,1697105018486,120,85.0,20.0,"[67, 815, 49, 526, 62, 57, 55, 55, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63]","[1697105015307, 1697105016122, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016871, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
72,396,2,[],200,EleutherAI/gpt-neox-20b,64,1,3620.0,1.0,1,H100,1697105014866,1697105018486,120,89.0,20.0,"[4, 1253, 48, 526, 62, 57, 55, 55, 272, 63, 58, 56, 58, 54, 667, 75, 70, 69, 55, 63]","[1697105014870, 1697105016123, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016871, 1697105016926, 1697105017198, 1697105017261, 1697105017319, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
73,4,3,[],200,EleutherAI/gpt-neox-20b,64,1,2191.0,1.0,1,H100,1697105019275,1697105021466,120,89.0,20.0,"[4, 640, 122, 70, 66, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 49, 61, 59, 47, 48]","[1697105019279, 1697105019919, 1697105020041, 1697105020111, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466]"
74,456,4,[],200,EleutherAI/gpt-neox-20b,64,1,2354.0,1.0,1,H100,1697105019112,1697105021466,120,90.0,20.0,"[8, 798, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 49, 61, 59, 47, 48]","[1697105019120, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466]"
75,533,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012549,1697105013554,120,,,[19],[1697105012568]
76,371,0,[],200,EleutherAI/gpt-neox-20b,64,1,972.0,1.0,1,H100,1697105012555,1697105013527,120,13.0,1.0,"[125, 847]","[1697105012680, 1697105013527]"
77,25,1,[],200,EleutherAI/gpt-neox-20b,64,1,205.0,1.0,1,H100,1697105013529,1697105013734,120,12.0,1.0,"[20, 185]","[1697105013549, 1697105013734]"
78,119,0,[],200,EleutherAI/gpt-neox-20b,64,1,359.0,1.0,1,H100,1697105012550,1697105012909,120,31.0,1.0,"[41, 318]","[1697105012591, 1697105012909]"
79,818,1,[],200,EleutherAI/gpt-neox-20b,64,1,206.0,1.0,1,H100,1697105013528,1697105013734,120,13.0,1.0,"[17, 189]","[1697105013545, 1697105013734]"
80,609,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012549,1697105013554,120,,,[18],[1697105012567]
81,596,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013735,1697105015236,120,,,[5],[1697105013740]
82,249,3,[],200,EleutherAI/gpt-neox-20b,64,1,1519.0,1.0,1,H100,1697105015240,1697105016759,120,874.0,5.0,"[22, 860, 50, 525, 62]","[1697105015262, 1697105016122, 1697105016172, 1697105016697, 1697105016759]"
83,24,4,[],200,EleutherAI/gpt-neox-20b,64,1,1470.0,1.0,1,H100,1697105016760,1697105018230,120,79.0,9.0,"[5, 433, 63, 59, 56, 57, 54, 667, 75]","[1697105016765, 1697105017198, 1697105017261, 1697105017320, 1697105017376, 1697105017433, 1697105017487, 1697105018154, 1697105018229]"
84,386,1,[],200,EleutherAI/gpt-neox-20b,64,1,1307.0,1.0,1,H100,1697105013558,1697105014865,120,140.0,6.0,"[113, 865, 46, 170, 57, 56]","[1697105013671, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
85,701,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013735,1697105015236,120,,,[5],[1697105013740]
86,626,1,[],200,EleutherAI/gpt-neox-20b,64,1,903.0,1.0,1,H100,1697105013556,1697105014459,120,10.0,1.0,"[30, 873]","[1697105013586, 1697105014459]"
87,286,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014461,1697105015236,120,,,[33],[1697105014494]
88,677,4,[],200,EleutherAI/gpt-neox-20b,64,1,894.0,1.0,1,H100,1697105021467,1697105022361,120,9.0,1.0,"[12, 882]","[1697105021479, 1697105022361]"
89,332,5,[],200,EleutherAI/gpt-neox-20b,64,1,673.0,1.0,1,H100,1697105022362,1697105023035,120,39.0,1.0,"[17, 656]","[1697105022379, 1697105023035]"
90,427,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012549,1697105013554,120,,,[15],[1697105012564]
91,38,2,[],200,EleutherAI/gpt-neox-20b,64,1,3620.0,1.0,1,H100,1697105014866,1697105018486,120,88.0,20.0,"[12, 1244, 49, 526, 62, 57, 55, 55, 272, 63, 58, 56, 58, 54, 667, 75, 70, 69, 55, 63]","[1697105014878, 1697105016122, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016871, 1697105016926, 1697105017198, 1697105017261, 1697105017319, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
92,81,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013558,1697105015235,120,,,"[113, 865, 46, 170, 57, 56]","[1697105013671, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
93,57,3,[],200,EleutherAI/gpt-neox-20b,64,1,820.0,1.0,1,H100,1697105015240,1697105016060,120,13.0,1.0,"[18, 802]","[1697105015258, 1697105016060]"
94,469,3,[],200,EleutherAI/gpt-neox-20b,64,1,819.0,1.0,1,H100,1697105015241,1697105016060,120,17.0,1.0,"[86, 733]","[1697105015327, 1697105016060]"
95,640,4,[],200,EleutherAI/gpt-neox-20b,64,1,429.0,1.0,1,H100,1697105016067,1697105016496,120,15.0,1.0,"[24, 405]","[1697105016091, 1697105016496]"
96,418,5,[],200,EleutherAI/gpt-neox-20b,64,1,764.0,1.0,1,H100,1697105016497,1697105017261,120,286.0,3.0,"[6, 695, 63]","[1697105016503, 1697105017198, 1697105017261]"
97,606,5,[],200,EleutherAI/gpt-neox-20b,64,1,787.0,1.0,1,H100,1697105018231,1697105019018,120,9.0,1.0,"[4, 783]","[1697105018235, 1697105019018]"
98,384,6,[],200,EleutherAI/gpt-neox-20b,64,1,2447.0,1.0,1,H100,1697105019019,1697105021466,120,92.0,20.0,"[4, 896, 122, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 49, 60, 60, 47, 48]","[1697105019023, 1697105019919, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021311, 1697105021371, 1697105021418, 1697105021466]"
99,38,7,[],200,EleutherAI/gpt-neox-20b,64,1,3259.0,1.0,1,H100,1697105021467,1697105024726,120,88.0,20.0,"[4, 974, 59, 56, 44, 586, 73, 67, 52, 56, 472, 69, 53, 65, 50, 60, 335, 72, 56, 56]","[1697105021471, 1697105022445, 1697105022504, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726]"
100,71,6,[],200,EleutherAI/gpt-neox-20b,64,1,1955.0,1.0,1,H100,1697105017262,1697105019217,120,364.0,11.0,"[8, 884, 76, 69, 70, 54, 63, 53, 571, 60, 47]","[1697105017270, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217]"
101,130,4,[],200,EleutherAI/gpt-neox-20b,64,1,431.0,1.0,1,H100,1697105016065,1697105016496,120,14.0,1.0,"[11, 420]","[1697105016076, 1697105016496]"
102,825,5,[],200,EleutherAI/gpt-neox-20b,64,1,2886.0,1.0,1,H100,1697105016497,1697105019383,120,96.0,20.0,"[13, 688, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63, 53, 572, 59, 47, 57, 55, 54]","[1697105016510, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383]"
103,747,3,[],200,EleutherAI/gpt-neox-20b,64,1,4703.0,1.0,1,H100,1697105018487,1697105023190,120,140.0,36.0,"[9, 615, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 49, 60, 60, 47, 48, 221, 48, 57, 54, 53, 545, 63, 53, 44, 586]","[1697105018496, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022507, 1697105022560, 1697105022604, 1697105023190]"
104,733,8,[],200,EleutherAI/gpt-neox-20b,64,1,518.0,1.0,1,H100,1697105024727,1697105025245,120,31.0,1.0,"[8, 510]","[1697105024735, 1697105025245]"
105,444,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012551,1697105013554,120,,,[125],[1697105012676]
106,481,9,[],200,EleutherAI/gpt-neox-20b,64,1,867.0,1.0,1,H100,1697105025246,1697105026113,120,10.0,1.0,"[17, 850]","[1697105025263, 1697105026113]"
107,221,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013556,1697105015237,120,,,"[22, 958, 46, 169, 58, 56]","[1697105013578, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
108,698,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012603,1697105013555,120,,,[93],[1697105012696]
109,202,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012555,1697105013554,120,,,[118],[1697105012673]
110,553,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012552,1697105013555,120,,,[69],[1697105012621]
111,836,0,[],200,EleutherAI/gpt-neox-20b,64,1,360.0,1.0,1,H100,1697105012549,1697105012909,120,11.0,1.0,"[30, 330]","[1697105012579, 1697105012909]"
112,490,1,[],200,EleutherAI/gpt-neox-20b,64,1,1899.0,1.0,1,H100,1697105012910,1697105014809,120,11.0,5.0,"[5, 1621, 46, 169, 58]","[1697105012915, 1697105014536, 1697105014582, 1697105014751, 1697105014809]"
113,267,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014810,1697105015236,120,,,[5],[1697105014815]
114,851,3,[],200,EleutherAI/gpt-neox-20b,64,1,820.0,1.0,1,H100,1697105015240,1697105016060,120,23.0,1.0,"[10, 810]","[1697105015250, 1697105016060]"
115,621,4,[],200,EleutherAI/gpt-neox-20b,64,1,3044.0,1.0,1,H100,1697105016067,1697105019111,120,88.0,20.0,"[17, 614, 61, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 76, 69, 70, 54, 63, 53, 572]","[1697105016084, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111]"
116,301,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013560,1697105015236,120,,,"[128, 848, 46, 169, 58, 56]","[1697105013688, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
117,907,1,[],200,EleutherAI/gpt-neox-20b,64,1,903.0,1.0,1,H100,1697105013556,1697105014459,120,10.0,1.0,"[22, 881]","[1697105013578, 1697105014459]"
118,795,1,[],200,EleutherAI/gpt-neox-20b,64,1,903.0,1.0,1,H100,1697105013557,1697105014460,120,12.0,1.0,"[46, 857]","[1697105013603, 1697105014460]"
119,449,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014461,1697105015235,120,,,[20],[1697105014481]
120,430,2,[],200,EleutherAI/gpt-neox-20b,64,1,820.0,1.0,1,H100,1697105015240,1697105016060,120,15.0,1.0,"[38, 782]","[1697105015278, 1697105016060]"
121,220,3,[],200,EleutherAI/gpt-neox-20b,64,1,884.0,1.0,1,H100,1697105015239,1697105016123,120,67.0,2.0,"[67, 816]","[1697105015306, 1697105016122]"
122,84,3,[],200,EleutherAI/gpt-neox-20b,64,1,429.0,1.0,1,H100,1697105016067,1697105016496,120,26.0,1.0,"[15, 414]","[1697105016082, 1697105016496]"
123,809,4,[],200,EleutherAI/gpt-neox-20b,64,1,372.0,1.0,1,H100,1697105016124,1697105016496,120,16.0,1.0,"[4, 368]","[1697105016128, 1697105016496]"
124,339,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012551,1697105013555,120,,,[57],[1697105012608]
125,208,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012551,1697105013555,120,,,[49],[1697105012600]
126,69,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012552,1697105013555,120,,,[132],[1697105012684]
127,85,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013556,1697105015235,120,,,"[26, 954, 46, 169, 58, 56]","[1697105013582, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
128,591,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012603,1697105013555,120,,,[90],[1697105012693]
129,580,5,[],200,EleutherAI/gpt-neox-20b,64,1,2886.0,1.0,1,H100,1697105016497,1697105019383,120,88.0,20.0,"[22, 679, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63, 53, 572, 59, 47, 57, 55, 54]","[1697105016519, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383]"
130,752,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013554,120,,,[21],[1697105012571]
131,202,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012603,1697105013555,120,,,[105],[1697105012708]
132,301,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013558,1697105015236,120,,,"[121, 857, 46, 169, 58, 56]","[1697105013679, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
133,821,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012910,1697105015236,120,,,"[9, 1617, 46, 169, 58, 56]","[1697105012919, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
134,590,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013554,120,,,[30],[1697105012580]
135,476,2,[],200,EleutherAI/gpt-neox-20b,64,1,6606.0,1.0,1,H100,1697105015240,1697105021846,120,6.0,50.0,"[38, 844, 50, 526, 61, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 54]","[1697105015278, 1697105016122, 1697105016172, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846]"
136,244,1,[],200,EleutherAI/gpt-neox-20b,64,1,903.0,1.0,1,H100,1697105013557,1697105014460,120,9.0,1.0,"[58, 845]","[1697105013615, 1697105014460]"
137,21,2,[],200,EleutherAI/gpt-neox-20b,64,1,753.0,1.0,1,H100,1697105014461,1697105015214,120,15.0,1.0,"[29, 724]","[1697105014490, 1697105015214]"
138,711,1,[],200,EleutherAI/gpt-neox-20b,64,1,1196.0,1.0,1,H100,1697105013556,1697105014752,120,457.0,4.0,"[38, 942, 46, 170]","[1697105013594, 1697105014536, 1697105014582, 1697105014752]"
139,165,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013558,1697105015235,120,,,"[41, 937, 46, 170, 57, 56]","[1697105013599, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
140,372,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014753,1697105015236,120,,,[4],[1697105014757]
141,477,4,[],200,EleutherAI/gpt-neox-20b,64,1,6239.0,1.0,1,H100,1697105018487,1697105024726,120,244.0,50.0,"[21, 603, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 67, 63, 50, 62, 218, 49, 60, 60, 47, 48, 221, 48, 57, 54, 53, 546, 62, 53, 44, 586, 73, 67, 52, 56, 472, 69, 53, 64, 50, 61, 335, 72, 56, 56]","[1697105018508, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020809, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445, 1697105022507, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726]"
142,861,2,[],200,EleutherAI/gpt-neox-20b,64,1,821.0,1.0,1,H100,1697105015239,1697105016060,120,10.0,1.0,"[75, 746]","[1697105015314, 1697105016060]"
143,99,0,[],200,EleutherAI/gpt-neox-20b,64,1,975.0,1.0,1,H100,1697105012552,1697105013527,120,10.0,1.0,"[136, 839]","[1697105012688, 1697105013527]"
144,928,1,[],200,EleutherAI/gpt-neox-20b,64,1,900.0,1.0,1,H100,1697105013560,1697105014460,120,20.0,1.0,"[131, 769]","[1697105013691, 1697105014460]"
145,96,0,[],200,EleutherAI/gpt-neox-20b,64,1,359.0,1.0,1,H100,1697105012550,1697105012909,120,31.0,1.0,"[22, 337]","[1697105012572, 1697105012909]"
146,536,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012556,1697105013555,120,,,[136],[1697105012692]
147,697,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014460,1697105015235,120,,,[21],[1697105014481]
148,329,3,[],200,EleutherAI/gpt-neox-20b,64,1,821.0,1.0,1,H100,1697105015239,1697105016060,120,15.0,1.0,"[27, 794]","[1697105015266, 1697105016060]"
149,522,3,[],200,EleutherAI/gpt-neox-20b,64,1,429.0,1.0,1,H100,1697105016067,1697105016496,120,20.0,1.0,"[13, 416]","[1697105016080, 1697105016496]"
150,143,3,[],200,EleutherAI/gpt-neox-20b,64,1,2136.0,1.0,1,H100,1697105015239,1697105017375,120,6.0,12.0,"[7, 876, 49, 526, 62, 57, 55, 55, 272, 63, 59, 55]","[1697105015246, 1697105016122, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016871, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375]"
151,304,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013556,1697105015235,120,,,"[43, 937, 46, 170, 57, 56]","[1697105013599, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
152,99,4,[],200,EleutherAI/gpt-neox-20b,64,1,428.0,1.0,1,H100,1697105016068,1697105016496,120,10.0,1.0,"[24, 404]","[1697105016092, 1697105016496]"
153,292,4,[],200,EleutherAI/gpt-neox-20b,64,1,637.0,1.0,1,H100,1697105016496,1697105017133,120,286.0,1.0,"[6, 631]","[1697105016502, 1697105017133]"
154,791,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012910,1697105015236,120,,,"[13, 1613, 46, 169, 58, 56]","[1697105012923, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
155,877,5,[],200,EleutherAI/gpt-neox-20b,64,1,3165.0,1.0,1,H100,1697105017134,1697105020299,120,85.0,20.0,"[5, 1015, 76, 69, 69, 55, 63, 53, 572, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60]","[1697105017139, 1697105018154, 1697105018230, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299]"
156,284,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012603,1697105013555,120,,,[115],[1697105012718]
157,778,7,[],200,EleutherAI/gpt-neox-20b,64,1,6439.0,1.0,1,H100,1697105019218,1697105025657,120,16.0,50.0,"[8, 692, 123, 70, 66, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 49, 61, 59, 47, 48, 221, 48, 57, 54, 53, 546, 63, 52, 44, 586, 73, 67, 52, 56, 472, 69, 53, 65, 49, 61, 335, 72, 56, 56, 62, 59, 619, 74, 66, 51]","[1697105019226, 1697105019918, 1697105020041, 1697105020111, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657]"
158,394,0,[],200,EleutherAI/gpt-neox-20b,64,1,972.0,1.0,1,H100,1697105012555,1697105013527,120,11.0,1.0,"[117, 855]","[1697105012672, 1697105013527]"
159,690,5,[],200,EleutherAI/gpt-neox-20b,64,1,636.0,1.0,1,H100,1697105016497,1697105017133,120,39.0,1.0,"[34, 602]","[1697105016531, 1697105017133]"
160,452,2,[],200,EleutherAI/gpt-neox-20b,64,1,1574.0,1.0,1,H100,1697105015242,1697105016816,120,216.0,4.0,"[129, 1124, 203, 61, 57]","[1697105015371, 1697105016495, 1697105016698, 1697105016759, 1697105016816]"
161,460,6,[],200,EleutherAI/gpt-neox-20b,64,1,3165.0,1.0,1,H100,1697105017134,1697105020299,120,87.0,20.0,"[17, 1003, 76, 69, 70, 54, 63, 53, 572, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60]","[1697105017151, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299]"
162,865,2,[],200,EleutherAI/gpt-neox-20b,64,1,821.0,1.0,1,H100,1697105015239,1697105016060,120,9.0,1.0,"[11, 810]","[1697105015250, 1697105016060]"
163,55,1,[],200,EleutherAI/gpt-neox-20b,64,1,205.0,1.0,1,H100,1697105013529,1697105013734,120,12.0,1.0,"[20, 185]","[1697105013549, 1697105013734]"
164,757,2,[],200,EleutherAI/gpt-neox-20b,64,1,1478.0,1.0,1,H100,1697105013736,1697105015214,120,20.0,1.0,"[8, 1470]","[1697105013744, 1697105015214]"
165,417,3,[],200,EleutherAI/gpt-neox-20b,64,1,154.0,1.0,1,H100,1697105015215,1697105015369,120,17.0,1.0,"[5, 149]","[1697105015220, 1697105015369]"
166,649,6,[],200,EleutherAI/gpt-neox-20b,64,1,3082.0,1.0,1,H100,1697105020300,1697105023382,120,244.0,20.0,"[5, 897, 48, 61, 60, 47, 48, 221, 48, 57, 54, 53, 546, 63, 52, 44, 586, 73, 67, 52]","[1697105020305, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382]"
167,219,3,[],200,EleutherAI/gpt-neox-20b,64,1,2566.0,1.0,1,H100,1697105016817,1697105019383,120,90.0,20.0,"[5, 376, 63, 59, 56, 57, 54, 667, 75, 70, 69, 55, 63, 53, 572, 59, 47, 57, 55, 54]","[1697105016822, 1697105017198, 1697105017261, 1697105017320, 1697105017376, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383]"
168,444,1,[],200,EleutherAI/gpt-neox-20b,64,1,1308.0,1.0,1,H100,1697105013557,1697105014865,120,457.0,6.0,"[46, 933, 46, 170, 57, 56]","[1697105013603, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
169,98,2,[],200,EleutherAI/gpt-neox-20b,64,1,502.0,1.0,1,H100,1697105014866,1697105015368,120,14.0,1.0,"[8, 494]","[1697105014874, 1697105015368]"
170,803,3,[],200,EleutherAI/gpt-neox-20b,64,1,1126.0,1.0,1,H100,1697105015370,1697105016496,120,20.0,1.0,"[22, 1104]","[1697105015392, 1697105016496]"
171,793,3,[],200,EleutherAI/gpt-neox-20b,64,1,4962.0,1.0,1,H100,1697105015215,1697105020177,120,92.0,31.0,"[5, 903, 48, 526, 62, 57, 55, 55, 272, 63, 58, 56, 58, 54, 667, 75, 70, 69, 55, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67]","[1697105015220, 1697105016123, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016871, 1697105016926, 1697105017198, 1697105017261, 1697105017319, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177]"
172,459,4,[],200,EleutherAI/gpt-neox-20b,64,1,879.0,1.0,1,H100,1697105016497,1697105017376,120,58.0,5.0,"[9, 692, 63, 59, 55]","[1697105016506, 1697105017198, 1697105017261, 1697105017320, 1697105017375]"
173,287,2,[],200,EleutherAI/gpt-neox-20b,64,1,820.0,1.0,1,H100,1697105015240,1697105016060,120,10.0,1.0,"[83, 737]","[1697105015323, 1697105016060]"
174,561,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014460,1697105015236,120,,,[9],[1697105014469]
175,57,3,[],200,EleutherAI/gpt-neox-20b,64,1,429.0,1.0,1,H100,1697105016067,1697105016496,120,13.0,1.0,"[21, 407]","[1697105016088, 1697105016495]"
176,332,3,[],200,EleutherAI/gpt-neox-20b,64,1,1253.0,1.0,1,H100,1697105015242,1697105016495,120,39.0,1.0,"[133, 1120]","[1697105015375, 1697105016495]"
177,80,4,[],200,EleutherAI/gpt-neox-20b,64,1,636.0,1.0,1,H100,1697105016497,1697105017133,120,13.0,1.0,"[18, 618]","[1697105016515, 1697105017133]"
178,646,4,[],200,EleutherAI/gpt-neox-20b,64,1,636.0,1.0,1,H100,1697105016497,1697105017133,120,14.0,1.0,"[29, 607]","[1697105016526, 1697105017133]"
179,175,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013555,120,,,[63],[1697105012613]
180,169,3,[],200,EleutherAI/gpt-neox-20b,64,1,1127.0,1.0,1,H100,1697105015369,1697105016496,120,10.0,1.0,"[18, 1108]","[1697105015387, 1697105016495]"
181,415,5,[],200,EleutherAI/gpt-neox-20b,64,1,4117.0,1.0,1,H100,1697105017134,1697105021251,120,109.0,29.0,"[13, 1007, 76, 69, 70, 54, 63, 53, 572, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48]","[1697105017147, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250]"
182,659,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105017136,1697105027572,120,,,"[19, 999, 76, 69, 70, 54, 63, 53, 572, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 54, 53, 545, 60, 56, 44, 586, 72, 68, 52, 56, 472, 69, 53, 64, 50, 61, 335, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 482, 132, 65, 49, 303, 54, 54, 69, 65]","[1697105017155, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022504, 1697105022560, 1697105022604, 1697105023190, 1697105023262, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026200, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026857, 1697105026926, 1697105026991]"
183,865,4,[],200,EleutherAI/gpt-neox-20b,64,1,636.0,1.0,1,H100,1697105016497,1697105017133,120,9.0,1.0,"[30, 606]","[1697105016527, 1697105017133]"
184,203,2,[],200,EleutherAI/gpt-neox-20b,64,1,2194.0,1.0,1,H100,1697105015239,1697105017433,120,364.0,13.0,"[79, 804, 49, 527, 61, 57, 56, 54, 272, 63, 59, 56, 57]","[1697105015318, 1697105016122, 1697105016171, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017376, 1697105017433]"
185,672,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012603,1697105013555,120,,,[101],[1697105012704]
186,563,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012553,1697105013554,120,,,[71],[1697105012624]
187,39,0,[],200,EleutherAI/gpt-neox-20b,64,1,971.0,1.0,1,H100,1697105012556,1697105013527,120,8.0,1.0,"[133, 838]","[1697105012689, 1697105013527]"
188,228,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012549,1697105013555,120,,,[39],[1697105012588]
189,448,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012603,1697105013555,120,,,[97],[1697105012700]
190,743,3,[],200,EleutherAI/gpt-neox-20b,64,1,587.0,1.0,1,H100,1697105021312,1697105021899,120,123.0,6.0,"[5, 370, 48, 57, 55, 52]","[1697105021317, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899]"
191,102,6,[],200,EleutherAI/gpt-neox-20b,64,1,3296.0,1.0,1,H100,1697105023036,1697105026332,120,84.0,20.0,"[8, 866, 69, 53, 65, 49, 62, 334, 72, 56, 56, 62, 59, 619, 75, 65, 51, 61, 483, 131]","[1697105023044, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024146, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025541, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332]"
192,497,5,[],200,EleutherAI/gpt-neox-20b,64,1,1020.0,1.0,1,H100,1697105017134,1697105018154,120,67.0,2.0,"[13, 1007]","[1697105017147, 1697105018154]"
193,217,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013557,1697105015235,120,,,"[58, 921, 46, 170, 57, 56]","[1697105013615, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
194,266,6,[],200,EleutherAI/gpt-neox-20b,64,1,863.0,1.0,1,H100,1697105018155,1697105019018,120,9.0,1.0,"[5, 858]","[1697105018160, 1697105019018]"
195,884,2,[],200,EleutherAI/gpt-neox-20b,64,1,3247.0,1.0,1,H100,1697105015239,1697105018486,120,90.0,20.0,"[47, 837, 48, 526, 62, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63]","[1697105015286, 1697105016123, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
196,855,7,[],200,EleutherAI/gpt-neox-20b,64,1,2447.0,1.0,1,H100,1697105019019,1697105021466,120,83.0,20.0,"[8, 891, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 49, 61, 59, 47, 48]","[1697105019027, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466]"
197,248,5,[],200,EleutherAI/gpt-neox-20b,64,1,2514.0,1.0,1,H100,1697105020676,1697105023190,120,182.0,17.0,"[5, 521, 49, 61, 59, 47, 48, 221, 48, 57, 54, 53, 545, 64, 52, 44, 586]","[1697105020681, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022508, 1697105022560, 1697105022604, 1697105023190]"
198,613,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013554,120,,,[33],[1697105012583]
199,788,1,[],200,EleutherAI/gpt-neox-20b,64,1,902.0,1.0,1,H100,1697105013557,1697105014459,120,31.0,1.0,"[9, 893]","[1697105013566, 1697105014459]"
200,252,3,[],200,EleutherAI/gpt-neox-20b,64,1,3000.0,1.0,1,H100,1697105021847,1697105024847,120,182.0,22.0,"[9, 589, 63, 52, 45, 585, 73, 67, 52, 56, 472, 69, 53, 65, 50, 61, 334, 72, 56, 56, 62, 59]","[1697105021856, 1697105022445, 1697105022508, 1697105022560, 1697105022605, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847]"
201,536,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014460,1697105015237,120,,,[13],[1697105014473]
202,266,1,[],200,EleutherAI/gpt-neox-20b,64,1,903.0,1.0,1,H100,1697105013556,1697105014459,120,9.0,1.0,"[30, 873]","[1697105013586, 1697105014459]"
203,306,3,[],200,EleutherAI/gpt-neox-20b,64,1,1574.0,1.0,1,H100,1697105015242,1697105016816,120,140.0,6.0,"[88, 793, 48, 527, 61, 57]","[1697105015330, 1697105016123, 1697105016171, 1697105016698, 1697105016759, 1697105016816]"
204,38,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014460,1697105015236,120,,,[5],[1697105014465]
205,631,3,[],200,EleutherAI/gpt-neox-20b,64,1,6605.0,1.0,1,H100,1697105015241,1697105021846,120,216.0,50.0,"[13, 868, 50, 525, 62, 57, 55, 55, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 54]","[1697105015254, 1697105016122, 1697105016172, 1697105016697, 1697105016759, 1697105016816, 1697105016871, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846]"
206,186,4,[],200,EleutherAI/gpt-neox-20b,64,1,3848.0,1.0,1,H100,1697105015369,1697105019217,120,123.0,22.0,"[17, 1312, 61, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 76, 69, 70, 54, 63, 53, 572, 59, 47]","[1697105015386, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217]"
207,729,4,[],200,EleutherAI/gpt-neox-20b,64,1,778.0,1.0,1,H100,1697105017376,1697105018154,120,874.0,2.0,"[5, 773]","[1697105017381, 1697105018154]"
208,891,4,[],200,EleutherAI/gpt-neox-20b,64,1,382.0,1.0,1,H100,1697105016817,1697105017199,120,52.0,2.0,"[4, 377]","[1697105016821, 1697105017198]"
209,477,5,[],200,EleutherAI/gpt-neox-20b,64,1,6571.0,1.0,1,H100,1697105018155,1697105024726,120,244.0,50.0,"[5, 951, 59, 47, 57, 55, 54, 536, 122, 70, 66, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 54, 53, 546, 63, 52, 44, 586, 73, 67, 52, 56, 472, 69, 53, 64, 50, 61, 335, 72, 56, 56]","[1697105018160, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019919, 1697105020041, 1697105020111, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726]"
210,490,1,[],200,EleutherAI/gpt-neox-20b,64,1,1249.0,1.0,1,H100,1697105013560,1697105014809,120,11.0,5.0,"[140, 836, 46, 169, 58]","[1697105013700, 1697105014536, 1697105014582, 1697105014751, 1697105014809]"
211,841,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013528,1697105015236,120,,,"[13, 995, 46, 169, 58, 56]","[1697105013541, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
212,267,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014810,1697105015236,120,,,[5],[1697105014815]
213,663,5,[],200,EleutherAI/gpt-neox-20b,64,1,3100.0,1.0,1,H100,1697105017199,1697105020299,120,79.0,20.0,"[5, 950, 76, 69, 70, 54, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60]","[1697105017204, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299]"
214,175,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012555,1697105013554,120,,,[114],[1697105012669]
215,11,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012550,1697105013555,120,,,[54],[1697105012604]
216,655,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013557,1697105015235,120,,,"[50, 929, 46, 170, 57, 56]","[1697105013607, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
217,738,1,[],200,EleutherAI/gpt-neox-20b,64,1,1337.0,1.0,1,H100,1697105013528,1697105014865,120,79.0,6.0,"[5, 1003, 46, 169, 58, 56]","[1697105013533, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
218,600,1,[],200,EleutherAI/gpt-neox-20b,64,1,902.0,1.0,1,H100,1697105013557,1697105014459,120,23.0,1.0,"[13, 889]","[1697105013570, 1697105014459]"
219,369,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014460,1697105015237,120,,,[13],[1697105014473]
220,400,2,[],200,EleutherAI/gpt-neox-20b,64,1,2006.0,1.0,1,H100,1697105014866,1697105016872,120,123.0,7.0,"[8, 1248, 49, 526, 62, 57, 56]","[1697105014874, 1697105016122, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016872]"
221,807,2,[],200,EleutherAI/gpt-neox-20b,64,1,3244.0,1.0,1,H100,1697105015242,1697105018486,120,90.0,20.0,"[77, 804, 48, 527, 61, 57, 56, 54, 272, 63, 59, 56, 57, 54, 667, 75, 70, 69, 55, 63]","[1697105015319, 1697105016123, 1697105016171, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017376, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
222,1,3,[],200,EleutherAI/gpt-neox-20b,64,1,6070.0,1.0,1,H100,1697105015242,1697105021312,120,47.0,43.0,"[89, 792, 48, 527, 61, 57, 56, 54, 272, 63, 59, 56, 57, 54, 667, 75, 70, 70, 54, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48, 61]","[1697105015331, 1697105016123, 1697105016171, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017376, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311]"
223,430,2,[],200,EleutherAI/gpt-neox-20b,64,1,821.0,1.0,1,H100,1697105015239,1697105016060,120,15.0,1.0,"[31, 790]","[1697105015270, 1697105016060]"
224,169,3,[],200,EleutherAI/gpt-neox-20b,64,1,260.0,1.0,1,H100,1697105016873,1697105017133,120,10.0,1.0,"[4, 256]","[1697105016877, 1697105017133]"
225,840,4,[],200,EleutherAI/gpt-neox-20b,64,1,806.0,1.0,1,H100,1697105017134,1697105017940,120,17.0,1.0,"[21, 785]","[1697105017155, 1697105017940]"
226,618,2,[],200,EleutherAI/gpt-neox-20b,64,1,1254.0,1.0,1,H100,1697105015242,1697105016496,120,9.0,1.0,"[137, 1116]","[1697105015379, 1697105016495]"
227,494,5,[],200,EleutherAI/gpt-neox-20b,64,1,2170.0,1.0,1,H100,1697105017941,1697105020111,120,6.0,10.0,"[5, 1165, 59, 47, 57, 55, 54, 535, 123, 70]","[1697105017946, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020111]"
228,273,3,[],200,EleutherAI/gpt-neox-20b,64,1,636.0,1.0,1,H100,1697105016497,1697105017133,120,19.0,1.0,"[26, 610]","[1697105016523, 1697105017133]"
229,84,3,[],200,EleutherAI/gpt-neox-20b,64,1,429.0,1.0,1,H100,1697105016067,1697105016496,120,26.0,1.0,"[32, 397]","[1697105016099, 1697105016496]"
230,43,4,[],200,EleutherAI/gpt-neox-20b,64,1,1405.0,1.0,1,H100,1697105017134,1697105018539,120,732.0,8.0,"[9, 1011, 76, 69, 70, 54, 63, 53]","[1697105017143, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539]"
231,789,4,[],200,EleutherAI/gpt-neox-20b,64,1,6766.0,1.0,1,H100,1697105016497,1697105023263,120,6.0,50.0,"[25, 676, 63, 59, 56, 57, 54, 667, 75, 70, 69, 55, 63, 53, 572, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 54, 53, 545, 60, 56, 44, 586, 73]","[1697105016522, 1697105017198, 1697105017261, 1697105017320, 1697105017376, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022504, 1697105022560, 1697105022604, 1697105023190, 1697105023263]"
232,31,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013556,1697105015237,120,,,"[18, 962, 46, 169, 58, 56]","[1697105013574, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
233,434,6,[],200,EleutherAI/gpt-neox-20b,64,1,2712.0,1.0,1,H100,1697105027575,1697105030287,120,85.0,20.0,"[53, 334, 770, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 60, 49, 60, 48, 48]","[1697105027628, 1697105027962, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287]"
234,612,2,[],200,EleutherAI/gpt-neox-20b,64,1,3244.0,1.0,1,H100,1697105015242,1697105018486,120,93.0,20.0,"[60, 820, 49, 527, 61, 57, 55, 55, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63]","[1697105015302, 1697105016122, 1697105016171, 1697105016698, 1697105016759, 1697105016816, 1697105016871, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
235,578,3,[],200,EleutherAI/gpt-neox-20b,64,1,530.0,1.0,1,H100,1697105018488,1697105019018,120,31.0,1.0,"[28, 502]","[1697105018516, 1697105019018]"
236,239,4,[],200,EleutherAI/gpt-neox-20b,64,1,3489.0,1.0,1,H100,1697105019019,1697105022508,120,39.0,27.0,"[8, 891, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 49, 61, 59, 47, 48, 221, 48, 57, 54, 53, 546, 62]","[1697105019027, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445, 1697105022507]"
237,921,0,[],200,EleutherAI/gpt-neox-20b,64,1,359.0,1.0,1,H100,1697105012550,1697105012909,120,31.0,1.0,"[45, 314]","[1697105012595, 1697105012909]"
238,271,6,[],200,EleutherAI/gpt-neox-20b,64,1,2333.0,1.0,1,H100,1697105020112,1697105022445,120,87.0,20.0,"[4, 490, 70, 66, 67, 63, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 55, 52, 546]","[1697105020116, 1697105020606, 1697105020676, 1697105020742, 1697105020809, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899, 1697105022445]"
239,575,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012910,1697105015236,120,,,"[9, 1617, 46, 169, 58, 56]","[1697105012919, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
240,346,2,[],200,EleutherAI/gpt-neox-20b,64,1,3247.0,1.0,1,H100,1697105015240,1697105018487,120,85.0,20.0,"[86, 797, 48, 527, 61, 57, 56, 54, 272, 63, 59, 56, 57, 54, 667, 75, 70, 70, 54, 63]","[1697105015326, 1697105016123, 1697105016171, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017376, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018369, 1697105018423, 1697105018486]"
241,493,3,[],200,EleutherAI/gpt-neox-20b,64,1,2497.0,1.0,1,H100,1697105018487,1697105020984,120,83.0,20.0,"[17, 607, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 67, 63, 50, 62]","[1697105018504, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020809, 1697105020872, 1697105020922, 1697105020984]"
242,781,2,[],200,EleutherAI/gpt-neox-20b,64,1,2022.0,1.0,1,H100,1697105015239,1697105017261,120,335.0,10.0,"[43, 841, 49, 525, 62, 57, 56, 54, 272, 63]","[1697105015282, 1697105016123, 1697105016172, 1697105016697, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261]"
243,435,3,[],200,EleutherAI/gpt-neox-20b,64,1,3722.0,1.0,1,H100,1697105017262,1697105020984,120,563.0,27.0,"[4, 888, 76, 69, 70, 54, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62]","[1697105017266, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984]"
244,404,4,[],200,EleutherAI/gpt-neox-20b,64,1,3640.0,1.0,1,H100,1697105021900,1697105025540,120,87.0,20.0,"[4, 1286, 73, 67, 52, 56, 472, 69, 53, 65, 50, 61, 334, 72, 56, 56, 62, 59, 619, 74]","[1697105021904, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540]"
245,7,3,[],200,EleutherAI/gpt-neox-20b,64,1,1690.0,1.0,1,H100,1697105018487,1697105020177,120,345.0,11.0,"[21, 603, 59, 47, 57, 55, 54, 535, 123, 69, 67]","[1697105018508, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177]"
246,87,7,[],200,EleutherAI/gpt-neox-20b,64,1,2624.0,1.0,1,H100,1697105030288,1697105032912,120,335.0,19.0,"[9, 741, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65]","[1697105030297, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912]"
247,691,7,[],200,EleutherAI/gpt-neox-20b,64,1,345.0,1.0,1,H100,1697105026333,1697105026678,120,47.0,1.0,"[9, 336]","[1697105026342, 1697105026678]"
248,461,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026679,1697105027572,120,,,[20],[1697105026699]
249,262,4,[],200,EleutherAI/gpt-neox-20b,64,1,639.0,1.0,1,H100,1697105020986,1697105021625,120,39.0,1.0,"[16, 623]","[1697105021002, 1697105021625]"
250,445,5,[],200,EleutherAI/gpt-neox-20b,64,1,646.0,1.0,1,H100,1697105023264,1697105023910,120,457.0,2.0,"[4, 642]","[1697105023268, 1697105023910]"
251,908,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013557,1697105015236,120,,,"[5, 974, 46, 169, 58, 56]","[1697105013562, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
252,214,6,[],200,EleutherAI/gpt-neox-20b,64,1,3016.0,1.0,1,H100,1697105023911,1697105026927,120,52.0,20.0,"[5, 626, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69]","[1697105023916, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927]"
253,852,5,[],200,EleutherAI/gpt-neox-20b,64,1,3100.0,1.0,1,H100,1697105021626,1697105024726,120,100.0,20.0,"[17, 802, 60, 55, 44, 586, 73, 67, 52, 56, 472, 69, 53, 65, 50, 61, 334, 72, 56, 56]","[1697105021643, 1697105022445, 1697105022505, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726]"
254,75,6,[],200,EleutherAI/gpt-neox-20b,64,1,2781.0,1.0,1,H100,1697105021251,1697105024032,120,345.0,18.0,"[5, 431, 48, 57, 55, 52, 546, 59, 56, 44, 586, 73, 67, 52, 56, 472, 69, 53]","[1697105021256, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899, 1697105022445, 1697105022504, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032]"
255,212,4,[],200,EleutherAI/gpt-neox-20b,64,1,640.0,1.0,1,H100,1697105020985,1697105021625,120,31.0,1.0,"[5, 635]","[1697105020990, 1697105021625]"
256,792,5,[],200,EleutherAI/gpt-neox-20b,64,1,735.0,1.0,1,H100,1697105021626,1697105022361,120,11.0,1.0,"[21, 714]","[1697105021647, 1697105022361]"
257,562,2,[],200,EleutherAI/gpt-neox-20b,64,1,5680.0,1.0,1,H100,1697105015242,1697105020922,120,67.0,39.0,"[93, 788, 48, 527, 61, 57, 56, 54, 272, 63, 58, 57, 57, 54, 667, 75, 70, 70, 54, 63, 53, 572, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50]","[1697105015335, 1697105016123, 1697105016171, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017319, 1697105017376, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922]"
258,540,6,[],200,EleutherAI/gpt-neox-20b,64,1,1020.0,1.0,1,H100,1697105022362,1697105023382,120,140.0,5.0,"[5, 823, 73, 67, 52]","[1697105022367, 1697105023190, 1697105023263, 1697105023330, 1697105023382]"
259,193,7,[],200,EleutherAI/gpt-neox-20b,64,1,2949.0,1.0,1,H100,1697105023383,1697105026332,120,79.0,20.0,"[17, 510, 69, 53, 65, 50, 61, 334, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131]","[1697105023400, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332]"
260,679,1,[],200,EleutherAI/gpt-neox-20b,64,1,206.0,1.0,1,H100,1697105013529,1697105013735,120,15.0,1.0,"[24, 181]","[1697105013553, 1697105013734]"
261,895,8,[],200,EleutherAI/gpt-neox-20b,64,1,345.0,1.0,1,H100,1697105026333,1697105026678,120,15.0,1.0,"[21, 324]","[1697105026354, 1697105026678]"
262,52,1,[],200,EleutherAI/gpt-neox-20b,64,1,1308.0,1.0,1,H100,1697105013557,1697105014865,120,58.0,6.0,"[9, 970, 46, 169, 58, 56]","[1697105013566, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
263,559,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026679,1697105027572,120,,,[24],[1697105026703]
264,328,10,[],200,EleutherAI/gpt-neox-20b,64,1,1357.0,1.0,1,H100,1697105027574,1697105028931,120,109.0,6.0,"[98, 1060, 55, 45, 44, 55]","[1697105027672, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931]"
265,918,11,[],200,EleutherAI/gpt-neox-20b,64,1,1021.0,1.0,1,H100,1697105028933,1697105029954,120,23.0,1.0,"[4, 1017]","[1697105028937, 1697105029954]"
266,133,10,[],200,EleutherAI/gpt-neox-20b,64,1,562.0,1.0,1,H100,1697105026116,1697105026678,120,15.0,1.0,"[20, 541]","[1697105026136, 1697105026677]"
267,645,2,[],200,EleutherAI/gpt-neox-20b,64,1,3620.0,1.0,1,H100,1697105014866,1697105018486,120,86.0,20.0,"[4, 1252, 49, 526, 62, 57, 55, 55, 272, 63, 58, 56, 58, 54, 667, 75, 70, 69, 55, 63]","[1697105014870, 1697105016122, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016871, 1697105016926, 1697105017198, 1697105017261, 1697105017319, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
268,685,12,[],200,EleutherAI/gpt-neox-20b,64,1,584.0,1.0,1,H100,1697105029955,1697105030539,120,364.0,2.0,"[4, 580]","[1697105029959, 1697105030539]"
269,71,2,[],200,EleutherAI/gpt-neox-20b,64,1,2080.0,1.0,1,H100,1697105015240,1697105017320,120,364.0,11.0,"[30, 853, 49, 525, 62, 57, 56, 54, 272, 63, 59]","[1697105015270, 1697105016123, 1697105016172, 1697105016697, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320]"
270,347,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105030540,1697105037422,120,,,"[33, 465, 70, 69, 68, 57, 509, 71, 56, 55, 65, 64, 62, 62, 306, 163, 60, 72, 65, 63, 370, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66, 58]","[1697105030573, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031938, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
271,789,4,[],200,EleutherAI/gpt-neox-20b,64,1,6766.0,1.0,1,H100,1697105016497,1697105023263,120,6.0,50.0,"[22, 679, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63, 53, 572, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 54, 53, 545, 60, 56, 44, 586, 73]","[1697105016519, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022504, 1697105022560, 1697105022604, 1697105023190, 1697105023263]"
272,764,1,[],200,EleutherAI/gpt-neox-20b,64,1,904.0,1.0,1,H100,1697105013556,1697105014460,120,39.0,1.0,"[67, 837]","[1697105013623, 1697105014460]"
273,536,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014461,1697105015235,120,,,[24],[1697105014485]
274,325,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013557,1697105015235,120,,,"[50, 929, 46, 170, 57, 56]","[1697105013607, 1697105014536, 1697105014582, 1697105014752, 1697105014809, 1697105014865]"
275,912,3,[],200,EleutherAI/gpt-neox-20b,64,1,2865.0,1.0,1,H100,1697105017434,1697105020299,120,92.0,20.0,"[5, 715, 76, 69, 70, 54, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60]","[1697105017439, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299]"
276,189,3,[],200,EleutherAI/gpt-neox-20b,64,1,3247.0,1.0,1,H100,1697105015239,1697105018486,120,88.0,20.0,"[71, 812, 49, 527, 61, 57, 55, 55, 272, 63, 59, 56, 57, 54, 667, 75, 70, 69, 55, 63]","[1697105015310, 1697105016122, 1697105016171, 1697105016698, 1697105016759, 1697105016816, 1697105016871, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017376, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
277,595,1,[],200,EleutherAI/gpt-neox-20b,64,1,902.0,1.0,1,H100,1697105013557,1697105014459,120,8.0,1.0,"[25, 877]","[1697105013582, 1697105014459]"
278,413,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013556,1697105015236,120,,,"[14, 966, 46, 169, 58, 56]","[1697105013570, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
279,372,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014460,1697105015236,120,,,[9],[1697105014469]
280,72,2,[],200,EleutherAI/gpt-neox-20b,64,1,3247.0,1.0,1,H100,1697105015239,1697105018486,120,84.0,20.0,"[55, 828, 49, 526, 62, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63]","[1697105015294, 1697105016122, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
281,185,2,[],200,EleutherAI/gpt-neox-20b,64,1,3869.0,1.0,1,H100,1697105015242,1697105019111,120,93.0,20.0,"[132, 1324, 61, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 76, 69, 70, 54, 63, 53, 572]","[1697105015374, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111]"
282,632,5,[],200,EleutherAI/gpt-neox-20b,64,1,2926.0,1.0,1,H100,1697105018540,1697105021466,120,91.0,20.0,"[4, 1374, 123, 69, 67, 62, 60, 307, 69, 67, 67, 63, 50, 62, 218, 49, 60, 60, 47, 48]","[1697105018544, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020809, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021311, 1697105021371, 1697105021418, 1697105021466]"
283,24,3,[],200,EleutherAI/gpt-neox-20b,64,1,1957.0,1.0,1,H100,1697105015241,1697105017198,120,79.0,9.0,"[21, 860, 50, 525, 62, 57, 56, 54, 272]","[1697105015262, 1697105016122, 1697105016172, 1697105016697, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198]"
284,565,6,[],200,EleutherAI/gpt-neox-20b,64,1,3061.0,1.0,1,H100,1697105019384,1697105022445,120,91.0,20.0,"[9, 1213, 70, 66, 66, 64, 50, 62, 218, 48, 61, 60, 48, 47, 221, 48, 57, 55, 52, 546]","[1697105019393, 1697105020606, 1697105020676, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021419, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899, 1697105022445]"
285,389,0,[],200,EleutherAI/gpt-neox-20b,64,1,976.0,1.0,1,H100,1697105012551,1697105013527,120,8.0,1.0,"[57, 919]","[1697105012608, 1697105013527]"
286,866,4,[],200,EleutherAI/gpt-neox-20b,64,1,2498.0,1.0,1,H100,1697105018487,1697105020985,120,93.0,20.0,"[17, 607, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 67, 63, 50, 62]","[1697105018504, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020809, 1697105020872, 1697105020922, 1697105020984]"
287,928,1,[],200,EleutherAI/gpt-neox-20b,64,1,900.0,1.0,1,H100,1697105013560,1697105014460,120,20.0,1.0,"[136, 764]","[1697105013696, 1697105014460]"
288,697,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014461,1697105015235,120,,,[32],[1697105014493]
289,445,3,[],200,EleutherAI/gpt-neox-20b,64,1,884.0,1.0,1,H100,1697105015239,1697105016123,120,457.0,2.0,"[59, 824]","[1697105015298, 1697105016122]"
290,100,4,[],200,EleutherAI/gpt-neox-20b,64,1,2105.0,1.0,1,H100,1697105016124,1697105018229,120,732.0,14.0,"[4, 570, 61, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 75]","[1697105016128, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229]"
291,802,5,[],200,EleutherAI/gpt-neox-20b,64,1,787.0,1.0,1,H100,1697105018231,1697105019018,120,9.0,1.0,"[4, 783]","[1697105018235, 1697105019018]"
292,704,4,[],200,EleutherAI/gpt-neox-20b,64,1,741.0,1.0,1,H100,1697105017199,1697105017940,120,14.0,1.0,"[5, 736]","[1697105017204, 1697105017940]"
293,463,6,[],200,EleutherAI/gpt-neox-20b,64,1,704.0,1.0,1,H100,1697105019019,1697105019723,120,39.0,1.0,"[16, 688]","[1697105019035, 1697105019723]"
294,231,7,[],200,EleutherAI/gpt-neox-20b,64,1,810.0,1.0,1,H100,1697105019724,1697105020534,120,13.0,1.0,"[13, 797]","[1697105019737, 1697105020534]"
295,702,4,[],200,EleutherAI/gpt-neox-20b,64,1,2267.0,1.0,1,H100,1697105020178,1697105022445,120,89.0,20.0,"[4, 424, 70, 66, 67, 63, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 55, 52, 546]","[1697105020182, 1697105020606, 1697105020676, 1697105020742, 1697105020809, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899, 1697105022445]"
296,659,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105018489,1697105027572,120,,,"[27, 595, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 67, 63, 50, 62, 218, 49, 60, 60, 47, 48, 221, 48, 57, 54, 53, 546, 62, 53, 44, 586, 73, 67, 52, 56, 472, 69, 53, 64, 50, 61, 335, 72, 56, 55, 63, 59, 619, 74, 66, 51, 61, 482, 132, 65, 49, 303, 54, 55, 68, 65]","[1697105018516, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020809, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445, 1697105022507, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024725, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026200, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026926, 1697105026991]"
297,804,7,[],200,EleutherAI/gpt-neox-20b,64,1,434.0,1.0,1,H100,1697105026928,1697105027362,120,20.0,1.0,"[12, 422]","[1697105026940, 1697105027362]"
298,568,8,[],200,EleutherAI/gpt-neox-20b,64,1,270.0,1.0,1,H100,1697105027363,1697105027633,120,11.0,1.0,"[5, 265]","[1697105027368, 1697105027633]"
299,821,8,[],200,EleutherAI/gpt-neox-20b,64,1,2847.0,1.0,1,H100,1697105020535,1697105023382,120,85.0,20.0,"[4, 663, 49, 60, 60, 47, 48, 221, 48, 57, 54, 53, 546, 63, 52, 44, 586, 73, 67, 52]","[1697105020539, 1697105021202, 1697105021251, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382]"
300,600,3,[],200,EleutherAI/gpt-neox-20b,64,1,154.0,1.0,1,H100,1697105015215,1697105015369,120,23.0,1.0,"[9, 144]","[1697105015224, 1697105015368]"
301,375,4,[],200,EleutherAI/gpt-neox-20b,64,1,3054.0,1.0,1,H100,1697105015369,1697105018423,120,874.0,17.0,"[14, 1315, 61, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 76, 69, 70, 54]","[1697105015383, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423]"
302,316,9,[],200,EleutherAI/gpt-neox-20b,64,1,2905.0,1.0,1,H100,1697105027634,1697105030539,120,86.0,20.0,"[147, 951, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 61, 48, 60, 48, 48, 252]","[1697105027781, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
303,63,5,[],200,EleutherAI/gpt-neox-20b,64,1,434.0,1.0,1,H100,1697105016699,1697105017133,120,39.0,1.0,"[8, 426]","[1697105016707, 1697105017133]"
304,25,4,[],200,EleutherAI/gpt-neox-20b,64,1,1265.0,1.0,1,H100,1697105024848,1697105026113,120,12.0,1.0,"[5, 1260]","[1697105024853, 1697105026113]"
305,625,6,[],200,EleutherAI/gpt-neox-20b,64,1,739.0,1.0,1,H100,1697105024727,1697105025466,120,364.0,2.0,"[4, 735]","[1697105024731, 1697105025466]"
306,649,6,[],200,EleutherAI/gpt-neox-20b,64,1,3165.0,1.0,1,H100,1697105017134,1697105020299,120,244.0,20.0,"[9, 1011, 76, 69, 69, 55, 63, 53, 572, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60]","[1697105017143, 1697105018154, 1697105018230, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299]"
307,606,5,[],200,EleutherAI/gpt-neox-20b,64,1,563.0,1.0,1,H100,1697105026115,1697105026678,120,9.0,1.0,"[9, 553]","[1697105026124, 1697105026677]"
308,839,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026678,1697105027572,120,,,[5],[1697105026683]
309,383,6,[],200,EleutherAI/gpt-neox-20b,64,1,683.0,1.0,1,H100,1697105026679,1697105027362,120,15.0,1.0,"[8, 675]","[1697105026687, 1697105027362]"
310,37,7,[],200,EleutherAI/gpt-neox-20b,64,1,270.0,1.0,1,H100,1697105027363,1697105027633,120,20.0,1.0,"[9, 261]","[1697105027372, 1697105027633]"
311,708,8,[],200,EleutherAI/gpt-neox-20b,64,1,948.0,1.0,1,H100,1697105027667,1697105028615,120,140.0,1.0,"[117, 831]","[1697105027784, 1697105028615]"
312,590,9,[],200,EleutherAI/gpt-neox-20b,64,1,2949.0,1.0,1,H100,1697105023383,1697105026332,120,88.0,20.0,"[13, 514, 69, 53, 65, 50, 61, 334, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131]","[1697105023396, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332]"
313,419,7,[],200,EleutherAI/gpt-neox-20b,64,1,3082.0,1.0,1,H100,1697105020300,1697105023382,120,88.0,20.0,"[5, 897, 48, 61, 60, 47, 48, 221, 48, 57, 54, 53, 546, 60, 55, 44, 586, 73, 67, 52]","[1697105020305, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445, 1697105022505, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382]"
314,362,9,[],200,EleutherAI/gpt-neox-20b,64,1,672.0,1.0,1,H100,1697105028617,1697105029289,120,14.0,1.0,"[28, 644]","[1697105028645, 1697105029289]"
315,139,10,[],200,EleutherAI/gpt-neox-20b,64,1,2894.0,1.0,1,H100,1697105029290,1697105032184,120,39.0,21.0,"[8, 724, 61, 48, 60, 48, 48, 252, 49, 450, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62]","[1697105029298, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184]"
316,492,12,[],200,EleutherAI/gpt-neox-20b,64,1,2964.0,1.0,1,H100,1697105027575,1697105030539,120,47.0,20.0,"[117, 1040, 56, 44, 44, 55, 487, 70, 68, 66, 62, 49, 61, 228, 61, 48, 60, 48, 48, 252]","[1697105027692, 1697105028732, 1697105028788, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
317,228,5,[],200,EleutherAI/gpt-neox-20b,64,1,2923.0,1.0,1,H100,1697105017376,1697105020299,120,100.0,20.0,"[5, 773, 76, 69, 70, 54, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60]","[1697105017381, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299]"
318,764,1,[],200,EleutherAI/gpt-neox-20b,64,1,900.0,1.0,1,H100,1697105013560,1697105014460,120,39.0,1.0,"[124, 775]","[1697105013684, 1697105014459]"
319,534,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105014460,1697105015236,120,,,[5],[1697105014465]
320,164,3,[],200,EleutherAI/gpt-neox-20b,64,1,819.0,1.0,1,H100,1697105015241,1697105016060,120,15.0,1.0,"[98, 721]","[1697105015339, 1697105016060]"
321,866,4,[],200,EleutherAI/gpt-neox-20b,64,1,3042.0,1.0,1,H100,1697105016069,1697105019111,120,93.0,20.0,"[18, 611, 61, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 75, 70, 70, 54, 63, 53, 572]","[1697105016087, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111]"
322,818,6,[],200,EleutherAI/gpt-neox-20b,64,1,838.0,1.0,1,H100,1697105020300,1697105021138,120,13.0,1.0,"[13, 825]","[1697105020313, 1697105021138]"
323,582,7,[],200,EleutherAI/gpt-neox-20b,64,1,485.0,1.0,1,H100,1697105021140,1697105021625,120,19.0,1.0,"[12, 473]","[1697105021152, 1697105021625]"
324,246,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026333,1697105027573,120,,,"[13, 403, 54, 55, 69, 64]","[1697105026346, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
325,641,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.11 GiB. GPU 0 has a total capacty of 79.11 GiB of which 8.06 GiB is free. Process 1645736 has 71.03 GiB memory in use. Of the allocated memory 49.02 GiB is allocated by PyTorch, and 21.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012603,1697105013555,120,,,[107],[1697105012710]
326,216,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013558,1697105015236,120,,,"[117, 861, 46, 169, 58, 56]","[1697105013675, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
327,410,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013557,1697105015236,120,,,"[5, 974, 46, 169, 58, 56]","[1697105013562, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
328,242,8,[],200,EleutherAI/gpt-neox-20b,64,1,1756.0,1.0,1,H100,1697105021626,1697105023382,120,345.0,9.0,"[17, 802, 60, 55, 44, 586, 73, 67, 52]","[1697105021643, 1697105022445, 1697105022505, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382]"
329,13,9,[],200,EleutherAI/gpt-neox-20b,64,1,2949.0,1.0,1,H100,1697105023383,1697105026332,120,90.0,20.0,"[21, 506, 69, 53, 65, 50, 61, 334, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131]","[1697105023404, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332]"
330,654,3,[],200,EleutherAI/gpt-neox-20b,64,1,730.0,1.0,1,H100,1697105018487,1697105019217,120,47.0,4.0,"[13, 611, 59, 47]","[1697105018500, 1697105019111, 1697105019170, 1697105019217]"
331,431,4,[],200,EleutherAI/gpt-neox-20b,64,1,2517.0,1.0,1,H100,1697105019218,1697105021735,120,732.0,22.0,"[4, 696, 123, 70, 66, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 49, 61, 59, 47, 48, 221, 48]","[1697105019222, 1697105019918, 1697105020041, 1697105020111, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735]"
332,120,9,[],200,EleutherAI/gpt-neox-20b,64,1,356.0,1.0,1,H100,1697105027575,1697105027931,120,17.0,1.0,"[20, 336]","[1697105027595, 1697105027931]"
333,240,6,[],200,EleutherAI/gpt-neox-20b,64,1,3061.0,1.0,1,H100,1697105019384,1697105022445,120,83.0,20.0,"[5, 1217, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 55, 52, 546]","[1697105019389, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899, 1697105022445]"
334,819,10,[],200,EleutherAI/gpt-neox-20b,64,1,682.0,1.0,1,H100,1697105027933,1697105028615,120,13.0,1.0,"[17, 665]","[1697105027950, 1697105028615]"
335,248,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013556,1697105015237,120,,,"[18, 962, 46, 169, 58, 56]","[1697105013574, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
336,483,11,[],200,EleutherAI/gpt-neox-20b,64,1,2629.0,1.0,1,H100,1697105028616,1697105031245,120,84.0,20.0,"[25, 777, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68]","[1697105028641, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245]"
337,14,2,[],200,EleutherAI/gpt-neox-20b,64,1,3869.0,1.0,1,H100,1697105015242,1697105019111,120,90.0,20.0,"[128, 1328, 61, 57, 56, 54, 272, 63, 58, 56, 58, 54, 667, 76, 69, 70, 54, 63, 53, 572]","[1697105015370, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017319, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111]"
338,449,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013735,1697105015236,120,,,[9],[1697105013744]
339,144,3,[],200,EleutherAI/gpt-neox-20b,64,1,2497.0,1.0,1,H100,1697105018487,1697105020984,120,96.0,20.0,"[5, 619, 59, 47, 57, 55, 54, 536, 122, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62]","[1697105018492, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019919, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984]"
340,110,3,[],200,EleutherAI/gpt-neox-20b,64,1,1458.0,1.0,1,H100,1697105015240,1697105016698,120,96.0,4.0,"[34, 848, 50, 526]","[1697105015274, 1697105016122, 1697105016172, 1697105016698]"
341,120,7,[],200,EleutherAI/gpt-neox-20b,64,1,838.0,1.0,1,H100,1697105020300,1697105021138,120,17.0,1.0,"[9, 829]","[1697105020309, 1697105021138]"
342,821,8,[],200,EleutherAI/gpt-neox-20b,64,1,3008.0,1.0,1,H100,1697105021139,1697105024147,120,85.0,20.0,"[5, 543, 48, 57, 54, 53, 545, 60, 56, 44, 586, 73, 67, 52, 56, 472, 69, 53, 64, 50]","[1697105021144, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022504, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024146]"
343,810,4,[],200,EleutherAI/gpt-neox-20b,64,1,3061.0,1.0,1,H100,1697105019384,1697105022445,120,91.0,20.0,"[5, 1217, 69, 67, 66, 64, 50, 62, 218, 49, 60, 60, 47, 48, 221, 48, 57, 55, 52, 546]","[1697105019389, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899, 1697105022445]"
344,849,3,[],200,EleutherAI/gpt-neox-20b,64,1,818.0,1.0,1,H100,1697105015242,1697105016060,120,10.0,1.0,"[92, 726]","[1697105015334, 1697105016060]"
345,625,4,[],200,EleutherAI/gpt-neox-20b,64,1,630.0,1.0,1,H100,1697105016068,1697105016698,120,364.0,2.0,"[27, 603]","[1697105016095, 1697105016698]"
346,278,5,[],200,EleutherAI/gpt-neox-20b,64,1,434.0,1.0,1,H100,1697105016699,1697105017133,120,13.0,1.0,"[8, 426]","[1697105016707, 1697105017133]"
347,624,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105021467,1697105027572,120,,,"[17, 961, 62, 53, 44, 586, 73, 67, 52, 56, 472, 69, 53, 65, 50, 60, 335, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 68, 65]","[1697105021484, 1697105022445, 1697105022507, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026926, 1697105026991]"
348,55,6,[],200,EleutherAI/gpt-neox-20b,64,1,806.0,1.0,1,H100,1697105017134,1697105017940,120,12.0,1.0,"[17, 789]","[1697105017151, 1697105017940]"
349,635,7,[],200,EleutherAI/gpt-neox-20b,64,1,1077.0,1.0,1,H100,1697105017941,1697105019018,120,23.0,1.0,"[9, 1068]","[1697105017950, 1697105019018]"
350,281,5,[],200,EleutherAI/gpt-neox-20b,64,1,611.0,1.0,1,H100,1697105019112,1697105019723,120,23.0,1.0,"[16, 595]","[1697105019128, 1697105019723]"
351,264,13,[],200,EleutherAI/gpt-neox-20b,64,1,2435.0,1.0,1,H100,1697105030540,1697105032975,120,86.0,20.0,"[12, 486, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 60, 72, 65, 63]","[1697105030552, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975]"
352,405,8,[],200,EleutherAI/gpt-neox-20b,64,1,2447.0,1.0,1,H100,1697105019019,1697105021466,120,87.0,20.0,"[4, 895, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 49, 60, 60, 47, 48]","[1697105019023, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021311, 1697105021371, 1697105021418, 1697105021466]"
353,49,6,[],200,EleutherAI/gpt-neox-20b,64,1,952.0,1.0,1,H100,1697105019724,1697105020676,120,109.0,3.0,"[4, 878, 70]","[1697105019728, 1697105020606, 1697105020676]"
354,635,7,[],200,EleutherAI/gpt-neox-20b,64,1,461.0,1.0,1,H100,1697105020677,1697105021138,120,23.0,1.0,"[4, 457]","[1697105020681, 1697105021138]"
355,71,2,[],200,EleutherAI/gpt-neox-20b,64,1,2080.0,1.0,1,H100,1697105015240,1697105017320,120,364.0,11.0,"[71, 812, 48, 527, 61, 57, 56, 54, 272, 63, 59]","[1697105015311, 1697105016123, 1697105016171, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320]"
356,767,3,[],200,EleutherAI/gpt-neox-20b,64,1,619.0,1.0,1,H100,1697105017321,1697105017940,120,11.0,1.0,"[5, 614]","[1697105017326, 1697105017940]"
357,414,3,[],200,EleutherAI/gpt-neox-20b,64,1,2497.0,1.0,1,H100,1697105018487,1697105020984,120,87.0,20.0,"[5, 619, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62]","[1697105018492, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984]"
358,421,4,[],200,EleutherAI/gpt-neox-20b,64,1,3043.0,1.0,1,H100,1697105017941,1697105020984,120,85.0,20.0,"[5, 1165, 59, 47, 57, 55, 54, 536, 122, 70, 66, 62, 60, 307, 69, 67, 66, 64, 50, 62]","[1697105017946, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019919, 1697105020041, 1697105020111, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984]"
359,405,8,[],200,EleutherAI/gpt-neox-20b,64,1,3008.0,1.0,1,H100,1697105021139,1697105024147,120,87.0,20.0,"[5, 543, 48, 57, 55, 52, 546, 59, 56, 44, 586, 73, 67, 52, 56, 472, 69, 53, 65, 50]","[1697105021144, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899, 1697105022445, 1697105022504, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147]"
360,309,3,[],200,EleutherAI/gpt-neox-20b,64,1,2459.0,1.0,1,H100,1697105020923,1697105023382,120,52.0,20.0,"[4, 275, 49, 61, 59, 47, 48, 221, 48, 57, 54, 53, 545, 64, 52, 44, 586, 73, 67, 52]","[1697105020927, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382]"
361,183,2,[],200,EleutherAI/gpt-neox-20b,64,1,6608.0,1.0,1,H100,1697105015238,1697105021846,120,17.0,50.0,"[8, 876, 49, 526, 62, 57, 55, 55, 272, 63, 58, 56, 58, 54, 667, 75, 70, 69, 55, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 54]","[1697105015246, 1697105016122, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016871, 1697105016926, 1697105017198, 1697105017261, 1697105017319, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846]"
362,763,3,[],200,EleutherAI/gpt-neox-20b,64,1,611.0,1.0,1,H100,1697105019112,1697105019723,120,20.0,1.0,"[12, 599]","[1697105019124, 1697105019723]"
363,540,4,[],200,EleutherAI/gpt-neox-20b,64,1,1085.0,1.0,1,H100,1697105019724,1697105020809,120,140.0,5.0,"[4, 878, 70, 66, 67]","[1697105019728, 1697105020606, 1697105020676, 1697105020742, 1697105020809]"
364,926,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013556,1697105015235,120,,,"[35, 945, 46, 169, 58, 56]","[1697105013591, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
365,810,2,[],200,EleutherAI/gpt-neox-20b,64,1,3247.0,1.0,1,H100,1697105015239,1697105018486,120,91.0,20.0,"[19, 864, 50, 525, 62, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63]","[1697105015258, 1697105016122, 1697105016172, 1697105016697, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
366,172,5,[],200,EleutherAI/gpt-neox-20b,64,1,571.0,1.0,1,H100,1697105025542,1697105026113,120,19.0,1.0,"[12, 559]","[1697105025554, 1697105026113]"
367,806,4,[],200,EleutherAI/gpt-neox-20b,64,1,2684.0,1.0,1,H100,1697105016699,1697105019383,120,89.0,20.0,"[4, 495, 63, 59, 56, 57, 54, 667, 75, 70, 69, 55, 63, 53, 572, 59, 47, 57, 55, 54]","[1697105016703, 1697105017198, 1697105017261, 1697105017320, 1697105017376, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383]"
368,578,3,[],200,EleutherAI/gpt-neox-20b,64,1,530.0,1.0,1,H100,1697105018488,1697105019018,120,31.0,1.0,"[24, 506]","[1697105018512, 1697105019018]"
369,849,6,[],200,EleutherAI/gpt-neox-20b,64,1,562.0,1.0,1,H100,1697105026116,1697105026678,120,10.0,1.0,"[16, 545]","[1697105026132, 1697105026677]"
370,502,7,[],200,EleutherAI/gpt-neox-20b,64,1,683.0,1.0,1,H100,1697105026679,1697105027362,120,19.0,1.0,"[12, 671]","[1697105026691, 1697105027362]"
371,238,4,[],200,EleutherAI/gpt-neox-20b,64,1,1220.0,1.0,1,H100,1697105019019,1697105020239,120,563.0,6.0,"[12, 887, 123, 69, 67, 62]","[1697105019031, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239]"
372,273,8,[],200,EleutherAI/gpt-neox-20b,64,1,270.0,1.0,1,H100,1697105027363,1697105027633,120,19.0,1.0,"[9, 261]","[1697105027372, 1697105027633]"
373,401,4,[],200,EleutherAI/gpt-neox-20b,64,1,3141.0,1.0,1,H100,1697105023191,1697105026332,120,84.0,20.0,"[5, 714, 69, 53, 65, 49, 62, 334, 72, 56, 56, 62, 59, 619, 75, 65, 51, 61, 483, 131]","[1697105023196, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024146, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025541, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332]"
374,863,9,[],200,EleutherAI/gpt-neox-20b,64,1,981.0,1.0,1,H100,1697105027634,1697105028615,120,10.0,1.0,"[142, 839]","[1697105027776, 1697105028615]"
375,284,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105025467,1697105027572,120,,,"[5, 729, 131, 65, 49, 303, 55, 54, 69, 64]","[1697105025472, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026804, 1697105026858, 1697105026927, 1697105026991]"
376,25,6,[],200,EleutherAI/gpt-neox-20b,64,1,633.0,1.0,1,H100,1697105023191,1697105023824,120,12.0,1.0,"[9, 624]","[1697105023200, 1697105023824]"
377,611,7,[],200,EleutherAI/gpt-neox-20b,64,1,577.0,1.0,1,H100,1697105023825,1697105024402,120,14.0,1.0,"[17, 560]","[1697105023842, 1697105024402]"
378,386,8,[],200,EleutherAI/gpt-neox-20b,64,1,1315.0,1.0,1,H100,1697105024403,1697105025718,120,140.0,6.0,"[5, 1058, 74, 66, 51, 61]","[1697105024408, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718]"
379,654,3,[],200,EleutherAI/gpt-neox-20b,64,1,980.0,1.0,1,H100,1697105017320,1697105018300,120,47.0,4.0,"[5, 829, 76, 69]","[1697105017325, 1697105018154, 1697105018230, 1697105018299]"
380,39,9,[],200,EleutherAI/gpt-neox-20b,64,1,959.0,1.0,1,H100,1697105025719,1697105026678,120,8.0,1.0,"[5, 954]","[1697105025724, 1697105026678]"
381,717,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026679,1697105027572,120,,,[8],[1697105026687]
382,172,5,[],200,EleutherAI/gpt-neox-20b,64,1,345.0,1.0,1,H100,1697105026333,1697105026678,120,19.0,1.0,"[17, 328]","[1697105026350, 1697105026678]"
383,372,11,[],200,EleutherAI/gpt-neox-20b,64,1,3670.0,1.0,1,H100,1697105027575,1697105031245,120,874.0,25.0,"[121, 1036, 55, 45, 44, 55, 487, 70, 68, 66, 62, 49, 61, 228, 61, 48, 60, 48, 48, 252, 49, 450, 70, 69, 68]","[1697105027696, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245]"
384,722,4,[],200,EleutherAI/gpt-neox-20b,64,1,639.0,1.0,1,H100,1697105020986,1697105021625,120,39.0,1.0,"[16, 623]","[1697105021002, 1697105021625]"
385,494,5,[],200,EleutherAI/gpt-neox-20b,64,1,1812.0,1.0,1,H100,1697105021626,1697105023438,120,6.0,10.0,"[5, 814, 63, 52, 44, 586, 73, 67, 52, 56]","[1697105021631, 1697105022445, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438]"
386,732,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026679,1697105027572,120,,,[16],[1697105026695]
387,503,7,[],200,EleutherAI/gpt-neox-20b,64,1,2712.0,1.0,1,H100,1697105027575,1697105030287,120,109.0,20.0,"[29, 358, 770, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 227, 61, 49, 60, 48, 48]","[1697105027604, 1697105027962, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030021, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287]"
388,432,4,[],200,EleutherAI/gpt-neox-20b,64,1,717.0,1.0,1,H100,1697105018301,1697105019018,120,13.0,1.0,"[4, 713]","[1697105018305, 1697105019018]"
389,85,5,[],200,EleutherAI/gpt-neox-20b,64,1,2447.0,1.0,1,H100,1697105019019,1697105021466,120,88.0,20.0,"[12, 888, 122, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 49, 61, 59, 47, 48]","[1697105019031, 1697105019919, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466]"
390,565,4,[],200,EleutherAI/gpt-neox-20b,64,1,2267.0,1.0,1,H100,1697105020178,1697105022445,120,91.0,20.0,"[4, 424, 70, 66, 67, 63, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 55, 52, 546]","[1697105020182, 1697105020606, 1697105020676, 1697105020742, 1697105020809, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899, 1697105022445]"
391,155,6,[],200,EleutherAI/gpt-neox-20b,64,1,3488.0,1.0,1,H100,1697105023439,1697105026927,120,90.0,20.0,"[5, 1098, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69]","[1697105023444, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927]"
392,312,5,[],200,EleutherAI/gpt-neox-20b,64,1,589.0,1.0,1,H100,1697105022446,1697105023035,120,23.0,1.0,"[20, 569]","[1697105022466, 1697105023035]"
393,792,6,[],200,EleutherAI/gpt-neox-20b,64,1,894.0,1.0,1,H100,1697105021467,1697105022361,120,11.0,1.0,"[8, 886]","[1697105021475, 1697105022361]"
394,446,7,[],200,EleutherAI/gpt-neox-20b,64,1,673.0,1.0,1,H100,1697105022362,1697105023035,120,26.0,1.0,"[13, 660]","[1697105022375, 1697105023035]"
395,223,8,[],200,EleutherAI/gpt-neox-20b,64,1,788.0,1.0,1,H100,1697105023036,1697105023824,120,16.0,1.0,"[12, 776]","[1697105023048, 1697105023824]"
396,807,9,[],200,EleutherAI/gpt-neox-20b,64,1,3102.0,1.0,1,H100,1697105023825,1697105026927,120,90.0,20.0,"[9, 708, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69]","[1697105023834, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927]"
397,60,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024148,1697105027573,120,,,"[8, 386, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105024156, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
398,116,5,[],200,EleutherAI/gpt-neox-20b,64,1,894.0,1.0,1,H100,1697105021467,1697105022361,120,23.0,1.0,"[8, 886]","[1697105021475, 1697105022361]"
399,811,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105022362,1697105027572,120,,,"[13, 815, 73, 67, 52, 56, 472, 69, 53, 65, 50, 61, 334, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 54, 69, 65]","[1697105022375, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026857, 1697105026926, 1697105026991]"
400,336,7,[],200,EleutherAI/gpt-neox-20b,64,1,1464.0,1.0,1,H100,1697105022446,1697105023910,120,58.0,7.0,"[4, 740, 73, 67, 52, 56, 472]","[1697105022450, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910]"
401,777,7,[],200,EleutherAI/gpt-neox-20b,64,1,369.0,1.0,1,H100,1697105024033,1697105024402,120,9.0,1.0,"[5, 364]","[1697105024038, 1697105024402]"
402,432,8,[],200,EleutherAI/gpt-neox-20b,64,1,842.0,1.0,1,H100,1697105024403,1697105025245,120,13.0,1.0,"[13, 829]","[1697105024416, 1697105025245]"
403,180,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105025246,1697105027572,120,,,"[8, 947, 131, 65, 49, 303, 55, 54, 69, 64]","[1697105025254, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026804, 1697105026858, 1697105026927, 1697105026991]"
404,467,7,[],200,EleutherAI/gpt-neox-20b,64,1,2964.0,1.0,1,H100,1697105027575,1697105030539,120,93.0,20.0,"[109, 1048, 55, 45, 44, 55, 487, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252]","[1697105027684, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
405,8,7,[],200,EleutherAI/gpt-neox-20b,64,1,817.0,1.0,1,H100,1697105022446,1697105023263,120,39.0,3.0,"[4, 740, 73]","[1697105022450, 1697105023190, 1697105023263]"
406,243,8,[],200,EleutherAI/gpt-neox-20b,64,1,637.0,1.0,1,H100,1697105030540,1697105031177,120,67.0,4.0,"[13, 485, 70, 69]","[1697105030553, 1697105031038, 1697105031108, 1697105031177]"
407,86,5,[],200,EleutherAI/gpt-neox-20b,64,1,2806.0,1.0,1,H100,1697105021736,1697105024542,120,335.0,17.0,"[4, 705, 63, 52, 44, 586, 73, 67, 52, 56, 472, 69, 53, 65, 50, 61, 334]","[1697105021740, 1697105022445, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542]"
408,897,8,[],200,EleutherAI/gpt-neox-20b,64,1,491.0,1.0,1,H100,1697105023911,1697105024402,120,9.0,1.0,"[9, 482]","[1697105023920, 1697105024402]"
409,670,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024403,1697105027573,120,,,"[9, 1054, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105024412, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
410,826,9,[],200,EleutherAI/gpt-neox-20b,64,1,2385.0,1.0,1,H100,1697105031178,1697105033563,120,87.0,20.0,"[4, 629, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 60, 49, 48, 60]","[1697105031182, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562]"
411,788,6,[],200,EleutherAI/gpt-neox-20b,64,1,702.0,1.0,1,H100,1697105024543,1697105025245,120,31.0,1.0,"[5, 697]","[1697105024548, 1697105025245]"
412,899,10,[],200,EleutherAI/gpt-neox-20b,64,1,2435.0,1.0,1,H100,1697105030540,1697105032975,120,100.0,20.0,"[25, 473, 70, 69, 68, 57, 509, 71, 56, 55, 65, 64, 62, 62, 306, 163, 60, 72, 65, 63]","[1697105030565, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031938, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975]"
413,303,7,[],200,EleutherAI/gpt-neox-20b,64,1,2949.0,1.0,1,H100,1697105023383,1697105026332,120,88.0,20.0,"[5, 522, 69, 53, 65, 49, 62, 334, 72, 56, 56, 62, 59, 619, 75, 65, 51, 61, 483, 131]","[1697105023388, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024146, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025541, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332]"
414,573,10,[],200,EleutherAI/gpt-neox-20b,64,1,1086.0,1.0,1,H100,1697105033564,1697105034650,120,874.0,2.0,"[12, 1074]","[1697105033576, 1697105034650]"
415,37,9,[],200,EleutherAI/gpt-neox-20b,64,1,894.0,1.0,1,H100,1697105021467,1697105022361,120,20.0,1.0,"[12, 882]","[1697105021479, 1697105022361]"
416,342,11,[],200,EleutherAI/gpt-neox-20b,64,1,2178.0,1.0,1,H100,1697105034651,1697105036829,120,364.0,14.0,"[5, 502, 50, 51, 58, 55, 477, 63, 62, 58, 56, 602, 70, 69]","[1697105034656, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829]"
417,734,10,[],200,EleutherAI/gpt-neox-20b,64,1,1076.0,1.0,1,H100,1697105022362,1697105023438,120,100.0,6.0,"[5, 823, 73, 67, 52, 56]","[1697105022367, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438]"
418,853,14,[],200,EleutherAI/gpt-neox-20b,64,1,3113.0,1.0,1,H100,1697105032976,1697105036089,120,364.0,22.0,"[20, 851, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 64, 61, 58, 57]","[1697105032996, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089]"
419,920,2,[],200,EleutherAI/gpt-neox-20b,64,1,1457.0,1.0,1,H100,1697105015240,1697105016697,120,96.0,4.0,"[50, 833, 48, 526]","[1697105015290, 1697105016123, 1697105016171, 1697105016697]"
420,141,12,[],200,EleutherAI/gpt-neox-20b,64,1,2315.0,1.0,1,H100,1697105031248,1697105033563,120,89.0,20.0,"[6, 557, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 60, 49, 48, 60]","[1697105031254, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562]"
421,582,3,[],200,EleutherAI/gpt-neox-20b,64,1,434.0,1.0,1,H100,1697105016699,1697105017133,120,19.0,1.0,"[4, 430]","[1697105016703, 1697105017133]"
422,394,11,[],200,EleutherAI/gpt-neox-20b,64,1,962.0,1.0,1,H100,1697105023440,1697105024402,120,11.0,1.0,"[4, 958]","[1697105023444, 1697105024402]"
423,351,4,[],200,EleutherAI/gpt-neox-20b,64,1,1289.0,1.0,1,H100,1697105017134,1697105018423,120,216.0,6.0,"[5, 1015, 75, 70, 69, 55]","[1697105017139, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423]"
424,164,12,[],200,EleutherAI/gpt-neox-20b,64,1,842.0,1.0,1,H100,1697105024403,1697105025245,120,15.0,1.0,"[17, 825]","[1697105024420, 1697105025245]"
425,748,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105025246,1697105027572,120,,,"[4, 951, 131, 65, 49, 303, 55, 54, 69, 64]","[1697105025250, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026804, 1697105026858, 1697105026927, 1697105026991]"
426,116,14,[],200,EleutherAI/gpt-neox-20b,64,1,521.0,1.0,1,H100,1697105037425,1697105037946,120,23.0,1.0,"[49, 472]","[1697105037474, 1697105037946]"
427,700,15,[],200,EleutherAI/gpt-neox-20b,64,1,4207.0,1.0,1,H100,1697105037947,1697105042154,120,140.0,33.0,"[12, 681, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58]","[1697105037959, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154]"
428,579,5,[],200,EleutherAI/gpt-neox-20b,64,1,589.0,1.0,1,H100,1697105022446,1697105023035,120,19.0,1.0,"[20, 569]","[1697105022466, 1697105023035]"
429,528,14,[],200,EleutherAI/gpt-neox-20b,64,1,2713.0,1.0,1,H100,1697105027574,1697105030287,120,52.0,20.0,"[5, 383, 769, 56, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 227, 61, 49, 60, 48, 48]","[1697105027579, 1697105027962, 1697105028731, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030021, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287]"
430,731,13,[],200,EleutherAI/gpt-neox-20b,64,1,3331.0,1.0,1,H100,1697105033564,1697105036895,120,89.0,20.0,"[4, 1082, 65, 59, 58, 57, 269, 51, 50, 58, 55, 477, 64, 61, 58, 57, 601, 70, 69, 66]","[1697105033568, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035209, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895]"
431,182,15,[],200,EleutherAI/gpt-neox-20b,64,1,3828.0,1.0,1,H100,1697105030288,1697105034116,120,47.0,31.0,"[21, 729, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55]","[1697105030309, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116]"
432,9,5,[],200,EleutherAI/gpt-neox-20b,64,1,3032.0,1.0,1,H100,1697105022509,1697105025541,120,85.0,20.0,"[7, 674, 73, 67, 52, 56, 472, 69, 53, 64, 50, 61, 335, 72, 56, 56, 62, 59, 619, 75]","[1697105022516, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025541]"
433,895,6,[],200,EleutherAI/gpt-neox-20b,64,1,788.0,1.0,1,H100,1697105023036,1697105023824,120,15.0,1.0,"[12, 776]","[1697105023048, 1697105023824]"
434,671,7,[],200,EleutherAI/gpt-neox-20b,64,1,577.0,1.0,1,H100,1697105023825,1697105024402,120,12.0,1.0,"[13, 564]","[1697105023838, 1697105024402]"
435,324,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024403,1697105027573,120,,,"[9, 1054, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105024412, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
436,577,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026928,1697105027572,120,,,[4],[1697105026932]
437,209,11,[],200,EleutherAI/gpt-neox-20b,64,1,1040.0,1.0,1,H100,1697105027575,1697105028615,120,20.0,1.0,"[137, 903]","[1697105027712, 1697105028615]"
438,905,12,[],200,EleutherAI/gpt-neox-20b,64,1,672.0,1.0,1,H100,1697105028616,1697105029288,120,11.0,1.0,"[13, 659]","[1697105028629, 1697105029288]"
439,562,13,[],200,EleutherAI/gpt-neox-20b,64,1,4827.0,1.0,1,H100,1697105029289,1697105034116,120,67.0,39.0,"[5, 728, 61, 48, 60, 48, 48, 252, 49, 450, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 72, 66, 62, 371, 60, 49, 48, 60, 284, 62, 59, 47, 47, 55]","[1697105029294, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032846, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033846, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116]"
440,400,6,[],200,EleutherAI/gpt-neox-20b,64,1,1796.0,1.0,1,H100,1697105021467,1697105023263,120,123.0,7.0,"[4, 974, 59, 56, 44, 586, 73]","[1697105021471, 1697105022445, 1697105022504, 1697105022560, 1697105022604, 1697105023190, 1697105023263]"
441,126,4,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105023264,1697105023824,120,19.0,1.0,"[4, 556]","[1697105023268, 1697105023824]"
442,100,9,[],200,EleutherAI/gpt-neox-20b,64,1,2447.0,1.0,1,H100,1697105027575,1697105030022,120,732.0,14.0,"[125, 1032, 55, 45, 44, 55, 487, 70, 68, 66, 62, 49, 61, 228]","[1697105027700, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022]"
443,597,10,[],200,EleutherAI/gpt-neox-20b,64,1,345.0,1.0,1,H100,1697105026333,1697105026678,120,39.0,1.0,"[17, 328]","[1697105026350, 1697105026678]"
444,345,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026679,1697105027572,120,,,[20],[1697105026699]
445,161,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105013528,1697105015236,120,,,"[9, 999, 46, 169, 58, 56]","[1697105013537, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
446,927,12,[],200,EleutherAI/gpt-neox-20b,64,1,2964.0,1.0,1,H100,1697105027575,1697105030539,120,83.0,20.0,"[117, 1040, 55, 45, 44, 55, 487, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252]","[1697105027692, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
447,794,8,[],200,EleutherAI/gpt-neox-20b,64,1,360.0,1.0,1,H100,1697105032913,1697105033273,120,11.0,1.0,"[5, 354]","[1697105032918, 1697105033272]"
448,449,9,[],200,EleutherAI/gpt-neox-20b,64,1,2700.0,1.0,1,H100,1697105033274,1697105035974,120,86.0,20.0,"[4, 569, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 64, 61]","[1697105033278, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974]"
449,148,7,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105023264,1697105023824,120,16.0,1.0,"[8, 552]","[1697105023272, 1697105023824]"
450,734,8,[],200,EleutherAI/gpt-neox-20b,64,1,963.0,1.0,1,H100,1697105023825,1697105024788,120,100.0,6.0,"[13, 704, 72, 56, 56, 62]","[1697105023838, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788]"
451,828,5,[],200,EleutherAI/gpt-neox-20b,64,1,963.0,1.0,1,H100,1697105023825,1697105024788,120,182.0,6.0,"[17, 700, 72, 56, 56, 62]","[1697105023842, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788]"
452,679,10,[],200,EleutherAI/gpt-neox-20b,64,1,453.0,1.0,1,H100,1697105030023,1697105030476,120,15.0,1.0,"[8, 445]","[1697105030031, 1697105030476]"
453,837,2,[],200,EleutherAI/gpt-neox-20b,64,1,3246.0,1.0,1,H100,1697105015240,1697105018486,120,85.0,20.0,"[54, 829, 48, 526, 62, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63]","[1697105015294, 1697105016123, 1697105016171, 1697105016697, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
454,218,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035975,1697105037422,120,,,"[12, 703, 71, 68, 66, 58]","[1697105035987, 1697105036690, 1697105036761, 1697105036829, 1697105036895, 1697105036953]"
455,702,13,[],200,EleutherAI/gpt-neox-20b,64,1,2435.0,1.0,1,H100,1697105030540,1697105032975,120,89.0,20.0,"[5, 493, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62]","[1697105030545, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974]"
456,492,3,[],200,EleutherAI/gpt-neox-20b,64,1,2497.0,1.0,1,H100,1697105018487,1697105020984,120,47.0,20.0,"[13, 611, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62]","[1697105018500, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984]"
457,474,5,[],200,EleutherAI/gpt-neox-20b,64,1,4566.0,1.0,1,H100,1697105017941,1697105022507,120,109.0,33.0,"[9, 1161, 59, 47, 57, 55, 54, 536, 122, 70, 66, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 54, 53, 545, 63]","[1697105017950, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019919, 1697105020041, 1697105020111, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022507]"
458,467,5,[],200,EleutherAI/gpt-neox-20b,64,1,3061.0,1.0,1,H100,1697105019384,1697105022445,120,93.0,20.0,"[10, 1212, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 48, 47, 221, 48, 57, 55, 52, 546]","[1697105019394, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021419, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899, 1697105022445]"
459,763,10,[],200,EleutherAI/gpt-neox-20b,64,1,356.0,1.0,1,H100,1697105027575,1697105027931,120,20.0,1.0,"[53, 303]","[1697105027628, 1697105027931]"
460,480,6,[],200,EleutherAI/gpt-neox-20b,64,1,1149.0,1.0,1,H100,1697105019385,1697105020534,120,26.0,1.0,"[12, 1137]","[1697105019397, 1697105020534]"
461,537,11,[],200,EleutherAI/gpt-neox-20b,64,1,2605.0,1.0,1,H100,1697105027934,1697105030539,120,83.0,20.0,"[20, 778, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 227, 61, 49, 60, 48, 48, 252]","[1697105027954, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030021, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
462,257,7,[],200,EleutherAI/gpt-neox-20b,64,1,603.0,1.0,1,H100,1697105020535,1697105021138,120,14.0,1.0,"[8, 595]","[1697105020543, 1697105021138]"
463,237,6,[],200,EleutherAI/gpt-neox-20b,64,1,3094.0,1.0,1,H100,1697105022446,1697105025540,120,87.0,20.0,"[8, 736, 73, 67, 52, 56, 472, 69, 53, 64, 51, 61, 334, 72, 56, 56, 62, 59, 619, 74]","[1697105022454, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540]"
464,821,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105025541,1697105027572,120,,,"[5, 655, 131, 65, 49, 303, 55, 54, 69, 64]","[1697105025546, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026804, 1697105026858, 1697105026927, 1697105026991]"
465,572,8,[],200,EleutherAI/gpt-neox-20b,64,1,357.0,1.0,1,H100,1697105027574,1697105027931,120,16.0,1.0,"[50, 307]","[1697105027624, 1697105027931]"
466,342,9,[],200,EleutherAI/gpt-neox-20b,64,1,2089.0,1.0,1,H100,1697105027933,1697105030022,120,364.0,14.0,"[21, 778, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228]","[1697105027954, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022]"
467,10,5,[],200,EleutherAI/gpt-neox-20b,64,1,1617.0,1.0,1,H100,1697105018424,1697105020041,120,563.0,9.0,"[4, 683, 59, 47, 57, 55, 54, 536, 122]","[1697105018428, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019919, 1697105020041]"
468,767,3,[],200,EleutherAI/gpt-neox-20b,64,1,513.0,1.0,1,H100,1697105021848,1697105022361,120,11.0,1.0,"[8, 505]","[1697105021856, 1697105022361]"
469,708,6,[],200,EleutherAI/gpt-neox-20b,64,1,491.0,1.0,1,H100,1697105020043,1697105020534,120,140.0,1.0,"[4, 487]","[1697105020047, 1697105020534]"
470,890,4,[],200,EleutherAI/gpt-neox-20b,64,1,2949.0,1.0,1,H100,1697105023383,1697105026332,120,93.0,20.0,"[13, 514, 69, 53, 65, 49, 62, 334, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131]","[1697105023396, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024146, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332]"
471,340,7,[],200,EleutherAI/gpt-neox-20b,64,1,2847.0,1.0,1,H100,1697105020535,1697105023382,120,85.0,20.0,"[4, 663, 49, 61, 59, 47, 48, 221, 48, 57, 54, 53, 546, 63, 52, 44, 586, 73, 67, 52]","[1697105020539, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382]"
472,191,12,[],200,EleutherAI/gpt-neox-20b,64,1,2435.0,1.0,1,H100,1697105030540,1697105032975,120,85.0,20.0,"[29, 469, 70, 69, 68, 57, 509, 71, 56, 55, 65, 64, 62, 62, 306, 163, 60, 72, 65, 63]","[1697105030569, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031938, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975]"
473,112,8,[],200,EleutherAI/gpt-neox-20b,64,1,527.0,1.0,1,H100,1697105023383,1697105023910,120,16.0,2.0,"[5, 522]","[1697105023388, 1697105023910]"
474,695,9,[],200,EleutherAI/gpt-neox-20b,64,1,3016.0,1.0,1,H100,1697105023911,1697105026927,120,92.0,20.0,"[4, 627, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69]","[1697105023915, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927]"
475,467,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026928,1697105027572,120,,,[4],[1697105026932]
476,120,11,[],200,EleutherAI/gpt-neox-20b,64,1,356.0,1.0,1,H100,1697105027575,1697105027931,120,17.0,1.0,"[25, 331]","[1697105027600, 1697105027931]"
477,825,12,[],200,EleutherAI/gpt-neox-20b,64,1,2606.0,1.0,1,H100,1697105027933,1697105030539,120,96.0,20.0,"[6, 793, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 61, 48, 60, 48, 48, 252]","[1697105027939, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
478,886,16,[],200,EleutherAI/gpt-neox-20b,64,1,976.0,1.0,1,H100,1697105034117,1697105035093,120,17.0,1.0,"[5, 970]","[1697105034122, 1697105035092]"
479,567,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105025541,1697105027572,120,,,"[9, 651, 131, 65, 49, 303, 55, 54, 69, 64]","[1697105025550, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026804, 1697105026858, 1697105026927, 1697105026991]"
480,540,17,[],200,EleutherAI/gpt-neox-20b,64,1,939.0,1.0,1,H100,1697105035093,1697105036032,120,140.0,5.0,"[5, 751, 64, 61, 58]","[1697105035098, 1697105035849, 1697105035913, 1697105035974, 1697105036032]"
481,389,3,[],200,EleutherAI/gpt-neox-20b,64,1,531.0,1.0,1,H100,1697105018487,1697105019018,120,8.0,1.0,"[25, 506]","[1697105018512, 1697105019018]"
482,255,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105031249,1697105037422,120,,,"[20, 542, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 61, 48, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66, 58]","[1697105031269, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033406, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
483,572,8,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105023264,1697105023824,120,16.0,1.0,"[12, 548]","[1697105023276, 1697105023824]"
484,478,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105030540,1697105037423,120,,,"[29, 469, 70, 69, 68, 57, 509, 71, 56, 55, 65, 64, 62, 62, 306, 163, 60, 72, 65, 63, 370, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 50, 59, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66, 58]","[1697105030569, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031938, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035258, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
485,342,9,[],200,EleutherAI/gpt-neox-20b,64,1,2507.0,1.0,1,H100,1697105023825,1697105026332,120,364.0,14.0,"[9, 708, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131]","[1697105023834, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332]"
486,737,10,[],200,EleutherAI/gpt-neox-20b,64,1,1157.0,1.0,1,H100,1697105027575,1697105028732,120,216.0,2.0,"[193, 847, 117]","[1697105027768, 1697105028615, 1697105028732]"
487,332,5,[],200,EleutherAI/gpt-neox-20b,64,1,589.0,1.0,1,H100,1697105022446,1697105023035,120,39.0,1.0,"[24, 565]","[1697105022470, 1697105023035]"
488,102,6,[],200,EleutherAI/gpt-neox-20b,64,1,3296.0,1.0,1,H100,1697105023036,1697105026332,120,84.0,20.0,"[4, 870, 69, 53, 65, 49, 61, 335, 72, 56, 56, 62, 59, 619, 75, 65, 51, 61, 483, 131]","[1697105023040, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025541, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332]"
489,506,11,[],200,EleutherAI/gpt-neox-20b,64,1,556.0,1.0,1,H100,1697105028733,1697105029289,120,16.0,1.0,"[8, 548]","[1697105028741, 1697105029289]"
490,167,12,[],200,EleutherAI/gpt-neox-20b,64,1,2832.0,1.0,1,H100,1697105029290,1697105032122,120,88.0,20.0,"[8, 724, 61, 48, 60, 48, 48, 252, 49, 450, 70, 69, 70, 55, 509, 71, 55, 56, 65, 64]","[1697105029298, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031247, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122]"
491,29,5,[],200,EleutherAI/gpt-neox-20b,64,1,905.0,1.0,1,H100,1697105018424,1697105019329,120,161.0,6.0,"[4, 683, 59, 47, 57, 55]","[1697105018428, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329]"
492,609,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 9.88 GiB. GPU 0 has a total capacty of 79.11 GiB of which 7.06 GiB is free. Process 1645736 has 72.04 GiB memory in use. Of the allocated memory 50.19 GiB is allocated by PyTorch, and 20.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105012910,1697105015236,120,,,"[5, 1621, 46, 169, 58, 56]","[1697105012915, 1697105014536, 1697105014582, 1697105014751, 1697105014809, 1697105014865]"
493,864,13,[],200,EleutherAI/gpt-neox-20b,64,1,2592.0,1.0,1,H100,1697105032123,1697105034715,120,83.0,20.0,"[4, 425, 163, 60, 72, 65, 63, 370, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65]","[1697105032127, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715]"
494,262,2,[],200,EleutherAI/gpt-neox-20b,64,1,820.0,1.0,1,H100,1697105015240,1697105016060,120,39.0,1.0,"[58, 762]","[1697105015298, 1697105016060]"
495,38,3,[],200,EleutherAI/gpt-neox-20b,64,1,3048.0,1.0,1,H100,1697105016063,1697105019111,120,88.0,20.0,"[11, 624, 61, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 76, 69, 70, 54, 63, 53, 572]","[1697105016074, 1697105016698, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111]"
496,528,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105034716,1697105037423,120,,,"[8, 434, 50, 51, 58, 55, 477, 63, 62, 58, 57, 601, 70, 69, 66, 58]","[1697105034724, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
497,926,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026333,1697105027572,120,,,"[13, 403, 55, 54, 69, 64]","[1697105026346, 1697105026749, 1697105026804, 1697105026858, 1697105026927, 1697105026991]"
498,620,4,[],200,EleutherAI/gpt-neox-20b,64,1,1494.0,1.0,1,H100,1697105019112,1697105020606,120,100.0,8.0,"[4, 802, 123, 69, 67, 62, 60, 307]","[1697105019116, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606]"
499,398,5,[],200,EleutherAI/gpt-neox-20b,64,1,2775.0,1.0,1,H100,1697105020607,1697105023382,120,87.0,20.0,"[5, 590, 49, 61, 59, 47, 48, 221, 48, 57, 54, 53, 545, 64, 52, 44, 586, 73, 67, 52]","[1697105020612, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382]"
500,449,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105025246,1697105027572,120,,,"[5, 950, 131, 65, 49, 303, 55, 54, 69, 64]","[1697105025251, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026804, 1697105026858, 1697105026927, 1697105026991]"
501,703,11,[],200,EleutherAI/gpt-neox-20b,64,1,1040.0,1.0,1,H100,1697105027575,1697105028615,120,12.0,1.0,"[121, 919]","[1697105027696, 1697105028615]"
502,629,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105028618,1697105037423,120,,,"[31, 769, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 305, 164, 59, 72, 66, 62, 371, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66, 58]","[1697105028649, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032551, 1697105032715, 1697105032774, 1697105032846, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
503,52,6,[],200,EleutherAI/gpt-neox-20b,64,1,764.0,1.0,1,H100,1697105023383,1697105024147,120,58.0,6.0,"[21, 506, 69, 53, 65, 50]","[1697105023404, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147]"
504,74,8,[],200,EleutherAI/gpt-neox-20b,64,1,2949.0,1.0,1,H100,1697105023383,1697105026332,120,88.0,20.0,"[9, 518, 69, 53, 65, 50, 61, 334, 72, 56, 56, 62, 59, 619, 75, 65, 51, 61, 483, 131]","[1697105023392, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025541, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332]"
505,747,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024148,1697105027572,120,,,"[8, 386, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105024156, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
506,527,5,[],200,EleutherAI/gpt-neox-20b,64,1,6545.0,1.0,1,H100,1697105019112,1697105025657,120,732.0,50.0,"[12, 795, 122, 70, 66, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 49, 61, 59, 47, 48, 221, 48, 57, 54, 53, 546, 63, 52, 44, 586, 73, 67, 52, 56, 472, 69, 53, 65, 49, 61, 335, 72, 56, 56, 62, 59, 619, 74, 66, 51]","[1697105019124, 1697105019919, 1697105020041, 1697105020111, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657]"
507,70,4,[],200,EleutherAI/gpt-neox-20b,64,1,639.0,1.0,1,H100,1697105020986,1697105021625,120,39.0,1.0,"[12, 627]","[1697105020998, 1697105021625]"
508,776,5,[],200,EleutherAI/gpt-neox-20b,64,1,819.0,1.0,1,H100,1697105021626,1697105022445,120,67.0,2.0,"[5, 814]","[1697105021631, 1697105022445]"
509,430,6,[],200,EleutherAI/gpt-neox-20b,64,1,589.0,1.0,1,H100,1697105022446,1697105023035,120,15.0,1.0,"[12, 577]","[1697105022458, 1697105023035]"
510,780,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026333,1697105027572,120,,,"[9, 407, 54, 55, 69, 64]","[1697105026342, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
511,433,10,[],200,EleutherAI/gpt-neox-20b,64,1,388.0,1.0,1,H100,1697105027574,1697105027962,120,109.0,2.0,"[13, 375]","[1697105027587, 1697105027962]"
512,408,8,[],200,EleutherAI/gpt-neox-20b,64,1,356.0,1.0,1,H100,1697105027575,1697105027931,120,16.0,1.0,"[33, 323]","[1697105027608, 1697105027931]"
513,175,9,[],200,EleutherAI/gpt-neox-20b,64,1,1554.0,1.0,1,H100,1697105027934,1697105029488,120,140.0,8.0,"[24, 774, 55, 45, 44, 55, 487, 70]","[1697105027958, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488]"
514,737,10,[],200,EleutherAI/gpt-neox-20b,64,1,533.0,1.0,1,H100,1697105029489,1697105030022,120,216.0,2.0,"[5, 460, 68]","[1697105029494, 1697105029954, 1697105030022]"
515,506,11,[],200,EleutherAI/gpt-neox-20b,64,1,453.0,1.0,1,H100,1697105030023,1697105030476,120,16.0,1.0,"[8, 445]","[1697105030031, 1697105030476]"
516,169,5,[],200,EleutherAI/gpt-neox-20b,64,1,640.0,1.0,1,H100,1697105020985,1697105021625,120,10.0,1.0,"[5, 635]","[1697105020990, 1697105021625]"
517,854,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026928,1697105027572,120,,,[8],[1697105026936]
518,623,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036090,1697105037423,120,,,[4],[1697105036094]
519,287,16,[],200,EleutherAI/gpt-neox-20b,64,1,521.0,1.0,1,H100,1697105037425,1697105037946,120,10.0,1.0,"[37, 484]","[1697105037462, 1697105037946]"
520,512,8,[],200,EleutherAI/gpt-neox-20b,64,1,356.0,1.0,1,H100,1697105027575,1697105027931,120,11.0,1.0,"[28, 328]","[1697105027603, 1697105027931]"
521,868,6,[],200,EleutherAI/gpt-neox-20b,64,1,3100.0,1.0,1,H100,1697105021626,1697105024726,120,85.0,20.0,"[9, 810, 63, 52, 44, 586, 73, 67, 52, 56, 472, 69, 53, 65, 50, 60, 335, 72, 56, 56]","[1697105021635, 1697105022445, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726]"
522,281,9,[],200,EleutherAI/gpt-neox-20b,64,1,682.0,1.0,1,H100,1697105027933,1697105028615,120,23.0,1.0,"[13, 669]","[1697105027946, 1697105028615]"
523,872,10,[],200,EleutherAI/gpt-neox-20b,64,1,2629.0,1.0,1,H100,1697105028616,1697105031245,120,91.0,20.0,"[17, 785, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68]","[1697105028633, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245]"
524,162,12,[],200,EleutherAI/gpt-neox-20b,64,1,2498.0,1.0,1,H100,1697105030477,1697105032975,120,90.0,20.0,"[8, 553, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62]","[1697105030485, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974]"
525,55,17,[],200,EleutherAI/gpt-neox-20b,64,1,616.0,1.0,1,H100,1697105037947,1697105038563,120,12.0,1.0,"[9, 607]","[1697105037956, 1697105038563]"
526,537,4,[],200,EleutherAI/gpt-neox-20b,64,1,3178.0,1.0,1,H100,1697105022362,1697105025540,120,83.0,20.0,"[9, 819, 73, 67, 52, 56, 472, 69, 53, 65, 50, 61, 334, 72, 56, 56, 62, 59, 619, 74]","[1697105022371, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540]"
527,42,4,[],200,EleutherAI/gpt-neox-20b,64,1,704.0,1.0,1,H100,1697105019019,1697105019723,120,10.0,1.0,"[16, 688]","[1697105019035, 1697105019723]"
528,742,5,[],200,EleutherAI/gpt-neox-20b,64,1,2721.0,1.0,1,H100,1697105019724,1697105022445,120,89.0,20.0,"[8, 874, 70, 66, 67, 63, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 55, 52, 546]","[1697105019732, 1697105020606, 1697105020676, 1697105020742, 1697105020809, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899, 1697105022445]"
529,616,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105038566,1697105058839,120,,,"[31, 818, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 60, 54, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 72, 65, 62, 59, 508, 70, 62, 58, 457, 74, 70, 55, 66, 64, 50, 451, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 75, 73, 71, 54, 55, 64, 618, 237, 74, 72, 70, 64, 311, 76, 74, 71, 56, 71, 66, 204, 70, 69, 69, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 66, 63, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 69, 67, 66, 51, 233, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 72, 54, 68, 66, 65, 427, 65, 64, 63]","[1697105038597, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043748, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045418, 1697105045468, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049428, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050816, 1697105050880, 1697105051191, 1697105051267, 1697105051341, 1697105051412, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051948, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054948, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057413, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
530,451,11,[],200,EleutherAI/gpt-neox-20b,64,1,486.0,1.0,1,H100,1697105030477,1697105030963,120,286.0,1.0,"[4, 482]","[1697105030481, 1697105030963]"
531,111,12,[],200,EleutherAI/gpt-neox-20b,64,1,1029.0,1.0,1,H100,1697105030964,1697105031993,120,79.0,5.0,"[9, 838, 71, 55, 56]","[1697105030973, 1697105031811, 1697105031882, 1697105031937, 1697105031993]"
532,168,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105025543,1697105027572,120,,,"[15, 643, 131, 65, 49, 303, 55, 54, 69, 64]","[1697105025558, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026804, 1697105026858, 1697105026927, 1697105026991]"
533,810,13,[],200,EleutherAI/gpt-neox-20b,64,1,2721.0,1.0,1,H100,1697105031994,1697105034715,120,91.0,20.0,"[4, 554, 163, 59, 73, 65, 63, 370, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65]","[1697105031998, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032975, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715]"
534,705,6,[],200,EleutherAI/gpt-neox-20b,64,1,3178.0,1.0,1,H100,1697105019330,1697105022508,120,79.0,27.0,"[4, 585, 122, 70, 66, 62, 60, 307, 69, 67, 66, 64, 50, 62, 218, 49, 60, 60, 47, 48, 221, 48, 57, 55, 52, 546, 62]","[1697105019334, 1697105019919, 1697105020041, 1697105020111, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021251, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899, 1697105022445, 1697105022507]"
535,646,11,[],200,EleutherAI/gpt-neox-20b,64,1,477.0,1.0,1,H100,1697105031249,1697105031726,120,14.0,1.0,"[16, 460]","[1697105031265, 1697105031725]"
536,837,13,[],200,EleutherAI/gpt-neox-20b,64,1,3162.0,1.0,1,H100,1697105037425,1697105040587,120,85.0,20.0,"[89, 1126, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53]","[1697105037514, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587]"
537,298,15,[],200,EleutherAI/gpt-neox-20b,64,1,521.0,1.0,1,H100,1697105037425,1697105037946,120,17.0,1.0,"[29, 492]","[1697105037454, 1697105037946]"
538,331,10,[],200,EleutherAI/gpt-neox-20b,64,1,1039.0,1.0,1,H100,1697105027576,1697105028615,120,26.0,1.0,"[196, 843]","[1697105027772, 1697105028615]"
539,869,6,[],200,EleutherAI/gpt-neox-20b,64,1,2158.0,1.0,1,H100,1697105027575,1697105029733,120,244.0,12.0,"[105, 1052, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49]","[1697105027680, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733]"
540,99,11,[],200,EleutherAI/gpt-neox-20b,64,1,672.0,1.0,1,H100,1697105028617,1697105029289,120,10.0,1.0,"[32, 639]","[1697105028649, 1697105029288]"
541,689,12,[],200,EleutherAI/gpt-neox-20b,64,1,664.0,1.0,1,H100,1697105029290,1697105029954,120,15.0,1.0,"[16, 648]","[1697105029306, 1697105029954]"
542,882,16,[],200,EleutherAI/gpt-neox-20b,64,1,1736.0,1.0,1,H100,1697105037947,1697105039683,120,345.0,11.0,"[4, 689, 58, 48, 55, 54, 560, 71, 68, 66, 63]","[1697105037951, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683]"
543,458,13,[],200,EleutherAI/gpt-neox-20b,64,1,521.0,1.0,1,H100,1697105029955,1697105030476,120,11.0,1.0,"[21, 500]","[1697105029976, 1697105030476]"
544,840,8,[],200,EleutherAI/gpt-neox-20b,64,1,486.0,1.0,1,H100,1697105021139,1697105021625,120,17.0,1.0,"[9, 477]","[1697105021148, 1697105021625]"
545,659,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105039685,1697105058840,120,,,"[12, 541, 68, 54, 64, 50, 60, 53, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 72, 65, 62, 59, 508, 70, 62, 58, 457, 74, 70, 55, 66, 64, 50, 451, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 75, 73, 71, 54, 55, 64, 618, 237, 74, 72, 70, 64, 311, 76, 75, 70, 56, 70, 67, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 66, 63, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 69, 67, 66, 51, 233, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 72, 54, 68, 66, 65, 427, 65, 63, 64]","[1697105039697, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040474, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043748, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045418, 1697105045468, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049428, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050816, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051412, 1697105051468, 1697105051538, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054948, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057413, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058221, 1697105058285]"
546,175,7,[],200,EleutherAI/gpt-neox-20b,64,1,1506.0,1.0,1,H100,1697105023036,1697105024542,120,140.0,8.0,"[4, 870, 69, 53, 65, 49, 62, 334]","[1697105023040, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024146, 1697105024208, 1697105024542]"
547,506,9,[],200,EleutherAI/gpt-neox-20b,64,1,456.0,1.0,1,H100,1697105024789,1697105025245,120,16.0,1.0,"[5, 451]","[1697105024794, 1697105025245]"
548,166,10,[],200,EleutherAI/gpt-neox-20b,64,1,867.0,1.0,1,H100,1697105025246,1697105026113,120,14.0,1.0,"[17, 850]","[1697105025263, 1697105026113]"
549,663,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026333,1697105027572,120,,,"[5, 411, 54, 55, 69, 64]","[1697105026338, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
550,439,14,[],200,EleutherAI/gpt-neox-20b,64,1,543.0,1.0,1,H100,1697105034716,1697105035259,120,13.0,4.0,"[4, 438, 50, 51]","[1697105034720, 1697105035158, 1697105035208, 1697105035259]"
551,54,8,[],200,EleutherAI/gpt-neox-20b,64,1,2964.0,1.0,1,H100,1697105027575,1697105030539,120,87.0,20.0,"[101, 1056, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 60, 49, 60, 48, 48, 252]","[1697105027676, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
552,209,15,[],200,EleutherAI/gpt-neox-20b,64,1,499.0,1.0,1,H100,1697105035260,1697105035759,120,20.0,1.0,"[12, 487]","[1697105035272, 1697105035759]"
553,337,7,[],200,EleutherAI/gpt-neox-20b,64,1,356.0,1.0,1,H100,1697105027575,1697105027931,120,12.0,1.0,"[8, 348]","[1697105027583, 1697105027931]"
554,927,8,[],200,EleutherAI/gpt-neox-20b,64,1,2606.0,1.0,1,H100,1697105027933,1697105030539,120,83.0,20.0,"[10, 789, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 60, 49, 60, 48, 48, 252]","[1697105027943, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
555,317,6,[],200,EleutherAI/gpt-neox-20b,64,1,6486.0,1.0,1,H100,1697105027575,1697105034061,120,244.0,50.0,"[105, 1052, 55, 45, 44, 55, 487, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 305, 164, 59, 72, 66, 62, 371, 60, 49, 48, 60, 285, 61, 59, 47, 47]","[1697105027680, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032551, 1697105032715, 1697105032774, 1697105032846, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061]"
556,316,6,[],200,EleutherAI/gpt-neox-20b,64,1,3082.0,1.0,1,H100,1697105020300,1697105023382,120,86.0,20.0,"[9, 893, 48, 61, 60, 47, 48, 221, 48, 57, 54, 53, 546, 61, 54, 44, 586, 73, 67, 52]","[1697105020309, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445, 1697105022506, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382]"
557,637,5,[],200,EleutherAI/gpt-neox-20b,64,1,3162.0,1.0,1,H100,1697105020985,1697105024147,120,96.0,20.0,"[13, 689, 48, 57, 54, 53, 545, 60, 56, 44, 586, 73, 67, 52, 56, 472, 69, 53, 64, 50]","[1697105020998, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022504, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024146]"
558,161,8,[],200,EleutherAI/gpt-neox-20b,64,1,1523.0,1.0,1,H100,1697105030288,1697105031811,120,109.0,7.0,"[5, 745, 70, 69, 68, 57, 509]","[1697105030293, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811]"
559,193,5,[],200,EleutherAI/gpt-neox-20b,64,1,2573.0,1.0,1,H100,1697105020809,1697105023382,120,79.0,20.0,"[5, 388, 49, 61, 59, 47, 48, 221, 48, 57, 54, 53, 545, 63, 53, 44, 586, 73, 67, 52]","[1697105020814, 1697105021202, 1697105021251, 1697105021312, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022507, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382]"
560,92,7,[],200,EleutherAI/gpt-neox-20b,64,1,2949.0,1.0,1,H100,1697105023383,1697105026332,120,85.0,20.0,"[9, 518, 69, 53, 65, 50, 61, 334, 72, 56, 56, 62, 59, 619, 75, 65, 51, 61, 483, 131]","[1697105023392, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025541, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332]"
561,252,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024727,1697105027571,120,,,"[4, 735, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105024731, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
562,855,7,[],200,EleutherAI/gpt-neox-20b,64,1,3095.0,1.0,1,H100,1697105022446,1697105025541,120,83.0,20.0,"[12, 732, 73, 67, 52, 56, 472, 69, 53, 64, 50, 61, 335, 72, 56, 56, 62, 59, 619, 74]","[1697105022458, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540]"
563,624,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105025542,1697105027572,120,,,"[16, 643, 131, 65, 49, 303, 55, 54, 69, 64]","[1697105025558, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026804, 1697105026858, 1697105026927, 1697105026991]"
564,673,2,[],200,EleutherAI/gpt-neox-20b,64,1,3248.0,1.0,1,H100,1697105015238,1697105018486,120,93.0,20.0,"[36, 848, 50, 525, 62, 57, 56, 54, 272, 63, 59, 55, 58, 54, 667, 75, 70, 69, 55, 63]","[1697105015274, 1697105016122, 1697105016172, 1697105016697, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018368, 1697105018423, 1697105018486]"
565,634,3,[],200,EleutherAI/gpt-neox-20b,64,1,429.0,1.0,1,H100,1697105016067,1697105016496,120,13.0,1.0,"[29, 400]","[1697105016096, 1697105016496]"
566,286,9,[],200,EleutherAI/gpt-neox-20b,64,1,2159.0,1.0,1,H100,1697105027574,1697105029733,120,161.0,12.0,"[94, 1064, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49]","[1697105027668, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733]"
567,723,11,[],200,EleutherAI/gpt-neox-20b,64,1,231.0,1.0,1,H100,1697105032185,1697105032416,120,14.0,1.0,"[5, 225]","[1697105032190, 1697105032415]"
568,492,12,[],200,EleutherAI/gpt-neox-20b,64,1,2843.0,1.0,1,H100,1697105032416,1697105035259,120,47.0,20.0,"[9, 920, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51]","[1697105032425, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259]"
569,401,4,[],200,EleutherAI/gpt-neox-20b,64,1,2879.0,1.0,1,H100,1697105021847,1697105024726,120,84.0,20.0,"[5, 593, 63, 52, 45, 585, 73, 67, 52, 56, 472, 69, 53, 65, 50, 61, 334, 72, 56, 56]","[1697105021852, 1697105022445, 1697105022508, 1697105022560, 1697105022605, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726]"
570,604,3,[],200,EleutherAI/gpt-neox-20b,64,1,1000.0,1.0,1,H100,1697105019111,1697105020111,120,161.0,4.0,"[5, 802, 123, 70]","[1697105019116, 1697105019918, 1697105020041, 1697105020111]"
571,375,4,[],200,EleutherAI/gpt-neox-20b,64,1,1681.0,1.0,1,H100,1697105020111,1697105021792,120,874.0,17.0,"[5, 490, 70, 66, 67, 63, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57]","[1697105020116, 1697105020606, 1697105020676, 1697105020742, 1697105020809, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792]"
572,29,5,[],200,EleutherAI/gpt-neox-20b,64,1,1397.0,1.0,1,H100,1697105021793,1697105023190,120,161.0,6.0,"[5, 647, 62, 53, 44, 586]","[1697105021798, 1697105022445, 1697105022507, 1697105022560, 1697105022604, 1697105023190]"
573,149,5,[],200,EleutherAI/gpt-neox-20b,64,1,1719.0,1.0,1,H100,1697105024727,1697105026446,120,563.0,10.0,"[16, 723, 74, 66, 51, 61, 483, 131, 65, 49]","[1697105024743, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446]"
574,705,6,[],200,EleutherAI/gpt-neox-20b,64,1,3800.0,1.0,1,H100,1697105023191,1697105026991,120,79.0,27.0,"[5, 714, 69, 53, 65, 49, 62, 334, 72, 56, 56, 62, 59, 619, 75, 65, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105023196, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024146, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025541, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
575,691,7,[],200,EleutherAI/gpt-neox-20b,64,1,345.0,1.0,1,H100,1697105026333,1697105026678,120,47.0,1.0,"[5, 340]","[1697105026338, 1697105026678]"
576,463,8,[],200,EleutherAI/gpt-neox-20b,64,1,683.0,1.0,1,H100,1697105026679,1697105027362,120,39.0,1.0,"[24, 659]","[1697105026703, 1697105027362]"
577,116,9,[],200,EleutherAI/gpt-neox-20b,64,1,270.0,1.0,1,H100,1697105027363,1697105027633,120,23.0,1.0,"[5, 265]","[1697105027368, 1697105027633]"
578,822,10,[],200,EleutherAI/gpt-neox-20b,64,1,2906.0,1.0,1,H100,1697105027633,1697105030539,120,88.0,20.0,"[147, 952, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 61, 48, 60, 48, 48, 252]","[1697105027780, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
579,154,13,[],200,EleutherAI/gpt-neox-20b,64,1,499.0,1.0,1,H100,1697105035260,1697105035759,120,13.0,1.0,"[16, 483]","[1697105035276, 1697105035759]"
580,850,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035760,1697105037422,120,,,"[13, 917, 70, 69, 66, 58]","[1697105035773, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
581,522,7,[],200,EleutherAI/gpt-neox-20b,64,1,220.0,1.0,1,H100,1697105029734,1697105029954,120,20.0,1.0,"[5, 215]","[1697105029739, 1697105029954]"
582,2,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036830,1697105037423,120,,,[5],[1697105036835]
583,701,13,[],200,EleutherAI/gpt-neox-20b,64,1,6251.0,1.0,1,H100,1697105037425,1697105043676,120,58.0,43.0,"[166, 1049, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520]","[1697105037591, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676]"
584,299,8,[],200,EleutherAI/gpt-neox-20b,64,1,521.0,1.0,1,H100,1697105029955,1697105030476,120,14.0,1.0,"[17, 504]","[1697105029972, 1697105030476]"
585,234,6,[],200,EleutherAI/gpt-neox-20b,64,1,3822.0,1.0,1,H100,1697105023036,1697105026858,120,457.0,25.0,"[8, 780, 86, 69, 53, 65, 49, 62, 334, 72, 56, 56, 62, 59, 619, 75, 65, 51, 61, 483, 131, 65, 49, 303, 54, 55]","[1697105023044, 1697105023824, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024146, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025541, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858]"
586,615,9,[],200,EleutherAI/gpt-neox-20b,64,1,3100.0,1.0,1,H100,1697105021626,1697105024726,120,93.0,20.0,"[13, 806, 60, 55, 44, 587, 72, 67, 52, 56, 472, 69, 53, 65, 50, 61, 334, 72, 56, 56]","[1697105021639, 1697105022445, 1697105022505, 1697105022560, 1697105022604, 1697105023191, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726]"
587,880,9,[],200,EleutherAI/gpt-neox-20b,64,1,561.0,1.0,1,H100,1697105030477,1697105031038,120,84.0,2.0,"[4, 557]","[1697105030481, 1697105031038]"
588,514,15,[],200,EleutherAI/gpt-neox-20b,64,1,3109.0,1.0,1,H100,1697105037425,1697105040534,120,85.0,20.0,"[45, 517, 653, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61]","[1697105037470, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534]"
589,660,10,[],200,EleutherAI/gpt-neox-20b,64,1,3022.0,1.0,1,H100,1697105031039,1697105034061,120,732.0,25.0,"[4, 768, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 60, 49, 48, 60, 285, 61, 59, 47, 47]","[1697105031043, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061]"
590,477,11,[],200,EleutherAI/gpt-neox-20b,64,1,6355.0,1.0,1,H100,1697105030540,1697105036895,120,244.0,50.0,"[21, 477, 70, 69, 68, 57, 509, 71, 56, 55, 65, 64, 62, 62, 306, 163, 60, 72, 65, 63, 370, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66]","[1697105030561, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031938, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895]"
591,11,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026859,1697105027572,120,,,[4],[1697105026863]
592,756,4,[],200,EleutherAI/gpt-neox-20b,64,1,395.0,1.0,1,H100,1697105020743,1697105021138,120,19.0,1.0,"[5, 390]","[1697105020748, 1697105021138]"
593,695,9,[],200,EleutherAI/gpt-neox-20b,64,1,2435.0,1.0,1,H100,1697105030540,1697105032975,120,92.0,20.0,"[21, 477, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 60, 72, 65, 63]","[1697105030561, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975]"
594,404,6,[],200,EleutherAI/gpt-neox-20b,64,1,3094.0,1.0,1,H100,1697105022446,1697105025540,120,87.0,20.0,"[8, 736, 73, 67, 52, 56, 472, 69, 53, 64, 51, 60, 335, 72, 56, 56, 62, 59, 619, 74]","[1697105022454, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024147, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540]"
595,417,5,[],200,EleutherAI/gpt-neox-20b,64,1,486.0,1.0,1,H100,1697105021139,1697105021625,120,17.0,1.0,"[9, 477]","[1697105021148, 1697105021625]"
596,186,6,[],200,EleutherAI/gpt-neox-20b,64,1,3221.0,1.0,1,H100,1697105021626,1697105024847,120,123.0,22.0,"[13, 806, 61, 54, 44, 586, 73, 67, 52, 56, 472, 69, 53, 65, 50, 60, 335, 72, 56, 56, 62, 59]","[1697105021639, 1697105022445, 1697105022506, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847]"
597,289,4,[],200,EleutherAI/gpt-neox-20b,64,1,2886.0,1.0,1,H100,1697105016497,1697105019383,120,89.0,20.0,"[10, 691, 63, 59, 55, 58, 54, 667, 75, 70, 70, 54, 63, 53, 572, 59, 47, 57, 55, 54]","[1697105016507, 1697105017198, 1697105017261, 1697105017320, 1697105017375, 1697105017433, 1697105017487, 1697105018154, 1697105018229, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383]"
598,835,6,[],200,EleutherAI/gpt-neox-20b,64,1,2712.0,1.0,1,H100,1697105027575,1697105030287,120,87.0,20.0,"[16, 371, 770, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 227, 61, 49, 60, 48, 48]","[1697105027591, 1697105027962, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030021, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287]"
599,287,18,[],200,EleutherAI/gpt-neox-20b,64,1,555.0,1.0,1,H100,1697105036034,1697105036589,120,10.0,1.0,"[4, 551]","[1697105036038, 1697105036589]"
600,59,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036590,1697105037423,120,,,[9],[1697105036599]
601,584,14,[],200,EleutherAI/gpt-neox-20b,64,1,724.0,1.0,1,H100,1697105040588,1697105041312,120,10.0,1.0,"[12, 712]","[1697105040600, 1697105041312]"
602,615,7,[],200,EleutherAI/gpt-neox-20b,64,1,2686.0,1.0,1,H100,1697105030288,1697105032974,120,93.0,20.0,"[13, 737, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62]","[1697105030301, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974]"
603,775,5,[],200,EleutherAI/gpt-neox-20b,64,1,505.0,1.0,1,H100,1697105019218,1697105019723,120,17.0,1.0,"[4, 501]","[1697105019222, 1697105019723]"
604,173,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105025541,1697105027572,120,,,"[5, 655, 131, 65, 49, 303, 55, 54, 69, 64]","[1697105025546, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026804, 1697105026858, 1697105026927, 1697105026991]"
605,547,6,[],200,EleutherAI/gpt-neox-20b,64,1,810.0,1.0,1,H100,1697105019724,1697105020534,120,12.0,1.0,"[8, 802]","[1697105019732, 1697105020534]"
606,200,7,[],200,EleutherAI/gpt-neox-20b,64,1,1200.0,1.0,1,H100,1697105020535,1697105021735,120,6.0,9.0,"[8, 659, 49, 60, 60, 47, 48, 221, 48]","[1697105020543, 1697105021202, 1697105021251, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735]"
607,878,8,[],200,EleutherAI/gpt-neox-20b,64,1,2990.0,1.0,1,H100,1697105021736,1697105024726,120,83.0,20.0,"[4, 705, 63, 52, 44, 586, 73, 67, 52, 56, 472, 69, 53, 65, 50, 61, 334, 72, 56, 56]","[1697105021740, 1697105022445, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726]"
608,642,20,[],200,EleutherAI/gpt-neox-20b,64,1,3109.0,1.0,1,H100,1697105037425,1697105040534,120,89.0,20.0,"[33, 529, 653, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61]","[1697105037458, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534]"
609,266,4,[],200,EleutherAI/gpt-neox-20b,64,1,640.0,1.0,1,H100,1697105020985,1697105021625,120,9.0,1.0,"[9, 631]","[1697105020994, 1697105021625]"
610,271,8,[],200,EleutherAI/gpt-neox-20b,64,1,2998.0,1.0,1,H100,1697105032976,1697105035974,120,87.0,20.0,"[4, 867, 61, 59, 47, 47, 55, 534, 65, 60, 57, 57, 269, 50, 51, 58, 55, 477, 63, 62]","[1697105032980, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034775, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974]"
611,849,5,[],200,EleutherAI/gpt-neox-20b,64,1,734.0,1.0,1,H100,1697105021627,1697105022361,120,10.0,1.0,"[24, 710]","[1697105021651, 1697105022361]"
612,356,12,[],200,EleutherAI/gpt-neox-20b,64,1,802.0,1.0,1,H100,1697105028616,1697105029418,120,874.0,2.0,"[5, 797]","[1697105028621, 1697105029418]"
613,781,2,[],200,EleutherAI/gpt-neox-20b,64,1,2022.0,1.0,1,H100,1697105015239,1697105017261,120,335.0,10.0,"[27, 856, 50, 525, 62, 57, 56, 54, 272, 63]","[1697105015266, 1697105016122, 1697105016172, 1697105016697, 1697105016759, 1697105016816, 1697105016872, 1697105016926, 1697105017198, 1697105017261]"
614,704,4,[],200,EleutherAI/gpt-neox-20b,64,1,312.0,1.0,1,H100,1697105021313,1697105021625,120,14.0,1.0,"[4, 308]","[1697105021317, 1697105021625]"
615,629,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105022364,1697105027572,120,,,"[15, 811, 73, 67, 52, 56, 472, 69, 53, 65, 50, 61, 334, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 54, 69, 65]","[1697105022379, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026857, 1697105026926, 1697105026991]"
616,729,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026447,1697105027573,120,,,[4],[1697105026451]
617,507,7,[],200,EleutherAI/gpt-neox-20b,64,1,2713.0,1.0,1,H100,1697105027574,1697105030287,120,83.0,20.0,"[5, 383, 769, 56, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 227, 61, 49, 60, 48, 48]","[1697105027579, 1697105027962, 1697105028731, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030021, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287]"
618,285,9,[],200,EleutherAI/gpt-neox-20b,64,1,4236.0,1.0,1,H100,1697105027575,1697105031811,120,100.0,27.0,"[193, 964, 55, 45, 44, 55, 487, 70, 68, 66, 62, 49, 61, 228, 61, 48, 60, 48, 48, 252, 49, 450, 70, 69, 68, 57, 509]","[1697105027768, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811]"
619,670,11,[],200,EleutherAI/gpt-neox-20b,64,1,2873.0,1.0,1,H100,1697105032976,1697105035849,120,67.0,18.0,"[24, 847, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477]","[1697105033000, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849]"
620,314,11,[],200,EleutherAI/gpt-neox-20b,64,1,1851.0,1.0,1,H100,1697105034062,1697105035913,120,335.0,13.0,"[5, 583, 65, 60, 57, 57, 269, 50, 51, 58, 55, 477, 63]","[1697105034067, 1697105034650, 1697105034715, 1697105034775, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912]"
621,80,8,[],200,EleutherAI/gpt-neox-20b,64,1,345.0,1.0,1,H100,1697105026333,1697105026678,120,13.0,1.0,"[21, 324]","[1697105026354, 1697105026678]"
622,85,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035913,1697105037422,120,,,"[5, 772, 70, 69, 66, 58]","[1697105035918, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
623,660,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026679,1697105027572,120,,,[12],[1697105026691]
624,808,11,[],200,EleutherAI/gpt-neox-20b,64,1,1215.0,1.0,1,H100,1697105037425,1697105038640,120,286.0,2.0,"[65, 1150]","[1697105037490, 1697105038640]"
625,408,10,[],200,EleutherAI/gpt-neox-20b,64,1,356.0,1.0,1,H100,1697105027575,1697105027931,120,16.0,1.0,"[32, 324]","[1697105027607, 1697105027931]"
626,1,10,[],200,EleutherAI/gpt-neox-20b,64,1,5349.0,1.0,1,H100,1697105030023,1697105035372,120,47.0,43.0,"[4, 512, 49, 450, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 72, 66, 62, 371, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55]","[1697105030027, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032846, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372]"
627,581,12,[],200,EleutherAI/gpt-neox-20b,64,1,2460.0,1.0,1,H100,1697105038641,1697105041101,120,47.0,20.0,"[5, 769, 71, 68, 66, 63, 58, 57, 441, 67, 54, 64, 49, 60, 54, 311, 45, 46, 56, 56]","[1697105038646, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040239, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101]"
628,527,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024727,1697105027571,120,,,"[8, 731, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105024735, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
629,675,13,[],200,EleutherAI/gpt-neox-20b,64,1,1321.0,1.0,1,H100,1697105037425,1697105038746,120,563.0,5.0,"[21, 541, 653, 58, 48]","[1697105037446, 1697105037987, 1697105038640, 1697105038698, 1697105038746]"
630,445,14,[],200,EleutherAI/gpt-neox-20b,64,1,668.0,1.0,1,H100,1697105038747,1697105039415,120,457.0,2.0,"[4, 664]","[1697105038751, 1697105039415]"
631,193,15,[],200,EleutherAI/gpt-neox-20b,64,1,2678.0,1.0,1,H100,1697105039418,1697105042096,120,79.0,20.0,"[7, 813, 68, 54, 64, 49, 61, 53, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62]","[1697105039425, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096]"
632,867,13,[],200,EleutherAI/gpt-neox-20b,64,1,2998.0,1.0,1,H100,1697105032976,1697105035974,120,91.0,20.0,"[8, 863, 61, 59, 47, 47, 55, 534, 65, 60, 57, 57, 269, 50, 51, 58, 55, 477, 64, 61]","[1697105032984, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034775, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974]"
633,564,4,[],200,EleutherAI/gpt-neox-20b,64,1,3082.0,1.0,1,H100,1697105020300,1697105023382,120,84.0,20.0,"[13, 889, 48, 61, 60, 47, 48, 221, 48, 57, 54, 53, 546, 63, 52, 44, 586, 73, 67, 52]","[1697105020313, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445, 1697105022508, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382]"
634,773,16,[],200,EleutherAI/gpt-neox-20b,64,1,3191.0,1.0,1,H100,1697105042097,1697105045288,120,90.0,20.0,"[8, 742, 74, 55, 62, 60, 58, 520, 73, 64, 62, 60, 507, 71, 61, 58, 458, 73, 71, 54]","[1697105042105, 1697105042847, 1697105042921, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288]"
635,296,8,[],200,EleutherAI/gpt-neox-20b,64,1,357.0,1.0,1,H100,1697105027574,1697105027931,120,6.0,1.0,"[9, 348]","[1697105027583, 1697105027931]"
636,524,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035975,1697105037422,120,,,"[7, 708, 70, 69, 66, 58]","[1697105035982, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
637,300,15,[],200,EleutherAI/gpt-neox-20b,64,1,1138.0,1.0,1,H100,1697105037425,1697105038563,120,9.0,1.0,"[178, 960]","[1697105037603, 1697105038563]"
638,211,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105041103,1697105058841,120,,,"[19, 912, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 73, 64, 62, 59, 508, 70, 62, 58, 457, 74, 70, 55, 66, 65, 50, 450, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 75, 73, 71, 54, 55, 64, 618, 237, 74, 72, 70, 64, 311, 76, 75, 70, 56, 71, 66, 204, 70, 69, 69, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 66, 63, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 69, 67, 66, 51, 233, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 72, 54, 68, 66, 65, 427, 65, 64, 63]","[1697105041122, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049428, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050816, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051412, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051948, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054948, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057413, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
639,312,5,[],200,EleutherAI/gpt-neox-20b,64,1,441.0,1.0,1,H100,1697105023383,1697105023824,120,23.0,1.0,"[25, 416]","[1697105023408, 1697105023824]"
640,882,16,[],200,EleutherAI/gpt-neox-20b,64,1,1796.0,1.0,1,H100,1697105038564,1697105040360,120,345.0,11.0,"[5, 846, 71, 68, 66, 63, 58, 57, 440, 68, 54]","[1697105038569, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360]"
641,894,6,[],200,EleutherAI/gpt-neox-20b,64,1,577.0,1.0,1,H100,1697105023825,1697105024402,120,14.0,1.0,"[21, 556]","[1697105023846, 1697105024402]"
642,666,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024403,1697105027573,120,,,"[5, 1058, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105024408, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
643,300,12,[],200,EleutherAI/gpt-neox-20b,64,1,688.0,1.0,1,H100,1697105031727,1697105032415,120,9.0,1.0,"[8, 680]","[1697105031735, 1697105032415]"
644,48,13,[],200,EleutherAI/gpt-neox-20b,64,1,1146.0,1.0,1,H100,1697105032416,1697105033562,120,6.0,6.0,"[5, 924, 60, 49, 48, 60]","[1697105032421, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562]"
645,745,14,[],200,EleutherAI/gpt-neox-20b,64,1,985.0,1.0,1,H100,1697105033563,1697105034548,120,17.0,1.0,"[9, 976]","[1697105033572, 1697105034548]"
646,66,5,[],200,EleutherAI/gpt-neox-20b,64,1,3061.0,1.0,1,H100,1697105019384,1697105022445,120,84.0,20.0,"[15, 1207, 69, 67, 66, 64, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 55, 52, 546]","[1697105019399, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021847, 1697105021899, 1697105022445]"
647,405,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105034549,1697105037423,120,,,"[5, 604, 50, 51, 58, 55, 477, 63, 62, 58, 57, 601, 70, 69, 66, 58]","[1697105034554, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
648,546,17,[],200,EleutherAI/gpt-neox-20b,64,1,2611.0,1.0,1,H100,1697105045289,1697105047900,120,93.0,20.0,"[8, 623, 133, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 438, 69, 68, 67, 52, 63]","[1697105045297, 1697105045920, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047650, 1697105047718, 1697105047785, 1697105047837, 1697105047900]"
649,874,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024543,1697105027571,120,,,"[5, 918, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105024548, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
650,219,8,[],200,EleutherAI/gpt-neox-20b,64,1,2713.0,1.0,1,H100,1697105027574,1697105030287,120,90.0,20.0,"[46, 342, 770, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 60, 49, 60, 48, 48]","[1697105027620, 1697105027962, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287]"
651,175,16,[],200,EleutherAI/gpt-neox-20b,64,1,2061.0,1.0,1,H100,1697105037425,1697105039486,120,140.0,8.0,"[98, 1117, 58, 48, 55, 54, 560, 71]","[1697105037523, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486]"
652,414,21,[],200,EleutherAI/gpt-neox-20b,64,1,2441.0,1.0,1,H100,1697105040535,1697105042976,120,87.0,20.0,"[8, 355, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56]","[1697105040543, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976]"
653,131,13,[],200,EleutherAI/gpt-neox-20b,64,1,535.0,1.0,1,H100,1697105029419,1697105029954,120,8.0,1.0,"[5, 530]","[1697105029424, 1697105029954]"
654,68,22,[],200,EleutherAI/gpt-neox-20b,64,1,543.0,1.0,1,H100,1697105042977,1697105043520,120,12.0,1.0,"[16, 527]","[1697105042993, 1697105043520]"
655,527,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024727,1697105027572,120,,,"[12, 727, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105024739, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
656,760,17,[],200,EleutherAI/gpt-neox-20b,64,1,8349.0,1.0,1,H100,1697105039488,1697105047837,120,335.0,64.0,"[4, 746, 68, 54, 64, 49, 61, 53, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 72, 65, 62, 59, 508, 70, 62, 58, 457, 74, 70, 55, 66, 64, 51, 450, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437, 69, 68, 68, 52]","[1697105039492, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043748, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045418, 1697105045469, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837]"
657,179,11,[],200,EleutherAI/gpt-neox-20b,64,1,1593.0,1.0,1,H100,1697105027963,1697105029556,120,161.0,4.0,"[5, 1450, 70, 68]","[1697105027968, 1697105029418, 1697105029488, 1697105029556]"
658,714,14,[],200,EleutherAI/gpt-neox-20b,64,1,2892.0,1.0,1,H100,1697105029955,1697105032847,120,83.0,20.0,"[13, 571, 49, 450, 70, 69, 70, 55, 509, 71, 55, 56, 65, 64, 62, 62, 305, 164, 59, 73]","[1697105029968, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031247, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032551, 1697105032715, 1697105032774, 1697105032847]"
659,40,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035975,1697105037422,120,,,"[9, 706, 70, 69, 66, 58]","[1697105035984, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
660,762,12,[],200,EleutherAI/gpt-neox-20b,64,1,2565.0,1.0,1,H100,1697105029557,1697105032122,120,92.0,20.0,"[5, 460, 61, 48, 60, 48, 48, 252, 49, 450, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64]","[1697105029562, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122]"
661,177,11,[],200,EleutherAI/gpt-neox-20b,64,1,681.0,1.0,1,H100,1697105027934,1697105028615,120,14.0,1.0,"[28, 653]","[1697105027962, 1697105028615]"
662,762,12,[],200,EleutherAI/gpt-neox-20b,64,1,2631.0,1.0,1,H100,1697105028616,1697105031247,120,92.0,20.0,"[17, 785, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68]","[1697105028633, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245]"
663,298,6,[],200,EleutherAI/gpt-neox-20b,64,1,455.0,1.0,1,H100,1697105025658,1697105026113,120,17.0,1.0,"[5, 450]","[1697105025663, 1697105026113]"
664,881,7,[],200,EleutherAI/gpt-neox-20b,64,1,876.0,1.0,1,H100,1697105026115,1697105026991,120,58.0,6.0,"[6, 628, 54, 55, 69, 64]","[1697105026121, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
665,359,14,[],200,EleutherAI/gpt-neox-20b,64,1,680.0,1.0,1,H100,1697105043677,1697105044357,120,10.0,1.0,"[5, 675]","[1697105043682, 1697105044357]"
666,480,6,[],200,EleutherAI/gpt-neox-20b,64,1,455.0,1.0,1,H100,1697105024790,1697105025245,120,26.0,1.0,"[8, 447]","[1697105024798, 1697105025245]"
667,348,10,[],200,EleutherAI/gpt-neox-20b,64,1,2998.0,1.0,1,H100,1697105032976,1697105035974,120,91.0,20.0,"[4, 867, 61, 59, 47, 47, 55, 534, 65, 60, 57, 57, 269, 50, 51, 58, 55, 477, 63, 62]","[1697105032980, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034775, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974]"
668,55,10,[],200,EleutherAI/gpt-neox-20b,64,1,220.0,1.0,1,H100,1697105029734,1697105029954,120,12.0,1.0,"[5, 215]","[1697105029739, 1697105029954]"
669,646,11,[],200,EleutherAI/gpt-neox-20b,64,1,520.0,1.0,1,H100,1697105029956,1697105030476,120,14.0,1.0,"[20, 500]","[1697105029976, 1697105030476]"
670,416,12,[],200,EleutherAI/gpt-neox-20b,64,1,6418.0,1.0,1,H100,1697105030477,1697105036895,120,286.0,50.0,"[12, 549, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66]","[1697105030489, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895]"
671,256,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105025246,1697105027572,120,,,"[12, 943, 131, 65, 49, 303, 55, 54, 69, 64]","[1697105025258, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026804, 1697105026858, 1697105026927, 1697105026991]"
672,897,13,[],200,EleutherAI/gpt-neox-20b,64,1,793.0,1.0,1,H100,1697105032976,1697105033769,120,9.0,1.0,"[28, 765]","[1697105033004, 1697105033769]"
673,840,8,[],200,EleutherAI/gpt-neox-20b,64,1,356.0,1.0,1,H100,1697105027575,1697105027931,120,17.0,1.0,"[45, 311]","[1697105027620, 1697105027931]"
674,610,9,[],200,EleutherAI/gpt-neox-20b,64,1,2606.0,1.0,1,H100,1697105027933,1697105030539,120,89.0,20.0,"[17, 782, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 60, 49, 60, 48, 48, 252]","[1697105027950, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
675,272,10,[],200,EleutherAI/gpt-neox-20b,64,1,2435.0,1.0,1,H100,1697105030540,1697105032975,120,86.0,20.0,"[25, 473, 70, 69, 68, 57, 509, 71, 56, 55, 65, 64, 62, 62, 306, 163, 60, 72, 65, 63]","[1697105030565, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031938, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975]"
676,548,14,[],200,EleutherAI/gpt-neox-20b,64,1,3125.0,1.0,1,H100,1697105033770,1697105036895,120,86.0,20.0,"[5, 875, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 64, 61, 58, 57, 601, 70, 69, 66]","[1697105033775, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895]"
677,657,8,[],200,EleutherAI/gpt-neox-20b,64,1,641.0,1.0,1,H100,1697105026992,1697105027633,120,10.0,1.0,"[5, 636]","[1697105026997, 1697105027633]"
678,136,15,[],200,EleutherAI/gpt-neox-20b,64,1,518.0,1.0,1,H100,1697105044358,1697105044876,120,31.0,1.0,"[12, 506]","[1697105044370, 1697105044876]"
679,309,9,[],200,EleutherAI/gpt-neox-20b,64,1,2905.0,1.0,1,H100,1697105027634,1697105030539,120,52.0,20.0,"[142, 956, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 61, 48, 60, 48, 48, 252]","[1697105027776, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
680,865,11,[],200,EleutherAI/gpt-neox-20b,64,1,562.0,1.0,1,H100,1697105026116,1697105026678,120,9.0,1.0,"[24, 538]","[1697105026140, 1697105026678]"
681,41,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105032976,1697105037422,120,,,"[21, 850, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 64, 61, 58, 57, 601, 70, 69, 66, 58]","[1697105032997, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
682,129,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105022518,1697105027572,120,,,"[7, 665, 73, 67, 52, 56, 472, 69, 53, 65, 49, 61, 335, 72, 56, 56, 62, 59, 619, 75, 65, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105022525, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025541, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
683,22,11,[],200,EleutherAI/gpt-neox-20b,64,1,1040.0,1.0,1,H100,1697105027575,1697105028615,120,16.0,1.0,"[141, 899]","[1697105027716, 1697105028615]"
684,114,14,[],200,EleutherAI/gpt-neox-20b,64,1,2498.0,1.0,1,H100,1697105030477,1697105032975,120,88.0,20.0,"[8, 553, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62]","[1697105030485, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974]"
685,435,3,[],200,EleutherAI/gpt-neox-20b,64,1,3722.0,1.0,1,H100,1697105017262,1697105020984,120,563.0,27.0,"[4, 888, 76, 69, 70, 54, 63, 53, 571, 60, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62]","[1697105017266, 1697105018154, 1697105018230, 1697105018299, 1697105018369, 1697105018423, 1697105018486, 1697105018539, 1697105019110, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984]"
686,601,12,[],200,EleutherAI/gpt-neox-20b,64,1,2629.0,1.0,1,H100,1697105028616,1697105031245,120,83.0,20.0,"[5, 797, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68]","[1697105028621, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245]"
687,716,12,[],200,EleutherAI/gpt-neox-20b,64,1,4142.0,1.0,1,H100,1697105037425,1697105041567,120,79.0,30.0,"[77, 1138, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 52, 312, 45, 46, 56, 55, 273, 55, 43, 43, 53]","[1697105037502, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040586, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041100, 1697105041373, 1697105041428, 1697105041471, 1697105041514, 1697105041567]"
688,373,13,[],200,EleutherAI/gpt-neox-20b,64,1,1033.0,1.0,1,H100,1697105041568,1697105042601,120,15.0,1.0,"[5, 1028]","[1697105041573, 1697105042601]"
689,150,14,[],200,EleutherAI/gpt-neox-20b,64,1,1074.0,1.0,1,H100,1697105042602,1697105043676,120,216.0,2.0,"[4, 1070]","[1697105042606, 1697105043676]"
690,734,15,[],200,EleutherAI/gpt-neox-20b,64,1,1412.0,1.0,1,H100,1697105043677,1697105045089,120,100.0,6.0,"[4, 761, 70, 62, 58, 457]","[1697105043681, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089]"
691,820,15,[],200,EleutherAI/gpt-neox-20b,64,1,1739.0,1.0,1,H100,1697105032976,1697105034715,120,161.0,9.0,"[12, 859, 61, 59, 47, 47, 55, 534, 65]","[1697105032988, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715]"
692,503,16,[],200,EleutherAI/gpt-neox-20b,64,1,2810.0,1.0,1,H100,1697105045090,1697105047900,120,109.0,20.0,"[5, 824, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 438, 69, 68, 67, 52, 63]","[1697105045095, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047650, 1697105047718, 1697105047785, 1697105047837, 1697105047900]"
693,129,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024729,1697105027572,120,,,"[18, 719, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105024747, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
694,443,5,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105023264,1697105023824,120,19.0,1.0,"[8, 552]","[1697105023272, 1697105023824]"
695,716,10,[],200,EleutherAI/gpt-neox-20b,64,1,4089.0,1.0,1,H100,1697105037425,1697105041514,120,79.0,30.0,"[5, 557, 653, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 63, 50, 60, 53, 312, 45, 46, 56, 55, 273, 55, 43, 43]","[1697105037430, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040423, 1697105040473, 1697105040533, 1697105040586, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041100, 1697105041373, 1697105041428, 1697105041471, 1697105041514]"
696,218,6,[],200,EleutherAI/gpt-neox-20b,64,1,1022.0,1.0,1,H100,1697105023825,1697105024847,120,109.0,7.0,"[5, 712, 72, 56, 56, 62, 59]","[1697105023830, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847]"
697,331,12,[],200,EleutherAI/gpt-neox-20b,64,1,738.0,1.0,1,H100,1697105035851,1697105036589,120,26.0,1.0,"[4, 734]","[1697105035855, 1697105036589]"
698,495,14,[],200,EleutherAI/gpt-neox-20b,64,1,505.0,1.0,1,H100,1697105036896,1697105037401,120,13.0,1.0,"[9, 496]","[1697105036905, 1697105037401]"
699,801,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024848,1697105027572,120,,,"[9, 1344, 131, 65, 49, 303, 55, 54, 68, 65]","[1697105024857, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026804, 1697105026858, 1697105026926, 1697105026991]"
700,211,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105020987,1697105027572,120,,,"[19, 681, 48, 57, 54, 53, 545, 60, 56, 44, 586, 73, 67, 52, 56, 472, 69, 53, 64, 50, 61, 335, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 54, 69, 65]","[1697105021006, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022444, 1697105022504, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026857, 1697105026926, 1697105026991]"
701,100,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036590,1697105037423,120,,,[13],[1697105036603]
702,597,8,[],200,EleutherAI/gpt-neox-20b,64,1,356.0,1.0,1,H100,1697105027575,1697105027931,120,39.0,1.0,"[36, 320]","[1697105027611, 1697105027931]"
703,690,14,[],200,EleutherAI/gpt-neox-20b,64,1,1138.0,1.0,1,H100,1697105037425,1697105038563,120,39.0,1.0,"[154, 984]","[1697105037579, 1697105038563]"
704,461,15,[],200,EleutherAI/gpt-neox-20b,64,1,3692.0,1.0,1,H100,1697105038564,1697105042256,120,216.0,30.0,"[13, 838, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 60, 54, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46]","[1697105038577, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256]"
705,860,9,[],200,EleutherAI/gpt-neox-20b,64,1,2903.0,1.0,1,H100,1697105031812,1697105034715,120,85.0,20.0,"[5, 735, 163, 59, 73, 65, 62, 371, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65]","[1697105031817, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715]"
706,359,14,[],200,EleutherAI/gpt-neox-20b,64,1,793.0,1.0,1,H100,1697105032976,1697105033769,120,10.0,1.0,"[29, 764]","[1697105033005, 1697105033769]"
707,237,15,[],200,EleutherAI/gpt-neox-20b,64,1,3200.0,1.0,1,H100,1697105041313,1697105044513,120,87.0,20.0,"[9, 712, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 73, 64, 62, 60, 507, 71]","[1697105041322, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513]"
708,136,15,[],200,EleutherAI/gpt-neox-20b,64,1,778.0,1.0,1,H100,1697105033770,1697105034548,120,31.0,1.0,"[9, 769]","[1697105033779, 1697105034548]"
709,721,16,[],200,EleutherAI/gpt-neox-20b,64,1,768.0,1.0,1,H100,1697105034549,1697105035317,120,286.0,5.0,"[5, 604, 50, 51, 58]","[1697105034554, 1697105035158, 1697105035208, 1697105035259, 1697105035317]"
710,719,16,[],200,EleutherAI/gpt-neox-20b,64,1,1374.0,1.0,1,H100,1697105044877,1697105046251,120,182.0,6.0,"[4, 1038, 134, 76, 69, 53]","[1697105044881, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251]"
711,734,8,[],200,EleutherAI/gpt-neox-20b,64,1,1356.0,1.0,1,H100,1697105027575,1697105028931,120,100.0,6.0,"[97, 1060, 55, 45, 44, 55]","[1697105027672, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931]"
712,358,5,[],200,EleutherAI/gpt-neox-20b,64,1,882.0,1.0,1,H100,1697105021626,1697105022508,120,216.0,3.0,"[9, 810, 63]","[1697105021635, 1697105022445, 1697105022508]"
713,86,10,[],200,EleutherAI/gpt-neox-20b,64,1,2235.0,1.0,1,H100,1697105030540,1697105032775,120,335.0,17.0,"[9, 489, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 60]","[1697105030549, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775]"
714,517,10,[],200,EleutherAI/gpt-neox-20b,64,1,377.0,1.0,1,H100,1697105034716,1697105035093,120,15.0,1.0,"[13, 364]","[1697105034729, 1697105035093]"
715,289,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035093,1697105037423,120,,,"[9, 747, 64, 61, 58, 57, 601, 70, 69, 66, 58]","[1697105035102, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
716,135,6,[],200,EleutherAI/gpt-neox-20b,64,1,679.0,1.0,1,H100,1697105022512,1697105023191,120,52.0,2.0,"[8, 670]","[1697105022520, 1697105023190]"
717,505,9,[],200,EleutherAI/gpt-neox-20b,64,1,3980.0,1.0,1,H100,1697105028932,1697105032912,120,100.0,27.0,"[5, 1085, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 72, 66]","[1697105028937, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032846, 1697105032912]"
718,715,7,[],200,EleutherAI/gpt-neox-20b,64,1,632.0,1.0,1,H100,1697105023192,1697105023824,120,20.0,1.0,"[8, 624]","[1697105023200, 1697105023824]"
719,358,7,[],200,EleutherAI/gpt-neox-20b,64,1,1740.0,1.0,1,H100,1697105026992,1697105028732,120,216.0,3.0,"[4, 966, 770]","[1697105026996, 1697105027962, 1697105028732]"
720,134,8,[],200,EleutherAI/gpt-neox-20b,64,1,2512.0,1.0,1,H100,1697105028733,1697105031245,120,86.0,20.0,"[4, 681, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68]","[1697105028737, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245]"
721,493,8,[],200,EleutherAI/gpt-neox-20b,64,1,3102.0,1.0,1,H100,1697105023825,1697105026927,120,83.0,20.0,"[5, 712, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69]","[1697105023830, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927]"
722,318,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036896,1697105037423,120,,,[9],[1697105036905]
723,255,14,[],200,EleutherAI/gpt-neox-20b,64,1,16050.0,1.0,1,H100,1697105037425,1697105053475,120,216.0,119.0,"[162, 1053, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 72, 65, 62, 59, 508, 70, 62, 58, 457, 74, 70, 55, 66, 64, 50, 451, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 75, 73, 71, 54, 55, 64, 618, 237, 74, 72, 70, 64, 311, 76, 74, 71, 56, 71, 66, 204, 70, 69, 69, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71]","[1697105037587, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043748, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045418, 1697105045468, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049428, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050816, 1697105050880, 1697105051191, 1697105051267, 1697105051341, 1697105051412, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051948, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475]"
724,908,16,[],200,EleutherAI/gpt-neox-20b,64,1,7149.0,1.0,1,H100,1697105037425,1697105044574,120,6.0,50.0,"[81, 1134, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 52, 312, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 57, 57, 46, 54, 537, 73, 55, 63, 60, 58, 520, 72, 65, 62, 59, 508, 70, 62]","[1697105037506, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040586, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042153, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043748, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574]"
725,160,8,[],200,EleutherAI/gpt-neox-20b,64,1,675.0,1.0,1,H100,1697105030288,1697105030963,120,13.0,1.0,"[17, 658]","[1697105030305, 1697105030963]"
726,860,9,[],200,EleutherAI/gpt-neox-20b,64,1,2598.0,1.0,1,H100,1697105030964,1697105033562,120,85.0,20.0,"[5, 842, 71, 56, 55, 65, 64, 62, 62, 306, 163, 60, 72, 65, 62, 371, 60, 49, 48, 60]","[1697105030969, 1697105031811, 1697105031882, 1697105031938, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562]"
727,807,9,[],200,EleutherAI/gpt-neox-20b,64,1,2686.0,1.0,1,H100,1697105030288,1697105032974,120,90.0,20.0,"[5, 745, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62]","[1697105030293, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974]"
728,772,23,[],200,EleutherAI/gpt-neox-20b,64,1,3309.0,1.0,1,H100,1697105043521,1697105046830,120,83.0,20.0,"[9, 912, 70, 62, 58, 458, 73, 71, 54, 66, 65, 50, 450, 134, 76, 69, 53, 65, 50, 464]","[1697105043530, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830]"
729,165,17,[],200,EleutherAI/gpt-neox-20b,64,1,3637.0,1.0,1,H100,1697105047902,1697105051539,120,83.0,20.0,"[13, 1439, 75, 72, 71, 55, 54, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71]","[1697105047915, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539]"
730,444,16,[],200,EleutherAI/gpt-neox-20b,64,1,944.0,1.0,1,H100,1697105042154,1697105043098,120,457.0,6.0,"[5, 688, 73, 56, 62, 60]","[1697105042159, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098]"
731,214,17,[],200,EleutherAI/gpt-neox-20b,64,1,3030.0,1.0,1,H100,1697105043099,1697105046129,120,52.0,20.0,"[5, 572, 73, 64, 62, 60, 507, 70, 62, 58, 458, 73, 71, 54, 66, 65, 50, 450, 134, 76]","[1697105043104, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046053, 1697105046129]"
732,426,24,[],200,EleutherAI/gpt-neox-20b,64,1,5048.0,1.0,1,H100,1697105046831,1697105051879,120,79.0,36.0,"[5, 745, 68, 69, 67, 52, 62, 665, 129, 73, 54, 55, 55, 71, 353, 75, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70]","[1697105046836, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049001, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879]"
733,517,10,[],200,EleutherAI/gpt-neox-20b,64,1,984.0,1.0,1,H100,1697105033564,1697105034548,120,15.0,1.0,"[20, 964]","[1697105033584, 1697105034548]"
734,287,11,[],200,EleutherAI/gpt-neox-20b,64,1,543.0,1.0,1,H100,1697105034550,1697105035093,120,10.0,1.0,"[12, 530]","[1697105034562, 1697105035092]"
735,643,9,[],200,EleutherAI/gpt-neox-20b,64,1,423.0,1.0,1,H100,1697105030540,1697105030963,120,18.0,1.0,"[9, 414]","[1697105030549, 1697105030963]"
736,875,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035093,1697105037423,120,,,"[9, 657, 90, 64, 61, 58, 57, 601, 70, 69, 66, 58]","[1697105035102, 1697105035759, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
737,576,10,[],200,EleutherAI/gpt-neox-20b,64,1,793.0,1.0,1,H100,1697105032976,1697105033769,120,14.0,1.0,"[16, 777]","[1697105032992, 1697105033769]"
738,88,7,[],200,EleutherAI/gpt-neox-20b,64,1,2833.0,1.0,1,H100,1697105034062,1697105036895,120,58.0,20.0,"[9, 579, 65, 60, 57, 57, 269, 50, 51, 58, 55, 477, 63, 62, 58, 57, 601, 70, 69, 66]","[1697105034071, 1697105034650, 1697105034715, 1697105034775, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895]"
739,802,18,[],200,EleutherAI/gpt-neox-20b,64,1,472.0,1.0,1,H100,1697105046130,1697105046602,120,9.0,1.0,"[12, 459]","[1697105046142, 1697105046601]"
740,324,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105033770,1697105037423,120,,,"[5, 875, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 64, 61, 59, 56, 601, 70, 69, 66, 58]","[1697105033775, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974, 1697105036033, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
741,415,10,[],200,EleutherAI/gpt-neox-20b,64,1,3810.0,1.0,1,H100,1697105030964,1697105034774,120,109.0,29.0,"[17, 830, 71, 55, 56, 65, 64, 62, 62, 306, 163, 60, 72, 65, 62, 371, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59]","[1697105030981, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774]"
742,835,7,[],200,EleutherAI/gpt-neox-20b,64,1,2712.0,1.0,1,H100,1697105027575,1697105030287,120,87.0,20.0,"[41, 346, 770, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 60, 49, 60, 48, 48]","[1697105027616, 1697105027962, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287]"
743,678,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036896,1697105037423,120,,,[5],[1697105036901]
744,447,9,[],200,EleutherAI/gpt-neox-20b,64,1,2373.0,1.0,1,H100,1697105037425,1697105039798,120,161.0,13.0,"[162, 1053, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57]","[1697105037587, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798]"
745,572,19,[],200,EleutherAI/gpt-neox-20b,64,1,892.0,1.0,1,H100,1697105046603,1697105047495,120,16.0,1.0,"[8, 884]","[1697105046611, 1697105047495]"
746,232,20,[],200,EleutherAI/gpt-neox-20b,64,1,3321.0,1.0,1,H100,1697105047496,1697105050817,120,93.0,20.0,"[4, 1064, 129, 73, 54, 55, 55, 70, 353, 75, 73, 71, 54, 55, 64, 618, 237, 74, 72, 71]","[1697105047500, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049428, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817]"
747,106,10,[],200,EleutherAI/gpt-neox-20b,64,1,3177.0,1.0,1,H100,1697105039799,1697105042976,120,161.0,20.0,"[4, 1095, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 55]","[1697105039803, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975]"
748,56,10,[],200,EleutherAI/gpt-neox-20b,64,1,2903.0,1.0,1,H100,1697105031812,1697105034715,120,86.0,20.0,"[5, 735, 163, 59, 73, 65, 62, 371, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65]","[1697105031817, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715]"
749,202,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105051880,1697105058841,120,,,"[5, 814, 74, 57, 57, 56, 67, 395, 70, 66, 63, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 72, 54, 68, 66, 65, 427, 65, 64, 63]","[1697105051885, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053405, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057413, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
750,886,9,[],200,EleutherAI/gpt-neox-20b,64,1,682.0,1.0,1,H100,1697105027933,1697105028615,120,17.0,1.0,"[13, 669]","[1697105027946, 1697105028615]"
751,660,10,[],200,EleutherAI/gpt-neox-20b,64,1,3377.0,1.0,1,H100,1697105028616,1697105031993,120,732.0,25.0,"[9, 793, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68, 57, 509, 71, 55, 56]","[1697105028625, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993]"
752,343,9,[],200,EleutherAI/gpt-neox-20b,64,1,2605.0,1.0,1,H100,1697105027934,1697105030539,120,84.0,20.0,"[24, 774, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 60, 49, 60, 48, 48, 252]","[1697105027958, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
753,4,21,[],200,EleutherAI/gpt-neox-20b,64,1,2125.0,1.0,1,H100,1697105050818,1697105052943,120,89.0,20.0,"[12, 361, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56]","[1697105050830, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943]"
754,284,16,[],200,EleutherAI/gpt-neox-20b,64,1,4039.0,1.0,1,H100,1697105040535,1697105044574,120,90.0,31.0,"[20, 343, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56, 62, 60, 58, 520, 73, 64, 62, 59, 508, 70, 62]","[1697105040555, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574]"
755,488,8,[],200,EleutherAI/gpt-neox-20b,64,1,674.0,1.0,1,H100,1697105030289,1697105030963,120,6.0,1.0,"[20, 654]","[1697105030309, 1697105030963]"
756,259,9,[],200,EleutherAI/gpt-neox-20b,64,1,2598.0,1.0,1,H100,1697105030964,1697105033562,120,87.0,20.0,"[5, 842, 71, 55, 56, 65, 64, 62, 62, 306, 163, 60, 71, 66, 62, 371, 60, 49, 48, 60]","[1697105030969, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032846, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562]"
757,315,11,[],200,EleutherAI/gpt-neox-20b,64,1,1915.0,1.0,1,H100,1697105031993,1697105033908,120,335.0,14.0,"[5, 554, 163, 59, 73, 65, 62, 371, 60, 49, 48, 60, 285, 61]","[1697105031998, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908]"
758,325,3,[],200,EleutherAI/gpt-neox-20b,64,1,2497.0,1.0,1,H100,1697105018487,1697105020984,120,85.0,20.0,"[9, 615, 59, 47, 57, 55, 54, 535, 123, 69, 67, 62, 60, 307, 69, 67, 66, 64, 50, 62]","[1697105018496, 1697105019111, 1697105019170, 1697105019217, 1697105019274, 1697105019329, 1697105019383, 1697105019918, 1697105020041, 1697105020110, 1697105020177, 1697105020239, 1697105020299, 1697105020606, 1697105020675, 1697105020742, 1697105020808, 1697105020872, 1697105020922, 1697105020984]"
759,674,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026334,1697105027572,120,,,"[24, 391, 55, 54, 69, 64]","[1697105026358, 1697105026749, 1697105026804, 1697105026858, 1697105026927, 1697105026991]"
760,796,5,[],200,EleutherAI/gpt-neox-20b,64,1,2964.0,1.0,1,H100,1697105027575,1697105030539,120,86.0,20.0,"[133, 1024, 55, 45, 44, 55, 487, 70, 68, 66, 62, 49, 61, 228, 61, 48, 60, 48, 48, 252]","[1697105027708, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
761,113,10,[],200,EleutherAI/gpt-neox-20b,64,1,422.0,1.0,1,H100,1697105030541,1697105030963,120,13.0,1.0,"[32, 390]","[1697105030573, 1697105030963]"
762,703,11,[],200,EleutherAI/gpt-neox-20b,64,1,760.0,1.0,1,H100,1697105030965,1697105031725,120,12.0,1.0,"[16, 744]","[1697105030981, 1697105031725]"
763,125,11,[],200,EleutherAI/gpt-neox-20b,64,1,611.0,1.0,1,H100,1697105035978,1697105036589,120,13.0,1.0,"[15, 596]","[1697105035993, 1697105036589]"
764,451,9,[],200,EleutherAI/gpt-neox-20b,64,1,356.0,1.0,1,H100,1697105027575,1697105027931,120,286.0,1.0,"[24, 332]","[1697105027599, 1697105027931]"
765,474,12,[],200,EleutherAI/gpt-neox-20b,64,1,4363.0,1.0,1,H100,1697105031726,1697105036089,120,109.0,33.0,"[5, 821, 163, 59, 73, 65, 62, 371, 61, 48, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 63, 62, 58, 56]","[1697105031731, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033406, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088]"
766,708,12,[],200,EleutherAI/gpt-neox-20b,64,1,810.0,1.0,1,H100,1697105036591,1697105037401,120,140.0,1.0,"[12, 798]","[1697105036603, 1697105037401]"
767,482,13,[],200,EleutherAI/gpt-neox-20b,64,1,3131.0,1.0,1,H100,1697105037402,1697105040533,120,91.0,20.0,"[9, 575, 654, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 63, 50, 60]","[1697105037411, 1697105037986, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040423, 1697105040473, 1697105040533]"
768,645,6,[],200,EleutherAI/gpt-neox-20b,64,1,3095.0,1.0,1,H100,1697105022446,1697105025541,120,86.0,20.0,"[16, 728, 73, 67, 52, 56, 472, 69, 53, 64, 50, 61, 335, 72, 56, 56, 62, 59, 619, 74]","[1697105022462, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540]"
769,871,12,[],200,EleutherAI/gpt-neox-20b,64,1,1376.0,1.0,1,H100,1697105037425,1697105038801,120,123.0,6.0,"[25, 537, 653, 58, 48, 55]","[1697105037450, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801]"
770,349,13,[],200,EleutherAI/gpt-neox-20b,64,1,2315.0,1.0,1,H100,1697105031248,1697105033563,120,88.0,20.0,"[9, 554, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 60, 49, 48, 61]","[1697105031257, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033563]"
771,422,7,[],200,EleutherAI/gpt-neox-20b,64,1,571.0,1.0,1,H100,1697105025542,1697105026113,120,26.0,1.0,"[12, 559]","[1697105025554, 1697105026113]"
772,648,13,[],200,EleutherAI/gpt-neox-20b,64,1,2299.0,1.0,1,H100,1697105038802,1697105041101,120,84.0,20.0,"[4, 609, 71, 68, 66, 63, 58, 57, 441, 67, 54, 64, 49, 60, 54, 311, 45, 46, 56, 56]","[1697105038806, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040239, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101]"
773,74,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026116,1697105027572,120,,,"[12, 621, 54, 55, 69, 64]","[1697105026128, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
774,780,9,[],200,EleutherAI/gpt-neox-20b,64,1,2712.0,1.0,1,H100,1697105027575,1697105030287,120,85.0,20.0,"[40, 347, 770, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 227, 61, 49, 60, 48, 48]","[1697105027615, 1697105027962, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030021, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287]"
775,1,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105033564,1697105037423,120,,,"[28, 1058, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 64, 61, 58, 57, 601, 70, 69, 66, 58]","[1697105033592, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
776,487,15,[],200,EleutherAI/gpt-neox-20b,64,1,2041.0,1.0,1,H100,1697105032848,1697105034889,120,123.0,17.0,"[12, 485, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 60, 57, 57]","[1697105032860, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034775, 1697105034832, 1697105034889]"
777,288,11,[],200,EleutherAI/gpt-neox-20b,64,1,3162.0,1.0,1,H100,1697105037425,1697105040587,120,93.0,20.0,"[78, 1137, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53]","[1697105037503, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587]"
778,542,13,[],200,EleutherAI/gpt-neox-20b,64,1,293.0,1.0,1,H100,1697105032123,1697105032416,120,11.0,1.0,"[5, 287]","[1697105032128, 1697105032415]"
779,198,14,[],200,EleutherAI/gpt-neox-20b,64,1,2842.0,1.0,1,H100,1697105032417,1697105035259,120,96.0,20.0,"[12, 916, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51]","[1697105032429, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259]"
780,490,17,[],200,EleutherAI/gpt-neox-20b,64,1,715.0,1.0,1,H100,1697105035318,1697105036033,120,11.0,5.0,"[4, 527, 64, 61, 59]","[1697105035322, 1697105035849, 1697105035913, 1697105035974, 1697105036033]"
781,151,18,[],200,EleutherAI/gpt-neox-20b,64,1,555.0,1.0,1,H100,1697105036034,1697105036589,120,39.0,1.0,"[4, 551]","[1697105036038, 1697105036589]"
782,13,16,[],200,EleutherAI/gpt-neox-20b,64,1,2571.0,1.0,1,H100,1697105044514,1697105047085,120,90.0,20.0,"[8, 568, 73, 71, 54, 66, 65, 50, 450, 134, 75, 70, 53, 65, 50, 465, 74, 56, 55, 69]","[1697105044522, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046053, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046831, 1697105046905, 1697105046961, 1697105047016, 1697105047085]"
783,849,19,[],200,EleutherAI/gpt-neox-20b,64,1,810.0,1.0,1,H100,1697105036591,1697105037401,120,10.0,1.0,"[16, 794]","[1697105036607, 1697105037401]"
784,107,10,[],200,EleutherAI/gpt-neox-20b,64,1,799.0,1.0,1,H100,1697105027933,1697105028732,120,216.0,2.0,"[5, 794]","[1697105027938, 1697105028732]"
785,699,11,[],200,EleutherAI/gpt-neox-20b,64,1,1216.0,1.0,1,H100,1697105035373,1697105036589,120,39.0,1.0,"[5, 1211]","[1697105035378, 1697105036589]"
786,361,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036590,1697105037422,120,,,[5],[1697105036595]
787,806,11,[],200,EleutherAI/gpt-neox-20b,64,1,2514.0,1.0,1,H100,1697105028733,1697105031247,120,89.0,20.0,"[8, 677, 70, 68, 66, 62, 49, 61, 228, 61, 48, 60, 48, 48, 252, 49, 450, 70, 69, 68]","[1697105028741, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245]"
788,132,13,[],200,EleutherAI/gpt-neox-20b,64,1,3162.0,1.0,1,H100,1697105037425,1697105040587,120,100.0,20.0,"[61, 1154, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53]","[1697105037486, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587]"
789,103,4,[],200,EleutherAI/gpt-neox-20b,64,1,640.0,1.0,1,H100,1697105020985,1697105021625,120,15.0,1.0,"[9, 631]","[1697105020994, 1697105021625]"
790,690,5,[],200,EleutherAI/gpt-neox-20b,64,1,734.0,1.0,1,H100,1697105021627,1697105022361,120,39.0,1.0,"[20, 714]","[1697105021647, 1697105022361]"
791,200,18,[],200,EleutherAI/gpt-neox-20b,64,1,2461.0,1.0,1,H100,1697105047902,1697105050363,120,6.0,9.0,"[21, 1431, 75, 72, 71, 55, 54, 64, 618]","[1697105047923, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681, 1697105049745, 1697105050363]"
792,460,6,[],200,EleutherAI/gpt-neox-20b,64,1,3178.0,1.0,1,H100,1697105022362,1697105025540,120,87.0,20.0,"[9, 819, 73, 67, 52, 56, 472, 69, 53, 65, 50, 61, 334, 72, 56, 56, 62, 59, 619, 74]","[1697105022371, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540]"
793,900,19,[],200,EleutherAI/gpt-neox-20b,64,1,1104.0,1.0,1,H100,1697105050364,1697105051468,120,67.0,6.0,"[5, 822, 76, 75, 71, 55]","[1697105050369, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468]"
794,477,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105022516,1697105027572,120,,,"[7, 667, 73, 67, 52, 56, 472, 69, 53, 64, 50, 61, 335, 72, 56, 56, 62, 59, 619, 75, 65, 51, 61, 483, 131, 65, 49, 303, 54, 54, 70, 64]","[1697105022523, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024096, 1697105024146, 1697105024207, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025541, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026857, 1697105026927, 1697105026991]"
795,146,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026928,1697105027572,120,,,[12],[1697105026940]
796,847,10,[],200,EleutherAI/gpt-neox-20b,64,1,1040.0,1.0,1,H100,1697105027575,1697105028615,120,10.0,1.0,"[133, 907]","[1697105027708, 1697105028615]"
797,503,11,[],200,EleutherAI/gpt-neox-20b,64,1,2629.0,1.0,1,H100,1697105028616,1697105031245,120,109.0,20.0,"[9, 793, 70, 68, 65, 63, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68]","[1697105028625, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245]"
798,312,4,[],200,EleutherAI/gpt-neox-20b,64,1,1041.0,1.0,1,H100,1697105027574,1697105028615,120,23.0,1.0,"[114, 927]","[1697105027688, 1697105028615]"
799,799,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035760,1697105037423,120,,,"[9, 921, 70, 69, 66, 58]","[1697105035769, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
800,121,7,[],200,EleutherAI/gpt-neox-20b,64,1,572.0,1.0,1,H100,1697105025541,1697105026113,120,13.0,1.0,"[9, 563]","[1697105025550, 1697105026113]"
801,818,8,[],200,EleutherAI/gpt-neox-20b,64,1,563.0,1.0,1,H100,1697105026115,1697105026678,120,13.0,1.0,"[7, 556]","[1697105026122, 1697105026678]"
802,570,17,[],200,EleutherAI/gpt-neox-20b,64,1,1138.0,1.0,1,H100,1697105037425,1697105038563,120,18.0,1.0,"[158, 980]","[1697105037583, 1697105038563]"
803,89,5,[],200,EleutherAI/gpt-neox-20b,64,1,2629.0,1.0,1,H100,1697105028616,1697105031245,120,52.0,20.0,"[13, 789, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68]","[1697105028629, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245]"
804,228,18,[],200,EleutherAI/gpt-neox-20b,64,1,2537.0,1.0,1,H100,1697105038564,1697105041101,120,100.0,20.0,"[9, 842, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53, 311, 45, 46, 56, 56]","[1697105038573, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101]"
805,479,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026679,1697105027572,120,,,[28],[1697105026707]
806,248,10,[],200,EleutherAI/gpt-neox-20b,64,1,2557.0,1.0,1,H100,1697105027574,1697105030131,120,182.0,17.0,"[17, 371, 770, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 227, 61, 49]","[1697105027591, 1697105027962, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030021, 1697105030082, 1697105030131]"
807,273,12,[],200,EleutherAI/gpt-neox-20b,64,1,476.0,1.0,1,H100,1697105031249,1697105031725,120,19.0,1.0,"[16, 460]","[1697105031265, 1697105031725]"
808,19,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105031727,1697105037422,120,,,"[8, 817, 163, 59, 73, 65, 62, 371, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66, 58]","[1697105031735, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
809,545,8,[],200,EleutherAI/gpt-neox-20b,64,1,1302.0,1.0,1,H100,1697105027575,1697105028877,120,216.0,5.0,"[141, 1016, 55, 45, 44]","[1697105027716, 1697105028732, 1697105028787, 1697105028832, 1697105028876]"
810,200,9,[],200,EleutherAI/gpt-neox-20b,64,1,1145.0,1.0,1,H100,1697105028877,1697105030022,120,6.0,9.0,"[5, 536, 70, 68, 66, 62, 49, 61, 228]","[1697105028882, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022]"
811,143,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105034890,1697105037423,120,,,"[4, 955, 63, 62, 58, 57, 601, 70, 69, 66, 58]","[1697105034894, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
812,543,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105031254,1697105037422,120,,,"[15, 542, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 61, 48, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66, 58]","[1697105031269, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033406, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
813,841,17,[],200,EleutherAI/gpt-neox-20b,64,1,2814.0,1.0,1,H100,1697105037424,1697105040238,120,123.0,15.0,"[6, 557, 653, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440]","[1697105037430, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238]"
814,899,10,[],200,EleutherAI/gpt-neox-20b,64,1,2824.0,1.0,1,H100,1697105030023,1697105032847,120,100.0,20.0,"[4, 512, 49, 450, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 305, 164, 59, 73]","[1697105030027, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032551, 1697105032715, 1697105032774, 1697105032847]"
815,805,11,[],200,EleutherAI/gpt-neox-20b,64,1,6768.0,1.0,1,H100,1697105042977,1697105049745,120,286.0,50.0,"[12, 687, 73, 64, 62, 60, 507, 70, 62, 58, 458, 73, 71, 54, 66, 65, 50, 450, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437, 69, 69, 67, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64]","[1697105042989, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745]"
816,253,3,[],200,EleutherAI/gpt-neox-20b,64,1,2879.0,1.0,1,H100,1697105021847,1697105024726,120,67.0,20.0,"[5, 593, 62, 53, 44, 586, 73, 67, 52, 56, 472, 69, 53, 65, 50, 61, 334, 72, 56, 56]","[1697105021852, 1697105022445, 1697105022507, 1697105022560, 1697105022604, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726]"
817,601,14,[],200,EleutherAI/gpt-neox-20b,64,1,3162.0,1.0,1,H100,1697105037425,1697105040587,120,83.0,20.0,"[146, 1069, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53]","[1697105037571, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587]"
818,378,15,[],200,EleutherAI/gpt-neox-20b,64,1,6555.0,1.0,1,H100,1697105040588,1697105047143,120,93.0,47.0,"[12, 774, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56, 62, 60, 58, 520, 73, 64, 62, 59, 508, 70, 62, 58, 457, 74, 70, 55, 66, 65, 50, 450, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55, 69, 58]","[1697105040600, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143]"
819,559,11,[],200,EleutherAI/gpt-neox-20b,64,1,2411.0,1.0,1,H100,1697105032848,1697105035259,120,86.0,20.0,"[8, 489, 61, 48, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51]","[1697105032856, 1697105033345, 1697105033406, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259]"
820,524,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026679,1697105027572,120,,,[16],[1697105026695]
821,474,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024148,1697105027572,120,,,"[4, 390, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 482, 132, 65, 49, 303, 54, 55, 69, 64]","[1697105024152, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026200, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
822,493,17,[],200,EleutherAI/gpt-neox-20b,64,1,2749.0,1.0,1,H100,1697105046252,1697105049001,120,83.0,20.0,"[4, 574, 75, 56, 56, 68, 58, 437, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 71]","[1697105046256, 1697105046830, 1697105046905, 1697105046961, 1697105047017, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049001]"
823,328,12,[],200,EleutherAI/gpt-neox-20b,64,1,829.0,1.0,1,H100,1697105035260,1697105036089,120,109.0,6.0,"[8, 581, 64, 61, 58, 57]","[1697105035268, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089]"
824,917,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036090,1697105037423,120,,,[8],[1697105036098]
825,687,14,[],200,EleutherAI/gpt-neox-20b,64,1,3109.0,1.0,1,H100,1697105037425,1697105040534,120,96.0,20.0,"[33, 529, 653, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 60]","[1697105037458, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533]"
826,669,11,[],200,EleutherAI/gpt-neox-20b,64,1,2483.0,1.0,1,H100,1697105032776,1697105035259,120,83.0,20.0,"[4, 565, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51]","[1697105032780, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259]"
827,440,12,[],200,EleutherAI/gpt-neox-20b,64,1,2314.0,1.0,1,H100,1697105031249,1697105033563,120,84.0,20.0,"[12, 550, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 61, 48, 48, 61]","[1697105031261, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033406, 1697105033454, 1697105033502, 1697105033563]"
828,345,15,[],200,EleutherAI/gpt-neox-20b,64,1,2441.0,1.0,1,H100,1697105040535,1697105042976,120,39.0,20.0,"[12, 351, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56]","[1697105040547, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976]"
829,556,12,[],200,EleutherAI/gpt-neox-20b,64,1,1367.0,1.0,1,H100,1697105049746,1697105051113,120,9.0,1.0,"[5, 1362]","[1697105049751, 1697105051113]"
830,210,13,[],200,EleutherAI/gpt-neox-20b,64,1,1086.0,1.0,1,H100,1697105033564,1697105034650,120,140.0,2.0,"[12, 1074]","[1697105033576, 1697105034650]"
831,212,13,[],200,EleutherAI/gpt-neox-20b,64,1,625.0,1.0,1,H100,1697105051114,1697105051739,120,31.0,1.0,"[4, 621]","[1697105051118, 1697105051739]"
832,910,14,[],200,EleutherAI/gpt-neox-20b,64,1,865.0,1.0,1,H100,1697105051740,1697105052605,120,8.0,1.0,"[21, 844]","[1697105051761, 1697105052605]"
833,570,15,[],200,EleutherAI/gpt-neox-20b,64,1,710.0,1.0,1,H100,1697105052606,1697105053316,120,18.0,1.0,"[13, 697]","[1697105052619, 1697105053316]"
834,339,16,[],200,EleutherAI/gpt-neox-20b,64,1,2763.0,1.0,1,H100,1697105053317,1697105056080,120,87.0,20.0,"[12, 730, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71]","[1697105053329, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080]"
835,798,14,[],200,EleutherAI/gpt-neox-20b,64,1,721.0,1.0,1,H100,1697105034651,1697105035372,120,79.0,6.0,"[5, 502, 50, 51, 58, 55]","[1697105034656, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372]"
836,136,14,[],200,EleutherAI/gpt-neox-20b,64,1,302.0,1.0,1,H100,1697105040535,1697105040837,120,31.0,1.0,"[12, 290]","[1697105040547, 1697105040837]"
837,813,15,[],200,EleutherAI/gpt-neox-20b,64,1,2911.0,1.0,1,H100,1697105040838,1697105043749,120,85.0,20.0,"[4, 532, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 74, 55, 62, 60, 58, 520, 73]","[1697105040842, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042921, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749]"
838,567,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035373,1697105037423,120,,,"[5, 1312, 70, 69, 66, 58]","[1697105035378, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
839,432,8,[],200,EleutherAI/gpt-neox-20b,64,1,455.0,1.0,1,H100,1697105025658,1697105026113,120,13.0,1.0,"[5, 450]","[1697105025663, 1697105026113]"
840,180,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026116,1697105027572,120,,,"[8, 625, 54, 55, 69, 64]","[1697105026124, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
841,880,10,[],200,EleutherAI/gpt-neox-20b,64,1,1157.0,1.0,1,H100,1697105027575,1697105028732,120,84.0,2.0,"[101, 1056]","[1697105027676, 1697105028732]"
842,534,11,[],200,EleutherAI/gpt-neox-20b,64,1,2514.0,1.0,1,H100,1697105028733,1697105031247,120,96.0,20.0,"[4, 681, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68]","[1697105028737, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245]"
843,928,17,[],200,EleutherAI/gpt-neox-20b,64,1,459.0,1.0,1,H100,1697105056081,1697105056540,120,20.0,1.0,"[12, 447]","[1697105056093, 1697105056540]"
844,698,18,[],200,EleutherAI/gpt-neox-20b,64,1,1060.0,1.0,1,H100,1697105056541,1697105057601,120,182.0,6.0,"[8, 792, 73, 53, 68, 66]","[1697105056549, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601]"
845,156,15,[],200,EleutherAI/gpt-neox-20b,64,1,3131.0,1.0,1,H100,1697105037402,1697105040533,120,86.0,20.0,"[5, 579, 654, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 63, 50, 60]","[1697105037407, 1697105037986, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040423, 1697105040473, 1697105040533]"
846,68,11,[],200,EleutherAI/gpt-neox-20b,64,1,317.0,1.0,1,H100,1697105034776,1697105035093,120,12.0,1.0,"[4, 313]","[1697105034780, 1697105035093]"
847,833,11,[],200,EleutherAI/gpt-neox-20b,64,1,1170.0,1.0,1,H100,1697105030132,1697105031302,120,563.0,8.0,"[4, 403, 49, 450, 70, 69, 68, 57]","[1697105030136, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302]"
848,745,12,[],200,EleutherAI/gpt-neox-20b,64,1,665.0,1.0,1,H100,1697105035094,1697105035759,120,17.0,1.0,"[16, 649]","[1697105035110, 1697105035759]"
849,350,19,[],200,EleutherAI/gpt-neox-20b,64,1,410.0,1.0,1,H100,1697105057602,1697105058012,120,216.0,1.0,"[4, 406]","[1697105057606, 1697105058012]"
850,127,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058012,1697105058840,120,,,[9],[1697105058021]
851,709,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058844,1697105063190,120,,,"[140, 1164, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105058984, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
852,854,16,[],200,EleutherAI/gpt-neox-20b,64,1,3908.0,1.0,1,H100,1697105040534,1697105044442,120,67.0,29.0,"[9, 355, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56, 62, 60, 58, 520, 72, 65, 62, 59, 508]","[1697105040543, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043748, 1697105043813, 1697105043875, 1697105043934, 1697105044442]"
853,576,12,[],200,EleutherAI/gpt-neox-20b,64,1,1113.0,1.0,1,H100,1697105031303,1697105032416,120,14.0,1.0,"[5, 1107]","[1697105031308, 1697105032415]"
854,509,13,[],200,EleutherAI/gpt-neox-20b,64,1,1001.0,1.0,1,H100,1697105035760,1697105036761,120,286.0,3.0,"[5, 925, 70]","[1697105035765, 1697105036690, 1697105036760]"
855,457,22,[],200,EleutherAI/gpt-neox-20b,64,1,882.0,1.0,1,H100,1697105063192,1697105064074,120,874.0,2.0,"[22, 860]","[1697105063214, 1697105064074]"
856,170,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036761,1697105037423,120,,,[5],[1697105036766]
857,510,17,[],200,EleutherAI/gpt-neox-20b,64,1,647.0,1.0,1,H100,1697105044443,1697105045090,120,79.0,2.0,"[5, 642]","[1697105044448, 1697105045090]"
858,230,13,[],200,EleutherAI/gpt-neox-20b,64,1,1086.0,1.0,1,H100,1697105032416,1697105033502,120,86.0,5.0,"[9, 920, 60, 49, 48]","[1697105032425, 1697105033345, 1697105033405, 1697105033454, 1697105033502]"
859,908,12,[],200,EleutherAI/gpt-neox-20b,64,1,7149.0,1.0,1,H100,1697105037425,1697105044574,120,6.0,50.0,"[58, 1157, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 52, 312, 45, 46, 56, 55, 273, 55, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 72, 65, 62, 59, 508, 70, 62]","[1697105037483, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040586, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041100, 1697105041373, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043748, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574]"
860,589,18,[],200,EleutherAI/gpt-neox-20b,64,1,2737.0,1.0,1,H100,1697105040239,1697105042976,120,92.0,20.0,"[5, 654, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56]","[1697105040244, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976]"
861,861,18,[],200,EleutherAI/gpt-neox-20b,64,1,199.0,1.0,1,H100,1697105051540,1697105051739,120,10.0,1.0,"[8, 191]","[1697105051548, 1697105051739]"
862,522,19,[],200,EleutherAI/gpt-neox-20b,64,1,865.0,1.0,1,H100,1697105051740,1697105052605,120,20.0,1.0,"[21, 844]","[1697105051761, 1697105052605]"
863,288,20,[],200,EleutherAI/gpt-neox-20b,64,1,2692.0,1.0,1,H100,1697105052606,1697105055298,120,93.0,20.0,"[5, 793, 71, 67, 62, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 69, 68, 65, 51, 233]","[1697105052611, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298]"
864,896,15,[],200,EleutherAI/gpt-neox-20b,64,1,499.0,1.0,1,H100,1697105035260,1697105035759,120,15.0,1.0,"[12, 487]","[1697105035272, 1697105035759]"
865,532,18,[],200,EleutherAI/gpt-neox-20b,64,1,2979.0,1.0,1,H100,1697105047838,1697105050817,120,92.0,20.0,"[5, 720, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71]","[1697105047843, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817]"
866,122,16,[],200,EleutherAI/gpt-neox-20b,64,1,3031.0,1.0,1,H100,1697105042257,1697105045288,120,88.0,20.0,"[4, 586, 73, 56, 62, 60, 58, 520, 73, 64, 62, 60, 507, 71, 61, 58, 458, 73, 71, 54]","[1697105042261, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288]"
867,557,16,[],200,EleutherAI/gpt-neox-20b,64,1,829.0,1.0,1,H100,1697105035760,1697105036589,120,31.0,1.0,"[13, 816]","[1697105035773, 1697105036589]"
868,869,6,[],200,EleutherAI/gpt-neox-20b,64,1,1405.0,1.0,1,H100,1697105023383,1697105024788,120,244.0,12.0,"[17, 510, 69, 53, 65, 50, 61, 334, 72, 56, 56, 62]","[1697105023400, 1697105023910, 1697105023979, 1697105024032, 1697105024097, 1697105024147, 1697105024208, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788]"
869,872,21,[],200,EleutherAI/gpt-neox-20b,64,1,2367.0,1.0,1,H100,1697105055299,1697105057666,120,91.0,20.0,"[9, 701, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 348, 72, 53, 68, 66, 65]","[1697105055308, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057342, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666]"
870,327,8,[],200,EleutherAI/gpt-neox-20b,64,1,2047.0,1.0,1,H100,1697105027575,1697105029622,120,563.0,10.0,"[129, 1028, 55, 45, 44, 55, 487, 70, 68, 66]","[1697105027704, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029622]"
871,186,19,[],200,EleutherAI/gpt-neox-20b,64,1,2586.0,1.0,1,H100,1697105050818,1697105053404,120,123.0,22.0,"[16, 357, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394]","[1697105050834, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404]"
872,96,9,[],200,EleutherAI/gpt-neox-20b,64,1,331.0,1.0,1,H100,1697105029623,1697105029954,120,31.0,1.0,"[5, 326]","[1697105029628, 1697105029954]"
873,849,10,[],200,EleutherAI/gpt-neox-20b,64,1,984.0,1.0,1,H100,1697105033564,1697105034548,120,10.0,1.0,"[24, 960]","[1697105033588, 1697105034548]"
874,615,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105034549,1697105037423,120,,,"[9, 600, 50, 51, 58, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66, 58]","[1697105034558, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
875,595,20,[],200,EleutherAI/gpt-neox-20b,64,1,80.0,1.0,1,H100,1697105037402,1697105037482,120,8.0,1.0,"[13, 67]","[1697105037415, 1697105037482]"
876,684,10,[],200,EleutherAI/gpt-neox-20b,64,1,2892.0,1.0,1,H100,1697105029955,1697105032847,120,100.0,20.0,"[8, 576, 49, 450, 70, 69, 69, 56, 509, 71, 55, 56, 65, 64, 62, 62, 305, 164, 59, 72]","[1697105029963, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031246, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032551, 1697105032715, 1697105032774, 1697105032846]"
877,245,21,[],200,EleutherAI/gpt-neox-20b,64,1,3104.0,1.0,1,H100,1697105037483,1697105040587,120,100.0,20.0,"[112, 1045, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53]","[1697105037595, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587]"
878,91,12,[],200,EleutherAI/gpt-neox-20b,64,1,639.0,1.0,1,H100,1697105033909,1697105034548,120,23.0,1.0,"[4, 635]","[1697105033913, 1697105034548]"
879,672,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105034549,1697105037423,120,,,"[9, 600, 50, 51, 58, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66, 58]","[1697105034558, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
880,147,18,[],200,EleutherAI/gpt-neox-20b,64,1,1189.0,1.0,1,H100,1697105049001,1697105050190,120,182.0,1.0,"[5, 1184]","[1697105049006, 1697105050190]"
881,824,19,[],200,EleutherAI/gpt-neox-20b,64,1,1151.0,1.0,1,H100,1697105050191,1697105051342,120,58.0,4.0,"[4, 996, 76, 75]","[1697105050195, 1697105051191, 1697105051267, 1697105051342]"
882,251,10,[],200,EleutherAI/gpt-neox-20b,64,1,1040.0,1.0,1,H100,1697105027575,1697105028615,120,31.0,1.0,"[137, 903]","[1697105027712, 1697105028615]"
883,832,11,[],200,EleutherAI/gpt-neox-20b,64,1,673.0,1.0,1,H100,1697105028616,1697105029289,120,15.0,1.0,"[21, 651]","[1697105028637, 1697105029288]"
884,561,20,[],200,EleutherAI/gpt-neox-20b,64,1,2257.0,1.0,1,H100,1697105051469,1697105053726,120,87.0,20.0,"[4, 336, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60]","[1697105051473, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726]"
885,302,10,[],200,EleutherAI/gpt-neox-20b,64,1,2964.0,1.0,1,H100,1697105027575,1697105030539,120,85.0,20.0,"[113, 1044, 55, 45, 44, 55, 487, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252]","[1697105027688, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
886,480,20,[],200,EleutherAI/gpt-neox-20b,64,1,396.0,1.0,1,H100,1697105051343,1697105051739,120,26.0,1.0,"[8, 388]","[1697105051351, 1697105051739]"
887,884,11,[],200,EleutherAI/gpt-neox-20b,64,1,2435.0,1.0,1,H100,1697105030540,1697105032975,120,90.0,20.0,"[5, 493, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 60, 72, 65, 63]","[1697105030545, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975]"
888,245,21,[],200,EleutherAI/gpt-neox-20b,64,1,2690.0,1.0,1,H100,1697105051740,1697105054430,120,100.0,20.0,"[5, 954, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68]","[1697105051745, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430]"
889,661,12,[],200,EleutherAI/gpt-neox-20b,64,1,1799.0,1.0,1,H100,1697105032976,1697105034775,120,161.0,10.0,"[12, 859, 61, 59, 47, 47, 55, 534, 65, 60]","[1697105032988, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034775]"
890,127,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036089,1697105037423,120,,,[5],[1697105036094]
891,130,8,[],200,EleutherAI/gpt-neox-20b,64,1,357.0,1.0,1,H100,1697105027574,1697105027931,120,14.0,1.0,"[58, 299]","[1697105027632, 1697105027931]"
892,835,22,[],200,EleutherAI/gpt-neox-20b,64,1,2910.0,1.0,1,H100,1697105054431,1697105057341,120,87.0,20.0,"[5, 862, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 70, 68, 67, 64, 52, 63, 347]","[1697105054436, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341]"
893,4,19,[],200,EleutherAI/gpt-neox-20b,64,1,3410.0,1.0,1,H100,1697105041102,1697105044512,120,89.0,20.0,"[4, 928, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 73, 64, 62, 59, 508, 70]","[1697105041106, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512]"
894,836,9,[],200,EleutherAI/gpt-neox-20b,64,1,682.0,1.0,1,H100,1697105027933,1697105028615,120,11.0,1.0,"[10, 672]","[1697105027943, 1697105028615]"
895,834,14,[],200,EleutherAI/gpt-neox-20b,64,1,3108.0,1.0,1,H100,1697105037425,1697105040533,120,85.0,20.0,"[9, 552, 654, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 60]","[1697105037434, 1697105037986, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533]"
896,317,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105034776,1697105037423,120,,,"[4, 378, 50, 51, 58, 55, 477, 63, 62, 58, 57, 601, 70, 69, 66, 58]","[1697105034780, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
897,108,23,[],200,EleutherAI/gpt-neox-20b,64,1,477.0,1.0,1,H100,1697105064075,1697105064552,120,182.0,2.0,"[4, 473]","[1697105064079, 1697105064552]"
898,814,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064553,1697105065210,120,,,[8],[1697105064561]
899,468,25,[],200,EleutherAI/gpt-neox-20b,64,1,3197.0,1.0,1,H100,1697105065212,1697105068409,120,31.0,20.0,"[33, 548, 53, 727, 68, 66, 65, 61, 59, 534, 68, 62, 60, 55, 407, 60, 58, 58, 47, 55, 53]","[1697105065245, 1697105065793, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067556, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409]"
900,287,18,[],200,EleutherAI/gpt-neox-20b,64,1,609.0,1.0,1,H100,1697105045091,1697105045700,120,10.0,1.0,"[8, 601]","[1697105045099, 1697105045700]"
901,869,15,[],200,EleutherAI/gpt-neox-20b,64,1,2259.0,1.0,1,H100,1697105037424,1697105039683,120,244.0,12.0,"[54, 509, 653, 58, 48, 55, 54, 560, 71, 68, 66, 63]","[1697105037478, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683]"
902,9,5,[],200,EleutherAI/gpt-neox-20b,64,1,2205.0,1.0,1,H100,1697105020240,1697105022445,120,85.0,20.0,"[5, 361, 70, 66, 67, 63, 50, 62, 218, 48, 61, 60, 47, 48, 221, 48, 57, 54, 53, 546]","[1697105020245, 1697105020606, 1697105020676, 1697105020742, 1697105020809, 1697105020872, 1697105020922, 1697105020984, 1697105021202, 1697105021250, 1697105021311, 1697105021371, 1697105021418, 1697105021466, 1697105021687, 1697105021735, 1697105021792, 1697105021846, 1697105021899, 1697105022445]"
903,274,12,[],200,EleutherAI/gpt-neox-20b,64,1,2195.0,1.0,1,H100,1697105037425,1697105039620,120,364.0,11.0,"[9, 553, 653, 58, 48, 55, 54, 560, 71, 68, 66]","[1697105037434, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620]"
904,659,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105040363,1697105058840,120,,,"[7, 528, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56, 62, 60, 58, 520, 73, 64, 62, 59, 508, 70, 62, 58, 457, 74, 70, 55, 66, 64, 51, 450, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 70, 64, 311, 76, 75, 70, 56, 71, 66, 204, 70, 69, 69, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 66, 63, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 72, 54, 68, 66, 65, 427, 65, 64, 63]","[1697105040370, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045418, 1697105045469, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050816, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051412, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051948, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057413, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
905,620,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057667,1697105058840,120,,,[13],[1697105057680]
906,44,13,[],200,EleutherAI/gpt-neox-20b,64,1,533.0,1.0,1,H100,1697105039621,1697105040154,120,12.0,1.0,"[5, 528]","[1697105039626, 1697105040154]"
907,721,14,[],200,EleutherAI/gpt-neox-20b,64,1,890.0,1.0,1,H100,1697105040155,1697105041045,120,286.0,5.0,"[4, 739, 45, 46, 56]","[1697105040159, 1697105040898, 1697105040943, 1697105040989, 1697105041045]"
908,273,23,[],200,EleutherAI/gpt-neox-20b,64,1,579.0,1.0,1,H100,1697105058843,1697105059422,120,19.0,1.0,"[37, 542]","[1697105058880, 1697105059422]"
909,22,22,[],200,EleutherAI/gpt-neox-20b,64,1,724.0,1.0,1,H100,1697105040588,1697105041312,120,16.0,1.0,"[16, 708]","[1697105040604, 1697105041312]"
910,51,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105059423,1697105063190,120,,,"[5, 720, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 62, 397, 74, 121, 125, 70, 55, 349, 61, 60, 58, 47]","[1697105059428, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105061999, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
911,606,23,[],200,EleutherAI/gpt-neox-20b,64,1,640.0,1.0,1,H100,1697105041313,1697105041953,120,9.0,1.0,"[17, 623]","[1697105041330, 1697105041953]"
912,165,10,[],200,EleutherAI/gpt-neox-20b,64,1,2346.0,1.0,1,H100,1697105032913,1697105035259,120,83.0,20.0,"[5, 427, 61, 48, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 60, 57, 57, 269, 50, 51]","[1697105032918, 1697105033345, 1697105033406, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034775, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259]"
913,644,13,[],200,EleutherAI/gpt-neox-20b,64,1,1138.0,1.0,1,H100,1697105037425,1697105038563,120,19.0,1.0,"[170, 968]","[1697105037595, 1697105038563]"
914,276,14,[],200,EleutherAI/gpt-neox-20b,64,1,1908.0,1.0,1,H100,1697105038565,1697105040473,120,732.0,13.0,"[28, 822, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49]","[1697105038593, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473]"
915,47,15,[],200,EleutherAI/gpt-neox-20b,64,1,2502.0,1.0,1,H100,1697105040474,1697105042976,120,90.0,20.0,"[5, 419, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56]","[1697105040479, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976]"
916,631,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063192,1697105065209,120,,,"[6, 876, 46, 432, 61, 59, 57, 45, 54]","[1697105063198, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
917,491,10,[],200,EleutherAI/gpt-neox-20b,64,1,673.0,1.0,1,H100,1697105028616,1697105029289,120,14.0,1.0,"[21, 651]","[1697105028637, 1697105029288]"
918,268,11,[],200,EleutherAI/gpt-neox-20b,64,1,664.0,1.0,1,H100,1697105029290,1697105029954,120,19.0,1.0,"[12, 652]","[1697105029302, 1697105029954]"
919,269,10,[],200,EleutherAI/gpt-neox-20b,64,1,518.0,1.0,1,H100,1697105024727,1697105025245,120,11.0,1.0,"[12, 506]","[1697105024739, 1697105025245]"
920,16,11,[],200,EleutherAI/gpt-neox-20b,64,1,867.0,1.0,1,H100,1697105025246,1697105026113,120,9.0,1.0,"[21, 846]","[1697105025267, 1697105026113]"
921,603,12,[],200,EleutherAI/gpt-neox-20b,64,1,562.0,1.0,1,H100,1697105026116,1697105026678,120,9.0,1.0,"[12, 549]","[1697105026128, 1697105026677]"
922,374,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026678,1697105027573,120,,,[5],[1697105026683]
923,34,14,[],200,EleutherAI/gpt-neox-20b,64,1,1040.0,1.0,1,H100,1697105027575,1697105028615,120,12.0,1.0,"[197, 843]","[1697105027772, 1697105028615]"
924,733,15,[],200,EleutherAI/gpt-neox-20b,64,1,673.0,1.0,1,H100,1697105028616,1697105029289,120,31.0,1.0,"[25, 648]","[1697105028641, 1697105029289]"
925,392,16,[],200,EleutherAI/gpt-neox-20b,64,1,664.0,1.0,1,H100,1697105029290,1697105029954,120,20.0,1.0,"[16, 648]","[1697105029306, 1697105029954]"
926,162,17,[],200,EleutherAI/gpt-neox-20b,64,1,2892.0,1.0,1,H100,1697105029955,1697105032847,120,90.0,20.0,"[12, 572, 49, 450, 70, 69, 70, 55, 509, 71, 55, 56, 65, 64, 62, 62, 305, 164, 59, 73]","[1697105029967, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031247, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032551, 1697105032715, 1697105032774, 1697105032847]"
927,408,26,[],200,EleutherAI/gpt-neox-20b,64,1,1133.0,1.0,1,H100,1697105065212,1697105066345,120,16.0,1.0,"[85, 1048]","[1697105065297, 1697105066345]"
928,65,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105066346,1697105070950,120,,,"[8, 1072, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105066354, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
929,751,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105032848,1697105037422,120,,,"[12, 485, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 60, 57, 57, 269, 50, 51, 58, 55, 477, 63, 62, 58, 57, 601, 70, 69, 66, 58]","[1697105032860, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034775, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
930,326,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036590,1697105037423,120,,,[5],[1697105036595]
931,57,12,[],200,EleutherAI/gpt-neox-20b,64,1,724.0,1.0,1,H100,1697105040588,1697105041312,120,13.0,1.0,"[20, 704]","[1697105040608, 1697105041312]"
932,869,19,[],200,EleutherAI/gpt-neox-20b,64,1,2136.0,1.0,1,H100,1697105045701,1697105047837,120,244.0,12.0,"[5, 1124, 75, 56, 55, 69, 58, 438, 69, 68, 67, 52]","[1697105045706, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047650, 1697105047718, 1697105047785, 1697105047837]"
933,648,13,[],200,EleutherAI/gpt-neox-20b,64,1,3200.0,1.0,1,H100,1697105041313,1697105044513,120,84.0,20.0,"[13, 708, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 73, 64, 62, 59, 508, 71]","[1697105041326, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044513]"
934,617,20,[],200,EleutherAI/gpt-neox-20b,64,1,2979.0,1.0,1,H100,1697105047838,1697105050817,120,87.0,20.0,"[9, 717, 129, 73, 55, 54, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 70]","[1697105047847, 1697105048564, 1697105048693, 1697105048766, 1697105048821, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050816]"
935,638,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024789,1697105027572,120,,,"[5, 672, 74, 66, 51, 61, 483, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105024794, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
936,388,21,[],200,EleutherAI/gpt-neox-20b,64,1,2127.0,1.0,1,H100,1697105050817,1697105052944,120,87.0,20.0,"[5, 369, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 57]","[1697105050822, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052944]"
937,836,4,[],200,EleutherAI/gpt-neox-20b,64,1,518.0,1.0,1,H100,1697105024727,1697105025245,120,11.0,1.0,"[16, 502]","[1697105024743, 1697105025245]"
938,419,14,[],200,EleutherAI/gpt-neox-20b,64,1,2571.0,1.0,1,H100,1697105044514,1697105047085,120,88.0,20.0,"[16, 560, 73, 71, 54, 66, 65, 50, 450, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55, 69]","[1697105044530, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085]"
939,611,5,[],200,EleutherAI/gpt-neox-20b,64,1,867.0,1.0,1,H100,1697105025246,1697105026113,120,14.0,1.0,"[9, 858]","[1697105025255, 1697105026113]"
940,264,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026116,1697105027572,120,,,"[16, 617, 54, 55, 69, 64]","[1697105026132, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
941,299,8,[],200,EleutherAI/gpt-neox-20b,64,1,1041.0,1.0,1,H100,1697105027574,1697105028615,120,14.0,1.0,"[126, 915]","[1697105027700, 1697105028615]"
942,70,9,[],200,EleutherAI/gpt-neox-20b,64,1,673.0,1.0,1,H100,1697105028616,1697105029289,120,39.0,1.0,"[29, 644]","[1697105028645, 1697105029289]"
943,376,15,[],200,EleutherAI/gpt-neox-20b,64,1,2703.0,1.0,1,H100,1697105041046,1697105043749,120,87.0,20.0,"[5, 323, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56, 62, 60, 58, 520, 73]","[1697105041051, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749]"
944,656,10,[],200,EleutherAI/gpt-neox-20b,64,1,664.0,1.0,1,H100,1697105029290,1697105029954,120,26.0,1.0,"[20, 644]","[1697105029310, 1697105029954]"
945,434,11,[],200,EleutherAI/gpt-neox-20b,64,1,2892.0,1.0,1,H100,1697105029955,1697105032847,120,85.0,20.0,"[9, 575, 49, 450, 70, 69, 69, 56, 509, 71, 55, 56, 65, 64, 62, 62, 305, 164, 59, 72]","[1697105029964, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031246, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032551, 1697105032715, 1697105032774, 1697105032846]"
946,437,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035260,1697105037423,120,,,"[8, 581, 64, 61, 59, 56, 601, 70, 69, 66, 58]","[1697105035268, 1697105035849, 1697105035913, 1697105035974, 1697105036033, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
947,85,12,[],200,EleutherAI/gpt-neox-20b,64,1,2411.0,1.0,1,H100,1697105032848,1697105035259,120,88.0,20.0,"[4, 493, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51]","[1697105032852, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259]"
948,311,12,[],200,EleutherAI/gpt-neox-20b,64,1,2315.0,1.0,1,H100,1697105031248,1697105033563,120,93.0,20.0,"[13, 550, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 61, 48, 48, 61]","[1697105031261, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033406, 1697105033454, 1697105033502, 1697105033563]"
949,791,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035260,1697105037423,120,,,"[16, 573, 64, 61, 58, 57, 601, 70, 69, 66, 58]","[1697105035276, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
950,444,14,[],200,EleutherAI/gpt-neox-20b,64,1,1430.0,1.0,1,H100,1697105037425,1697105038855,120,457.0,6.0,"[94, 1121, 58, 48, 55, 54]","[1697105037519, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855]"
951,630,16,[],200,EleutherAI/gpt-neox-20b,64,1,543.0,1.0,1,H100,1697105042977,1697105043520,120,6.0,1.0,"[16, 527]","[1697105042993, 1697105043520]"
952,851,12,[],200,EleutherAI/gpt-neox-20b,64,1,521.0,1.0,1,H100,1697105029955,1697105030476,120,23.0,1.0,"[17, 504]","[1697105029972, 1697105030476]"
953,410,17,[],200,EleutherAI/gpt-neox-20b,64,1,1948.0,1.0,1,H100,1697105043521,1697105045469,120,364.0,12.0,"[5, 916, 70, 62, 58, 458, 73, 71, 54, 66, 65, 50]","[1697105043526, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469]"
954,893,13,[],200,EleutherAI/gpt-neox-20b,64,1,1753.0,1.0,1,H100,1697105033564,1697105035317,120,335.0,10.0,"[20, 1066, 65, 59, 58, 57, 269, 50, 51, 58]","[1697105033584, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317]"
955,63,18,[],200,EleutherAI/gpt-neox-20b,64,1,1132.0,1.0,1,H100,1697105045470,1697105046602,120,39.0,1.0,"[5, 1126]","[1697105045475, 1697105046601]"
956,768,19,[],200,EleutherAI/gpt-neox-20b,64,1,1235.0,1.0,1,H100,1697105046602,1697105047837,120,47.0,6.0,"[5, 973, 69, 68, 68, 52]","[1697105046607, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837]"
957,421,20,[],200,EleutherAI/gpt-neox-20b,64,1,2979.0,1.0,1,H100,1697105047838,1697105050817,120,85.0,20.0,"[5, 721, 129, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 70]","[1697105047843, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050816]"
958,298,6,[],200,EleutherAI/gpt-neox-20b,64,1,254.0,1.0,1,H100,1697105024148,1697105024402,120,17.0,1.0,"[4, 250]","[1697105024152, 1697105024402]"
959,68,7,[],200,EleutherAI/gpt-neox-20b,64,1,842.0,1.0,1,H100,1697105024403,1697105025245,120,12.0,1.0,"[13, 829]","[1697105024416, 1697105025245]"
960,216,15,[],200,EleutherAI/gpt-neox-20b,64,1,3240.0,1.0,1,H100,1697105038856,1697105042096,120,91.0,20.0,"[5, 1377, 68, 54, 64, 49, 61, 53, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62]","[1697105038861, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096]"
961,622,13,[],200,EleutherAI/gpt-neox-20b,64,1,486.0,1.0,1,H100,1697105030477,1697105030963,120,20.0,1.0,"[12, 474]","[1697105030489, 1697105030963]"
962,283,14,[],200,EleutherAI/gpt-neox-20b,64,1,2598.0,1.0,1,H100,1697105030964,1697105033562,120,85.0,20.0,"[9, 838, 71, 55, 56, 65, 64, 62, 62, 306, 163, 60, 72, 65, 62, 371, 60, 49, 48, 60]","[1697105030973, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562]"
963,669,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035318,1697105037423,120,,,"[4, 527, 64, 61, 58, 57, 601, 70, 69, 66, 58]","[1697105035322, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
964,242,26,[],200,EleutherAI/gpt-neox-20b,64,1,1939.0,1.0,1,H100,1697105068410,1697105070349,120,345.0,9.0,"[9, 880, 61, 60, 59, 55, 676, 70, 69]","[1697105068419, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349]"
965,332,14,[],200,EleutherAI/gpt-neox-20b,64,1,975.0,1.0,1,H100,1697105034118,1697105035093,120,39.0,1.0,"[4, 970]","[1697105034122, 1697105035092]"
966,833,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070350,1697105070950,120,,,[4],[1697105070354]
967,916,15,[],200,EleutherAI/gpt-neox-20b,64,1,665.0,1.0,1,H100,1697105035094,1697105035759,120,8.0,1.0,"[12, 653]","[1697105035106, 1697105035759]"
968,602,28,[],200,EleutherAI/gpt-neox-20b,64,1,1007.0,1.0,1,H100,1697105070953,1697105071960,120,15.0,1.0,"[73, 934]","[1697105071026, 1697105071960]"
969,261,29,[],200,EleutherAI/gpt-neox-20b,64,1,937.0,1.0,1,H100,1697105071961,1697105072898,120,874.0,2.0,"[6, 931]","[1697105071967, 1697105072898]"
970,693,16,[],200,EleutherAI/gpt-neox-20b,64,1,930.0,1.0,1,H100,1697105035760,1697105036690,120,67.0,2.0,"[17, 913]","[1697105035777, 1697105036690]"
971,346,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036691,1697105037423,120,,,[5],[1697105036696]
972,318,15,[],200,EleutherAI/gpt-neox-20b,64,1,1377.0,1.0,1,H100,1697105037424,1697105038801,120,6.0,6.0,"[50, 513, 653, 58, 48, 55]","[1697105037474, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801]"
973,124,18,[],200,EleutherAI/gpt-neox-20b,64,1,1215.0,1.0,1,H100,1697105037425,1697105038640,120,83.0,2.0,"[174, 1041]","[1697105037599, 1697105038640]"
974,892,20,[],200,EleutherAI/gpt-neox-20b,64,1,2675.0,1.0,1,H100,1697105053405,1697105056080,120,87.0,20.0,"[5, 649, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71]","[1697105053410, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080]"
975,524,16,[],200,EleutherAI/gpt-neox-20b,64,1,3472.0,1.0,1,H100,1697105039684,1697105043156,120,100.0,30.0,"[9, 545, 68, 54, 64, 49, 61, 53, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58]","[1697105039693, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156]"
976,576,12,[],200,EleutherAI/gpt-neox-20b,64,1,664.0,1.0,1,H100,1697105029290,1697105029954,120,14.0,1.0,"[12, 652]","[1697105029302, 1697105029954]"
977,346,13,[],200,EleutherAI/gpt-neox-20b,64,1,2892.0,1.0,1,H100,1697105029955,1697105032847,120,85.0,20.0,"[5, 579, 49, 450, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 305, 164, 59, 72]","[1697105029960, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032551, 1697105032715, 1697105032774, 1697105032846]"
978,193,21,[],200,EleutherAI/gpt-neox-20b,64,1,2126.0,1.0,1,H100,1697105050818,1697105052944,120,79.0,20.0,"[4, 369, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 57]","[1697105050822, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052944]"
979,766,28,[],200,EleutherAI/gpt-neox-20b,64,1,1007.0,1.0,1,H100,1697105070953,1697105071960,120,11.0,1.0,"[117, 890]","[1697105071070, 1697105071960]"
980,426,29,[],200,EleutherAI/gpt-neox-20b,64,1,4424.0,1.0,1,H100,1697105071961,1697105076385,120,79.0,36.0,"[28, 909, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60, 53, 476, 67, 64, 52, 64, 63, 59, 357, 60, 49, 60, 57, 56, 265, 45]","[1697105071989, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385]"
981,330,21,[],200,EleutherAI/gpt-neox-20b,64,1,2415.0,1.0,1,H100,1697105053727,1697105056142,120,345.0,14.0,"[4, 1081, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62]","[1697105053731, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142]"
982,196,30,[],200,EleutherAI/gpt-neox-20b,64,1,363.0,1.0,1,H100,1697105076386,1697105076749,120,13.0,1.0,"[4, 359]","[1697105076390, 1697105076749]"
983,871,31,[],200,EleutherAI/gpt-neox-20b,64,1,1076.0,1.0,1,H100,1697105076750,1697105077826,120,123.0,6.0,"[12, 832, 63, 49, 61, 59]","[1697105076762, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826]"
984,524,32,[],200,EleutherAI/gpt-neox-20b,64,1,4362.0,1.0,1,H100,1697105077827,1697105082189,120,100.0,30.0,"[5, 365, 56, 55, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 424, 72, 71, 68]","[1697105077832, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189]"
985,7,14,[],200,EleutherAI/gpt-neox-20b,64,1,1330.0,1.0,1,H100,1697105033503,1697105034833,120,345.0,11.0,"[4, 340, 61, 59, 47, 47, 55, 534, 65, 59, 59]","[1697105033507, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034833]"
986,229,16,[],200,EleutherAI/gpt-neox-20b,64,1,1137.0,1.0,1,H100,1697105037426,1697105038563,120,15.0,1.0,"[89, 1048]","[1697105037515, 1697105038563]"
987,652,8,[],200,EleutherAI/gpt-neox-20b,64,1,867.0,1.0,1,H100,1697105025246,1697105026113,120,14.0,1.0,"[13, 854]","[1697105025259, 1697105026113]"
988,429,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026116,1697105027572,120,,,"[20, 613, 54, 55, 69, 64]","[1697105026136, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
989,928,17,[],200,EleutherAI/gpt-neox-20b,64,1,706.0,1.0,1,H100,1697105038565,1697105039271,120,20.0,1.0,"[32, 674]","[1697105038597, 1697105039271]"
990,586,22,[],200,EleutherAI/gpt-neox-20b,64,1,2354.0,1.0,1,H100,1697105052944,1697105055298,120,85.0,20.0,"[5, 455, 71, 67, 62, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 70, 67, 65, 51, 233]","[1697105052949, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054882, 1697105054949, 1697105055014, 1697105055065, 1697105055298]"
991,582,18,[],200,EleutherAI/gpt-neox-20b,64,1,882.0,1.0,1,H100,1697105039272,1697105040154,120,19.0,1.0,"[12, 870]","[1697105039284, 1697105040154]"
992,82,10,[],200,EleutherAI/gpt-neox-20b,64,1,2712.0,1.0,1,H100,1697105027575,1697105030287,120,67.0,20.0,"[20, 367, 770, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 227, 61, 49, 60, 48, 48]","[1697105027595, 1697105027962, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030021, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287]"
993,820,17,[],200,EleutherAI/gpt-neox-20b,64,1,1541.0,1.0,1,H100,1697105045289,1697105046830,120,161.0,9.0,"[9, 622, 133, 76, 69, 53, 65, 50, 464]","[1697105045298, 1697105045920, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830]"
994,30,30,[],200,EleutherAI/gpt-neox-20b,64,1,2538.0,1.0,1,H100,1697105072899,1697105075437,120,93.0,20.0,"[4, 648, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60, 53, 476, 67, 64, 52, 64, 63, 59]","[1697105072903, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436]"
995,355,19,[],200,EleutherAI/gpt-neox-20b,64,1,2821.0,1.0,1,H100,1697105040155,1697105042976,120,90.0,20.0,"[5, 738, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 55]","[1697105040160, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975]"
996,914,18,[],200,EleutherAI/gpt-neox-20b,64,1,3162.0,1.0,1,H100,1697105037425,1697105040587,120,84.0,20.0,"[158, 1057, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53]","[1697105037583, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587]"
997,531,9,[],200,EleutherAI/gpt-neox-20b,64,1,2713.0,1.0,1,H100,1697105027574,1697105030287,120,52.0,20.0,"[13, 375, 770, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 227, 61, 49, 60, 48, 48]","[1697105027587, 1697105027962, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030021, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287]"
998,788,11,[],200,EleutherAI/gpt-neox-20b,64,1,674.0,1.0,1,H100,1697105030289,1697105030963,120,31.0,1.0,"[24, 650]","[1697105030313, 1697105030963]"
999,436,12,[],200,EleutherAI/gpt-neox-20b,64,1,2598.0,1.0,1,H100,1697105030964,1697105033562,120,86.0,20.0,"[13, 834, 71, 55, 56, 65, 64, 62, 62, 306, 163, 60, 72, 65, 62, 371, 60, 49, 48, 60]","[1697105030977, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562]"
1000,7,20,[],200,EleutherAI/gpt-neox-20b,64,1,2114.0,1.0,1,H100,1697105042976,1697105045090,120,345.0,11.0,"[5, 695, 73, 64, 62, 60, 507, 71, 61, 58, 458]","[1697105042981, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045090]"
1001,684,19,[],200,EleutherAI/gpt-neox-20b,64,1,3161.0,1.0,1,H100,1697105040588,1697105043749,120,100.0,20.0,"[4, 782, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56, 62, 60, 58, 520, 73]","[1697105040592, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749]"
1002,704,19,[],200,EleutherAI/gpt-neox-20b,64,1,630.0,1.0,1,H100,1697105038641,1697105039271,120,14.0,1.0,"[5, 625]","[1697105038646, 1697105039271]"
1003,707,31,[],200,EleutherAI/gpt-neox-20b,64,1,843.0,1.0,1,H100,1697105075438,1697105076281,120,8.0,1.0,"[8, 834]","[1697105075446, 1697105076280]"
1004,452,20,[],200,EleutherAI/gpt-neox-20b,64,1,1088.0,1.0,1,H100,1697105039272,1697105040360,120,216.0,4.0,"[8, 874, 84, 68, 54]","[1697105039280, 1697105040154, 1697105040238, 1697105040306, 1697105040360]"
1005,225,21,[],200,EleutherAI/gpt-neox-20b,64,1,476.0,1.0,1,H100,1697105040361,1697105040837,120,23.0,1.0,"[5, 471]","[1697105040366, 1697105040837]"
1006,809,22,[],200,EleutherAI/gpt-neox-20b,64,1,474.0,1.0,1,H100,1697105040838,1697105041312,120,16.0,1.0,"[12, 462]","[1697105040850, 1697105041312]"
1007,586,23,[],200,EleutherAI/gpt-neox-20b,64,1,3200.0,1.0,1,H100,1697105041313,1697105044513,120,85.0,20.0,"[9, 712, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 73, 64, 62, 60, 507, 71]","[1697105041322, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513]"
1008,240,24,[],200,EleutherAI/gpt-neox-20b,64,1,2571.0,1.0,1,H100,1697105044514,1697105047085,120,83.0,20.0,"[12, 564, 73, 71, 54, 66, 65, 50, 450, 133, 76, 70, 53, 65, 50, 465, 74, 56, 55, 69]","[1697105044526, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046831, 1697105046905, 1697105046961, 1697105047016, 1697105047085]"
1009,684,21,[],200,EleutherAI/gpt-neox-20b,64,1,2809.0,1.0,1,H100,1697105045091,1697105047900,120,100.0,20.0,"[4, 824, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 438, 69, 68, 67, 52, 63]","[1697105045095, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047650, 1697105047718, 1697105047785, 1697105047837, 1697105047900]"
1010,593,6,[],200,EleutherAI/gpt-neox-20b,64,1,1586.0,1.0,1,H100,1697105022446,1697105024032,120,335.0,9.0,"[16, 728, 73, 67, 52, 56, 472, 69, 53]","[1697105022462, 1697105023190, 1697105023263, 1697105023330, 1697105023382, 1697105023438, 1697105023910, 1697105023979, 1697105024032]"
1011,428,20,[],200,EleutherAI/gpt-neox-20b,64,1,1537.0,1.0,1,H100,1697105043751,1697105045288,120,31.0,9.0,"[6, 600, 85, 70, 62, 58, 457, 74, 71, 54]","[1697105043757, 1697105044357, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045234, 1697105045288]"
1012,363,32,[],200,EleutherAI/gpt-neox-20b,64,1,3254.0,1.0,1,H100,1697105076281,1697105079535,120,286.0,22.0,"[5, 545, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 56, 61, 60, 59, 58, 916, 72]","[1697105076286, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078309, 1697105078370, 1697105078430, 1697105078489, 1697105078547, 1697105079463, 1697105079535]"
1013,341,7,[],200,EleutherAI/gpt-neox-20b,64,1,2894.0,1.0,1,H100,1697105024033,1697105026927,120,87.0,20.0,"[5, 504, 72, 56, 56, 62, 59, 619, 74, 66, 51, 61, 482, 132, 65, 49, 303, 54, 55, 69]","[1697105024038, 1697105024542, 1697105024614, 1697105024670, 1697105024726, 1697105024788, 1697105024847, 1697105025466, 1697105025540, 1697105025606, 1697105025657, 1697105025718, 1697105026200, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927]"
1014,15,25,[],200,EleutherAI/gpt-neox-20b,64,1,2595.0,1.0,1,H100,1697105047086,1697105049681,120,100.0,20.0,"[16, 479, 68, 69, 67, 52, 62, 665, 129, 73, 55, 54, 55, 70, 354, 75, 72, 71, 55, 54]","[1697105047102, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048564, 1697105048693, 1697105048766, 1697105048821, 1697105048875, 1697105048930, 1697105049000, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681]"
1015,469,16,[],200,EleutherAI/gpt-neox-20b,64,1,377.0,1.0,1,H100,1697105034716,1697105035093,120,17.0,1.0,"[8, 369]","[1697105034724, 1697105035093]"
1016,215,17,[],200,EleutherAI/gpt-neox-20b,64,1,665.0,1.0,1,H100,1697105035094,1697105035759,120,12.0,1.0,"[16, 649]","[1697105035110, 1697105035759]"
1017,49,22,[],200,EleutherAI/gpt-neox-20b,64,1,531.0,1.0,1,H100,1697105052944,1697105053475,120,109.0,3.0,"[9, 451, 71]","[1697105052953, 1697105053404, 1697105053475]"
1018,748,23,[],200,EleutherAI/gpt-neox-20b,64,1,1822.0,1.0,1,H100,1697105053476,1697105055298,120,182.0,14.0,"[9, 574, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233]","[1697105053485, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298]"
1019,913,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035760,1697105037422,120,,,"[9, 921, 70, 69, 66, 58]","[1697105035769, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
1020,453,22,[],200,EleutherAI/gpt-neox-20b,64,1,1302.0,1.0,1,H100,1697105047902,1697105049204,120,26.0,1.0,"[17, 1285]","[1697105047919, 1697105049204]"
1021,109,23,[],200,EleutherAI/gpt-neox-20b,64,1,2932.0,1.0,1,H100,1697105049205,1697105052137,120,90.0,20.0,"[4, 1154, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52]","[1697105049209, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137]"
1022,370,11,[],200,EleutherAI/gpt-neox-20b,64,1,438.0,1.0,1,H100,1697105041515,1697105041953,120,31.0,1.0,"[5, 433]","[1697105041520, 1697105041953]"
1023,145,12,[],200,EleutherAI/gpt-neox-20b,64,1,1795.0,1.0,1,H100,1697105041954,1697105043749,120,161.0,9.0,"[13, 880, 73, 55, 63, 60, 58, 520, 73]","[1697105041967, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749]"
1024,639,11,[],200,EleutherAI/gpt-neox-20b,64,1,656.0,1.0,1,H100,1697105034716,1697105035372,120,100.0,6.0,"[4, 438, 50, 51, 58, 55]","[1697105034720, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372]"
1025,728,13,[],200,EleutherAI/gpt-neox-20b,64,1,605.0,1.0,1,H100,1697105043752,1697105044357,120,20.0,1.0,"[12, 593]","[1697105043764, 1697105044357]"
1026,603,23,[],200,EleutherAI/gpt-neox-20b,64,1,669.0,1.0,1,H100,1697105057343,1697105058012,120,9.0,1.0,"[10, 659]","[1697105057353, 1697105058012]"
1027,433,10,[],200,EleutherAI/gpt-neox-20b,64,1,750.0,1.0,1,H100,1697105030288,1697105031038,120,109.0,2.0,"[9, 741]","[1697105030297, 1697105031038]"
1028,263,24,[],200,EleutherAI/gpt-neox-20b,64,1,638.0,1.0,1,H100,1697105058013,1697105058651,120,15.0,1.0,"[20, 618]","[1697105058033, 1697105058651]"
1029,204,11,[],200,EleutherAI/gpt-neox-20b,64,1,1019.0,1.0,1,H100,1697105031039,1697105032058,120,67.0,6.0,"[4, 768, 71, 55, 56, 65]","[1697105031043, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058]"
1030,86,14,[],200,EleutherAI/gpt-neox-20b,64,1,2935.0,1.0,1,H100,1697105037425,1697105040360,120,335.0,17.0,"[53, 509, 653, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54]","[1697105037478, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360]"
1031,676,15,[],200,EleutherAI/gpt-neox-20b,64,1,476.0,1.0,1,H100,1697105040361,1697105040837,120,19.0,1.0,"[5, 471]","[1697105040366, 1697105040837]"
1032,33,25,[],200,EleutherAI/gpt-neox-20b,64,1,1712.0,1.0,1,H100,1697105058651,1697105060363,120,140.0,7.0,"[5, 811, 681, 59, 57, 45, 54]","[1697105058656, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363]"
1033,617,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105060364,1697105063190,120,,,"[4, 1436, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060368, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1034,447,16,[],200,EleutherAI/gpt-neox-20b,64,1,2009.0,1.0,1,H100,1697105040838,1697105042847,120,161.0,13.0,"[20, 516, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537]","[1697105040858, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847]"
1035,501,14,[],200,EleutherAI/gpt-neox-20b,64,1,518.0,1.0,1,H100,1697105044358,1697105044876,120,19.0,1.0,"[4, 514]","[1697105044362, 1697105044876]"
1036,156,15,[],200,EleutherAI/gpt-neox-20b,64,1,3023.0,1.0,1,H100,1697105044877,1697105047900,120,86.0,20.0,"[8, 1034, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 438, 68, 69, 67, 52, 62]","[1697105044885, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899]"
1037,881,12,[],200,EleutherAI/gpt-neox-20b,64,1,854.0,1.0,1,H100,1697105032059,1697105032913,120,58.0,6.0,"[5, 488, 163, 59, 73, 65]","[1697105032064, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912]"
1038,312,18,[],200,EleutherAI/gpt-neox-20b,64,1,1136.0,1.0,1,H100,1697105058843,1697105059979,120,23.0,1.0,"[133, 1003]","[1697105058976, 1697105059979]"
1039,538,13,[],200,EleutherAI/gpt-neox-20b,64,1,2346.0,1.0,1,H100,1697105032913,1697105035259,120,89.0,20.0,"[9, 423, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 60, 57, 57, 269, 50, 51]","[1697105032922, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034775, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259]"
1040,59,19,[],200,EleutherAI/gpt-neox-20b,64,1,2844.0,1.0,1,H100,1697105059980,1697105062824,120,91.0,20.0,"[25, 759, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 54, 350, 61, 60, 58, 47]","[1697105060005, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062248, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1041,641,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062825,1697105065209,120,,,"[12, 1237, 46, 432, 61, 59, 56, 46, 54]","[1697105062837, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064728, 1697105064774, 1697105064828]"
1042,585,20,[],200,EleutherAI/gpt-neox-20b,64,1,6829.0,1.0,1,H100,1697105044513,1697105051342,120,244.0,50.0,"[9, 568, 73, 71, 54, 66, 65, 50, 451, 132, 76, 70, 53, 65, 50, 465, 74, 56, 55, 69, 58, 438, 68, 69, 67, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75]","[1697105044522, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045920, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046831, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342]"
1043,314,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035260,1697105037423,120,,,"[4, 585, 64, 61, 59, 56, 601, 70, 69, 66, 58]","[1697105035264, 1697105035849, 1697105035913, 1697105035974, 1697105036033, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
1044,895,15,[],200,EleutherAI/gpt-neox-20b,64,1,1137.0,1.0,1,H100,1697105037426,1697105038563,120,15.0,1.0,"[81, 1056]","[1697105037507, 1697105038563]"
1045,673,16,[],200,EleutherAI/gpt-neox-20b,64,1,2537.0,1.0,1,H100,1697105038564,1697105041101,120,93.0,20.0,"[17, 834, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 60, 54, 311, 45, 46, 56, 56]","[1697105038581, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101]"
1046,328,17,[],200,EleutherAI/gpt-neox-20b,64,1,1154.0,1.0,1,H100,1697105041102,1697105042256,120,109.0,6.0,"[4, 928, 62, 58, 56, 46]","[1697105041106, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256]"
1047,98,18,[],200,EleutherAI/gpt-neox-20b,64,1,344.0,1.0,1,H100,1697105042257,1697105042601,120,14.0,1.0,"[8, 336]","[1697105042265, 1697105042601]"
1048,565,6,[],200,EleutherAI/gpt-neox-20b,64,1,2435.0,1.0,1,H100,1697105030540,1697105032975,120,91.0,20.0,"[17, 481, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 60, 72, 65, 63]","[1697105030557, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975]"
1049,687,19,[],200,EleutherAI/gpt-neox-20b,64,1,3527.0,1.0,1,H100,1697105042602,1697105046129,120,96.0,20.0,"[8, 1066, 73, 64, 62, 60, 507, 71, 61, 58, 458, 73, 71, 54, 66, 65, 50, 450, 133, 77]","[1697105042610, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046052, 1697105046129]"
1050,366,23,[],200,EleutherAI/gpt-neox-20b,64,1,950.0,1.0,1,H100,1697105055299,1697105056249,120,85.0,6.0,"[13, 697, 71, 62, 59, 48]","[1697105055312, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249]"
1051,313,7,[],200,EleutherAI/gpt-neox-20b,64,1,793.0,1.0,1,H100,1697105032976,1697105033769,120,20.0,1.0,"[25, 768]","[1697105033001, 1697105033769]"
1052,894,8,[],200,EleutherAI/gpt-neox-20b,64,1,778.0,1.0,1,H100,1697105033770,1697105034548,120,14.0,1.0,"[9, 769]","[1697105033779, 1697105034548]"
1053,451,20,[],200,EleutherAI/gpt-neox-20b,64,1,473.0,1.0,1,H100,1697105046129,1697105046602,120,286.0,1.0,"[5, 468]","[1697105046134, 1697105046602]"
1054,19,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105056250,1697105058840,120,,,"[9, 351, 70, 68, 67, 64, 52, 63, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105056259, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1055,12,7,[],200,EleutherAI/gpt-neox-20b,64,1,356.0,1.0,1,H100,1697105027575,1697105027931,120,11.0,1.0,"[57, 299]","[1697105027632, 1697105027931]"
1056,709,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105027937,1697105037423,120,,,"[25, 770, 56, 44, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 305, 164, 59, 72, 66, 62, 371, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 50, 59, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66, 58]","[1697105027962, 1697105028732, 1697105028788, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032551, 1697105032715, 1697105032774, 1697105032846, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035258, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
1057,83,21,[],200,EleutherAI/gpt-neox-20b,64,1,2751.0,1.0,1,H100,1697105046603,1697105049354,120,123.0,15.0,"[4, 973, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 71, 353]","[1697105046607, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049001, 1697105049354]"
1058,674,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105034550,1697105037423,120,,,"[12, 596, 50, 51, 58, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66, 58]","[1697105034562, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
1059,303,10,[],200,EleutherAI/gpt-neox-20b,64,1,2686.0,1.0,1,H100,1697105030288,1697105032974,120,88.0,20.0,"[13, 737, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62]","[1697105030301, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974]"
1060,328,10,[],200,EleutherAI/gpt-neox-20b,64,1,1431.0,1.0,1,H100,1697105037425,1697105038856,120,109.0,6.0,"[74, 1141, 58, 48, 55, 54]","[1697105037499, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855]"
1061,596,17,[],200,EleutherAI/gpt-neox-20b,64,1,2595.0,1.0,1,H100,1697105047086,1697105049681,120,87.0,20.0,"[21, 474, 69, 68, 67, 52, 62, 665, 129, 73, 54, 55, 55, 70, 354, 74, 73, 71, 55, 54]","[1697105047107, 1697105047581, 1697105047650, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049354, 1697105049428, 1697105049501, 1697105049572, 1697105049627, 1697105049681]"
1062,695,25,[],200,EleutherAI/gpt-neox-20b,64,1,3351.0,1.0,1,H100,1697105058843,1697105062194,120,92.0,20.0,"[33, 591, 681, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70]","[1697105058876, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194]"
1063,99,11,[],200,EleutherAI/gpt-neox-20b,64,1,1297.0,1.0,1,H100,1697105038857,1697105040154,120,10.0,1.0,"[8, 1289]","[1697105038865, 1697105040154]"
1064,689,12,[],200,EleutherAI/gpt-neox-20b,64,1,682.0,1.0,1,H100,1697105040155,1697105040837,120,15.0,1.0,"[9, 673]","[1697105040164, 1697105040837]"
1065,458,13,[],200,EleutherAI/gpt-neox-20b,64,1,474.0,1.0,1,H100,1697105040838,1697105041312,120,11.0,1.0,"[16, 458]","[1697105040854, 1697105041312]"
1066,717,9,[],200,EleutherAI/gpt-neox-20b,64,1,2315.0,1.0,1,H100,1697105031248,1697105033563,120,89.0,20.0,"[6, 557, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 60, 49, 48, 61]","[1697105031254, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033563]"
1067,117,14,[],200,EleutherAI/gpt-neox-20b,64,1,721.0,1.0,1,H100,1697105041313,1697105042034,120,364.0,2.0,"[5, 716]","[1697105041318, 1697105042034]"
1068,885,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105032976,1697105037422,120,,,"[8, 863, 61, 59, 47, 47, 55, 534, 65, 60, 57, 57, 269, 50, 51, 58, 55, 477, 64, 61, 58, 57, 601, 70, 69, 66, 58]","[1697105032984, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034775, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
1069,363,18,[],200,EleutherAI/gpt-neox-20b,64,1,3017.0,1.0,1,H100,1697105049682,1697105052699,120,286.0,22.0,"[12, 669, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 69, 69, 68, 52, 59, 503]","[1697105049694, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051948, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699]"
1070,816,15,[],200,EleutherAI/gpt-neox-20b,64,1,941.0,1.0,1,H100,1697105042035,1697105042976,120,182.0,4.0,"[5, 807, 73, 56]","[1697105042040, 1697105042847, 1697105042920, 1697105042976]"
1071,544,21,[],200,EleutherAI/gpt-neox-20b,64,1,459.0,1.0,1,H100,1697105056081,1697105056540,120,26.0,1.0,"[13, 446]","[1697105056094, 1697105056540]"
1072,291,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105056541,1697105058840,120,,,"[12, 788, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105056553, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1073,444,16,[],200,EleutherAI/gpt-neox-20b,64,1,959.0,1.0,1,H100,1697105042976,1697105043935,120,457.0,6.0,"[9, 691, 73, 64, 62, 60]","[1697105042985, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935]"
1074,575,19,[],200,EleutherAI/gpt-neox-20b,64,1,3162.0,1.0,1,H100,1697105037425,1697105040587,120,86.0,20.0,"[150, 1065, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53]","[1697105037575, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587]"
1075,216,17,[],200,EleutherAI/gpt-neox-20b,64,1,3149.0,1.0,1,H100,1697105043936,1697105047085,120,91.0,20.0,"[8, 1145, 74, 71, 54, 66, 65, 50, 451, 133, 75, 70, 53, 65, 50, 464, 75, 56, 55, 69]","[1697105043944, 1697105045089, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045920, 1697105046053, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085]"
1076,25,19,[],200,EleutherAI/gpt-neox-20b,64,1,616.0,1.0,1,H100,1697105052700,1697105053316,120,12.0,1.0,"[5, 611]","[1697105052705, 1697105053316]"
1077,72,15,[],200,EleutherAI/gpt-neox-20b,64,1,2595.0,1.0,1,H100,1697105047086,1697105049681,120,84.0,20.0,"[16, 479, 68, 69, 67, 52, 63, 664, 129, 73, 54, 55, 55, 70, 354, 75, 72, 71, 55, 54]","[1697105047102, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047900, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681]"
1078,110,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105026928,1697105027572,120,,,[8],[1697105026936]
1079,694,9,[],200,EleutherAI/gpt-neox-20b,64,1,2219.0,1.0,1,H100,1697105027575,1697105029794,120,161.0,13.0,"[129, 1028, 55, 45, 44, 55, 487, 70, 68, 66, 62, 49, 61]","[1697105027704, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794]"
1080,723,20,[],200,EleutherAI/gpt-neox-20b,64,1,668.0,1.0,1,H100,1697105053317,1697105053985,120,14.0,1.0,"[13, 655]","[1697105053330, 1697105053985]"
1081,814,24,[],200,EleutherAI/gpt-neox-20b,64,1,2293.0,1.0,1,H100,1697105052138,1697105054431,120,89.0,20.0,"[4, 557, 74, 57, 57, 56, 67, 394, 71, 66, 63, 62, 60, 333, 56, 55, 70, 69, 53, 68]","[1697105052142, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430]"
1082,301,14,[],200,EleutherAI/gpt-neox-20b,64,1,4951.0,1.0,1,H100,1697105041102,1697105046053,120,109.0,31.0,"[12, 920, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 73, 64, 62, 59, 508, 70, 62, 58, 457, 74, 70, 55, 66, 65, 50, 450, 133]","[1697105041114, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046052]"
1083,381,21,[],200,EleutherAI/gpt-neox-20b,64,1,826.0,1.0,1,H100,1697105053986,1697105054812,120,140.0,2.0,"[5, 821]","[1697105053991, 1697105054812]"
1084,151,22,[],200,EleutherAI/gpt-neox-20b,64,1,411.0,1.0,1,H100,1697105054813,1697105055224,120,39.0,1.0,"[9, 402]","[1697105054822, 1697105055224]"
1085,661,12,[],200,EleutherAI/gpt-neox-20b,64,1,2129.0,1.0,1,H100,1697105037425,1697105039554,120,161.0,10.0,"[29, 533, 653, 58, 48, 55, 54, 560, 71, 68]","[1697105037454, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554]"
1086,707,15,[],200,EleutherAI/gpt-neox-20b,64,1,1138.0,1.0,1,H100,1697105037425,1697105038563,120,8.0,1.0,"[57, 1081]","[1697105037482, 1697105038563]"
1087,360,16,[],200,EleutherAI/gpt-neox-20b,64,1,707.0,1.0,1,H100,1697105038564,1697105039271,120,16.0,1.0,"[17, 690]","[1697105038581, 1697105039271]"
1088,132,17,[],200,EleutherAI/gpt-neox-20b,64,1,2824.0,1.0,1,H100,1697105039272,1697105042096,120,100.0,20.0,"[4, 962, 68, 54, 64, 49, 60, 54, 311, 45, 46, 56, 55, 274, 54, 43, 43, 53, 467, 62]","[1697105039276, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041100, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096]"
1089,314,13,[],200,EleutherAI/gpt-neox-20b,64,1,1546.0,1.0,1,H100,1697105039555,1697105041101,120,335.0,13.0,"[5, 678, 68, 54, 64, 49, 61, 53, 311, 45, 46, 56, 56]","[1697105039560, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101]"
1090,94,16,[],200,EleutherAI/gpt-neox-20b,64,1,2299.0,1.0,1,H100,1697105038802,1697105041101,120,86.0,20.0,"[5, 608, 71, 68, 66, 63, 58, 57, 441, 67, 54, 64, 49, 60, 54, 311, 45, 46, 56, 56]","[1697105038807, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040239, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101]"
1091,254,12,[],200,EleutherAI/gpt-neox-20b,64,1,505.0,1.0,1,H100,1697105036896,1697105037401,120,58.0,1.0,"[17, 488]","[1697105036913, 1697105037401]"
1092,52,15,[],200,EleutherAI/gpt-neox-20b,64,1,1325.0,1.0,1,H100,1697105033564,1697105034889,120,58.0,6.0,"[16, 1070, 65, 59, 58, 57]","[1697105033580, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889]"
1093,196,14,[],200,EleutherAI/gpt-neox-20b,64,1,1138.0,1.0,1,H100,1697105037425,1697105038563,120,13.0,1.0,"[62, 1076]","[1697105037487, 1697105038563]"
1094,900,15,[],200,EleutherAI/gpt-neox-20b,64,1,1119.0,1.0,1,H100,1697105038564,1697105039683,120,67.0,6.0,"[5, 846, 71, 68, 66, 63]","[1697105038569, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683]"
1095,781,22,[],200,EleutherAI/gpt-neox-20b,64,1,1988.0,1.0,1,H100,1697105049354,1697105051342,120,335.0,10.0,"[5, 1004, 237, 74, 72, 71, 63, 311, 76, 75]","[1697105049359, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342]"
1096,437,23,[],200,EleutherAI/gpt-neox-20b,64,1,3539.0,1.0,1,H100,1697105051343,1697105054882,120,91.0,29.0,"[4, 462, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382, 69]","[1697105051347, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881]"
1097,553,16,[],200,EleutherAI/gpt-neox-20b,64,1,2412.0,1.0,1,H100,1697105039684,1697105042096,120,88.0,20.0,"[5, 549, 68, 54, 64, 49, 61, 53, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62]","[1697105039689, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096]"
1098,84,21,[],200,EleutherAI/gpt-neox-20b,64,1,412.0,1.0,1,H100,1697105045289,1697105045701,120,26.0,1.0,"[12, 399]","[1697105045301, 1697105045700]"
1099,782,22,[],200,EleutherAI/gpt-neox-20b,64,1,3299.0,1.0,1,H100,1697105045701,1697105049000,120,90.0,20.0,"[9, 1120, 75, 56, 55, 69, 58, 438, 68, 69, 67, 52, 63, 663, 130, 73, 54, 55, 55, 70]","[1697105045710, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047900, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000]"
1100,611,16,[],200,EleutherAI/gpt-neox-20b,64,1,869.0,1.0,1,H100,1697105034890,1697105035759,120,14.0,1.0,"[8, 861]","[1697105034898, 1697105035759]"
1101,377,17,[],200,EleutherAI/gpt-neox-20b,64,1,829.0,1.0,1,H100,1697105035760,1697105036589,120,13.0,1.0,"[21, 808]","[1697105035781, 1697105036589]"
1102,376,24,[],200,EleutherAI/gpt-neox-20b,64,1,3334.0,1.0,1,H100,1697105041954,1697105045288,120,87.0,20.0,"[8, 885, 73, 55, 63, 60, 58, 520, 72, 65, 62, 59, 508, 71, 61, 58, 457, 74, 70, 55]","[1697105041962, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043748, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288]"
1103,450,11,[],200,EleutherAI/gpt-neox-20b,64,1,2411.0,1.0,1,H100,1697105032848,1697105035259,120,91.0,20.0,"[4, 493, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51]","[1697105032852, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259]"
1104,38,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036590,1697105037423,120,,,[9],[1697105036599]
1105,738,19,[],200,EleutherAI/gpt-neox-20b,64,1,1431.0,1.0,1,H100,1697105037425,1697105038856,120,79.0,6.0,"[69, 1146, 58, 48, 55, 54]","[1697105037494, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855]"
1106,123,16,[],200,EleutherAI/gpt-neox-20b,64,1,544.0,1.0,1,H100,1697105042977,1697105043521,120,14.0,1.0,"[20, 523]","[1697105042997, 1697105043520]"
1107,705,17,[],200,EleutherAI/gpt-neox-20b,64,1,4129.0,1.0,1,H100,1697105043521,1697105047650,120,79.0,27.0,"[13, 908, 70, 62, 58, 458, 73, 71, 54, 66, 64, 51, 451, 133, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 438, 68]","[1697105043534, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045418, 1697105045469, 1697105045920, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047649]"
1108,835,7,[],200,EleutherAI/gpt-neox-20b,64,1,2712.0,1.0,1,H100,1697105027575,1697105030287,120,87.0,20.0,"[37, 350, 770, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 227, 61, 49, 60, 48, 48]","[1697105027612, 1697105027962, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030021, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287]"
1109,403,24,[],200,EleutherAI/gpt-neox-20b,64,1,710.0,1.0,1,H100,1697105055299,1697105056009,120,874.0,2.0,"[4, 706]","[1697105055303, 1697105056009]"
1110,180,25,[],200,EleutherAI/gpt-neox-20b,64,1,1525.0,1.0,1,H100,1697105056010,1697105057535,120,123.0,12.0,"[4, 596, 70, 68, 67, 64, 52, 63, 347, 73, 53, 68]","[1697105056014, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535]"
1111,468,25,[],200,EleutherAI/gpt-neox-20b,64,1,2911.0,1.0,1,H100,1697105054431,1697105057342,120,31.0,20.0,"[17, 776, 74, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 70, 68, 67, 64, 52, 63, 347]","[1697105054448, 1697105055224, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341]"
1112,828,23,[],200,EleutherAI/gpt-neox-20b,64,1,1024.0,1.0,1,H100,1697105055225,1697105056249,120,182.0,6.0,"[4, 780, 71, 62, 59, 48]","[1697105055229, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249]"
1113,37,25,[],200,EleutherAI/gpt-neox-20b,64,1,412.0,1.0,1,H100,1697105045289,1697105045701,120,20.0,1.0,"[13, 399]","[1697105045302, 1697105045701]"
1114,587,16,[],200,EleutherAI/gpt-neox-20b,64,1,605.0,1.0,1,H100,1697105043752,1697105044357,120,13.0,1.0,"[17, 588]","[1697105043769, 1697105044357]"
1115,865,11,[],200,EleutherAI/gpt-neox-20b,64,1,499.0,1.0,1,H100,1697105035260,1697105035759,120,9.0,1.0,"[20, 479]","[1697105035280, 1697105035759]"
1116,868,17,[],200,EleutherAI/gpt-neox-20b,64,1,2510.0,1.0,1,H100,1697105044575,1697105047085,120,85.0,20.0,"[9, 506, 73, 71, 54, 66, 65, 50, 450, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69]","[1697105044584, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085]"
1117,919,22,[],200,EleutherAI/gpt-neox-20b,64,1,397.0,1.0,1,H100,1697105056143,1697105056540,120,14.0,1.0,"[4, 393]","[1697105056147, 1697105056540]"
1118,294,13,[],200,EleutherAI/gpt-neox-20b,64,1,387.0,1.0,1,H100,1697105027575,1697105027962,120,9.0,2.0,"[49, 338]","[1697105027624, 1697105027962]"
1119,874,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058843,1697105063190,120,,,"[28, 596, 681, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 54, 350, 61, 60, 58, 47]","[1697105058871, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062248, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1120,419,14,[],200,EleutherAI/gpt-neox-20b,64,1,3108.0,1.0,1,H100,1697105037425,1697105040533,120,88.0,20.0,"[13, 549, 653, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 63, 50, 60]","[1697105037438, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040423, 1697105040473, 1697105040533]"
1121,489,8,[],200,EleutherAI/gpt-neox-20b,64,1,3773.0,1.0,1,H100,1697105030288,1697105034061,120,79.0,30.0,"[17, 733, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 60, 49, 48, 60, 285, 61, 59, 47, 47]","[1697105030305, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061]"
1122,771,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 13.08 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.74 GiB is free. Process 1645736 has 68.35 GiB memory in use. Of the allocated memory 54.04 GiB is allocated by PyTorch, and 13.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105024848,1697105027572,120,,,"[5, 1348, 131, 65, 49, 303, 54, 55, 69, 64]","[1697105024853, 1697105026201, 1697105026332, 1697105026397, 1697105026446, 1697105026749, 1697105026803, 1697105026858, 1697105026927, 1697105026991]"
1123,883,14,[],200,EleutherAI/gpt-neox-20b,64,1,1325.0,1.0,1,H100,1697105027963,1697105029288,120,563.0,1.0,"[5, 1320]","[1697105027968, 1697105029288]"
1124,240,19,[],200,EleutherAI/gpt-neox-20b,64,1,3153.0,1.0,1,H100,1697105042976,1697105046129,120,83.0,20.0,"[13, 687, 73, 64, 62, 60, 507, 71, 61, 58, 458, 73, 71, 54, 66, 65, 50, 450, 134, 76]","[1697105042989, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046053, 1697105046129]"
1125,655,15,[],200,EleutherAI/gpt-neox-20b,64,1,1819.0,1.0,1,H100,1697105029289,1697105031108,120,335.0,11.0,"[5, 728, 61, 48, 60, 48, 48, 252, 49, 450, 70]","[1697105029294, 1697105030022, 1697105030083, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108]"
1126,443,23,[],200,EleutherAI/gpt-neox-20b,64,1,1188.0,1.0,1,H100,1697105049002,1697105050190,120,19.0,1.0,"[8, 1180]","[1697105049010, 1697105050190]"
1127,210,24,[],200,EleutherAI/gpt-neox-20b,64,1,1000.0,1.0,1,H100,1697105050191,1697105051191,120,140.0,2.0,"[4, 996]","[1697105050195, 1697105051191]"
1128,800,25,[],200,EleutherAI/gpt-neox-20b,64,1,2534.0,1.0,1,H100,1697105051192,1697105053726,120,140.0,20.0,"[5, 612, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60]","[1697105051197, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726]"
1129,391,20,[],200,EleutherAI/gpt-neox-20b,64,1,3240.0,1.0,1,H100,1697105038856,1697105042096,120,79.0,20.0,"[5, 1377, 68, 54, 64, 49, 60, 54, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62]","[1697105038861, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096]"
1130,110,12,[],200,EleutherAI/gpt-neox-20b,64,1,714.0,1.0,1,H100,1697105035260,1697105035974,120,96.0,4.0,"[4, 585, 64, 61]","[1697105035264, 1697105035849, 1697105035913, 1697105035974]"
1131,782,22,[],200,EleutherAI/gpt-neox-20b,64,1,2353.0,1.0,1,H100,1697105052945,1697105055298,120,90.0,20.0,"[12, 447, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382, 70, 67, 65, 51, 233]","[1697105052957, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054882, 1697105054949, 1697105055014, 1697105055065, 1697105055298]"
1132,776,16,[],200,EleutherAI/gpt-neox-20b,64,1,750.0,1.0,1,H100,1697105042097,1697105042847,120,67.0,2.0,"[4, 746]","[1697105042101, 1697105042847]"
1133,301,17,[],200,EleutherAI/gpt-neox-20b,64,1,4742.0,1.0,1,H100,1697105043157,1697105047899,120,109.0,31.0,"[4, 1281, 70, 62, 58, 458, 73, 71, 54, 66, 65, 50, 450, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 438, 68, 69, 67, 52, 62]","[1697105043161, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899]"
1134,484,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105056250,1697105058840,120,,,"[4, 356, 70, 68, 67, 64, 52, 63, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105056254, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1135,546,17,[],200,EleutherAI/gpt-neox-20b,64,1,3281.0,1.0,1,H100,1697105042848,1697105046129,120,93.0,20.0,"[5, 823, 73, 64, 62, 60, 507, 71, 61, 58, 458, 73, 71, 54, 66, 65, 50, 450, 133, 77]","[1697105042853, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046052, 1697105046129]"
1136,256,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058843,1697105063190,120,,,"[89, 1216, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 54, 350, 61, 60, 58, 47]","[1697105058932, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062248, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1137,492,10,[],200,EleutherAI/gpt-neox-20b,64,1,3331.0,1.0,1,H100,1697105033564,1697105036895,120,47.0,20.0,"[16, 1070, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 64, 61, 58, 57, 601, 70, 69, 66]","[1697105033580, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895]"
1138,883,18,[],200,EleutherAI/gpt-neox-20b,64,1,1302.0,1.0,1,H100,1697105047902,1697105049204,120,563.0,1.0,"[7, 1295]","[1697105047909, 1697105049204]"
1139,152,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036896,1697105037423,120,,,[13],[1697105036909]
1140,331,17,[],200,EleutherAI/gpt-neox-20b,64,1,504.0,1.0,1,H100,1697105042097,1697105042601,120,26.0,1.0,"[12, 492]","[1697105042109, 1697105042601]"
1141,851,12,[],200,EleutherAI/gpt-neox-20b,64,1,1137.0,1.0,1,H100,1697105037426,1697105038563,120,23.0,1.0,"[165, 972]","[1697105037591, 1697105038563]"
1142,914,18,[],200,EleutherAI/gpt-neox-20b,64,1,3527.0,1.0,1,H100,1697105042602,1697105046129,120,84.0,20.0,"[4, 1070, 73, 64, 62, 60, 507, 71, 61, 58, 458, 73, 71, 54, 66, 65, 50, 450, 133, 77]","[1697105042606, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046052, 1697105046129]"
1143,570,26,[],200,EleutherAI/gpt-neox-20b,64,1,936.0,1.0,1,H100,1697105053727,1697105054663,120,18.0,1.0,"[12, 923]","[1697105053739, 1697105054662]"
1144,229,27,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105054664,1697105055224,120,15.0,1.0,"[16, 544]","[1697105054680, 1697105055224]"
1145,6,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105055225,1697105058839,120,,,"[12, 772, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 52, 63, 348, 72, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105055237, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057342, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1146,169,21,[],200,EleutherAI/gpt-neox-20b,64,1,504.0,1.0,1,H100,1697105042097,1697105042601,120,10.0,1.0,"[8, 496]","[1697105042105, 1697105042601]"
1147,714,14,[],200,EleutherAI/gpt-neox-20b,64,1,3161.0,1.0,1,H100,1697105040588,1697105043749,120,83.0,20.0,"[4, 782, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56, 62, 60, 58, 520, 73]","[1697105040592, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749]"
1148,511,13,[],200,EleutherAI/gpt-neox-20b,64,1,8452.0,1.0,1,H100,1697105038564,1697105047016,120,364.0,64.0,"[25, 826, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 60, 54, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 72, 65, 62, 59, 508, 70, 62, 58, 457, 74, 70, 55, 66, 64, 51, 450, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55]","[1697105038589, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043748, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045418, 1697105045469, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016]"
1149,798,18,[],200,EleutherAI/gpt-neox-20b,64,1,751.0,1.0,1,H100,1697105047086,1697105047837,120,79.0,6.0,"[4, 491, 68, 69, 67, 52]","[1697105047090, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837]"
1150,811,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035977,1697105037422,120,,,"[14, 699, 71, 68, 66, 58]","[1697105035991, 1697105036690, 1697105036761, 1697105036829, 1697105036895, 1697105036953]"
1151,464,10,[],200,EleutherAI/gpt-neox-20b,64,1,680.0,1.0,1,H100,1697105029796,1697105030476,120,12.0,1.0,"[4, 676]","[1697105029800, 1697105030476]"
1152,125,11,[],200,EleutherAI/gpt-neox-20b,64,1,486.0,1.0,1,H100,1697105030477,1697105030963,120,13.0,1.0,"[16, 470]","[1697105030493, 1697105030963]"
1153,684,19,[],200,EleutherAI/gpt-neox-20b,64,1,2871.0,1.0,1,H100,1697105046130,1697105049001,120,100.0,20.0,"[4, 696, 75, 56, 56, 68, 58, 438, 68, 68, 68, 52, 62, 665, 129, 73, 54, 55, 55, 70]","[1697105046134, 1697105046830, 1697105046905, 1697105046961, 1697105047017, 1697105047085, 1697105047143, 1697105047581, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000]"
1154,464,14,[],200,EleutherAI/gpt-neox-20b,64,1,1138.0,1.0,1,H100,1697105037425,1697105038563,120,12.0,1.0,"[73, 1065]","[1697105037498, 1697105038563]"
1155,419,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035373,1697105037423,120,,,"[9, 1308, 70, 69, 66, 58]","[1697105035382, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
1156,213,15,[],200,EleutherAI/gpt-neox-20b,64,1,1119.0,1.0,1,H100,1697105038564,1697105039683,120,123.0,6.0,"[9, 842, 71, 68, 66, 63]","[1697105038573, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683]"
1157,71,13,[],200,EleutherAI/gpt-neox-20b,64,1,2258.0,1.0,1,H100,1697105037425,1697105039683,120,364.0,11.0,"[102, 1113, 58, 48, 55, 54, 560, 71, 68, 66, 63]","[1697105037527, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683]"
1158,911,16,[],200,EleutherAI/gpt-neox-20b,64,1,1305.0,1.0,1,H100,1697105039684,1697105040989,120,335.0,11.0,"[5, 549, 68, 54, 64, 49, 61, 53, 311, 45, 46]","[1697105039689, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989]"
1159,395,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063192,1697105065210,120,,,"[82, 800, 46, 432, 61, 59, 57, 45, 54]","[1697105063274, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1160,748,14,[],200,EleutherAI/gpt-neox-20b,64,1,1690.0,1.0,1,H100,1697105039684,1697105041374,120,182.0,14.0,"[9, 545, 68, 54, 64, 49, 61, 53, 311, 45, 46, 56, 56, 273]","[1697105039693, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374]"
1161,362,21,[],200,EleutherAI/gpt-neox-20b,64,1,396.0,1.0,1,H100,1697105051343,1697105051739,120,14.0,1.0,"[5, 391]","[1697105051348, 1697105051739]"
1162,839,15,[],200,EleutherAI/gpt-neox-20b,64,1,764.0,1.0,1,H100,1697105053476,1697105054240,120,58.0,5.0,"[5, 578, 56, 55, 70]","[1697105053481, 1697105054059, 1697105054115, 1697105054170, 1697105054240]"
1163,15,22,[],200,EleutherAI/gpt-neox-20b,64,1,2690.0,1.0,1,H100,1697105051740,1697105054430,120,100.0,20.0,"[8, 951, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68]","[1697105051748, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430]"
1164,570,17,[],200,EleutherAI/gpt-neox-20b,64,1,323.0,1.0,1,H100,1697105040990,1697105041313,120,18.0,1.0,"[5, 317]","[1697105040995, 1697105041312]"
1165,609,16,[],200,EleutherAI/gpt-neox-20b,64,1,2507.0,1.0,1,H100,1697105054241,1697105056748,120,88.0,20.0,"[4, 567, 70, 67, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 70, 68]","[1697105054245, 1697105054812, 1697105054882, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748]"
1166,343,18,[],200,EleutherAI/gpt-neox-20b,64,1,3200.0,1.0,1,H100,1697105041313,1697105044513,120,84.0,20.0,"[13, 708, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 73, 64, 62, 60, 507, 71]","[1697105041326, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513]"
1167,247,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057347,1697105058839,120,,,"[9, 737, 65, 64, 63]","[1697105057356, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1168,716,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105054431,1697105058839,120,,,"[9, 858, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 70, 68, 67, 64, 52, 63, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105054440, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1169,51,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105065212,1697105070950,120,,,"[65, 569, 727, 68, 66, 65, 62, 58, 534, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105065277, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1170,517,15,[],200,EleutherAI/gpt-neox-20b,64,1,578.0,1.0,1,H100,1697105041375,1697105041953,120,15.0,1.0,"[8, 570]","[1697105041383, 1697105041953]"
1171,829,27,[],200,EleutherAI/gpt-neox-20b,64,1,1137.0,1.0,1,H100,1697105058842,1697105059979,120,20.0,1.0,"[70, 1067]","[1697105058912, 1697105059979]"
1172,1,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105032848,1697105037422,120,,,"[8, 489, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 63, 62, 58, 56, 602, 70, 69, 66, 58]","[1697105032856, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036032, 1697105036088, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
1173,606,28,[],200,EleutherAI/gpt-neox-20b,64,1,658.0,1.0,1,H100,1697105059980,1697105060638,120,9.0,1.0,"[13, 645]","[1697105059993, 1697105060638]"
1174,146,16,[],200,EleutherAI/gpt-neox-20b,64,1,3079.0,1.0,1,H100,1697105043752,1697105046831,120,96.0,20.0,"[9, 681, 70, 62, 58, 457, 74, 71, 54, 66, 64, 51, 451, 133, 76, 69, 53, 65, 50, 464]","[1697105043761, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045418, 1697105045469, 1697105045920, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830]"
1175,414,21,[],200,EleutherAI/gpt-neox-20b,64,1,3197.0,1.0,1,H100,1697105065212,1697105068409,120,87.0,20.0,"[9, 625, 727, 68, 66, 65, 61, 59, 533, 69, 62, 60, 55, 406, 60, 59, 59, 46, 55, 53]","[1697105065221, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067425, 1697105067494, 1697105067556, 1697105067616, 1697105067671, 1697105068077, 1697105068137, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409]"
1176,262,29,[],200,EleutherAI/gpt-neox-20b,64,1,971.0,1.0,1,H100,1697105060639,1697105061610,120,39.0,1.0,"[20, 951]","[1697105060659, 1697105061610]"
1177,31,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105061611,1697105063191,120,,,"[5, 982, 61, 60, 58, 47]","[1697105061616, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1178,591,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063194,1697105065210,120,,,"[120, 760, 46, 432, 61, 59, 57, 45, 54]","[1697105063314, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1179,361,32,[],200,EleutherAI/gpt-neox-20b,64,1,1680.0,1.0,1,H100,1697105065212,1697105066892,120,67.0,7.0,"[97, 1264, 68, 66, 65, 61, 59]","[1697105065309, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892]"
1180,16,33,[],200,EleutherAI/gpt-neox-20b,64,1,1104.0,1.0,1,H100,1697105066894,1697105067998,120,9.0,1.0,"[8, 1096]","[1697105066902, 1697105067998]"
1181,715,34,[],200,EleutherAI/gpt-neox-20b,64,1,640.0,1.0,1,H100,1697105067999,1697105068639,120,20.0,1.0,"[8, 632]","[1697105068007, 1697105068639]"
1182,376,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068639,1697105070951,120,,,"[9, 651, 61, 60, 59, 56, 675, 70, 69, 67, 56]","[1697105068648, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069535, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1183,763,26,[],200,EleutherAI/gpt-neox-20b,64,1,476.0,1.0,1,H100,1697105057536,1697105058012,120,20.0,1.0,"[4, 471]","[1697105057540, 1697105058011]"
1184,735,17,[],200,EleutherAI/gpt-neox-20b,64,1,2850.0,1.0,1,H100,1697105046831,1697105049681,120,85.0,20.0,"[9, 741, 68, 69, 67, 52, 62, 664, 130, 73, 54, 55, 55, 71, 353, 75, 72, 71, 55, 54]","[1697105046840, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049001, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681]"
1185,537,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058013,1697105058840,120,,,[12],[1697105058025]
1186,187,28,[],200,EleutherAI/gpt-neox-20b,64,1,1467.0,1.0,1,H100,1697105058843,1697105060310,120,161.0,6.0,"[41, 583, 681, 59, 57, 46]","[1697105058884, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310]"
1187,864,29,[],200,EleutherAI/gpt-neox-20b,64,1,2513.0,1.0,1,H100,1697105060311,1697105062824,120,83.0,20.0,"[8, 445, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060319, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1188,519,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062825,1697105065209,120,,,"[21, 1228, 46, 432, 61, 59, 57, 45, 54]","[1697105062846, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1189,508,18,[],200,EleutherAI/gpt-neox-20b,64,1,2455.0,1.0,1,H100,1697105049682,1697105052137,120,86.0,20.0,"[8, 673, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 69, 69, 68, 52]","[1697105049690, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051948, 1697105052017, 1697105052085, 1697105052137]"
1190,147,36,[],200,EleutherAI/gpt-neox-20b,64,1,1006.0,1.0,1,H100,1697105070954,1697105071960,120,182.0,1.0,"[125, 881]","[1697105071079, 1697105071960]"
1191,707,15,[],200,EleutherAI/gpt-neox-20b,64,1,521.0,1.0,1,H100,1697105037425,1697105037946,120,8.0,1.0,"[21, 500]","[1697105037446, 1697105037946]"
1192,289,31,[],200,EleutherAI/gpt-neox-20b,64,1,3197.0,1.0,1,H100,1697105065212,1697105068409,120,89.0,20.0,"[5, 629, 727, 68, 66, 65, 61, 59, 533, 69, 62, 60, 55, 406, 60, 59, 59, 46, 55, 53]","[1697105065217, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067425, 1697105067494, 1697105067556, 1697105067616, 1697105067671, 1697105068077, 1697105068137, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409]"
1193,360,16,[],200,EleutherAI/gpt-neox-20b,64,1,616.0,1.0,1,H100,1697105037947,1697105038563,120,16.0,1.0,"[5, 611]","[1697105037952, 1697105038563]"
1194,304,16,[],200,EleutherAI/gpt-neox-20b,64,1,2453.0,1.0,1,H100,1697105031109,1697105033562,120,86.0,20.0,"[4, 698, 71, 55, 56, 65, 64, 62, 62, 306, 163, 59, 73, 65, 62, 371, 60, 49, 48, 60]","[1697105031113, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032774, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562]"
1195,202,18,[],200,EleutherAI/gpt-neox-20b,64,1,9168.0,1.0,1,H100,1697105046130,1697105055298,120,874.0,72.0,"[12, 688, 75, 56, 56, 68, 58, 437, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 70, 354, 75, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 69, 69, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 66, 63, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 69, 67, 66, 51, 233]","[1697105046142, 1697105046830, 1697105046905, 1697105046961, 1697105047017, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051948, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054948, 1697105055014, 1697105055065, 1697105055298]"
1196,385,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058842,1697105063190,120,,,"[58, 567, 681, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 54, 350, 61, 59, 59, 47]","[1697105058900, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062248, 1697105062598, 1697105062659, 1697105062718, 1697105062777, 1697105062824]"
1197,17,20,[],200,EleutherAI/gpt-neox-20b,64,1,472.0,1.0,1,H100,1697105046130,1697105046602,120,23.0,1.0,"[8, 464]","[1697105046138, 1697105046602]"
1198,733,26,[],200,EleutherAI/gpt-neox-20b,64,1,900.0,1.0,1,H100,1697105045702,1697105046602,120,31.0,1.0,"[12, 887]","[1697105045714, 1697105046601]"
1199,600,21,[],200,EleutherAI/gpt-neox-20b,64,1,892.0,1.0,1,H100,1697105046603,1697105047495,120,23.0,1.0,"[8, 884]","[1697105046611, 1697105047495]"
1200,394,27,[],200,EleutherAI/gpt-neox-20b,64,1,892.0,1.0,1,H100,1697105046603,1697105047495,120,11.0,1.0,"[12, 880]","[1697105046615, 1697105047495]"
1201,284,7,[],200,EleutherAI/gpt-neox-20b,64,1,4483.0,1.0,1,H100,1697105027575,1697105032058,120,90.0,31.0,"[109, 1048, 55, 45, 44, 55, 487, 70, 68, 66, 62, 49, 61, 228, 60, 49, 60, 48, 48, 252, 49, 450, 70, 69, 68, 57, 509, 71, 55, 56, 65]","[1697105027684, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029622, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539, 1697105030588, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058]"
1202,375,22,[],200,EleutherAI/gpt-neox-20b,64,1,3104.0,1.0,1,H100,1697105047496,1697105050600,120,874.0,17.0,"[8, 1059, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237]","[1697105047504, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600]"
1203,167,28,[],200,EleutherAI/gpt-neox-20b,64,1,3321.0,1.0,1,H100,1697105047496,1697105050817,120,88.0,20.0,"[4, 1064, 129, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71]","[1697105047500, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817]"
1204,36,23,[],200,EleutherAI/gpt-neox-20b,64,1,2343.0,1.0,1,H100,1697105050601,1697105052944,120,457.0,20.0,"[4, 586, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56]","[1697105050605, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943]"
1205,519,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035760,1697105037422,120,,,"[17, 913, 70, 69, 66, 58]","[1697105035777, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
1206,825,12,[],200,EleutherAI/gpt-neox-20b,64,1,2598.0,1.0,1,H100,1697105030964,1697105033562,120,96.0,20.0,"[13, 834, 71, 55, 56, 65, 64, 62, 62, 306, 163, 60, 72, 65, 62, 371, 60, 49, 48, 60]","[1697105030977, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032974, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562]"
1207,297,13,[],200,EleutherAI/gpt-neox-20b,64,1,10155.0,1.0,1,H100,1697105037425,1697105047580,120,563.0,72.0,"[86, 1129, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53, 311, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 72, 65, 62, 59, 508, 70, 62, 58, 457, 74, 70, 55, 66, 64, 51, 450, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437]","[1697105037511, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043748, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045418, 1697105045469, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580]"
1208,128,33,[],200,EleutherAI/gpt-neox-20b,64,1,561.0,1.0,1,H100,1697105079536,1697105080097,120,9.0,1.0,"[4, 557]","[1697105079540, 1697105080097]"
1209,719,34,[],200,EleutherAI/gpt-neox-20b,64,1,1389.0,1.0,1,H100,1697105080098,1697105081487,120,182.0,6.0,"[12, 1023, 142, 74, 70, 68]","[1697105080110, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487]"
1210,265,9,[],200,EleutherAI/gpt-neox-20b,64,1,2833.0,1.0,1,H100,1697105034062,1697105036895,120,86.0,20.0,"[5, 583, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 63, 62, 59, 56, 601, 70, 69, 66]","[1697105034067, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035912, 1697105035974, 1697105036033, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895]"
1211,486,35,[],200,EleutherAI/gpt-neox-20b,64,1,2925.0,1.0,1,H100,1697105081488,1697105084413,120,14.0,20.0,"[5, 486, 71, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 415, 71, 68, 51, 67, 64, 592]","[1697105081493, 1697105081979, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083500, 1697105083571, 1697105083639, 1697105083690, 1697105083757, 1697105083821, 1697105084413]"
1212,551,8,[],200,EleutherAI/gpt-neox-20b,64,1,2964.0,1.0,1,H100,1697105027575,1697105030539,120,90.0,20.0,"[93, 1064, 55, 45, 44, 55, 487, 70, 68, 65, 63, 49, 61, 228, 60, 49, 60, 48, 48, 252]","[1697105027668, 1697105028732, 1697105028787, 1697105028832, 1697105028876, 1697105028931, 1697105029418, 1697105029488, 1697105029556, 1697105029621, 1697105029684, 1697105029733, 1697105029794, 1697105030022, 1697105030082, 1697105030131, 1697105030191, 1697105030239, 1697105030287, 1697105030539]"
1213,588,15,[],200,EleutherAI/gpt-neox-20b,64,1,259.0,1.0,1,H100,1697105034834,1697105035093,120,11.0,1.0,"[4, 255]","[1697105034838, 1697105035093]"
1214,730,37,[],200,EleutherAI/gpt-neox-20b,64,1,1706.0,1.0,1,H100,1697105071961,1697105073667,120,364.0,12.0,"[24, 913, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64]","[1697105071985, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667]"
1215,364,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035095,1697105037423,120,,,"[19, 735, 64, 61, 58, 57, 601, 70, 69, 66, 58]","[1697105035114, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
1216,136,17,[],200,EleutherAI/gpt-neox-20b,64,1,707.0,1.0,1,H100,1697105038564,1697105039271,120,31.0,1.0,"[25, 682]","[1697105038589, 1697105039271]"
1217,836,13,[],200,EleutherAI/gpt-neox-20b,64,1,80.0,1.0,1,H100,1697105037402,1697105037482,120,11.0,1.0,"[9, 71]","[1697105037411, 1697105037482]"
1218,506,38,[],200,EleutherAI/gpt-neox-20b,64,1,565.0,1.0,1,H100,1697105073668,1697105074233,120,16.0,1.0,"[5, 560]","[1697105073673, 1697105074233]"
1219,205,9,[],200,EleutherAI/gpt-neox-20b,64,1,2435.0,1.0,1,H100,1697105030540,1697105032975,120,87.0,20.0,"[17, 481, 70, 69, 68, 57, 509, 71, 55, 56, 65, 64, 62, 62, 306, 163, 60, 72, 65, 63]","[1697105030557, 1697105031038, 1697105031108, 1697105031177, 1697105031245, 1697105031302, 1697105031811, 1697105031882, 1697105031937, 1697105031993, 1697105032058, 1697105032122, 1697105032184, 1697105032246, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975]"
1220,718,18,[],200,EleutherAI/gpt-neox-20b,64,1,882.0,1.0,1,H100,1697105039272,1697105040154,120,13.0,1.0,"[4, 878]","[1697105039276, 1697105040154]"
1221,17,17,[],200,EleutherAI/gpt-neox-20b,64,1,521.0,1.0,1,H100,1697105037425,1697105037946,120,23.0,1.0,"[45, 476]","[1697105037470, 1697105037946]"
1222,349,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062195,1697105063190,120,,,"[25, 378, 61, 60, 58, 47]","[1697105062220, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1223,718,18,[],200,EleutherAI/gpt-neox-20b,64,1,616.0,1.0,1,H100,1697105037947,1697105038563,120,13.0,1.0,"[8, 608]","[1697105037955, 1697105038563]"
1224,382,19,[],200,EleutherAI/gpt-neox-20b,64,1,2537.0,1.0,1,H100,1697105038564,1697105041101,120,47.0,20.0,"[13, 838, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 60, 54, 311, 45, 46, 56, 56]","[1697105038577, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101]"
1225,120,27,[],200,EleutherAI/gpt-neox-20b,64,1,817.0,1.0,1,H100,1697105063192,1697105064009,120,17.0,1.0,"[14, 803]","[1697105063206, 1697105064009]"
1226,360,17,[],200,EleutherAI/gpt-neox-20b,64,1,446.0,1.0,1,H100,1697105056749,1697105057195,120,16.0,1.0,"[5, 441]","[1697105056754, 1697105057195]"
1227,12,18,[],200,EleutherAI/gpt-neox-20b,64,1,816.0,1.0,1,H100,1697105057196,1697105058012,120,11.0,1.0,"[17, 798]","[1697105057213, 1697105058011]"
1228,718,19,[],200,EleutherAI/gpt-neox-20b,64,1,638.0,1.0,1,H100,1697105058013,1697105058651,120,13.0,1.0,"[8, 630]","[1697105058021, 1697105058651]"
1229,487,15,[],200,EleutherAI/gpt-neox-20b,64,1,2500.0,1.0,1,H100,1697105043751,1697105046251,120,123.0,17.0,"[7, 684, 70, 62, 58, 457, 74, 71, 54, 66, 64, 51, 451, 133, 76, 69, 53]","[1697105043758, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045418, 1697105045469, 1697105045920, 1697105046053, 1697105046129, 1697105046198, 1697105046251]"
1230,168,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105052140,1697105058841,120,,,"[18, 541, 74, 57, 57, 56, 67, 395, 70, 66, 63, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105052158, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053405, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1231,312,20,[],200,EleutherAI/gpt-neox-20b,64,1,1188.0,1.0,1,H100,1697105049002,1697105050190,120,23.0,1.0,"[12, 1176]","[1697105049014, 1697105050190]"
1232,688,23,[],200,EleutherAI/gpt-neox-20b,64,1,926.0,1.0,1,H100,1697105056541,1697105057467,120,345.0,4.0,"[16, 784, 73, 53]","[1697105056557, 1697105057341, 1697105057414, 1697105057467]"
1233,80,21,[],200,EleutherAI/gpt-neox-20b,64,1,922.0,1.0,1,H100,1697105050191,1697105051113,120,13.0,1.0,"[16, 906]","[1697105050207, 1697105051113]"
1234,670,22,[],200,EleutherAI/gpt-neox-20b,64,1,2490.0,1.0,1,H100,1697105051114,1697105053604,120,67.0,18.0,"[4, 691, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62]","[1697105051118, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604]"
1235,464,24,[],200,EleutherAI/gpt-neox-20b,64,1,579.0,1.0,1,H100,1697105058843,1697105059422,120,12.0,1.0,"[45, 534]","[1697105058888, 1697105059422]"
1236,147,36,[],200,EleutherAI/gpt-neox-20b,64,1,547.0,1.0,1,H100,1697105084414,1697105084961,120,182.0,1.0,"[9, 538]","[1697105084423, 1697105084961]"
1237,323,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057468,1697105058839,120,,,"[4, 621, 65, 64, 63]","[1697105057472, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1238,845,37,[],200,EleutherAI/gpt-neox-20b,64,1,6432.0,1.0,1,H100,1697105084962,1697105091394,120,244.0,50.0,"[8, 935, 72, 70, 68, 67, 63, 349, 69, 53, 53, 66, 62, 291, 72, 54, 71, 69, 67, 63, 546, 63, 61, 48, 55, 264, 61, 60, 59, 56, 55, 272, 58, 47, 57, 46, 55, 319, 65, 57, 56, 45, 56, 529, 65, 48, 61, 59, 58, 529]","[1697105084970, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089766, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090687, 1697105090748, 1697105090807, 1697105090865, 1697105091394]"
1239,113,25,[],200,EleutherAI/gpt-neox-20b,64,1,556.0,1.0,1,H100,1697105059423,1697105059979,120,13.0,1.0,"[13, 543]","[1697105059436, 1697105059979]"
1240,819,26,[],200,EleutherAI/gpt-neox-20b,64,1,658.0,1.0,1,H100,1697105059980,1697105060638,120,13.0,1.0,"[25, 633]","[1697105060005, 1697105060638]"
1241,440,23,[],200,EleutherAI/gpt-neox-20b,64,1,2475.0,1.0,1,H100,1697105053605,1697105056080,120,84.0,20.0,"[5, 449, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71]","[1697105053610, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080]"
1242,471,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105060639,1697105063190,120,,,"[8, 1157, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060647, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1243,710,28,[],200,EleutherAI/gpt-neox-20b,64,1,475.0,1.0,1,H100,1697105064010,1697105064485,120,14.0,1.0,"[25, 450]","[1697105064035, 1697105064485]"
1244,878,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068410,1697105070950,120,,,"[5, 884, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105068415, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1245,480,29,[],200,EleutherAI/gpt-neox-20b,64,1,702.0,1.0,1,H100,1697105064486,1697105065188,120,26.0,1.0,"[21, 681]","[1697105064507, 1697105065188]"
1246,597,26,[],200,EleutherAI/gpt-neox-20b,64,1,508.0,1.0,1,H100,1697105049682,1697105050190,120,39.0,1.0,"[17, 491]","[1697105049699, 1697105050190]"
1247,374,27,[],200,EleutherAI/gpt-neox-20b,64,1,2752.0,1.0,1,H100,1697105050191,1697105052943,120,85.0,20.0,"[8, 992, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56]","[1697105050199, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943]"
1248,346,20,[],200,EleutherAI/gpt-neox-20b,64,1,3161.0,1.0,1,H100,1697105040588,1697105043749,120,85.0,20.0,"[8, 778, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56, 62, 60, 58, 520, 73]","[1697105040596, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749]"
1249,136,30,[],200,EleutherAI/gpt-neox-20b,64,1,101.0,1.0,1,H100,1697105065189,1697105065290,120,31.0,1.0,"[8, 93]","[1697105065197, 1697105065290]"
1250,721,18,[],200,EleutherAI/gpt-neox-20b,64,1,941.0,1.0,1,H100,1697105042097,1697105043038,120,286.0,5.0,"[4, 746, 73, 56, 62]","[1697105042101, 1697105042847, 1697105042920, 1697105042976, 1697105043038]"
1251,195,17,[],200,EleutherAI/gpt-neox-20b,64,1,8961.0,1.0,1,H100,1697105042848,1697105051809,120,286.0,64.0,"[5, 823, 73, 64, 62, 60, 507, 71, 61, 58, 458, 73, 71, 54, 66, 65, 50, 450, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 70, 56, 71, 66, 204]","[1697105042853, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051412, 1697105051468, 1697105051539, 1697105051605, 1697105051809]"
1252,158,39,[],200,EleutherAI/gpt-neox-20b,64,1,2660.0,1.0,1,H100,1697105074234,1697105076894,120,85.0,20.0,"[12, 821, 67, 64, 52, 64, 63, 60, 356, 60, 49, 60, 57, 56, 265, 45, 53, 51, 342, 63]","[1697105074246, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894]"
1253,95,25,[],200,EleutherAI/gpt-neox-20b,64,1,1136.0,1.0,1,H100,1697105058843,1697105059979,120,12.0,1.0,"[129, 1007]","[1697105058972, 1697105059979]"
1254,371,20,[],200,EleutherAI/gpt-neox-20b,64,1,254.0,1.0,1,H100,1697105058652,1697105058906,120,13.0,1.0,"[12, 242]","[1697105058664, 1697105058906]"
1255,677,26,[],200,EleutherAI/gpt-neox-20b,64,1,658.0,1.0,1,H100,1697105059980,1697105060638,120,9.0,1.0,"[29, 629]","[1697105060009, 1697105060638]"
1256,454,27,[],200,EleutherAI/gpt-neox-20b,64,1,1555.0,1.0,1,H100,1697105060639,1697105062194,120,182.0,6.0,"[4, 1161, 74, 122, 124, 70]","[1697105060643, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194]"
1257,106,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062195,1697105063190,120,,,"[21, 382, 61, 60, 58, 47]","[1697105062216, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1258,491,19,[],200,EleutherAI/gpt-neox-20b,64,1,481.0,1.0,1,H100,1697105043039,1697105043520,120,14.0,1.0,"[5, 476]","[1697105043044, 1697105043520]"
1259,143,21,[],200,EleutherAI/gpt-neox-20b,64,1,2376.0,1.0,1,H100,1697105058907,1697105061283,120,6.0,12.0,"[85, 1156, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64]","[1697105058992, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283]"
1260,153,20,[],200,EleutherAI/gpt-neox-20b,64,1,1053.0,1.0,1,H100,1697105043521,1697105044574,120,335.0,4.0,"[5, 916, 70, 62]","[1697105043526, 1697105044442, 1697105044512, 1697105044574]"
1261,812,29,[],200,EleutherAI/gpt-neox-20b,64,1,817.0,1.0,1,H100,1697105063192,1697105064009,120,16.0,1.0,"[14, 803]","[1697105063206, 1697105064009]"
1262,852,21,[],200,EleutherAI/gpt-neox-20b,64,1,2510.0,1.0,1,H100,1697105044575,1697105047085,120,100.0,20.0,"[5, 510, 73, 71, 54, 66, 65, 50, 450, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69]","[1697105044580, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085]"
1263,184,13,[],200,EleutherAI/gpt-neox-20b,64,1,3331.0,1.0,1,H100,1697105033564,1697105036895,120,87.0,20.0,"[8, 1078, 65, 59, 59, 56, 269, 50, 51, 58, 55, 477, 64, 61, 58, 57, 601, 70, 69, 66]","[1697105033572, 1697105034650, 1697105034715, 1697105034774, 1697105034833, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895]"
1264,214,24,[],200,EleutherAI/gpt-neox-20b,64,1,2460.0,1.0,1,H100,1697105054882,1697105057342,120,52.0,20.0,"[5, 411, 63, 47, 62, 58, 481, 71, 62, 59, 48, 57, 305, 69, 68, 67, 64, 52, 63, 348]","[1697105054887, 1697105055298, 1697105055361, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056611, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057342]"
1265,676,17,[],200,EleutherAI/gpt-neox-20b,64,1,301.0,1.0,1,H100,1697105044575,1697105044876,120,19.0,1.0,"[5, 296]","[1697105044580, 1697105044876]"
1266,752,29,[],200,EleutherAI/gpt-neox-20b,64,1,449.0,1.0,1,H100,1697105050818,1697105051267,120,39.0,3.0,"[8, 365, 76]","[1697105050826, 1697105051191, 1697105051267]"
1267,423,18,[],200,EleutherAI/gpt-neox-20b,64,1,3022.0,1.0,1,H100,1697105044877,1697105047899,120,84.0,20.0,"[4, 1038, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 438, 68, 69, 67, 52, 62]","[1697105044881, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899]"
1268,600,22,[],200,EleutherAI/gpt-neox-20b,64,1,409.0,1.0,1,H100,1697105047086,1697105047495,120,23.0,1.0,"[12, 397]","[1697105047098, 1697105047495]"
1269,173,16,[],200,EleutherAI/gpt-neox-20b,64,1,3334.0,1.0,1,H100,1697105041954,1697105045288,120,96.0,20.0,"[12, 881, 73, 55, 63, 60, 58, 520, 73, 64, 62, 59, 508, 71, 61, 58, 458, 73, 70, 55]","[1697105041966, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045233, 1697105045288]"
1270,575,19,[],200,EleutherAI/gpt-neox-20b,64,1,2979.0,1.0,1,H100,1697105047838,1697105050817,120,86.0,20.0,"[9, 717, 129, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71]","[1697105047847, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817]"
1271,47,15,[],200,EleutherAI/gpt-neox-20b,64,1,2947.0,1.0,1,H100,1697105046054,1697105049001,120,90.0,20.0,"[4, 772, 75, 56, 55, 69, 58, 438, 68, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 70]","[1697105046058, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000]"
1272,184,13,[],200,EleutherAI/gpt-neox-20b,64,1,3162.0,1.0,1,H100,1697105037425,1697105040587,120,87.0,20.0,"[85, 1130, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53]","[1697105037510, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587]"
1273,749,16,[],200,EleutherAI/gpt-neox-20b,64,1,2455.0,1.0,1,H100,1697105049682,1697105052137,120,47.0,20.0,"[16, 665, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 69, 69, 68, 52]","[1697105049698, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051948, 1697105052017, 1697105052085, 1697105052137]"
1274,475,18,[],200,EleutherAI/gpt-neox-20b,64,1,2850.0,1.0,1,H100,1697105046831,1697105049681,120,89.0,20.0,"[5, 745, 68, 68, 68, 52, 62, 665, 129, 73, 54, 55, 55, 71, 353, 75, 72, 71, 54, 55]","[1697105046836, 1697105047581, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049001, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681]"
1275,734,24,[],200,EleutherAI/gpt-neox-20b,64,1,722.0,1.0,1,H100,1697105052944,1697105053666,120,100.0,6.0,"[9, 451, 71, 67, 62, 62]","[1697105052953, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666]"
1276,395,25,[],200,EleutherAI/gpt-neox-20b,64,1,2413.0,1.0,1,H100,1697105053667,1697105056080,120,88.0,20.0,"[5, 387, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71]","[1697105053672, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080]"
1277,465,30,[],200,EleutherAI/gpt-neox-20b,64,1,603.0,1.0,1,H100,1697105064010,1697105064613,120,364.0,3.0,"[4, 538, 61]","[1697105064014, 1697105064552, 1697105064613]"
1278,237,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064619,1697105065210,120,,,[8],[1697105064627]
1279,658,19,[],200,EleutherAI/gpt-neox-20b,64,1,985.0,1.0,1,H100,1697105049205,1697105050190,120,11.0,1.0,"[8, 977]","[1697105049213, 1697105050190]"
1280,822,32,[],200,EleutherAI/gpt-neox-20b,64,1,3197.0,1.0,1,H100,1697105065212,1697105068409,120,88.0,20.0,"[61, 573, 727, 68, 66, 65, 62, 58, 534, 68, 63, 59, 55, 407, 60, 58, 58, 47, 55, 53]","[1697105065273, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409]"
1281,315,20,[],200,EleutherAI/gpt-neox-20b,64,1,1946.0,1.0,1,H100,1697105050191,1697105052137,120,335.0,14.0,"[8, 992, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52]","[1697105050199, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137]"
1282,52,17,[],200,EleutherAI/gpt-neox-20b,64,1,1326.0,1.0,1,H100,1697105033563,1697105034889,120,58.0,6.0,"[5, 1082, 65, 59, 58, 57]","[1697105033568, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889]"
1283,847,26,[],200,EleutherAI/gpt-neox-20b,64,1,817.0,1.0,1,H100,1697105063192,1697105064009,120,10.0,1.0,"[38, 779]","[1697105063230, 1697105064009]"
1284,616,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064012,1697105065209,120,,,"[35, 505, 61, 59, 57, 45, 54]","[1697105064047, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1285,751,22,[],200,EleutherAI/gpt-neox-20b,64,1,7143.0,1.0,1,H100,1697105042602,1697105049745,120,216.0,50.0,"[12, 1062, 73, 64, 62, 60, 507, 71, 61, 58, 458, 73, 71, 54, 66, 65, 50, 450, 133, 77, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 75, 73, 71, 54, 55, 64]","[1697105042614, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046052, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049428, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745]"
1286,833,31,[],200,EleutherAI/gpt-neox-20b,64,1,2135.0,1.0,1,H100,1697105065291,1697105067426,120,563.0,8.0,"[99, 1183, 68, 66, 65, 62, 58, 534]","[1697105065390, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067426]"
1287,592,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068410,1697105070951,120,,,"[21, 868, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105068431, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1288,90,14,[],200,EleutherAI/gpt-neox-20b,64,1,851.0,1.0,1,H100,1697105041102,1697105041953,120,19.0,1.0,"[12, 839]","[1697105041114, 1697105041953]"
1289,673,15,[],200,EleutherAI/gpt-neox-20b,64,1,3334.0,1.0,1,H100,1697105041954,1697105045288,120,93.0,20.0,"[4, 889, 73, 55, 63, 60, 58, 520, 73, 64, 62, 59, 508, 71, 61, 58, 457, 74, 70, 55]","[1697105041958, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288]"
1290,677,6,[],200,EleutherAI/gpt-neox-20b,64,1,477.0,1.0,1,H100,1697105031248,1697105031725,120,9.0,1.0,"[9, 468]","[1697105031257, 1697105031725]"
1291,446,7,[],200,EleutherAI/gpt-neox-20b,64,1,688.0,1.0,1,H100,1697105031727,1697105032415,120,26.0,1.0,"[4, 684]","[1697105031731, 1697105032415]"
1292,767,14,[],200,EleutherAI/gpt-neox-20b,64,1,724.0,1.0,1,H100,1697105040588,1697105041312,120,11.0,1.0,"[16, 708]","[1697105040604, 1697105041312]"
1293,4,19,[],200,EleutherAI/gpt-neox-20b,64,1,2571.0,1.0,1,H100,1697105044514,1697105047085,120,89.0,20.0,"[16, 560, 73, 71, 54, 66, 65, 50, 450, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69]","[1697105044530, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085]"
1294,498,19,[],200,EleutherAI/gpt-neox-20b,64,1,682.0,1.0,1,H100,1697105040155,1697105040837,120,9.0,1.0,"[8, 674]","[1697105040163, 1697105040837]"
1295,846,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036896,1697105037423,120,,,[13],[1697105036909]
1296,619,11,[],200,EleutherAI/gpt-neox-20b,64,1,1137.0,1.0,1,H100,1697105037426,1697105038563,120,10.0,1.0,"[65, 1072]","[1697105037491, 1697105038563]"
1297,273,12,[],200,EleutherAI/gpt-neox-20b,64,1,707.0,1.0,1,H100,1697105038564,1697105039271,120,19.0,1.0,"[29, 678]","[1697105038593, 1697105039271]"
1298,44,13,[],200,EleutherAI/gpt-neox-20b,64,1,882.0,1.0,1,H100,1697105039272,1697105040154,120,12.0,1.0,"[8, 874]","[1697105039280, 1697105040154]"
1299,141,16,[],200,EleutherAI/gpt-neox-20b,64,1,2749.0,1.0,1,H100,1697105046252,1697105049001,120,89.0,20.0,"[4, 574, 75, 56, 56, 68, 58, 437, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 71]","[1697105046256, 1697105046830, 1697105046905, 1697105046961, 1697105047017, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049001]"
1300,404,17,[],200,EleutherAI/gpt-neox-20b,64,1,2292.0,1.0,1,H100,1697105052138,1697105054430,120,87.0,20.0,"[4, 557, 74, 57, 57, 56, 67, 394, 71, 66, 63, 62, 60, 333, 56, 55, 70, 69, 53, 68]","[1697105052142, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430]"
1301,605,14,[],200,EleutherAI/gpt-neox-20b,64,1,682.0,1.0,1,H100,1697105040155,1697105040837,120,8.0,1.0,"[13, 669]","[1697105040168, 1697105040837]"
1302,855,16,[],200,EleutherAI/gpt-neox-20b,64,1,3637.0,1.0,1,H100,1697105047902,1697105051539,120,83.0,20.0,"[13, 1439, 75, 72, 71, 55, 54, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71]","[1697105047915, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539]"
1303,374,15,[],200,EleutherAI/gpt-neox-20b,64,1,2911.0,1.0,1,H100,1697105040838,1697105043749,120,85.0,20.0,"[8, 528, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56, 62, 60, 58, 520, 73]","[1697105040846, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749]"
1304,247,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063194,1697105065210,120,,,"[132, 748, 46, 432, 61, 59, 57, 45, 54]","[1697105063326, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1305,181,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105054431,1697105058839,120,,,"[13, 854, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 70, 68, 67, 64, 52, 63, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105054444, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1306,766,14,[],200,EleutherAI/gpt-neox-20b,64,1,504.0,1.0,1,H100,1697105036897,1697105037401,120,11.0,1.0,"[16, 488]","[1697105036913, 1697105037401]"
1307,275,28,[],200,EleutherAI/gpt-neox-20b,64,1,1495.0,1.0,1,H100,1697105065212,1697105066707,120,161.0,4.0,"[105, 1256, 68, 66]","[1697105065317, 1697105066573, 1697105066641, 1697105066707]"
1308,645,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063192,1697105065209,120,,,"[18, 864, 46, 432, 61, 59, 57, 45, 54]","[1697105063210, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1309,451,18,[],200,EleutherAI/gpt-neox-20b,64,1,822.0,1.0,1,H100,1697105047650,1697105048472,120,286.0,1.0,"[5, 817]","[1697105047655, 1697105048472]"
1310,221,19,[],200,EleutherAI/gpt-neox-20b,64,1,5002.0,1.0,1,H100,1697105048473,1697105053475,120,364.0,36.0,"[4, 877, 75, 72, 71, 55, 54, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71]","[1697105048477, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475]"
1311,588,29,[],200,EleutherAI/gpt-neox-20b,64,1,1137.0,1.0,1,H100,1697105058842,1697105059979,120,11.0,1.0,"[66, 1071]","[1697105058908, 1697105059979]"
1312,73,15,[],200,EleutherAI/gpt-neox-20b,64,1,302.0,1.0,1,H100,1697105040535,1697105040837,120,9.0,1.0,"[16, 286]","[1697105040551, 1697105040837]"
1313,334,30,[],200,EleutherAI/gpt-neox-20b,64,1,658.0,1.0,1,H100,1697105059980,1697105060638,120,15.0,1.0,"[29, 629]","[1697105060009, 1697105060638]"
1314,255,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105047496,1697105058841,120,,,"[20, 1048, 129, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105047516, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1315,917,31,[],200,EleutherAI/gpt-neox-20b,64,1,1165.0,1.0,1,H100,1697105060639,1697105061804,120,123.0,2.0,"[4, 1161]","[1697105060643, 1697105061804]"
1316,764,19,[],200,EleutherAI/gpt-neox-20b,64,1,580.0,1.0,1,H100,1697105058842,1697105059422,120,39.0,1.0,"[62, 518]","[1697105058904, 1697105059422]"
1317,107,8,[],200,EleutherAI/gpt-neox-20b,64,1,929.0,1.0,1,H100,1697105032416,1697105033345,120,216.0,2.0,"[5, 924]","[1697105032421, 1697105033345]"
1318,679,13,[],200,EleutherAI/gpt-neox-20b,64,1,301.0,1.0,1,H100,1697105044575,1697105044876,120,15.0,1.0,"[9, 292]","[1697105044584, 1697105044876]"
1319,774,16,[],200,EleutherAI/gpt-neox-20b,64,1,474.0,1.0,1,H100,1697105040838,1697105041312,120,8.0,1.0,"[8, 466]","[1697105040846, 1697105041312]"
1320,77,19,[],200,EleutherAI/gpt-neox-20b,64,1,3637.0,1.0,1,H100,1697105047902,1697105051539,120,92.0,20.0,"[5, 1447, 75, 72, 71, 55, 54, 64, 618, 237, 74, 72, 70, 64, 311, 76, 74, 72, 55, 71]","[1697105047907, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050816, 1697105050880, 1697105051191, 1697105051267, 1697105051341, 1697105051413, 1697105051468, 1697105051539]"
1321,608,14,[],200,EleutherAI/gpt-neox-20b,64,1,3104.0,1.0,1,H100,1697105037483,1697105040587,120,96.0,20.0,"[124, 1033, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 61, 53]","[1697105037607, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040534, 1697105040587]"
1322,430,17,[],200,EleutherAI/gpt-neox-20b,64,1,640.0,1.0,1,H100,1697105041313,1697105041953,120,15.0,1.0,"[17, 623]","[1697105041330, 1697105041953]"
1323,200,18,[],200,EleutherAI/gpt-neox-20b,64,1,1795.0,1.0,1,H100,1697105041954,1697105043749,120,6.0,9.0,"[4, 889, 73, 55, 63, 60, 58, 520, 73]","[1697105041958, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749]"
1324,542,15,[],200,EleutherAI/gpt-neox-20b,64,1,639.0,1.0,1,H100,1697105041314,1697105041953,120,11.0,1.0,"[20, 619]","[1697105041334, 1697105041953]"
1325,335,14,[],200,EleutherAI/gpt-neox-20b,64,1,8598.0,1.0,1,H100,1697105044877,1697105053475,120,58.0,62.0,"[16, 1026, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 438, 69, 68, 67, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 70, 56, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71]","[1697105044893, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047650, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051412, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475]"
1326,552,23,[],200,EleutherAI/gpt-neox-20b,64,1,2367.0,1.0,1,H100,1697105055299,1697105057666,120,87.0,20.0,"[21, 689, 71, 62, 59, 48, 57, 304, 70, 68, 67, 64, 51, 64, 347, 73, 53, 68, 66, 65]","[1697105055320, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666]"
1327,228,20,[],200,EleutherAI/gpt-neox-20b,64,1,2126.0,1.0,1,H100,1697105050818,1697105052944,120,100.0,20.0,"[8, 365, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 57]","[1697105050826, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052944]"
1328,880,10,[],200,EleutherAI/gpt-neox-20b,64,1,871.0,1.0,1,H100,1697105032976,1697105033847,120,84.0,2.0,"[17, 854]","[1697105032993, 1697105033847]"
1329,533,11,[],200,EleutherAI/gpt-neox-20b,64,1,802.0,1.0,1,H100,1697105033848,1697105034650,120,216.0,2.0,"[4, 798]","[1697105033852, 1697105034650]"
1330,704,20,[],200,EleutherAI/gpt-neox-20b,64,1,409.0,1.0,1,H100,1697105047086,1697105047495,120,14.0,1.0,"[8, 401]","[1697105047094, 1697105047495]"
1331,310,12,[],200,EleutherAI/gpt-neox-20b,64,1,441.0,1.0,1,H100,1697105034652,1697105035093,120,26.0,1.0,"[8, 432]","[1697105034660, 1697105035092]"
1332,358,21,[],200,EleutherAI/gpt-neox-20b,64,1,1197.0,1.0,1,H100,1697105047496,1697105048693,120,216.0,3.0,"[8, 1059, 130]","[1697105047504, 1697105048563, 1697105048693]"
1333,894,13,[],200,EleutherAI/gpt-neox-20b,64,1,665.0,1.0,1,H100,1697105035094,1697105035759,120,14.0,1.0,"[12, 653]","[1697105035106, 1697105035759]"
1334,664,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035760,1697105037422,120,,,"[21, 909, 70, 69, 66, 58]","[1697105035781, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
1335,48,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036896,1697105037423,120,,,[5],[1697105036901]
1336,748,14,[],200,EleutherAI/gpt-neox-20b,64,1,2813.0,1.0,1,H100,1697105037425,1697105040238,120,182.0,14.0,"[154, 1061, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440]","[1697105037579, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238]"
1337,635,18,[],200,EleutherAI/gpt-neox-20b,64,1,869.0,1.0,1,H100,1697105034890,1697105035759,120,23.0,1.0,"[5, 864]","[1697105034895, 1697105035759]"
1338,410,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035760,1697105037423,120,,,"[5, 925, 70, 69, 66, 58]","[1697105035765, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
1339,296,24,[],200,EleutherAI/gpt-neox-20b,64,1,983.0,1.0,1,H100,1697105057668,1697105058651,120,6.0,1.0,"[24, 959]","[1697105057692, 1697105058651]"
1340,63,20,[],200,EleutherAI/gpt-neox-20b,64,1,1138.0,1.0,1,H100,1697105037425,1697105038563,120,39.0,1.0,"[70, 1068]","[1697105037495, 1697105038563]"
1341,769,21,[],200,EleutherAI/gpt-neox-20b,64,1,2537.0,1.0,1,H100,1697105038564,1697105041101,120,47.0,20.0,"[21, 830, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 60, 53, 312, 45, 46, 56, 56]","[1697105038585, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040586, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101]"
1342,732,22,[],200,EleutherAI/gpt-neox-20b,64,1,1540.0,1.0,1,H100,1697105061284,1697105062824,120,345.0,12.0,"[4, 516, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105061288, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1343,679,17,[],200,EleutherAI/gpt-neox-20b,64,1,851.0,1.0,1,H100,1697105041102,1697105041953,120,15.0,1.0,"[16, 835]","[1697105041118, 1697105041953]"
1344,63,21,[],200,EleutherAI/gpt-neox-20b,64,1,467.0,1.0,1,H100,1697105052138,1697105052605,120,39.0,1.0,"[16, 451]","[1697105052154, 1697105052605]"
1345,647,22,[],200,EleutherAI/gpt-neox-20b,64,1,2692.0,1.0,1,H100,1697105052606,1697105055298,120,83.0,20.0,"[9, 789, 71, 67, 62, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 70, 67, 65, 51, 233]","[1697105052615, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054882, 1697105054949, 1697105055014, 1697105055065, 1697105055298]"
1346,537,15,[],200,EleutherAI/gpt-neox-20b,64,1,3131.0,1.0,1,H100,1697105037402,1697105040533,120,83.0,20.0,"[5, 579, 654, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 63, 50, 60]","[1697105037407, 1697105037986, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040423, 1697105040473, 1697105040533]"
1347,797,25,[],200,EleutherAI/gpt-neox-20b,64,1,664.0,1.0,1,H100,1697105057347,1697105058011,120,26.0,1.0,"[12, 652]","[1697105057359, 1697105058011]"
1348,573,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058012,1697105058840,120,,,[5],[1697105058017]
1349,228,27,[],200,EleutherAI/gpt-neox-20b,64,1,3351.0,1.0,1,H100,1697105058843,1697105062194,120,100.0,20.0,"[12, 612, 681, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 121, 125, 70]","[1697105058855, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105061999, 1697105062124, 1697105062194]"
1350,418,23,[],200,EleutherAI/gpt-neox-20b,64,1,781.0,1.0,1,H100,1697105055299,1697105056080,120,286.0,3.0,"[8, 702, 71]","[1697105055307, 1697105056009, 1697105056080]"
1351,78,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105056081,1697105058840,120,,,"[4, 525, 70, 68, 67, 64, 52, 63, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105056085, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1352,502,23,[],200,EleutherAI/gpt-neox-20b,64,1,548.0,1.0,1,H100,1697105062825,1697105063373,120,19.0,1.0,"[8, 540]","[1697105062833, 1697105063373]"
1353,830,29,[],200,EleutherAI/gpt-neox-20b,64,1,2214.0,1.0,1,H100,1697105065212,1697105067426,120,140.0,9.0,"[21, 613, 727, 68, 66, 65, 61, 59, 534]","[1697105065233, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426]"
1354,157,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063374,1697105065211,120,,,"[8, 1170, 61, 59, 57, 45, 54]","[1697105063382, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1355,54,8,[],200,EleutherAI/gpt-neox-20b,64,1,2656.0,1.0,1,H100,1697105032059,1697105034715,120,87.0,20.0,"[5, 488, 163, 60, 72, 65, 63, 370, 60, 49, 48, 60, 285, 61, 59, 47, 47, 55, 534, 65]","[1697105032064, 1697105032552, 1697105032715, 1697105032775, 1697105032847, 1697105032912, 1697105032975, 1697105033345, 1697105033405, 1697105033454, 1697105033502, 1697105033562, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715]"
1356,776,25,[],200,EleutherAI/gpt-neox-20b,64,1,625.0,1.0,1,H100,1697105058842,1697105059467,120,67.0,2.0,"[62, 563]","[1697105058904, 1697105059467]"
1357,515,17,[],200,EleutherAI/gpt-neox-20b,64,1,199.0,1.0,1,H100,1697105051540,1697105051739,120,11.0,1.0,"[12, 187]","[1697105051552, 1697105051739]"
1358,436,26,[],200,EleutherAI/gpt-neox-20b,64,1,3356.0,1.0,1,H100,1697105059468,1697105062824,120,86.0,20.0,"[5, 1291, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105059473, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1359,643,9,[],200,EleutherAI/gpt-neox-20b,64,1,377.0,1.0,1,H100,1697105034716,1697105035093,120,18.0,1.0,"[12, 365]","[1697105034728, 1697105035093]"
1360,284,18,[],200,EleutherAI/gpt-neox-20b,64,1,4269.0,1.0,1,H100,1697105051740,1697105056009,120,90.0,31.0,"[13, 946, 74, 57, 57, 56, 67, 394, 71, 66, 63, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481]","[1697105051753, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009]"
1361,412,10,[],200,EleutherAI/gpt-neox-20b,64,1,1736.0,1.0,1,H100,1697105035093,1697105036829,120,244.0,9.0,"[5, 751, 64, 61, 58, 57, 601, 70, 69]","[1697105035098, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829]"
1362,195,16,[],200,EleutherAI/gpt-neox-20b,64,1,9313.0,1.0,1,H100,1697105041954,1697105051267,120,286.0,64.0,"[17, 876, 73, 56, 62, 60, 58, 520, 73, 64, 62, 60, 507, 71, 61, 58, 458, 73, 70, 55, 66, 65, 50, 450, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76]","[1697105041971, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267]"
1363,340,34,[],200,EleutherAI/gpt-neox-20b,64,1,2826.0,1.0,1,H100,1697105070953,1697105073779,120,85.0,20.0,"[65, 1015, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 61, 51]","[1697105071018, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073779]"
1364,810,20,[],200,EleutherAI/gpt-neox-20b,64,1,2604.0,1.0,1,H100,1697105053476,1697105056080,120,91.0,20.0,"[13, 570, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71]","[1697105053489, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080]"
1365,582,21,[],200,EleutherAI/gpt-neox-20b,64,1,459.0,1.0,1,H100,1697105056081,1697105056540,120,19.0,1.0,"[17, 442]","[1697105056098, 1697105056540]"
1366,231,22,[],200,EleutherAI/gpt-neox-20b,64,1,654.0,1.0,1,H100,1697105056541,1697105057195,120,13.0,1.0,"[20, 634]","[1697105056561, 1697105057195]"
1367,9,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057196,1697105058840,120,,,"[13, 884, 65, 64, 63]","[1697105057209, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1368,43,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105036830,1697105037423,120,,,[4],[1697105036834]
1369,616,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105047087,1697105058840,120,,,"[24, 470, 69, 68, 67, 52, 62, 665, 129, 73, 54, 55, 55, 70, 354, 74, 73, 71, 55, 54, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 69, 67, 66, 51, 233, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 72, 54, 68, 66, 65, 427, 65, 64, 63]","[1697105047111, 1697105047581, 1697105047650, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049354, 1697105049428, 1697105049501, 1697105049572, 1697105049627, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054948, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057413, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1370,742,12,[],200,EleutherAI/gpt-neox-20b,64,1,3110.0,1.0,1,H100,1697105037424,1697105040534,120,89.0,20.0,"[18, 545, 653, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 60]","[1697105037442, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533]"
1371,135,22,[],200,EleutherAI/gpt-neox-20b,64,1,660.0,1.0,1,H100,1697105048694,1697105049354,120,52.0,2.0,"[4, 656]","[1697105048698, 1697105049354]"
1372,728,29,[],200,EleutherAI/gpt-neox-20b,64,1,1007.0,1.0,1,H100,1697105070953,1697105071960,120,20.0,1.0,"[45, 962]","[1697105070998, 1697105071960]"
1373,496,30,[],200,EleutherAI/gpt-neox-20b,64,1,1642.0,1.0,1,H100,1697105071961,1697105073603,120,335.0,11.0,"[9, 928, 73, 68, 63, 60, 48, 60, 47, 234, 52]","[1697105071970, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073270, 1697105073317, 1697105073551, 1697105073603]"
1374,321,15,[],200,EleutherAI/gpt-neox-20b,64,1,1321.0,1.0,1,H100,1697105037425,1697105038746,120,182.0,4.0,"[150, 1065, 58, 48]","[1697105037575, 1697105038640, 1697105038698, 1697105038746]"
1375,877,25,[],200,EleutherAI/gpt-neox-20b,64,1,3542.0,1.0,1,H100,1697105058652,1697105062194,120,85.0,20.0,"[8, 807, 681, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 397, 75, 121, 125, 70]","[1697105058660, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061803, 1697105061878, 1697105061999, 1697105062124, 1697105062194]"
1376,309,25,[],200,EleutherAI/gpt-neox-20b,64,1,3197.0,1.0,1,H100,1697105065212,1697105068409,120,52.0,20.0,"[33, 601, 727, 68, 66, 65, 61, 59, 534, 68, 62, 60, 55, 407, 60, 58, 58, 47, 55, 53]","[1697105065245, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067556, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409]"
1377,199,16,[],200,EleutherAI/gpt-neox-20b,64,1,302.0,1.0,1,H100,1697105040535,1697105040837,120,13.0,1.0,"[20, 282]","[1697105040555, 1697105040837]"
1378,897,17,[],200,EleutherAI/gpt-neox-20b,64,1,474.0,1.0,1,H100,1697105040838,1697105041312,120,9.0,1.0,"[12, 462]","[1697105040850, 1697105041312]"
1379,556,18,[],200,EleutherAI/gpt-neox-20b,64,1,639.0,1.0,1,H100,1697105041314,1697105041953,120,9.0,1.0,"[20, 619]","[1697105041334, 1697105041953]"
1380,601,30,[],200,EleutherAI/gpt-neox-20b,64,1,2989.0,1.0,1,H100,1697105067427,1697105070416,120,83.0,20.0,"[4, 647, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67]","[1697105067431, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
1381,329,19,[],200,EleutherAI/gpt-neox-20b,64,1,647.0,1.0,1,H100,1697105041954,1697105042601,120,15.0,1.0,"[20, 627]","[1697105041974, 1697105042601]"
1382,919,20,[],200,EleutherAI/gpt-neox-20b,64,1,918.0,1.0,1,H100,1697105042602,1697105043520,120,14.0,1.0,"[8, 910]","[1697105042610, 1697105043520]"
1383,425,22,[],200,EleutherAI/gpt-neox-20b,64,1,3410.0,1.0,1,H100,1697105041102,1697105044512,120,88.0,20.0,"[8, 924, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 73, 64, 62, 59, 508, 70]","[1697105041110, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512]"
1384,690,21,[],200,EleutherAI/gpt-neox-20b,64,1,835.0,1.0,1,H100,1697105043522,1697105044357,120,39.0,1.0,"[12, 823]","[1697105043534, 1697105044357]"
1385,344,22,[],200,EleutherAI/gpt-neox-20b,64,1,518.0,1.0,1,H100,1697105044358,1697105044876,120,13.0,1.0,"[12, 506]","[1697105044370, 1697105044876]"
1386,92,23,[],200,EleutherAI/gpt-neox-20b,64,1,3023.0,1.0,1,H100,1697105044877,1697105047900,120,85.0,20.0,"[8, 1034, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 438, 69, 68, 67, 52, 62]","[1697105044885, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047650, 1697105047718, 1697105047785, 1697105047837, 1697105047899]"
1387,694,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105061805,1697105063191,120,,,"[5, 788, 61, 60, 58, 47]","[1697105061810, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1388,349,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063193,1697105065210,120,,,"[114, 767, 46, 432, 61, 59, 57, 45, 54]","[1697105063307, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1389,114,34,[],200,EleutherAI/gpt-neox-20b,64,1,3486.0,1.0,1,H100,1697105065212,1697105068698,120,88.0,20.0,"[93, 1268, 68, 66, 65, 61, 59, 534, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55, 53, 289]","[1697105065305, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698]"
1390,856,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105065212,1697105070950,120,,,"[77, 558, 726, 68, 66, 65, 61, 59, 534, 68, 63, 59, 55, 407, 60, 58, 58, 47, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105065289, 1697105065847, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1391,0,21,[],200,EleutherAI/gpt-neox-20b,64,1,7065.0,1.0,1,H100,1697105043752,1697105050817,120,244.0,50.0,"[16, 674, 70, 62, 58, 457, 74, 71, 54, 66, 64, 51, 451, 133, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 438, 68, 69, 67, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71]","[1697105043768, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045418, 1697105045469, 1697105045920, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817]"
1392,197,23,[],200,EleutherAI/gpt-neox-20b,64,1,956.0,1.0,1,H100,1697105044513,1697105045469,120,6.0,8.0,"[5, 572, 73, 71, 54, 66, 65, 50]","[1697105044518, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469]"
1393,787,24,[],200,EleutherAI/gpt-neox-20b,64,1,1615.0,1.0,1,H100,1697105045470,1697105047085,120,123.0,6.0,"[4, 1356, 75, 56, 55, 69]","[1697105045474, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085]"
1394,555,25,[],200,EleutherAI/gpt-neox-20b,64,1,409.0,1.0,1,H100,1697105047086,1697105047495,120,11.0,1.0,"[20, 389]","[1697105047106, 1697105047495]"
1395,705,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068699,1697105070951,120,,,"[9, 591, 61, 60, 59, 56, 675, 70, 69, 67, 56]","[1697105068708, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069535, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1396,302,26,[],200,EleutherAI/gpt-neox-20b,64,1,3321.0,1.0,1,H100,1697105047496,1697105050817,120,85.0,20.0,"[12, 1055, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71]","[1697105047508, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817]"
1397,886,27,[],200,EleutherAI/gpt-neox-20b,64,1,295.0,1.0,1,H100,1697105050818,1697105051113,120,17.0,1.0,"[16, 279]","[1697105050834, 1697105051113]"
1398,656,28,[],200,EleutherAI/gpt-neox-20b,64,1,625.0,1.0,1,H100,1697105051114,1697105051739,120,26.0,1.0,"[12, 613]","[1697105051126, 1697105051739]"
1399,318,29,[],200,EleutherAI/gpt-neox-20b,64,1,1203.0,1.0,1,H100,1697105051740,1697105052943,120,6.0,6.0,"[17, 942, 74, 57, 57, 56]","[1697105051757, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943]"
1400,788,24,[],200,EleutherAI/gpt-neox-20b,64,1,1302.0,1.0,1,H100,1697105047902,1697105049204,120,31.0,1.0,"[11, 1291]","[1697105047913, 1697105049204]"
1401,83,30,[],200,EleutherAI/gpt-neox-20b,64,1,1868.0,1.0,1,H100,1697105052944,1697105054812,120,123.0,15.0,"[5, 455, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382]","[1697105052949, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812]"
1402,672,31,[],200,EleutherAI/gpt-neox-20b,64,1,2529.0,1.0,1,H100,1697105054813,1697105057342,120,93.0,20.0,"[9, 476, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 70, 68, 67, 64, 52, 63, 348]","[1697105054822, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057342]"
1403,474,36,[],200,EleutherAI/gpt-neox-20b,64,1,4424.0,1.0,1,H100,1697105070953,1697105075377,120,109.0,33.0,"[78, 1002, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 61, 51, 43, 481, 64, 50, 61, 60, 53, 476, 66, 65, 52, 64, 63]","[1697105071031, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073779, 1697105073822, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250, 1697105075314, 1697105075377]"
1404,449,25,[],200,EleutherAI/gpt-neox-20b,64,1,2932.0,1.0,1,H100,1697105049205,1697105052137,120,86.0,20.0,"[4, 1154, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52]","[1697105049209, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137]"
1405,441,32,[],200,EleutherAI/gpt-neox-20b,64,1,669.0,1.0,1,H100,1697105057343,1697105058012,120,6.0,1.0,"[9, 660]","[1697105057352, 1697105058012]"
1406,101,33,[],200,EleutherAI/gpt-neox-20b,64,1,638.0,1.0,1,H100,1697105058013,1697105058651,120,13.0,1.0,"[16, 622]","[1697105058029, 1697105058651]"
1407,133,37,[],200,EleutherAI/gpt-neox-20b,64,1,342.0,1.0,1,H100,1697105075378,1697105075720,120,15.0,1.0,"[5, 337]","[1697105075383, 1697105075720]"
1408,831,38,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105075721,1697105076281,120,11.0,1.0,"[16, 544]","[1697105075737, 1697105076281]"
1409,579,39,[],200,EleutherAI/gpt-neox-20b,64,1,467.0,1.0,1,H100,1697105076282,1697105076749,120,19.0,1.0,"[12, 455]","[1697105076294, 1697105076749]"
1410,234,40,[],200,EleutherAI/gpt-neox-20b,64,1,3630.0,1.0,1,H100,1697105076750,1697105080380,120,457.0,25.0,"[12, 737, 95, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 72, 68, 67, 52, 58, 390, 70, 71, 69]","[1697105076762, 1697105077499, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380]"
1411,393,13,[],200,EleutherAI/gpt-neox-20b,64,1,2564.0,1.0,1,H100,1697105040534,1697105043098,120,182.0,22.0,"[17, 347, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56, 62, 60]","[1697105040551, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098]"
1412,356,15,[],200,EleutherAI/gpt-neox-20b,64,1,786.0,1.0,1,H100,1697105040588,1697105041374,120,874.0,2.0,"[8, 778]","[1697105040596, 1697105041374]"
1413,9,16,[],200,EleutherAI/gpt-neox-20b,64,1,3138.0,1.0,1,H100,1697105041375,1697105044513,120,85.0,20.0,"[4, 655, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 73, 64, 62, 59, 508, 71]","[1697105041379, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044513]"
1414,154,20,[],200,EleutherAI/gpt-neox-20b,64,1,474.0,1.0,1,H100,1697105040838,1697105041312,120,13.0,1.0,"[16, 458]","[1697105040854, 1697105041312]"
1415,852,21,[],200,EleutherAI/gpt-neox-20b,64,1,3200.0,1.0,1,H100,1697105041313,1697105044513,120,100.0,20.0,"[5, 716, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 73, 64, 62, 59, 508, 70]","[1697105041318, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512]"
1416,844,19,[],200,EleutherAI/gpt-neox-20b,64,1,530.0,1.0,1,H100,1697105056010,1697105056540,120,10.0,1.0,"[4, 526]","[1697105056014, 1697105056540]"
1417,152,20,[],200,EleutherAI/gpt-neox-20b,64,1,3410.0,1.0,1,H100,1697105041102,1697105044512,120,87.0,20.0,"[8, 924, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 73, 64, 62, 59, 508, 70]","[1697105041110, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512]"
1418,162,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105056081,1697105058840,120,,,"[8, 521, 70, 68, 67, 64, 52, 63, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105056089, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1419,483,15,[],200,EleutherAI/gpt-neox-20b,64,1,2442.0,1.0,1,H100,1697105040534,1697105042976,120,84.0,20.0,"[5, 359, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56]","[1697105040539, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976]"
1420,878,17,[],200,EleutherAI/gpt-neox-20b,64,1,2611.0,1.0,1,H100,1697105045289,1697105047900,120,83.0,20.0,"[5, 626, 133, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 438, 69, 68, 67, 52, 63]","[1697105045294, 1697105045920, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047650, 1697105047718, 1697105047785, 1697105047837, 1697105047900]"
1421,532,18,[],200,EleutherAI/gpt-neox-20b,64,1,3637.0,1.0,1,H100,1697105047902,1697105051539,120,92.0,20.0,"[10, 1442, 75, 72, 71, 55, 54, 64, 618, 237, 74, 72, 71, 63, 311, 76, 74, 72, 55, 71]","[1697105047912, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051341, 1697105051413, 1697105051468, 1697105051539]"
1422,305,19,[],200,EleutherAI/gpt-neox-20b,64,1,2186.0,1.0,1,H100,1697105051540,1697105053726,120,86.0,20.0,"[4, 265, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60]","[1697105051544, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726]"
1423,259,16,[],200,EleutherAI/gpt-neox-20b,64,1,3152.0,1.0,1,H100,1697105042977,1697105046129,120,87.0,20.0,"[8, 691, 73, 64, 62, 60, 507, 71, 61, 58, 458, 73, 71, 54, 66, 65, 50, 450, 134, 76]","[1697105042985, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046053, 1697105046129]"
1424,45,29,[],200,EleutherAI/gpt-neox-20b,64,1,566.0,1.0,1,H100,1697105066708,1697105067274,120,19.0,1.0,"[5, 561]","[1697105066713, 1697105067274]"
1425,886,20,[],200,EleutherAI/gpt-neox-20b,64,1,936.0,1.0,1,H100,1697105053727,1697105054663,120,17.0,1.0,"[8, 927]","[1697105053735, 1697105054662]"
1426,663,21,[],200,EleutherAI/gpt-neox-20b,64,1,2679.0,1.0,1,H100,1697105054663,1697105057342,120,79.0,20.0,"[5, 630, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 70, 68, 67, 64, 52, 63, 348]","[1697105054668, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057342]"
1427,634,30,[],200,EleutherAI/gpt-neox-20b,64,1,723.0,1.0,1,H100,1697105067275,1697105067998,120,13.0,1.0,"[17, 706]","[1697105067292, 1697105067998]"
1428,841,17,[],200,EleutherAI/gpt-neox-20b,64,1,2563.0,1.0,1,H100,1697105046130,1697105048693,120,123.0,15.0,"[8, 692, 75, 56, 56, 68, 58, 437, 69, 68, 68, 52, 62, 664, 130]","[1697105046138, 1697105046830, 1697105046905, 1697105046961, 1697105047017, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693]"
1429,499,30,[],200,EleutherAI/gpt-neox-20b,64,1,2458.0,1.0,1,H100,1697105051268,1697105053726,120,88.0,20.0,"[5, 536, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60]","[1697105051273, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726]"
1430,156,31,[],200,EleutherAI/gpt-neox-20b,64,1,2471.0,1.0,1,H100,1697105073604,1697105076075,120,86.0,20.0,"[4, 695, 64, 50, 61, 60, 53, 476, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60, 57, 56]","[1697105073608, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
1431,448,18,[],200,EleutherAI/gpt-neox-20b,64,1,1981.0,1.0,1,H100,1697105041954,1697105043935,120,335.0,12.0,"[8, 885, 73, 55, 63, 60, 58, 520, 73, 64, 62, 60]","[1697105041962, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935]"
1432,81,19,[],200,EleutherAI/gpt-neox-20b,64,1,2315.0,1.0,1,H100,1697105043936,1697105046251,120,732.0,13.0,"[4, 1149, 74, 70, 55, 66, 65, 50, 451, 133, 76, 69, 53]","[1697105043940, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045920, 1697105046053, 1697105046129, 1697105046198, 1697105046251]"
1433,6,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062195,1697105063190,120,,,"[17, 386, 61, 60, 58, 47]","[1697105062212, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1434,777,20,[],200,EleutherAI/gpt-neox-20b,64,1,350.0,1.0,1,H100,1697105046252,1697105046602,120,9.0,1.0,"[8, 342]","[1697105046260, 1697105046602]"
1435,438,21,[],200,EleutherAI/gpt-neox-20b,64,1,892.0,1.0,1,H100,1697105046603,1697105047495,120,9.0,1.0,"[12, 880]","[1697105046615, 1697105047495]"
1436,587,29,[],200,EleutherAI/gpt-neox-20b,64,1,817.0,1.0,1,H100,1697105063192,1697105064009,120,13.0,1.0,"[10, 807]","[1697105063202, 1697105064009]"
1437,280,14,[],200,EleutherAI/gpt-neox-20b,64,1,2664.0,1.0,1,H100,1697105047017,1697105049681,120,91.0,20.0,"[5, 559, 68, 69, 67, 52, 62, 665, 129, 73, 54, 55, 55, 70, 354, 75, 72, 71, 55, 54]","[1697105047022, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681]"
1438,359,30,[],200,EleutherAI/gpt-neox-20b,64,1,474.0,1.0,1,H100,1697105064011,1697105064485,120,10.0,1.0,"[32, 442]","[1697105064043, 1697105064485]"
1439,107,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064486,1697105065210,120,,,[8],[1697105064494]
1440,690,32,[],200,EleutherAI/gpt-neox-20b,64,1,1133.0,1.0,1,H100,1697105065212,1697105066345,120,39.0,1.0,"[162, 971]","[1697105065374, 1697105066345]"
1441,464,33,[],200,EleutherAI/gpt-neox-20b,64,1,928.0,1.0,1,H100,1697105066346,1697105067274,120,12.0,1.0,"[24, 904]","[1697105066370, 1697105067274]"
1442,27,15,[],200,EleutherAI/gpt-neox-20b,64,1,5846.0,1.0,1,H100,1697105049682,1697105055528,120,15.0,50.0,"[12, 669, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 69, 69, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58]","[1697105049694, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051948, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528]"
1443,118,34,[],200,EleutherAI/gpt-neox-20b,64,1,3141.0,1.0,1,H100,1697105067275,1697105070416,120,85.0,20.0,"[8, 795, 60, 58, 58, 47, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67]","[1697105067283, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
1444,746,16,[],200,EleutherAI/gpt-neox-20b,64,1,3015.0,1.0,1,H100,1697105049002,1697105052017,120,345.0,18.0,"[8, 1353, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68]","[1697105049010, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017]"
1445,215,26,[],200,EleutherAI/gpt-neox-20b,64,1,467.0,1.0,1,H100,1697105052138,1697105052605,120,12.0,1.0,"[12, 455]","[1697105052150, 1697105052605]"
1446,800,27,[],200,EleutherAI/gpt-neox-20b,64,1,2692.0,1.0,1,H100,1697105052606,1697105055298,120,140.0,20.0,"[9, 789, 71, 67, 62, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 70, 67, 65, 51, 233]","[1697105052615, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054882, 1697105054949, 1697105055014, 1697105055065, 1697105055298]"
1447,523,19,[],200,EleutherAI/gpt-neox-20b,64,1,2316.0,1.0,1,H100,1697105037425,1697105039741,120,345.0,13.0,"[13, 549, 653, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58]","[1697105037438, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741]"
1448,820,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070417,1697105070951,120,,,[9],[1697105070426]
1449,838,40,[],200,EleutherAI/gpt-neox-20b,64,1,2827.0,1.0,1,H100,1697105076895,1697105079722,120,90.0,20.0,"[9, 690, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 72, 68, 67, 52]","[1697105076904, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722]"
1450,591,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058843,1697105063190,120,,,"[137, 1168, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105058980, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1451,172,20,[],200,EleutherAI/gpt-neox-20b,64,1,412.0,1.0,1,H100,1697105039742,1697105040154,120,19.0,1.0,"[5, 407]","[1697105039747, 1697105040154]"
1452,849,21,[],200,EleutherAI/gpt-neox-20b,64,1,682.0,1.0,1,H100,1697105040155,1697105040837,120,10.0,1.0,"[13, 669]","[1697105040168, 1697105040837]"
1453,617,22,[],200,EleutherAI/gpt-neox-20b,64,1,2911.0,1.0,1,H100,1697105040838,1697105043749,120,87.0,20.0,"[4, 532, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56, 62, 60, 58, 520, 73]","[1697105040842, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749]"
1454,715,17,[],200,EleutherAI/gpt-neox-20b,64,1,362.0,1.0,1,H100,1697105044514,1697105044876,120,20.0,1.0,"[20, 342]","[1697105044534, 1697105044876]"
1455,715,23,[],200,EleutherAI/gpt-neox-20b,64,1,835.0,1.0,1,H100,1697105049355,1697105050190,120,20.0,1.0,"[4, 831]","[1697105049359, 1697105050190]"
1456,241,17,[],200,EleutherAI/gpt-neox-20b,64,1,518.0,1.0,1,H100,1697105044358,1697105044876,120,19.0,1.0,"[8, 510]","[1697105044366, 1697105044876]"
1457,18,18,[],200,EleutherAI/gpt-neox-20b,64,1,823.0,1.0,1,H100,1697105044877,1697105045700,120,15.0,1.0,"[12, 811]","[1697105044889, 1697105045700]"
1458,599,19,[],200,EleutherAI/gpt-neox-20b,64,1,7773.0,1.0,1,H100,1697105045702,1697105053475,120,58.0,55.0,"[8, 1120, 75, 56, 55, 69, 58, 438, 68, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71]","[1697105045710, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475]"
1459,4,21,[],200,EleutherAI/gpt-neox-20b,64,1,2353.0,1.0,1,H100,1697105052945,1697105055298,120,89.0,20.0,"[20, 439, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382, 70, 67, 65, 51, 233]","[1697105052965, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054882, 1697105054949, 1697105055014, 1697105055065, 1697105055298]"
1460,401,15,[],200,EleutherAI/gpt-neox-20b,64,1,2737.0,1.0,1,H100,1697105040239,1697105042976,120,84.0,20.0,"[5, 654, 45, 46, 56, 56, 273, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 56]","[1697105040244, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042976]"
1461,179,16,[],200,EleutherAI/gpt-neox-20b,64,1,837.0,1.0,1,H100,1697105042976,1697105043813,120,161.0,4.0,"[5, 695, 73, 64]","[1697105042981, 1697105043676, 1697105043749, 1697105043813]"
1462,759,17,[],200,EleutherAI/gpt-neox-20b,64,1,3017.0,1.0,1,H100,1697105043814,1697105046831,120,92.0,20.0,"[5, 623, 70, 62, 58, 457, 74, 70, 55, 66, 65, 50, 451, 133, 76, 69, 53, 65, 50, 464]","[1697105043819, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045920, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830]"
1463,316,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057343,1697105058839,120,,,"[13, 737, 65, 64, 63]","[1697105057356, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1464,362,9,[],200,EleutherAI/gpt-neox-20b,64,1,1138.0,1.0,1,H100,1697105037425,1697105038563,120,14.0,1.0,"[94, 1044]","[1697105037519, 1697105038563]"
1465,139,10,[],200,EleutherAI/gpt-neox-20b,64,1,2810.0,1.0,1,H100,1697105038564,1697105041374,120,39.0,21.0,"[21, 830, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 60, 54, 311, 45, 46, 56, 56, 273]","[1697105038585, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374]"
1466,720,11,[],200,EleutherAI/gpt-neox-20b,64,1,882.0,1.0,1,H100,1697105041374,1697105042256,120,286.0,6.0,"[5, 655, 62, 58, 56, 46]","[1697105041379, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256]"
1467,496,12,[],200,EleutherAI/gpt-neox-20b,64,1,1618.0,1.0,1,H100,1697105042257,1697105043875,120,335.0,11.0,"[4, 586, 74, 55, 62, 60, 58, 520, 73, 64, 62]","[1697105042261, 1697105042847, 1697105042921, 1697105042976, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875]"
1468,153,31,[],200,EleutherAI/gpt-neox-20b,64,1,1222.0,1.0,1,H100,1697105053727,1697105054949,120,335.0,4.0,"[4, 1081, 69, 68]","[1697105053731, 1697105054812, 1697105054881, 1697105054949]"
1469,149,13,[],200,EleutherAI/gpt-neox-20b,64,1,1478.0,1.0,1,H100,1697105043876,1697105045354,120,563.0,10.0,"[4, 562, 70, 62, 58, 457, 74, 70, 55, 66]","[1697105043880, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354]"
1470,88,23,[],200,EleutherAI/gpt-neox-20b,64,1,3351.0,1.0,1,H100,1697105058843,1697105062194,120,58.0,20.0,"[32, 592, 681, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70]","[1697105058875, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194]"
1471,857,32,[],200,EleutherAI/gpt-neox-20b,64,1,274.0,1.0,1,H100,1697105054950,1697105055224,120,18.0,1.0,"[4, 270]","[1697105054954, 1697105055224]"
1472,494,32,[],200,EleutherAI/gpt-neox-20b,64,1,1322.0,1.0,1,H100,1697105067427,1697105068749,120,6.0,10.0,"[4, 647, 60, 58, 59, 46, 55, 53, 289, 51]","[1697105067431, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749]"
1473,510,33,[],200,EleutherAI/gpt-neox-20b,64,1,784.0,1.0,1,H100,1697105055225,1697105056009,120,79.0,2.0,"[8, 776]","[1697105055233, 1697105056009]"
1474,535,20,[],200,EleutherAI/gpt-neox-20b,64,1,2826.0,1.0,1,H100,1697105059423,1697105062249,120,84.0,20.0,"[9, 716, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 55]","[1697105059432, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249]"
1475,589,18,[],200,EleutherAI/gpt-neox-20b,64,1,2845.0,1.0,1,H100,1697105048694,1697105051539,120,92.0,20.0,"[5, 655, 75, 73, 70, 55, 54, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71]","[1697105048699, 1697105049354, 1697105049429, 1697105049502, 1697105049572, 1697105049627, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539]"
1476,856,32,[],200,EleutherAI/gpt-neox-20b,64,1,10037.0,1.0,1,H100,1697105076078,1697105086115,120,286.0,72.0,"[16, 737, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 58, 916, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 141, 75, 69, 69, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 71, 68, 52, 66, 65, 592, 77, 56, 74, 73, 72, 272, 56, 72, 55, 55, 68, 561, 73, 70, 68]","[1697105076094, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078547, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081418, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083756, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085165, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115]"
1477,866,20,[],200,EleutherAI/gpt-neox-20b,64,1,3406.0,1.0,1,H100,1697105058843,1697105062249,120,93.0,20.0,"[77, 1228, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 55]","[1697105058920, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249]"
1478,287,34,[],200,EleutherAI/gpt-neox-20b,64,1,530.0,1.0,1,H100,1697105056010,1697105056540,120,10.0,1.0,"[8, 522]","[1697105056018, 1697105056540]"
1479,868,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105056541,1697105058840,120,,,"[8, 792, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105056549, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1480,648,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062195,1697105063190,120,,,"[29, 374, 61, 60, 58, 47]","[1697105062224, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1481,70,22,[],200,EleutherAI/gpt-neox-20b,64,1,810.0,1.0,1,H100,1697105068410,1697105069220,120,39.0,1.0,"[25, 784]","[1697105068435, 1697105069219]"
1482,768,23,[],200,EleutherAI/gpt-neox-20b,64,1,1252.0,1.0,1,H100,1697105069220,1697105070472,120,47.0,6.0,"[5, 985, 70, 69, 67, 56]","[1697105069225, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1483,410,17,[],200,EleutherAI/gpt-neox-20b,64,1,1648.0,1.0,1,H100,1697105052018,1697105053666,120,364.0,12.0,"[5, 676, 74, 57, 57, 56, 67, 395, 70, 66, 63, 62]","[1697105052023, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053405, 1697105053475, 1697105053541, 1697105053604, 1697105053666]"
1484,638,36,[],200,EleutherAI/gpt-neox-20b,64,1,3406.0,1.0,1,H100,1697105058843,1697105062249,120,88.0,20.0,"[153, 1152, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 55]","[1697105058996, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249]"
1485,429,24,[],200,EleutherAI/gpt-neox-20b,64,1,7121.0,1.0,1,H100,1697105070473,1697105077594,120,244.0,50.0,"[9, 1551, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 233, 53, 64, 61, 50, 44, 481, 64, 50, 61, 60, 53, 476, 66, 65, 52, 64, 63, 59, 357, 60, 49, 59, 58, 55, 266, 45, 53, 51, 342, 63, 56, 55, 55, 534]","[1697105070482, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073550, 1697105073603, 1697105073667, 1697105073728, 1697105073778, 1697105073822, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075902, 1697105075961, 1697105076019, 1697105076074, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594]"
1486,182,18,[],200,EleutherAI/gpt-neox-20b,64,1,3327.0,1.0,1,H100,1697105053667,1697105056994,120,47.0,31.0,"[4, 388, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 49, 61, 58, 481, 71, 62, 59, 48, 57, 304, 70, 68, 67, 64, 52, 63]","[1697105053671, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055409, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994]"
1487,418,25,[],200,EleutherAI/gpt-neox-20b,64,1,928.0,1.0,1,H100,1697105063192,1697105064120,120,286.0,3.0,"[30, 852, 46]","[1697105063222, 1697105064074, 1697105064120]"
1488,80,26,[],200,EleutherAI/gpt-neox-20b,64,1,1066.0,1.0,1,H100,1697105064122,1697105065188,120,13.0,1.0,"[4, 1062]","[1697105064126, 1697105065188]"
1489,779,27,[],200,EleutherAI/gpt-neox-20b,64,1,2305.0,1.0,1,H100,1697105065189,1697105067494,120,563.0,10.0,"[4, 653, 727, 68, 66, 65, 61, 59, 533, 69]","[1697105065193, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067425, 1697105067494]"
1490,921,35,[],200,EleutherAI/gpt-neox-20b,64,1,1201.0,1.0,1,H100,1697105073780,1697105074981,120,31.0,1.0,"[52, 1149]","[1697105073832, 1697105074981]"
1491,698,36,[],200,EleutherAI/gpt-neox-20b,64,1,1037.0,1.0,1,H100,1697105074982,1697105076019,120,182.0,6.0,"[9, 802, 60, 49, 60, 57]","[1697105074991, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019]"
1492,440,28,[],200,EleutherAI/gpt-neox-20b,64,1,2921.0,1.0,1,H100,1697105067495,1697105070416,120,84.0,20.0,"[5, 578, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67]","[1697105067500, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
1493,196,21,[],200,EleutherAI/gpt-neox-20b,64,1,918.0,1.0,1,H100,1697105062250,1697105063168,120,13.0,1.0,"[20, 898]","[1697105062270, 1697105063168]"
1494,351,37,[],200,EleutherAI/gpt-neox-20b,64,1,811.0,1.0,1,H100,1697105076020,1697105076831,120,216.0,6.0,"[5, 315, 45, 53, 52, 341]","[1697105076025, 1697105076340, 1697105076385, 1697105076438, 1697105076490, 1697105076831]"
1495,123,38,[],200,EleutherAI/gpt-neox-20b,64,1,667.0,1.0,1,H100,1697105076832,1697105077499,120,14.0,1.0,"[5, 662]","[1697105076837, 1697105077499]"
1496,716,39,[],200,EleutherAI/gpt-neox-20b,64,1,4689.0,1.0,1,H100,1697105077500,1697105082189,120,79.0,30.0,"[5, 692, 56, 55, 62, 60, 59, 57, 917, 72, 68, 67, 52, 58, 390, 70, 71, 69, 66, 51, 636, 141, 75, 70, 68, 67, 424, 72, 71, 68]","[1697105077505, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189]"
1497,475,36,[],200,EleutherAI/gpt-neox-20b,64,1,2825.0,1.0,1,H100,1697105070953,1697105073778,120,89.0,20.0,"[17, 1063, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 61, 50]","[1697105070970, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073778]"
1498,298,37,[],200,EleutherAI/gpt-neox-20b,64,1,918.0,1.0,1,H100,1697105062250,1697105063168,120,17.0,1.0,"[16, 902]","[1697105062266, 1697105063168]"
1499,816,17,[],200,EleutherAI/gpt-neox-20b,64,1,1672.0,1.0,1,H100,1697105049002,1697105050674,120,182.0,4.0,"[12, 1349, 237, 74]","[1697105049014, 1697105050363, 1697105050600, 1697105050674]"
1500,33,16,[],200,EleutherAI/gpt-neox-20b,64,1,1786.0,1.0,1,H100,1697105047144,1697105048930,120,140.0,7.0,"[4, 1416, 129, 73, 54, 55, 55]","[1697105047148, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930]"
1501,787,26,[],200,EleutherAI/gpt-neox-20b,64,1,1467.0,1.0,1,H100,1697105058843,1697105060310,120,123.0,6.0,"[28, 596, 681, 59, 57, 46]","[1697105058871, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310]"
1502,68,38,[],200,EleutherAI/gpt-neox-20b,64,1,204.0,1.0,1,H100,1697105063169,1697105063373,120,12.0,1.0,"[9, 195]","[1697105063178, 1697105063373]"
1503,743,39,[],200,EleutherAI/gpt-neox-20b,64,1,1400.0,1.0,1,H100,1697105063374,1697105064774,120,123.0,6.0,"[4, 1174, 61, 59, 57, 45]","[1697105063378, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774]"
1504,556,27,[],200,EleutherAI/gpt-neox-20b,64,1,327.0,1.0,1,H100,1697105060311,1697105060638,120,9.0,1.0,"[12, 315]","[1697105060323, 1697105060638]"
1505,608,41,[],200,EleutherAI/gpt-neox-20b,64,1,3044.0,1.0,1,H100,1697105079723,1697105082767,120,96.0,20.0,"[8, 439, 70, 71, 69, 66, 51, 636, 141, 75, 70, 68, 67, 425, 71, 71, 68, 65, 434, 79]","[1697105079731, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081979, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082767]"
1506,191,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105060639,1697105063190,120,,,"[12, 1153, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060651, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1507,732,17,[],200,EleutherAI/gpt-neox-20b,64,1,1815.0,1.0,1,H100,1697105048931,1697105050746,120,345.0,12.0,"[5, 418, 75, 73, 70, 55, 54, 64, 618, 237, 74, 72]","[1697105048936, 1697105049354, 1697105049429, 1697105049502, 1697105049572, 1697105049627, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746]"
1508,484,22,[],200,EleutherAI/gpt-neox-20b,64,1,4915.0,1.0,1,H100,1697105044514,1697105049429,120,86.0,36.0,"[12, 564, 73, 71, 54, 66, 65, 50, 450, 133, 76, 70, 53, 65, 50, 465, 74, 56, 55, 69, 58, 438, 68, 69, 67, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 76]","[1697105044526, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046831, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429]"
1509,406,31,[],200,EleutherAI/gpt-neox-20b,64,1,801.0,1.0,1,H100,1697105067999,1697105068800,120,244.0,4.0,"[4, 636, 59, 51, 50]","[1697105068003, 1697105068639, 1697105068698, 1697105068749, 1697105068799]"
1510,396,18,[],200,EleutherAI/gpt-neox-20b,64,1,2197.0,1.0,1,H100,1697105050747,1697105052944,120,89.0,20.0,"[5, 439, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56]","[1697105050752, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943]"
1511,827,21,[],200,EleutherAI/gpt-neox-20b,64,1,2572.0,1.0,1,H100,1697105044513,1697105047085,120,96.0,20.0,"[5, 572, 73, 71, 54, 66, 65, 50, 451, 132, 76, 70, 53, 65, 50, 465, 74, 56, 55, 69]","[1697105044518, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045920, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046831, 1697105046905, 1697105046961, 1697105047016, 1697105047085]"
1512,56,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068800,1697105070950,120,,,"[5, 1405, 70, 69, 67, 56]","[1697105068805, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1513,253,23,[],200,EleutherAI/gpt-neox-20b,64,1,2707.0,1.0,1,H100,1697105049430,1697105052137,120,67.0,20.0,"[4, 929, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52]","[1697105049434, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137]"
1514,733,33,[],200,EleutherAI/gpt-neox-20b,64,1,1007.0,1.0,1,H100,1697105070953,1697105071960,120,31.0,1.0,"[9, 998]","[1697105070962, 1697105071960]"
1515,384,34,[],200,EleutherAI/gpt-neox-20b,64,1,2577.0,1.0,1,H100,1697105071961,1697105074538,120,92.0,20.0,"[10, 927, 73, 68, 63, 60, 48, 60, 47, 234, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60]","[1697105071971, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538]"
1516,166,19,[],200,EleutherAI/gpt-neox-20b,64,1,371.0,1.0,1,H100,1697105052945,1697105053316,120,14.0,1.0,"[20, 351]","[1697105052965, 1697105053316]"
1517,754,20,[],200,EleutherAI/gpt-neox-20b,64,1,1045.0,1.0,1,H100,1697105053317,1697105054362,120,88.0,7.0,"[11, 731, 56, 55, 70, 69, 53]","[1697105053328, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362]"
1518,536,18,[],200,EleutherAI/gpt-neox-20b,64,1,2850.0,1.0,1,H100,1697105046831,1697105049681,120,83.0,20.0,"[9, 741, 68, 69, 67, 52, 62, 664, 130, 73, 55, 54, 55, 71, 353, 75, 72, 71, 55, 54]","[1697105046840, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048821, 1697105048875, 1697105048930, 1697105049001, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681]"
1519,524,21,[],200,EleutherAI/gpt-neox-20b,64,1,3303.0,1.0,1,H100,1697105054363,1697105057666,120,100.0,30.0,"[5, 444, 70, 67, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 70, 68, 67, 64, 52, 63, 347, 73, 53, 68, 66, 65]","[1697105054368, 1697105054812, 1697105054882, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666]"
1520,25,28,[],200,EleutherAI/gpt-neox-20b,64,1,371.0,1.0,1,H100,1697105052945,1697105053316,120,12.0,1.0,"[16, 355]","[1697105052961, 1697105053316]"
1521,724,29,[],200,EleutherAI/gpt-neox-20b,64,1,668.0,1.0,1,H100,1697105053317,1697105053985,120,11.0,1.0,"[17, 651]","[1697105053334, 1697105053985]"
1522,80,26,[],200,EleutherAI/gpt-neox-20b,64,1,809.0,1.0,1,H100,1697105068410,1697105069219,120,13.0,1.0,"[17, 792]","[1697105068427, 1697105069219]"
1523,356,30,[],200,EleutherAI/gpt-neox-20b,64,1,826.0,1.0,1,H100,1697105053986,1697105054812,120,874.0,2.0,"[5, 821]","[1697105053991, 1697105054812]"
1524,667,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069220,1697105070950,120,,,"[9, 981, 70, 69, 67, 56]","[1697105069229, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1525,124,31,[],200,EleutherAI/gpt-neox-20b,64,1,485.0,1.0,1,H100,1697105054813,1697105055298,120,83.0,2.0,"[5, 480]","[1697105054818, 1697105055298]"
1526,375,20,[],200,EleutherAI/gpt-neox-20b,64,1,1994.0,1.0,1,H100,1697105053476,1697105055470,120,874.0,17.0,"[9, 574, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62]","[1697105053485, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470]"
1527,155,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057667,1697105058839,120,,,[9],[1697105057676]
1528,855,23,[],200,EleutherAI/gpt-neox-20b,64,1,3352.0,1.0,1,H100,1697105058842,1697105062194,120,83.0,20.0,"[17, 608, 681, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70]","[1697105058859, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194]"
1529,437,28,[],200,EleutherAI/gpt-neox-20b,64,1,4181.0,1.0,1,H100,1697105070953,1697105075134,120,91.0,29.0,"[5, 1075, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 61, 50, 44, 481, 64, 50, 61, 60, 53, 476, 66]","[1697105070958, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073778, 1697105073822, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133]"
1530,478,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105033564,1697105037422,120,,,"[24, 1062, 65, 59, 58, 57, 269, 50, 51, 58, 55, 477, 64, 61, 58, 57, 601, 70, 69, 66, 58]","[1697105033588, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035208, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974, 1697105036032, 1697105036089, 1697105036690, 1697105036760, 1697105036829, 1697105036895, 1697105036953]"
1531,901,17,[],200,EleutherAI/gpt-neox-20b,64,1,470.0,1.0,1,H100,1697105051269,1697105051739,120,17.0,1.0,"[4, 466]","[1697105051273, 1697105051739]"
1532,557,18,[],200,EleutherAI/gpt-neox-20b,64,1,865.0,1.0,1,H100,1697105051740,1697105052605,120,31.0,1.0,"[17, 848]","[1697105051757, 1697105052605]"
1533,329,19,[],200,EleutherAI/gpt-neox-20b,64,1,710.0,1.0,1,H100,1697105052606,1697105053316,120,15.0,1.0,"[17, 693]","[1697105052623, 1697105053316]"
1534,614,20,[],200,EleutherAI/gpt-neox-20b,64,1,654.0,1.0,1,H100,1697105056541,1697105057195,120,15.0,1.0,"[16, 638]","[1697105056557, 1697105057195]"
1535,919,20,[],200,EleutherAI/gpt-neox-20b,64,1,668.0,1.0,1,H100,1697105053317,1697105053985,120,14.0,1.0,"[13, 655]","[1697105053330, 1697105053985]"
1536,647,33,[],200,EleutherAI/gpt-neox-20b,64,1,2827.0,1.0,1,H100,1697105070952,1697105073779,120,83.0,20.0,"[62, 1019, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 61, 51]","[1697105071014, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073779]"
1537,689,21,[],200,EleutherAI/gpt-neox-20b,64,1,676.0,1.0,1,H100,1697105053987,1697105054663,120,15.0,1.0,"[12, 664]","[1697105053999, 1697105054663]"
1538,5,41,[],200,EleutherAI/gpt-neox-20b,64,1,3119.0,1.0,1,H100,1697105080381,1697105083500,120,84.0,20.0,"[4, 748, 142, 74, 70, 68, 67, 424, 72, 71, 68, 65, 435, 78, 71, 71, 54, 53, 69, 414]","[1697105080385, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082689, 1697105082767, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499]"
1539,319,22,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105054664,1697105055224,120,31.0,1.0,"[8, 552]","[1697105054672, 1697105055224]"
1540,272,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057196,1697105058840,120,,,"[9, 888, 65, 64, 63]","[1697105057205, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1541,89,23,[],200,EleutherAI/gpt-neox-20b,64,1,2441.0,1.0,1,H100,1697105055225,1697105057666,120,52.0,20.0,"[9, 775, 71, 62, 59, 48, 57, 305, 68, 69, 67, 64, 52, 63, 348, 72, 53, 68, 66, 65]","[1697105055234, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056611, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057342, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666]"
1542,41,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063192,1697105065210,120,,,"[34, 848, 46, 432, 61, 59, 57, 45, 54]","[1697105063226, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1543,469,18,[],200,EleutherAI/gpt-neox-20b,64,1,438.0,1.0,1,H100,1697105050675,1697105051113,120,17.0,1.0,"[5, 433]","[1697105050680, 1697105051113]"
1544,598,42,[],200,EleutherAI/gpt-neox-20b,64,1,1776.0,1.0,1,H100,1697105083500,1697105085276,120,345.0,12.0,"[13, 900, 77, 56, 74, 73, 72, 272, 56, 73, 54, 56]","[1697105083513, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085276]"
1545,746,21,[],200,EleutherAI/gpt-neox-20b,64,1,3144.0,1.0,1,H100,1697105065212,1697105068356,120,345.0,18.0,"[90, 1271, 68, 66, 65, 61, 59, 534, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55]","[1697105065302, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356]"
1546,724,27,[],200,EleutherAI/gpt-neox-20b,64,1,579.0,1.0,1,H100,1697105058843,1697105059422,120,11.0,1.0,"[44, 535]","[1697105058887, 1697105059422]"
1547,365,43,[],200,EleutherAI/gpt-neox-20b,64,1,363.0,1.0,1,H100,1697105085277,1697105085640,120,23.0,1.0,"[8, 355]","[1697105085285, 1697105085640]"
1548,493,28,[],200,EleutherAI/gpt-neox-20b,64,1,2826.0,1.0,1,H100,1697105059423,1697105062249,120,83.0,20.0,"[9, 716, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 55]","[1697105059432, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249]"
1549,26,44,[],200,EleutherAI/gpt-neox-20b,64,1,874.0,1.0,1,H100,1697105085641,1697105086515,120,18.0,1.0,"[9, 865]","[1697105085650, 1697105086515]"
1550,463,24,[],200,EleutherAI/gpt-neox-20b,64,1,922.0,1.0,1,H100,1697105050191,1697105051113,120,39.0,1.0,"[12, 910]","[1697105050203, 1697105051113]"
1551,113,25,[],200,EleutherAI/gpt-neox-20b,64,1,625.0,1.0,1,H100,1697105051114,1697105051739,120,13.0,1.0,"[8, 617]","[1697105051122, 1697105051739]"
1552,814,26,[],200,EleutherAI/gpt-neox-20b,64,1,2690.0,1.0,1,H100,1697105051740,1697105054430,120,89.0,20.0,"[13, 946, 74, 57, 57, 56, 67, 394, 71, 66, 63, 62, 60, 333, 56, 55, 70, 69, 53, 68]","[1697105051753, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430]"
1553,33,16,[],200,EleutherAI/gpt-neox-20b,64,1,1411.0,1.0,1,H100,1697105043752,1697105045163,120,140.0,7.0,"[13, 677, 70, 62, 58, 457, 74]","[1697105043765, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163]"
1554,102,15,[],200,EleutherAI/gpt-neox-20b,64,1,2604.0,1.0,1,H100,1697105053476,1697105056080,120,84.0,20.0,"[5, 578, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71]","[1697105053481, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080]"
1555,263,42,[],200,EleutherAI/gpt-neox-20b,64,1,638.0,1.0,1,H100,1697105082768,1697105083406,120,15.0,1.0,"[12, 626]","[1697105082780, 1697105083406]"
1556,41,43,[],200,EleutherAI/gpt-neox-20b,64,1,5505.0,1.0,1,H100,1697105083407,1697105088912,120,39.0,43.0,"[8, 998, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562, 72, 70, 68, 67, 63, 348, 70, 53, 53, 66, 62, 291, 71, 55, 70, 70, 67, 63, 546, 63, 61, 48, 55, 264, 61, 60, 59, 56, 55]","[1697105083415, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912]"
1557,732,17,[],200,EleutherAI/gpt-neox-20b,64,1,1852.0,1.0,1,H100,1697105045164,1697105047016,120,345.0,12.0,"[4, 751, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55]","[1697105045168, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016]"
1558,89,16,[],200,EleutherAI/gpt-neox-20b,64,1,2354.0,1.0,1,H100,1697105038747,1697105041101,120,52.0,20.0,"[4, 664, 71, 68, 66, 63, 58, 57, 441, 67, 54, 64, 49, 60, 54, 311, 45, 46, 56, 56]","[1697105038751, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040239, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040587, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101]"
1559,201,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062825,1697105065209,120,,,"[5, 1244, 46, 432, 61, 59, 56, 46, 54]","[1697105062830, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064728, 1697105064774, 1697105064828]"
1560,161,35,[],200,EleutherAI/gpt-neox-20b,64,1,839.0,1.0,1,H100,1697105074539,1697105075378,120,109.0,7.0,"[4, 524, 67, 65, 51, 65, 62]","[1697105074543, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377]"
1561,743,36,[],200,EleutherAI/gpt-neox-20b,64,1,641.0,1.0,1,H100,1697105075378,1697105076019,120,123.0,6.0,"[5, 410, 61, 48, 60, 57]","[1697105075383, 1697105075793, 1697105075854, 1697105075902, 1697105075962, 1697105076019]"
1562,30,21,[],200,EleutherAI/gpt-neox-20b,64,1,2195.0,1.0,1,H100,1697105055471,1697105057666,120,93.0,20.0,"[4, 534, 71, 62, 59, 48, 57, 304, 70, 68, 67, 64, 51, 64, 347, 73, 53, 68, 66, 65]","[1697105055475, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666]"
1563,448,16,[],200,EleutherAI/gpt-neox-20b,64,1,1727.0,1.0,1,H100,1697105045289,1697105047016,120,335.0,12.0,"[4, 627, 133, 76, 69, 53, 65, 50, 464, 75, 56, 55]","[1697105045293, 1697105045920, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016]"
1564,509,24,[],200,EleutherAI/gpt-neox-20b,64,1,464.0,1.0,1,H100,1697105062195,1697105062659,120,286.0,3.0,"[17, 386, 61]","[1697105062212, 1697105062598, 1697105062659]"
1565,705,22,[],200,EleutherAI/gpt-neox-20b,64,1,2908.0,1.0,1,H100,1697105050818,1697105053726,120,79.0,27.0,"[12, 361, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 57, 66, 394, 71, 67, 62, 62, 60]","[1697105050830, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052944, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726]"
1566,791,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105065212,1697105070950,120,,,"[29, 605, 727, 68, 66, 65, 61, 59, 534, 68, 62, 60, 55, 407, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105065241, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067556, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1567,282,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062660,1697105063191,120,,,[5],[1697105062665]
1568,863,26,[],200,EleutherAI/gpt-neox-20b,64,1,815.0,1.0,1,H100,1697105063194,1697105064009,120,10.0,1.0,"[125, 690]","[1697105063319, 1697105064009]"
1569,640,27,[],200,EleutherAI/gpt-neox-20b,64,1,475.0,1.0,1,H100,1697105064010,1697105064485,120,15.0,1.0,"[21, 454]","[1697105064031, 1697105064485]"
1570,729,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057667,1697105058840,120,,,[17],[1697105057684]
1571,391,23,[],200,EleutherAI/gpt-neox-20b,64,1,3351.0,1.0,1,H100,1697105058843,1697105062194,120,79.0,20.0,"[24, 600, 681, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70]","[1697105058867, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194]"
1572,294,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064486,1697105065210,120,,,[12],[1697105064498]
1573,65,29,[],200,EleutherAI/gpt-neox-20b,64,1,5068.0,1.0,1,H100,1697105065212,1697105070280,120,39.0,30.0,"[41, 593, 727, 68, 66, 65, 61, 59, 534, 68, 62, 60, 55, 407, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70]","[1697105065253, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067556, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280]"
1574,910,14,[],200,EleutherAI/gpt-neox-20b,64,1,1136.0,1.0,1,H100,1697105058843,1697105059979,120,8.0,1.0,"[145, 991]","[1697105058988, 1697105059979]"
1575,571,15,[],200,EleutherAI/gpt-neox-20b,64,1,784.0,1.0,1,H100,1697105059980,1697105060764,120,67.0,2.0,"[4, 780]","[1697105059984, 1697105060764]"
1576,342,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105060765,1697105063190,120,,,"[4, 1035, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060769, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1577,654,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070281,1697105070950,120,,,[5],[1697105070286]
1578,424,31,[],200,EleutherAI/gpt-neox-20b,64,1,2825.0,1.0,1,H100,1697105070953,1697105073778,120,88.0,20.0,"[133, 947, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 62, 49]","[1697105071086, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778]"
1579,924,17,[],200,EleutherAI/gpt-neox-20b,64,1,816.0,1.0,1,H100,1697105063193,1697105064009,120,9.0,1.0,"[118, 698]","[1697105063311, 1697105064009]"
1580,701,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064010,1697105065209,120,,,"[25, 517, 61, 59, 57, 45, 54]","[1697105064035, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1581,349,19,[],200,EleutherAI/gpt-neox-20b,64,1,3197.0,1.0,1,H100,1697105065212,1697105068409,120,88.0,20.0,"[17, 617, 727, 68, 66, 65, 61, 59, 534, 68, 62, 60, 55, 406, 61, 58, 59, 46, 55, 53]","[1697105065229, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067556, 1697105067616, 1697105067671, 1697105068077, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409]"
1582,777,20,[],200,EleutherAI/gpt-neox-20b,64,1,199.0,1.0,1,H100,1697105051540,1697105051739,120,9.0,1.0,"[4, 195]","[1697105051544, 1697105051739]"
1583,438,21,[],200,EleutherAI/gpt-neox-20b,64,1,865.0,1.0,1,H100,1697105051740,1697105052605,120,9.0,1.0,"[25, 840]","[1697105051765, 1697105052605]"
1584,212,22,[],200,EleutherAI/gpt-neox-20b,64,1,710.0,1.0,1,H100,1697105052606,1697105053316,120,31.0,1.0,"[13, 697]","[1697105052619, 1697105053316]"
1585,173,32,[],200,EleutherAI/gpt-neox-20b,64,1,2295.0,1.0,1,H100,1697105073780,1697105076075,120,96.0,20.0,"[20, 504, 63, 50, 61, 60, 53, 476, 66, 66, 51, 64, 64, 59, 356, 60, 49, 60, 57, 56]","[1697105073800, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075199, 1697105075250, 1697105075314, 1697105075378, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
1586,486,40,[],200,EleutherAI/gpt-neox-20b,64,1,2576.0,1.0,1,H100,1697105082190,1697105084766,120,14.0,20.0,"[9, 490, 77, 72, 71, 54, 53, 70, 413, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 73]","[1697105082199, 1697105082689, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083086, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084766]"
1587,50,22,[],200,EleutherAI/gpt-neox-20b,64,1,1364.0,1.0,1,H100,1697105058843,1697105060207,120,90.0,4.0,"[40, 584, 681, 59]","[1697105058883, 1697105059467, 1697105060148, 1697105060207]"
1588,277,23,[],200,EleutherAI/gpt-neox-20b,64,1,605.0,1.0,1,H100,1697105043752,1697105044357,120,18.0,1.0,"[9, 596]","[1697105043761, 1697105044357]"
1589,632,23,[],200,EleutherAI/gpt-neox-20b,64,1,2616.0,1.0,1,H100,1697105060208,1697105062824,120,91.0,20.0,"[5, 551, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060213, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1590,47,24,[],200,EleutherAI/gpt-neox-20b,64,1,2727.0,1.0,1,H100,1697105044358,1697105047085,120,90.0,20.0,"[4, 727, 74, 71, 54, 66, 65, 50, 451, 133, 75, 70, 53, 65, 50, 464, 75, 56, 55, 69]","[1697105044362, 1697105045089, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045920, 1697105046053, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085]"
1591,363,18,[],200,EleutherAI/gpt-neox-20b,64,1,3816.0,1.0,1,H100,1697105044877,1697105048693,120,286.0,22.0,"[12, 1030, 134, 76, 69, 53, 65, 50, 464, 75, 56, 55, 69, 58, 438, 68, 69, 67, 52, 62, 664, 130]","[1697105044889, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693]"
1592,246,19,[],200,EleutherAI/gpt-neox-20b,64,1,5701.0,1.0,1,H100,1697105051114,1697105056815,120,58.0,47.0,"[8, 687, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 69, 69, 67]","[1697105051122, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815]"
1593,396,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064775,1697105065210,120,,,[9],[1697105064784]
1594,173,41,[],200,EleutherAI/gpt-neox-20b,64,1,3486.0,1.0,1,H100,1697105065212,1697105068698,120,96.0,20.0,"[174, 1187, 68, 66, 65, 62, 58, 534, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55, 53, 289]","[1697105065386, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698]"
1595,400,22,[],200,EleutherAI/gpt-neox-20b,64,1,1063.0,1.0,1,H100,1697105068357,1697105069420,120,123.0,7.0,"[4, 337, 51, 51, 499, 61, 60]","[1697105068361, 1697105068698, 1697105068749, 1697105068800, 1697105069299, 1697105069360, 1697105069420]"
1596,900,19,[],200,EleutherAI/gpt-neox-20b,64,1,950.0,1.0,1,H100,1697105055299,1697105056249,120,67.0,6.0,"[5, 705, 71, 62, 59, 48]","[1697105055304, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249]"
1597,561,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105056250,1697105058840,120,,,"[5, 355, 70, 68, 67, 64, 52, 63, 347, 72, 54, 68, 66, 65, 427, 65, 64, 63]","[1697105056255, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341, 1697105057413, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1598,186,19,[],200,EleutherAI/gpt-neox-20b,64,1,3017.0,1.0,1,H100,1697105049682,1697105052699,120,123.0,22.0,"[4, 677, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503]","[1697105049686, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699]"
1599,761,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068699,1697105070951,120,,,"[13, 587, 61, 60, 59, 56, 675, 70, 69, 67, 56]","[1697105068712, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069535, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1600,332,21,[],200,EleutherAI/gpt-neox-20b,64,1,579.0,1.0,1,H100,1697105058843,1697105059422,120,39.0,1.0,"[8, 571]","[1697105058851, 1697105059422]"
1601,915,22,[],200,EleutherAI/gpt-neox-20b,64,1,556.0,1.0,1,H100,1697105059423,1697105059979,120,182.0,1.0,"[5, 551]","[1697105059428, 1697105059979]"
1602,501,38,[],200,EleutherAI/gpt-neox-20b,64,1,499.0,1.0,1,H100,1697105091396,1697105091895,120,19.0,1.0,"[8, 491]","[1697105091404, 1697105091895]"
1603,691,23,[],200,EleutherAI/gpt-neox-20b,64,1,658.0,1.0,1,H100,1697105059980,1697105060638,120,47.0,1.0,"[21, 637]","[1697105060001, 1697105060638]"
1604,278,39,[],200,EleutherAI/gpt-neox-20b,64,1,609.0,1.0,1,H100,1697105091897,1697105092506,120,13.0,1.0,"[20, 589]","[1697105091917, 1697105092506]"
1605,345,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105060639,1697105063190,120,,,"[20, 1145, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060659, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1606,863,40,[],200,EleutherAI/gpt-neox-20b,64,1,660.0,1.0,1,H100,1697105092507,1697105093167,120,10.0,1.0,"[17, 643]","[1697105092524, 1697105093167]"
1607,806,9,[],200,EleutherAI/gpt-neox-20b,64,1,2628.0,1.0,1,H100,1697105033346,1697105035974,120,89.0,20.0,"[4, 497, 61, 59, 47, 47, 55, 534, 65, 59, 58, 57, 269, 51, 50, 58, 55, 477, 64, 61]","[1697105033350, 1697105033847, 1697105033908, 1697105033967, 1697105034014, 1697105034061, 1697105034116, 1697105034650, 1697105034715, 1697105034774, 1697105034832, 1697105034889, 1697105035158, 1697105035209, 1697105035259, 1697105035317, 1697105035372, 1697105035849, 1697105035913, 1697105035974]"
1608,654,26,[],200,EleutherAI/gpt-neox-20b,64,1,524.0,1.0,1,H100,1697105062195,1697105062719,120,47.0,4.0,"[5, 398, 61, 60]","[1697105062200, 1697105062598, 1697105062659, 1697105062719]"
1609,308,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062720,1697105063191,120,,,[4],[1697105062724]
1610,80,17,[],200,EleutherAI/gpt-neox-20b,64,1,477.0,1.0,1,H100,1697105047018,1697105047495,120,13.0,1.0,"[8, 469]","[1697105047026, 1697105047495]"
1611,778,18,[],200,EleutherAI/gpt-neox-20b,64,1,6674.0,1.0,1,H100,1697105047496,1697105054170,120,16.0,50.0,"[16, 1051, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55]","[1697105047512, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170]"
1612,79,28,[],200,EleutherAI/gpt-neox-20b,64,1,816.0,1.0,1,H100,1697105063193,1697105064009,120,12.0,1.0,"[98, 718]","[1697105063291, 1697105064009]"
1613,668,29,[],200,EleutherAI/gpt-neox-20b,64,1,764.0,1.0,1,H100,1697105064010,1697105064774,120,109.0,6.0,"[9, 533, 61, 59, 57, 45]","[1697105064019, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774]"
1614,439,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064775,1697105065211,120,,,[5],[1697105064780]
1615,101,31,[],200,EleutherAI/gpt-neox-20b,64,1,1133.0,1.0,1,H100,1697105065212,1697105066345,120,13.0,1.0,"[170, 963]","[1697105065382, 1697105066345]"
1616,799,32,[],200,EleutherAI/gpt-neox-20b,64,1,3133.0,1.0,1,H100,1697105066346,1697105069479,120,84.0,20.0,"[16, 1064, 68, 63, 59, 56, 406, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59]","[1697105066362, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067672, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479]"
1617,431,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069480,1697105070950,120,,,"[4, 726, 71, 68, 67, 56]","[1697105069484, 1697105070210, 1697105070281, 1697105070349, 1697105070416, 1697105070472]"
1618,203,34,[],200,EleutherAI/gpt-neox-20b,64,1,2258.0,1.0,1,H100,1697105070953,1697105073211,120,364.0,13.0,"[33, 1047, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49]","[1697105070986, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211]"
1619,886,20,[],200,EleutherAI/gpt-neox-20b,64,1,616.0,1.0,1,H100,1697105052700,1697105053316,120,17.0,1.0,"[5, 611]","[1697105052705, 1697105053316]"
1620,211,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070419,1697105070950,120,,,[27],[1697105070446]
1621,801,23,[],200,EleutherAI/gpt-neox-20b,64,1,2763.0,1.0,1,H100,1697105053317,1697105056080,120,47.0,20.0,"[11, 731, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71]","[1697105053328, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080]"
1622,794,30,[],200,EleutherAI/gpt-neox-20b,64,1,1008.0,1.0,1,H100,1697105070952,1697105071960,120,11.0,1.0,"[50, 958]","[1697105071002, 1697105071960]"
1623,126,20,[],200,EleutherAI/gpt-neox-20b,64,1,809.0,1.0,1,H100,1697105068410,1697105069219,120,19.0,1.0,"[13, 796]","[1697105068423, 1697105069219]"
1624,25,24,[],200,EleutherAI/gpt-neox-20b,64,1,1136.0,1.0,1,H100,1697105058843,1697105059979,120,12.0,1.0,"[81, 1055]","[1697105058924, 1697105059979]"
1625,547,21,[],200,EleutherAI/gpt-neox-20b,64,1,669.0,1.0,1,H100,1697105053317,1697105053986,120,12.0,1.0,"[17, 651]","[1697105053334, 1697105053985]"
1626,708,21,[],200,EleutherAI/gpt-neox-20b,64,1,748.0,1.0,1,H100,1697105069221,1697105069969,120,140.0,1.0,"[20, 728]","[1697105069241, 1697105069969]"
1627,570,31,[],200,EleutherAI/gpt-neox-20b,64,1,697.0,1.0,1,H100,1697105071961,1697105072658,120,18.0,1.0,"[25, 672]","[1697105071986, 1697105072658]"
1628,480,22,[],200,EleutherAI/gpt-neox-20b,64,1,959.0,1.0,1,H100,1697105069970,1697105070929,120,26.0,1.0,"[16, 943]","[1697105069986, 1697105070929]"
1629,228,23,[],200,EleutherAI/gpt-neox-20b,64,1,2848.0,1.0,1,H100,1697105070930,1697105073778,120,100.0,20.0,"[13, 1090, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 61, 50]","[1697105070943, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073778]"
1630,889,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063192,1697105065210,120,,,"[82, 800, 46, 432, 61, 59, 57, 45, 54]","[1697105063274, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1631,316,22,[],200,EleutherAI/gpt-neox-20b,64,1,2762.0,1.0,1,H100,1697105053986,1697105056748,120,86.0,20.0,"[9, 817, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 70, 68]","[1697105053995, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748]"
1632,97,29,[],200,EleutherAI/gpt-neox-20b,64,1,2633.0,1.0,1,H100,1697105075134,1697105077767,120,6.0,20.0,"[5, 654, 61, 48, 60, 57, 56, 265, 45, 53, 51, 342, 64, 55, 55, 55, 534, 63, 49, 61]","[1697105075139, 1697105075793, 1697105075854, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076895, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767]"
1633,223,32,[],200,EleutherAI/gpt-neox-20b,64,1,825.0,1.0,1,H100,1697105072659,1697105073484,120,16.0,1.0,"[9, 816]","[1697105072668, 1697105073484]"
1634,895,33,[],200,EleutherAI/gpt-neox-20b,64,1,747.0,1.0,1,H100,1697105073486,1697105074233,120,15.0,1.0,"[12, 735]","[1697105073498, 1697105074233]"
1635,549,30,[],200,EleutherAI/gpt-neox-20b,64,1,3197.0,1.0,1,H100,1697105065212,1697105068409,120,93.0,20.0,"[65, 569, 727, 68, 66, 65, 62, 58, 534, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55, 53]","[1697105065277, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409]"
1636,665,34,[],200,EleutherAI/gpt-neox-20b,64,1,2660.0,1.0,1,H100,1697105074234,1697105076894,120,90.0,20.0,"[4, 829, 67, 64, 52, 64, 63, 59, 357, 60, 49, 60, 57, 56, 265, 45, 53, 51, 342, 63]","[1697105074238, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894]"
1637,396,18,[],200,EleutherAI/gpt-neox-20b,64,1,2664.0,1.0,1,H100,1697105047017,1697105049681,120,89.0,20.0,"[5, 559, 68, 69, 67, 52, 62, 665, 129, 73, 55, 54, 55, 70, 354, 75, 72, 71, 55, 54]","[1697105047022, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048564, 1697105048693, 1697105048766, 1697105048821, 1697105048875, 1697105048930, 1697105049000, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681]"
1638,531,43,[],200,EleutherAI/gpt-neox-20b,64,1,2825.0,1.0,1,H100,1697105070953,1697105073778,120,52.0,20.0,"[134, 946, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 62, 49]","[1697105071087, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778]"
1639,851,14,[],200,EleutherAI/gpt-neox-20b,64,1,346.0,1.0,1,H100,1697105045355,1697105045701,120,23.0,1.0,"[5, 340]","[1697105045360, 1697105045700]"
1640,810,24,[],200,EleutherAI/gpt-neox-20b,64,1,2296.0,1.0,1,H100,1697105073779,1697105076075,120,91.0,20.0,"[13, 512, 63, 50, 61, 60, 53, 476, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60, 57, 56]","[1697105073792, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
1641,514,15,[],200,EleutherAI/gpt-neox-20b,64,1,3299.0,1.0,1,H100,1697105045701,1697105049000,120,85.0,20.0,"[5, 1124, 75, 56, 55, 69, 58, 438, 69, 68, 67, 52, 63, 663, 130, 73, 54, 55, 55, 70]","[1697105045706, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047581, 1697105047650, 1697105047718, 1697105047785, 1697105047837, 1697105047900, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000]"
1642,191,44,[],200,EleutherAI/gpt-neox-20b,64,1,2295.0,1.0,1,H100,1697105073780,1697105076075,120,85.0,20.0,"[24, 500, 63, 50, 61, 60, 53, 476, 66, 65, 52, 64, 63, 59, 357, 61, 48, 60, 57, 56]","[1697105073804, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075854, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
1643,888,45,[],200,EleutherAI/gpt-neox-20b,64,1,671.0,1.0,1,H100,1697105076078,1697105076749,120,19.0,1.0,"[28, 643]","[1697105076106, 1697105076749]"
1644,285,16,[],200,EleutherAI/gpt-neox-20b,64,1,4009.0,1.0,1,H100,1697105049001,1697105053010,120,100.0,27.0,"[5, 1357, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67]","[1697105049006, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010]"
1645,549,46,[],200,EleutherAI/gpt-neox-20b,64,1,2972.0,1.0,1,H100,1697105076750,1697105079722,120,93.0,20.0,"[8, 836, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 72, 68, 67, 52]","[1697105076758, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722]"
1646,319,47,[],200,EleutherAI/gpt-neox-20b,64,1,374.0,1.0,1,H100,1697105079723,1697105080097,120,31.0,1.0,"[12, 362]","[1697105079735, 1697105080097]"
1647,714,32,[],200,EleutherAI/gpt-neox-20b,64,1,2367.0,1.0,1,H100,1697105055299,1697105057666,120,83.0,20.0,"[13, 697, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 348, 72, 53, 68, 66, 65]","[1697105055312, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057342, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666]"
1648,903,48,[],200,EleutherAI/gpt-neox-20b,64,1,1456.0,1.0,1,H100,1697105080098,1697105081554,120,244.0,7.0,"[8, 1027, 142, 74, 70, 68, 67]","[1697105080106, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554]"
1649,320,35,[],200,EleutherAI/gpt-neox-20b,64,1,5226.0,1.0,1,H100,1697105076895,1697105082121,120,109.0,36.0,"[13, 686, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 72, 68, 67, 52, 58, 390, 70, 71, 69, 66, 51, 636, 141, 75, 69, 69, 67, 424, 72, 71]","[1697105076908, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081418, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121]"
1650,788,19,[],200,EleutherAI/gpt-neox-20b,64,1,598.0,1.0,1,H100,1697105043759,1697105044357,120,31.0,1.0,"[14, 584]","[1697105043773, 1697105044357]"
1651,558,20,[],200,EleutherAI/gpt-neox-20b,64,1,2727.0,1.0,1,H100,1697105044358,1697105047085,120,58.0,20.0,"[8, 723, 74, 71, 54, 66, 65, 50, 451, 133, 75, 70, 53, 65, 50, 464, 75, 56, 55, 69]","[1697105044366, 1697105045089, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045920, 1697105046053, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085]"
1652,488,26,[],200,EleutherAI/gpt-neox-20b,64,1,1007.0,1.0,1,H100,1697105070953,1697105071960,120,6.0,1.0,"[13, 994]","[1697105070966, 1697105071960]"
1653,259,27,[],200,EleutherAI/gpt-neox-20b,64,1,2577.0,1.0,1,H100,1697105071961,1697105074538,120,87.0,20.0,"[17, 920, 73, 68, 63, 60, 48, 60, 47, 234, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60]","[1697105071978, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538]"
1654,156,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062195,1697105063190,120,,,"[13, 390, 61, 60, 58, 47]","[1697105062208, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1655,715,25,[],200,EleutherAI/gpt-neox-20b,64,1,817.0,1.0,1,H100,1697105063192,1697105064009,120,20.0,1.0,"[78, 739]","[1697105063270, 1697105064009]"
1656,842,28,[],200,EleutherAI/gpt-neox-20b,64,1,1846.0,1.0,1,H100,1697105074539,1697105076385,120,161.0,16.0,"[8, 520, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60, 57, 56, 265, 45]","[1697105074547, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385]"
1657,485,26,[],200,EleutherAI/gpt-neox-20b,64,1,605.0,1.0,1,H100,1697105064010,1697105064615,120,67.0,3.0,"[13, 529, 61]","[1697105064023, 1697105064552, 1697105064613]"
1658,145,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064616,1697105065210,120,,,[6],[1697105064622]
1659,218,21,[],200,EleutherAI/gpt-neox-20b,64,1,814.0,1.0,1,H100,1697105047086,1697105047900,120,109.0,7.0,"[4, 491, 68, 69, 67, 52, 62]","[1697105047090, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899]"
1660,618,29,[],200,EleutherAI/gpt-neox-20b,64,1,363.0,1.0,1,H100,1697105076386,1697105076749,120,9.0,1.0,"[9, 354]","[1697105076395, 1697105076749]"
1661,97,36,[],200,EleutherAI/gpt-neox-20b,64,1,2644.0,1.0,1,H100,1697105082122,1697105084766,120,6.0,20.0,"[5, 562, 77, 72, 71, 54, 53, 70, 413, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72]","[1697105082127, 1697105082689, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083086, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765]"
1662,919,22,[],200,EleutherAI/gpt-neox-20b,64,1,1302.0,1.0,1,H100,1697105047902,1697105049204,120,14.0,1.0,"[17, 1285]","[1697105047919, 1697105049204]"
1663,846,28,[],200,EleutherAI/gpt-neox-20b,64,1,1560.0,1.0,1,H100,1697105065212,1697105066772,120,140.0,6.0,"[9, 625, 727, 68, 66, 65]","[1697105065221, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772]"
1664,272,30,[],200,EleutherAI/gpt-neox-20b,64,1,2972.0,1.0,1,H100,1697105076750,1697105079722,120,86.0,20.0,"[16, 828, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 72, 68, 67, 52]","[1697105076766, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722]"
1665,572,23,[],200,EleutherAI/gpt-neox-20b,64,1,985.0,1.0,1,H100,1697105049205,1697105050190,120,16.0,1.0,"[8, 977]","[1697105049213, 1697105050190]"
1666,323,24,[],200,EleutherAI/gpt-neox-20b,64,1,2753.0,1.0,1,H100,1697105050191,1697105052944,120,84.0,20.0,"[12, 988, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56]","[1697105050203, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943]"
1667,50,31,[],200,EleutherAI/gpt-neox-20b,64,1,589.0,1.0,1,H100,1697105079722,1697105080311,120,90.0,4.0,"[5, 443, 70, 71]","[1697105079727, 1697105080170, 1697105080240, 1697105080311]"
1668,633,32,[],200,EleutherAI/gpt-neox-20b,64,1,3187.0,1.0,1,H100,1697105080312,1697105083499,120,90.0,20.0,"[5, 816, 142, 74, 70, 68, 67, 424, 72, 71, 68, 65, 435, 78, 71, 71, 54, 53, 69, 414]","[1697105080317, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082689, 1697105082767, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499]"
1669,499,29,[],200,EleutherAI/gpt-neox-20b,64,1,2706.0,1.0,1,H100,1697105066773,1697105069479,120,88.0,20.0,"[5, 648, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59]","[1697105066778, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479]"
1670,94,25,[],200,EleutherAI/gpt-neox-20b,64,1,2354.0,1.0,1,H100,1697105052944,1697105055298,120,86.0,20.0,"[13, 447, 71, 67, 62, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 70, 67, 65, 51, 233]","[1697105052957, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054882, 1697105054949, 1697105055014, 1697105055065, 1697105055298]"
1671,277,30,[],200,EleutherAI/gpt-neox-20b,64,1,489.0,1.0,1,H100,1697105069480,1697105069969,120,18.0,1.0,"[8, 481]","[1697105069488, 1697105069969]"
1672,862,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069970,1697105070950,120,,,[4],[1697105069974]
1673,637,32,[],200,EleutherAI/gpt-neox-20b,64,1,2827.0,1.0,1,H100,1697105070952,1697105073779,120,96.0,20.0,"[34, 1047, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 48, 60, 47, 234, 52, 64, 62, 50]","[1697105070986, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073779]"
1674,410,33,[],200,EleutherAI/gpt-neox-20b,64,1,1776.0,1.0,1,H100,1697105083500,1697105085276,120,364.0,12.0,"[9, 904, 77, 56, 74, 73, 72, 272, 56, 73, 54, 56]","[1697105083509, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085276]"
1675,291,33,[],200,EleutherAI/gpt-neox-20b,64,1,2295.0,1.0,1,H100,1697105073780,1697105076075,120,79.0,20.0,"[24, 500, 63, 50, 61, 60, 53, 476, 66, 65, 52, 64, 63, 59, 357, 60, 49, 60, 57, 56]","[1697105073804, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
1676,614,25,[],200,EleutherAI/gpt-neox-20b,64,1,658.0,1.0,1,H100,1697105059980,1697105060638,120,15.0,1.0,"[9, 649]","[1697105059989, 1697105060638]"
1677,144,41,[],200,EleutherAI/gpt-neox-20b,64,1,2817.0,1.0,1,H100,1697105084767,1697105087584,120,96.0,20.0,"[20, 1118, 72, 70, 68, 67, 63, 348, 70, 53, 53, 66, 62, 291, 72, 54, 71, 69, 67, 63]","[1697105084787, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584]"
1678,366,25,[],200,EleutherAI/gpt-neox-20b,64,1,1480.0,1.0,1,H100,1697105063192,1697105064672,120,85.0,6.0,"[18, 864, 46, 432, 61, 59]","[1697105063210, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672]"
1679,66,34,[],200,EleutherAI/gpt-neox-20b,64,1,2307.0,1.0,1,H100,1697105085277,1697105087584,120,84.0,20.0,"[4, 624, 73, 69, 68, 67, 63, 349, 69, 53, 53, 66, 62, 291, 72, 54, 71, 69, 67, 63]","[1697105085281, 1697105085905, 1697105085978, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584]"
1680,314,18,[],200,EleutherAI/gpt-neox-20b,64,1,2502.0,1.0,1,H100,1697105058843,1697105061345,120,335.0,13.0,"[77, 1228, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62]","[1697105058920, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345]"
1681,764,35,[],200,EleutherAI/gpt-neox-20b,64,1,971.0,1.0,1,H100,1697105087586,1697105088557,120,39.0,1.0,"[20, 951]","[1697105087606, 1697105088557]"
1682,383,26,[],200,EleutherAI/gpt-neox-20b,64,1,971.0,1.0,1,H100,1697105060639,1697105061610,120,15.0,1.0,"[16, 955]","[1697105060655, 1697105061610]"
1683,63,23,[],200,EleutherAI/gpt-neox-20b,64,1,446.0,1.0,1,H100,1697105056749,1697105057195,120,39.0,1.0,"[5, 441]","[1697105056754, 1697105057195]"
1684,645,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057196,1697105058840,120,,,"[5, 892, 65, 64, 63]","[1697105057201, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1685,842,42,[],200,EleutherAI/gpt-neox-20b,64,1,2303.0,1.0,1,H100,1697105087585,1697105089888,120,161.0,16.0,"[13, 1024, 60, 61, 58, 56, 56, 271, 58, 47, 57, 46, 55, 320, 64, 57]","[1697105087598, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888]"
1686,84,19,[],200,EleutherAI/gpt-neox-20b,64,1,263.0,1.0,1,H100,1697105061347,1697105061610,120,26.0,1.0,"[7, 256]","[1697105061354, 1697105061610]"
1687,760,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105061611,1697105063191,120,,,"[9, 978, 61, 60, 58, 47]","[1697105061620, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1688,39,27,[],200,EleutherAI/gpt-neox-20b,64,1,919.0,1.0,1,H100,1697105061611,1697105062530,120,8.0,1.0,"[5, 914]","[1697105061616, 1697105062530]"
1689,414,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063193,1697105065210,120,,,"[130, 751, 46, 432, 61, 59, 57, 45, 54]","[1697105063323, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1690,186,22,[],200,EleutherAI/gpt-neox-20b,64,1,3587.0,1.0,1,H100,1697105065212,1697105068799,120,123.0,22.0,"[94, 1267, 68, 66, 65, 61, 59, 534, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55, 53, 289, 51, 50]","[1697105065306, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799]"
1691,38,34,[],200,EleutherAI/gpt-neox-20b,64,1,2469.0,1.0,1,H100,1697105076078,1697105078547,120,88.0,20.0,"[32, 721, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 58]","[1697105076110, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078547]"
1692,737,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062531,1697105063190,120,,,"[4, 633]","[1697105062535, 1697105063168]"
1693,393,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063193,1697105065211,120,,,"[117, 764, 46, 432, 61, 59, 57, 45, 54]","[1697105063310, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1694,679,17,[],200,EleutherAI/gpt-neox-20b,64,1,851.0,1.0,1,H100,1697105041102,1697105041953,120,15.0,1.0,"[16, 835]","[1697105041118, 1697105041953]"
1695,448,18,[],200,EleutherAI/gpt-neox-20b,64,1,1981.0,1.0,1,H100,1697105041954,1697105043935,120,335.0,12.0,"[16, 877, 74, 54, 63, 60, 58, 520, 73, 64, 62, 60]","[1697105041970, 1697105042847, 1697105042921, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043935]"
1696,104,19,[],200,EleutherAI/gpt-neox-20b,64,1,3149.0,1.0,1,H100,1697105043936,1697105047085,120,93.0,20.0,"[4, 1149, 74, 71, 54, 66, 65, 50, 451, 133, 75, 70, 53, 65, 50, 464, 75, 56, 55, 69]","[1697105043940, 1697105045089, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045920, 1697105046053, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085]"
1697,768,23,[],200,EleutherAI/gpt-neox-20b,64,1,1672.0,1.0,1,H100,1697105068800,1697105070472,120,47.0,6.0,"[5, 1405, 70, 69, 67, 56]","[1697105068805, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1698,264,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068750,1697105070950,120,,,"[5, 544, 61, 60, 59, 56, 675, 70, 69, 67, 56]","[1697105068755, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069535, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1699,739,35,[],200,EleutherAI/gpt-neox-20b,64,1,1550.0,1.0,1,H100,1697105078547,1697105080097,120,216.0,1.0,"[5, 1545]","[1697105078552, 1697105080097]"
1700,319,31,[],200,EleutherAI/gpt-neox-20b,64,1,810.0,1.0,1,H100,1697105068410,1697105069220,120,31.0,1.0,"[21, 788]","[1697105068431, 1697105069219]"
1701,392,36,[],200,EleutherAI/gpt-neox-20b,64,1,715.0,1.0,1,H100,1697105080098,1697105080813,120,20.0,1.0,"[29, 686]","[1697105080127, 1697105080813]"
1702,903,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069220,1697105070950,120,,,"[13, 977, 70, 69, 67, 56]","[1697105069233, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1703,680,33,[],200,EleutherAI/gpt-neox-20b,64,1,2150.0,1.0,1,H100,1697105070952,1697105073102,120,123.0,11.0,"[54, 1027, 57, 47, 46, 56, 55, 604, 73, 68, 63]","[1697105071006, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102]"
1704,333,34,[],200,EleutherAI/gpt-neox-20b,64,1,1376.0,1.0,1,H100,1697105073103,1697105074479,120,563.0,11.0,"[4, 444, 52, 64, 62, 49, 44, 482, 63, 50, 62]","[1697105073107, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074479]"
1705,481,22,[],200,EleutherAI/gpt-neox-20b,64,1,409.0,1.0,1,H100,1697105047086,1697105047495,120,10.0,1.0,"[24, 385]","[1697105047110, 1697105047495]"
1706,170,37,[],200,EleutherAI/gpt-neox-20b,64,1,2757.0,1.0,1,H100,1697105080814,1697105083571,120,335.0,15.0,"[13, 1151, 72, 71, 68, 65, 435, 77, 73, 70, 54, 53, 69, 414, 72]","[1697105080827, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082689, 1697105082766, 1697105082839, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083571]"
1707,105,35,[],200,EleutherAI/gpt-neox-20b,64,1,4010.0,1.0,1,H100,1697105074479,1697105078489,120,364.0,36.0,"[5, 583, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60, 57, 56, 265, 45, 53, 51, 342, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59]","[1697105074484, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489]"
1708,258,23,[],200,EleutherAI/gpt-neox-20b,64,1,6674.0,1.0,1,H100,1697105047496,1697105054170,120,244.0,50.0,"[16, 1051, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55]","[1697105047512, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170]"
1709,848,34,[],200,EleutherAI/gpt-neox-20b,64,1,1007.0,1.0,1,H100,1697105070953,1697105071960,120,47.0,1.0,"[21, 986]","[1697105070974, 1697105071960]"
1710,596,35,[],200,EleutherAI/gpt-neox-20b,64,1,2577.0,1.0,1,H100,1697105071961,1697105074538,120,87.0,20.0,"[8, 929, 73, 68, 63, 60, 48, 59, 48, 234, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60]","[1697105071969, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538]"
1711,515,37,[],200,EleutherAI/gpt-neox-20b,64,1,260.0,1.0,1,H100,1697105076021,1697105076281,120,11.0,1.0,"[8, 252]","[1697105076029, 1697105076281]"
1712,176,38,[],200,EleutherAI/gpt-neox-20b,64,1,550.0,1.0,1,H100,1697105076281,1697105076831,120,216.0,2.0,"[5, 545]","[1697105076286, 1697105076831]"
1713,877,39,[],200,EleutherAI/gpt-neox-20b,64,1,2890.0,1.0,1,H100,1697105076832,1697105079722,120,85.0,20.0,"[5, 757, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 72, 68, 67, 52]","[1697105076837, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722]"
1714,438,19,[],200,EleutherAI/gpt-neox-20b,64,1,491.0,1.0,1,H100,1697105054172,1697105054663,120,9.0,1.0,"[8, 483]","[1697105054180, 1697105054663]"
1715,302,33,[],200,EleutherAI/gpt-neox-20b,64,1,2576.0,1.0,1,H100,1697105082190,1697105084766,120,85.0,20.0,"[5, 494, 77, 72, 71, 54, 53, 70, 413, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 73]","[1697105082195, 1697105082689, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083086, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084766]"
1716,207,20,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105054664,1697105055224,120,10.0,1.0,"[12, 548]","[1697105054676, 1697105055224]"
1717,796,21,[],200,EleutherAI/gpt-neox-20b,64,1,2441.0,1.0,1,H100,1697105055225,1697105057666,120,86.0,20.0,"[17, 767, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 52, 63, 348, 72, 53, 68, 66, 65]","[1697105055242, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057342, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666]"
1718,678,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057667,1697105058839,120,,,[5],[1697105057672]
1719,196,25,[],200,EleutherAI/gpt-neox-20b,64,1,530.0,1.0,1,H100,1697105077595,1697105078125,120,13.0,1.0,"[13, 517]","[1697105077608, 1697105078125]"
1720,450,25,[],200,EleutherAI/gpt-neox-20b,64,1,3352.0,1.0,1,H100,1697105058842,1697105062194,120,91.0,20.0,"[50, 575, 681, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70]","[1697105058892, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194]"
1721,407,24,[],200,EleutherAI/gpt-neox-20b,64,1,548.0,1.0,1,H100,1697105062825,1697105063373,120,16.0,1.0,"[17, 531]","[1697105062842, 1697105063373]"
1722,61,25,[],200,EleutherAI/gpt-neox-20b,64,1,1111.0,1.0,1,H100,1697105063374,1697105064485,120,9.0,1.0,"[12, 1099]","[1697105063386, 1697105064485]"
1723,725,45,[],200,EleutherAI/gpt-neox-20b,64,1,2669.0,1.0,1,H100,1697105086516,1697105089185,120,90.0,20.0,"[4, 668, 71, 55, 71, 69, 67, 63, 547, 63, 60, 48, 55, 265, 60, 61, 58, 56, 55, 272]","[1697105086520, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088194, 1697105088254, 1697105088302, 1697105088357, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088912, 1697105089184]"
1724,623,44,[],200,EleutherAI/gpt-neox-20b,64,1,918.0,1.0,1,H100,1697105088913,1697105089831,120,140.0,3.0,"[5, 849, 64]","[1697105088918, 1697105089767, 1697105089831]"
1725,365,36,[],200,EleutherAI/gpt-neox-20b,64,1,442.0,1.0,1,H100,1697105074539,1697105074981,120,23.0,1.0,"[12, 430]","[1697105074551, 1697105074981]"
1726,27,37,[],200,EleutherAI/gpt-neox-20b,64,1,7068.0,1.0,1,H100,1697105074982,1697105082050,120,15.0,50.0,"[13, 798, 60, 49, 60, 57, 56, 265, 45, 53, 51, 342, 64, 55, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 141, 75, 69, 69, 67, 424, 72]","[1697105074995, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076895, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081418, 1697105081487, 1697105081554, 1697105081978, 1697105082050]"
1727,398,45,[],200,EleutherAI/gpt-neox-20b,64,1,2807.0,1.0,1,H100,1697105089832,1697105092639,120,87.0,20.0,"[4, 738, 65, 49, 60, 59, 58, 529, 71, 67, 65, 64, 312, 74, 67, 52, 52, 65, 299, 57]","[1697105089836, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639]"
1728,681,26,[],200,EleutherAI/gpt-neox-20b,64,1,617.0,1.0,1,H100,1697105055300,1697105055917,120,23.0,1.0,"[20, 597]","[1697105055320, 1697105055917]"
1729,380,46,[],200,EleutherAI/gpt-neox-20b,64,1,6253.0,1.0,1,H100,1697105089186,1697105095439,120,216.0,50.0,"[8, 573, 64, 57, 56, 45, 56, 529, 65, 48, 61, 59, 58, 530, 70, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 56, 67, 52, 66, 64, 360, 77, 69, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54, 246, 54, 67, 65, 50, 65, 336]","[1697105089194, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090687, 1697105090748, 1697105090807, 1697105090865, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438]"
1730,451,27,[],200,EleutherAI/gpt-neox-20b,64,1,622.0,1.0,1,H100,1697105055918,1697105056540,120,286.0,1.0,"[9, 613]","[1697105055927, 1697105056540]"
1731,756,33,[],200,EleutherAI/gpt-neox-20b,64,1,671.0,1.0,1,H100,1697105076078,1697105076749,120,19.0,1.0,"[20, 651]","[1697105076098, 1697105076749]"
1732,533,34,[],200,EleutherAI/gpt-neox-20b,64,1,844.0,1.0,1,H100,1697105076750,1697105077594,120,216.0,2.0,"[4, 840]","[1697105076754, 1697105077594]"
1733,189,35,[],200,EleutherAI/gpt-neox-20b,64,1,2902.0,1.0,1,H100,1697105077595,1697105080497,120,88.0,20.0,"[9, 593, 56, 55, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51]","[1697105077604, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497]"
1734,569,24,[],200,EleutherAI/gpt-neox-20b,64,1,459.0,1.0,1,H100,1697105056081,1697105056540,120,16.0,1.0,"[8, 451]","[1697105056089, 1697105056540]"
1735,111,28,[],200,EleutherAI/gpt-neox-20b,64,1,994.0,1.0,1,H100,1697105056541,1697105057535,120,79.0,5.0,"[4, 796, 73, 53, 68]","[1697105056545, 1697105057341, 1697105057414, 1697105057467, 1697105057535]"
1736,95,24,[],200,EleutherAI/gpt-neox-20b,64,1,459.0,1.0,1,H100,1697105056081,1697105056540,120,12.0,1.0,"[20, 439]","[1697105056101, 1697105056540]"
1737,230,25,[],200,EleutherAI/gpt-neox-20b,64,1,994.0,1.0,1,H100,1697105056541,1697105057535,120,86.0,5.0,"[4, 796, 72, 54, 68]","[1697105056545, 1697105057341, 1697105057413, 1697105057467, 1697105057535]"
1738,812,29,[],200,EleutherAI/gpt-neox-20b,64,1,476.0,1.0,1,H100,1697105057536,1697105058012,120,16.0,1.0,"[8, 468]","[1697105057544, 1697105058012]"
1739,0,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057536,1697105058839,120,,,"[5, 552, 65, 64, 63]","[1697105057541, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1740,583,27,[],200,EleutherAI/gpt-neox-20b,64,1,3351.0,1.0,1,H100,1697105058843,1697105062194,120,96.0,20.0,"[12, 612, 681, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 121, 125, 70]","[1697105058855, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105061999, 1697105062124, 1697105062194]"
1741,465,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058013,1697105058840,120,,,[12],[1697105058025]
1742,238,31,[],200,EleutherAI/gpt-neox-20b,64,1,1520.0,1.0,1,H100,1697105058843,1697105060363,120,563.0,6.0,"[65, 1240, 59, 57, 45, 54]","[1697105058908, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363]"
1743,820,32,[],200,EleutherAI/gpt-neox-20b,64,1,2296.0,1.0,1,H100,1697105060364,1697105062660,120,161.0,9.0,"[4, 1436, 74, 122, 124, 70, 55, 349, 61]","[1697105060368, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659]"
1744,51,46,[],200,EleutherAI/gpt-neox-20b,64,1,4388.0,1.0,1,H100,1697105092640,1697105097028,120,364.0,36.0,"[16, 592, 76, 69, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54, 247, 53, 67, 65, 50, 65, 336, 66, 50, 64, 64, 511, 75, 56, 74, 55, 72, 54, 244, 70, 68, 67]","[1697105092656, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028]"
1745,332,28,[],200,EleutherAI/gpt-neox-20b,64,1,335.0,1.0,1,H100,1697105062195,1697105062530,120,39.0,1.0,"[33, 302]","[1697105062228, 1697105062530]"
1746,910,29,[],200,EleutherAI/gpt-neox-20b,64,1,637.0,1.0,1,H100,1697105062531,1697105063168,120,8.0,1.0,"[12, 625]","[1697105062543, 1697105063168]"
1747,800,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105056541,1697105058840,120,,,"[12, 788, 72, 54, 68, 66, 65, 427, 65, 64, 63]","[1697105056553, 1697105057341, 1697105057413, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1748,685,30,[],200,EleutherAI/gpt-neox-20b,64,1,905.0,1.0,1,H100,1697105063169,1697105064074,120,364.0,2.0,"[5, 900]","[1697105063174, 1697105064074]"
1749,385,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058843,1697105063190,120,,,"[56, 568, 681, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 54, 350, 61, 60, 58, 47]","[1697105058899, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062248, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1750,339,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064075,1697105065209,120,,,"[8, 469, 61, 59, 57, 45, 54]","[1697105064083, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1751,454,26,[],200,EleutherAI/gpt-neox-20b,64,1,1467.0,1.0,1,H100,1697105058843,1697105060310,120,182.0,6.0,"[20, 604, 681, 59, 57, 46]","[1697105058863, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310]"
1752,798,30,[],200,EleutherAI/gpt-neox-20b,64,1,662.0,1.0,1,H100,1697105077768,1697105078430,120,79.0,6.0,"[4, 425, 56, 55, 62, 60]","[1697105077772, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430]"
1753,451,31,[],200,EleutherAI/gpt-neox-20b,64,1,761.0,1.0,1,H100,1697105078431,1697105079192,120,286.0,1.0,"[4, 757]","[1697105078435, 1697105079192]"
1754,115,32,[],200,EleutherAI/gpt-neox-20b,64,1,1133.0,1.0,1,H100,1697105065212,1697105066345,120,13.0,1.0,"[98, 1035]","[1697105065310, 1697105066345]"
1755,171,23,[],200,EleutherAI/gpt-neox-20b,64,1,548.0,1.0,1,H100,1697105069421,1697105069969,120,6.0,1.0,"[5, 543]","[1697105069426, 1697105069969]"
1756,700,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105066346,1697105070950,120,,,"[4, 1076, 68, 63, 59, 55, 407, 59, 59, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105066350, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068137, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1757,199,32,[],200,EleutherAI/gpt-neox-20b,64,1,904.0,1.0,1,H100,1697105079193,1697105080097,120,13.0,1.0,"[9, 895]","[1697105079202, 1697105080097]"
1758,893,33,[],200,EleutherAI/gpt-neox-20b,64,1,2023.0,1.0,1,H100,1697105080098,1697105082121,120,335.0,10.0,"[4, 1031, 141, 75, 70, 68, 67, 425, 71, 71]","[1697105080102, 1697105081133, 1697105081274, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081979, 1697105082050, 1697105082121]"
1759,726,38,[],200,EleutherAI/gpt-neox-20b,64,1,6142.0,1.0,1,H100,1697105082051,1697105088193,120,67.0,47.0,"[5, 633, 77, 72, 71, 54, 53, 70, 413, 72, 67, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562, 72, 70, 68, 67, 63, 348, 70, 53, 53, 66, 62, 291, 71, 55, 70, 70, 67, 63, 546, 63]","[1697105082056, 1697105082689, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083086, 1697105083499, 1697105083571, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193]"
1760,95,25,[],200,EleutherAI/gpt-neox-20b,64,1,815.0,1.0,1,H100,1697105063194,1697105064009,120,12.0,1.0,"[84, 731]","[1697105063278, 1697105064009]"
1761,553,34,[],200,EleutherAI/gpt-neox-20b,64,1,2644.0,1.0,1,H100,1697105082122,1697105084766,120,88.0,20.0,"[5, 562, 77, 72, 71, 54, 53, 70, 413, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72]","[1697105082127, 1697105082689, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083086, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765]"
1762,793,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064010,1697105065209,120,,,"[21, 521, 61, 59, 57, 45, 54]","[1697105064031, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1763,802,34,[],200,EleutherAI/gpt-neox-20b,64,1,254.0,1.0,1,H100,1697105058652,1697105058906,120,9.0,1.0,"[12, 242]","[1697105058664, 1697105058906]"
1764,454,27,[],200,EleutherAI/gpt-neox-20b,64,1,1560.0,1.0,1,H100,1697105065212,1697105066772,120,182.0,6.0,"[13, 621, 727, 68, 66, 65]","[1697105065225, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772]"
1765,545,24,[],200,EleutherAI/gpt-neox-20b,64,1,1710.0,1.0,1,H100,1697105070473,1697105072183,120,216.0,5.0,"[5, 1555, 57, 47, 46]","[1697105070478, 1697105072033, 1697105072090, 1697105072137, 1697105072183]"
1766,225,28,[],200,EleutherAI/gpt-neox-20b,64,1,501.0,1.0,1,H100,1697105066773,1697105067274,120,23.0,1.0,"[9, 492]","[1697105066782, 1697105067274]"
1767,357,23,[],200,EleutherAI/gpt-neox-20b,64,1,4495.0,1.0,1,H100,1697105053727,1697105058222,120,52.0,33.0,"[8, 1077, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 70, 68, 67, 64, 52, 63, 347, 73, 53, 68, 66, 65, 427, 65, 64]","[1697105053735, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222]"
1768,200,25,[],200,EleutherAI/gpt-neox-20b,64,1,1133.0,1.0,1,H100,1697105072184,1697105073317,120,6.0,9.0,"[4, 710, 73, 68, 63, 60, 49, 59, 47]","[1697105072188, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317]"
1769,483,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057667,1697105058840,120,,,[21],[1697105057688]
1770,139,34,[],200,EleutherAI/gpt-neox-20b,64,1,3755.0,1.0,1,H100,1697105058843,1697105062598,120,39.0,21.0,"[125, 1180, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 54, 350]","[1697105058968, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062248, 1697105062598]"
1771,780,20,[],200,EleutherAI/gpt-neox-20b,64,1,2595.0,1.0,1,H100,1697105047086,1697105049681,120,85.0,20.0,"[8, 487, 68, 69, 67, 52, 62, 665, 129, 73, 54, 55, 55, 70, 354, 75, 72, 71, 55, 54]","[1697105047094, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681]"
1772,895,22,[],200,EleutherAI/gpt-neox-20b,64,1,203.0,1.0,1,H100,1697105063170,1697105063373,120,15.0,1.0,"[8, 195]","[1697105063178, 1697105063373]"
1773,808,29,[],200,EleutherAI/gpt-neox-20b,64,1,803.0,1.0,1,H100,1697105067275,1697105068078,120,286.0,2.0,"[21, 782]","[1697105067296, 1697105068078]"
1774,584,30,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105068079,1697105068639,120,10.0,1.0,"[8, 552]","[1697105068087, 1697105068639]"
1775,236,31,[],200,EleutherAI/gpt-neox-20b,64,1,579.0,1.0,1,H100,1697105068640,1697105069219,120,8.0,1.0,"[16, 563]","[1697105068656, 1697105069219]"
1776,9,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069220,1697105070950,120,,,"[9, 981, 70, 69, 67, 56]","[1697105069229, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1777,787,26,[],200,EleutherAI/gpt-neox-20b,64,1,1596.0,1.0,1,H100,1697105078126,1697105079722,120,123.0,6.0,"[9, 1328, 72, 68, 67, 52]","[1697105078135, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722]"
1778,556,27,[],200,EleutherAI/gpt-neox-20b,64,1,374.0,1.0,1,H100,1697105079723,1697105080097,120,9.0,1.0,"[16, 358]","[1697105079739, 1697105080097]"
1779,215,28,[],200,EleutherAI/gpt-neox-20b,64,1,715.0,1.0,1,H100,1697105080098,1697105080813,120,12.0,1.0,"[25, 690]","[1697105080123, 1697105080813]"
1780,764,19,[],200,EleutherAI/gpt-neox-20b,64,1,1016.0,1.0,1,H100,1697105056995,1697105058011,120,39.0,1.0,"[5, 1011]","[1697105057000, 1697105058011]"
1781,892,29,[],200,EleutherAI/gpt-neox-20b,64,1,3599.0,1.0,1,H100,1697105080814,1697105084413,120,87.0,20.0,"[9, 1155, 72, 71, 68, 65, 435, 77, 73, 70, 54, 53, 69, 414, 72, 68, 51, 67, 64, 592]","[1697105080823, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082689, 1697105082766, 1697105082839, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083571, 1697105083639, 1697105083690, 1697105083757, 1697105083821, 1697105084413]"
1782,680,37,[],200,EleutherAI/gpt-neox-20b,64,1,2004.0,1.0,1,H100,1697105084766,1697105086770,120,123.0,11.0,"[13, 1125, 73, 70, 68, 67, 63, 348, 70, 56, 51]","[1697105084779, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086719, 1697105086770]"
1783,224,19,[],200,EleutherAI/gpt-neox-20b,64,1,2455.0,1.0,1,H100,1697105049682,1697105052137,120,85.0,20.0,"[4, 677, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52]","[1697105049686, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137]"
1784,785,35,[],200,EleutherAI/gpt-neox-20b,64,1,272.0,1.0,1,H100,1697105073212,1697105073484,120,10.0,1.0,"[4, 268]","[1697105073216, 1697105073484]"
1785,99,26,[],200,EleutherAI/gpt-neox-20b,64,1,334.0,1.0,1,H100,1697105062196,1697105062530,120,10.0,1.0,"[36, 298]","[1697105062232, 1697105062530]"
1786,561,36,[],200,EleutherAI/gpt-neox-20b,64,1,2590.0,1.0,1,H100,1697105073485,1697105076075,120,87.0,20.0,"[9, 809, 64, 50, 61, 60, 53, 476, 67, 65, 51, 64, 63, 60, 356, 60, 49, 60, 57, 56]","[1697105073494, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075314, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
1787,806,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062531,1697105063190,120,,,[8],[1697105062539]
1788,567,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062660,1697105063190,120,,,[5],[1697105062665]
1789,422,25,[],200,EleutherAI/gpt-neox-20b,64,1,1136.0,1.0,1,H100,1697105058843,1697105059979,120,26.0,1.0,"[133, 1003]","[1697105058976, 1697105059979]"
1790,81,26,[],200,EleutherAI/gpt-neox-20b,64,1,2144.0,1.0,1,H100,1697105059980,1697105062124,120,732.0,13.0,"[8, 776, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124]","[1697105059988, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124]"
1791,457,28,[],200,EleutherAI/gpt-neox-20b,64,1,881.0,1.0,1,H100,1697105063193,1697105064074,120,874.0,2.0,"[94, 787]","[1697105063287, 1697105064074]"
1792,233,29,[],200,EleutherAI/gpt-neox-20b,64,1,410.0,1.0,1,H100,1697105064075,1697105064485,120,6.0,1.0,"[13, 397]","[1697105064088, 1697105064485]"
1793,779,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062125,1697105063191,120,,,"[5, 468, 61, 60, 58, 47]","[1697105062130, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1794,816,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064486,1697105065210,120,,,[4],[1697105064490]
1795,440,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063194,1697105065210,120,,,"[96, 784, 46, 432, 61, 59, 57, 45, 54]","[1697105063290, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1796,209,29,[],200,EleutherAI/gpt-neox-20b,64,1,581.0,1.0,1,H100,1697105065212,1697105065793,120,20.0,1.0,"[37, 544]","[1697105065249, 1697105065793]"
1797,798,30,[],200,EleutherAI/gpt-neox-20b,64,1,1040.0,1.0,1,H100,1697105065794,1697105066834,120,79.0,6.0,"[9, 770, 68, 66, 65, 62]","[1697105065803, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834]"
1798,567,31,[],200,EleutherAI/gpt-neox-20b,64,1,2644.0,1.0,1,H100,1697105066835,1697105069479,120,90.0,20.0,"[4, 587, 68, 63, 59, 56, 406, 60, 58, 58, 47, 55, 53, 289, 51, 50, 500, 61, 60, 59]","[1697105066839, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067672, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479]"
1799,263,31,[],200,EleutherAI/gpt-neox-20b,64,1,512.0,1.0,1,H100,1697105070417,1697105070929,120,15.0,1.0,"[17, 495]","[1697105070434, 1697105070929]"
1800,221,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063194,1697105065210,120,,,"[124, 756, 46, 432, 61, 59, 57, 45, 54]","[1697105063318, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1801,35,32,[],200,EleutherAI/gpt-neox-20b,64,1,2848.0,1.0,1,H100,1697105070930,1697105073778,120,87.0,20.0,"[9, 1094, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 61, 50]","[1697105070939, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073778]"
1802,587,31,[],200,EleutherAI/gpt-neox-20b,64,1,1133.0,1.0,1,H100,1697105065212,1697105066345,120,13.0,1.0,"[113, 1020]","[1697105065325, 1697105066345]"
1803,336,32,[],200,EleutherAI/gpt-neox-20b,64,1,1732.0,1.0,1,H100,1697105066346,1697105068078,120,58.0,7.0,"[8, 1072, 68, 63, 59, 55, 407]","[1697105066354, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078]"
1804,753,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069970,1697105070950,120,,,[8],[1697105069978]
1805,921,35,[],200,EleutherAI/gpt-neox-20b,64,1,1133.0,1.0,1,H100,1697105065212,1697105066345,120,31.0,1.0,"[109, 1024]","[1697105065321, 1697105066345]"
1806,585,25,[],200,EleutherAI/gpt-neox-20b,64,1,7007.0,1.0,1,H100,1697105076078,1697105083085,120,244.0,50.0,"[24, 729, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 58, 916, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 141, 75, 69, 69, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69]","[1697105076102, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078547, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081418, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085]"
1807,921,33,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105068079,1697105068639,120,31.0,1.0,"[8, 552]","[1697105068087, 1697105068639]"
1808,697,34,[],200,EleutherAI/gpt-neox-20b,64,1,1776.0,1.0,1,H100,1697105068640,1697105070416,120,123.0,10.0,"[8, 651, 61, 60, 59, 56, 675, 70, 69, 67]","[1697105068648, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069535, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
1809,612,41,[],200,EleutherAI/gpt-neox-20b,64,1,3026.0,1.0,1,H100,1697105093168,1697105096194,120,93.0,20.0,"[5, 999, 74, 57, 72, 71, 55, 54, 247, 53, 67, 65, 50, 65, 336, 66, 50, 64, 64, 512]","[1697105093173, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194]"
1810,842,24,[],200,EleutherAI/gpt-neox-20b,64,1,2033.0,1.0,1,H100,1697105052138,1697105054171,120,161.0,16.0,"[8, 553, 74, 57, 57, 56, 67, 394, 71, 66, 63, 62, 60, 333, 56, 55]","[1697105052146, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170]"
1811,531,25,[],200,EleutherAI/gpt-neox-20b,64,1,2826.0,1.0,1,H100,1697105070952,1697105073778,120,52.0,20.0,"[6, 1075, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 61, 50]","[1697105070958, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073778]"
1812,144,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062250,1697105063190,120,,,[12],[1697105062262]
1813,582,36,[],200,EleutherAI/gpt-neox-20b,64,1,928.0,1.0,1,H100,1697105066346,1697105067274,120,19.0,1.0,"[20, 908]","[1697105066366, 1697105067274]"
1814,351,37,[],200,EleutherAI/gpt-neox-20b,64,1,1026.0,1.0,1,H100,1697105067275,1697105068301,120,216.0,6.0,"[8, 795, 60, 58, 59, 46]","[1697105067283, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301]"
1815,851,30,[],200,EleutherAI/gpt-neox-20b,64,1,816.0,1.0,1,H100,1697105063193,1697105064009,120,23.0,1.0,"[110, 706]","[1697105063303, 1697105064009]"
1816,350,35,[],200,EleutherAI/gpt-neox-20b,64,1,512.0,1.0,1,H100,1697105070417,1697105070929,120,216.0,1.0,"[5, 507]","[1697105070422, 1697105070929]"
1817,504,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064010,1697105065209,120,,,"[13, 529, 61, 59, 57, 45, 54]","[1697105064023, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1818,126,36,[],200,EleutherAI/gpt-neox-20b,64,1,222.0,1.0,1,H100,1697105070930,1697105071152,120,19.0,1.0,"[13, 209]","[1697105070943, 1697105071152]"
1819,711,37,[],200,EleutherAI/gpt-neox-20b,64,1,1886.0,1.0,1,H100,1697105071153,1697105073039,120,457.0,4.0,"[4, 1740, 74, 68]","[1697105071157, 1697105072897, 1697105072971, 1697105073039]"
1820,480,38,[],200,EleutherAI/gpt-neox-20b,64,1,443.0,1.0,1,H100,1697105073041,1697105073484,120,26.0,1.0,"[8, 435]","[1697105073049, 1697105073484]"
1821,576,28,[],200,EleutherAI/gpt-neox-20b,64,1,618.0,1.0,1,H100,1697105055299,1697105055917,120,14.0,1.0,"[17, 601]","[1697105055316, 1697105055917]"
1822,647,49,[],200,EleutherAI/gpt-neox-20b,64,1,3210.0,1.0,1,H100,1697105081555,1697105084765,120,83.0,20.0,"[5, 1128, 78, 72, 71, 54, 53, 69, 414, 72, 68, 51, 67, 64, 592, 77, 56, 74, 73, 72]","[1697105081560, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083571, 1697105083639, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765]"
1823,555,23,[],200,EleutherAI/gpt-neox-20b,64,1,1111.0,1.0,1,H100,1697105063374,1697105064485,120,11.0,1.0,"[12, 1099]","[1697105063386, 1697105064485]"
1824,142,39,[],200,EleutherAI/gpt-neox-20b,64,1,2590.0,1.0,1,H100,1697105073485,1697105076075,120,52.0,20.0,"[5, 813, 64, 50, 61, 60, 53, 476, 67, 64, 52, 64, 63, 60, 356, 60, 49, 60, 57, 56]","[1697105073490, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
1825,230,29,[],200,EleutherAI/gpt-neox-20b,64,1,897.0,1.0,1,H100,1697105055918,1697105056815,120,86.0,5.0,"[5, 687, 70, 68, 67]","[1697105055923, 1697105056610, 1697105056680, 1697105056748, 1697105056815]"
1826,7,30,[],200,EleutherAI/gpt-neox-20b,64,1,1469.0,1.0,1,H100,1697105056816,1697105058285,120,345.0,11.0,"[4, 522, 72, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105056820, 1697105057342, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1827,319,24,[],200,EleutherAI/gpt-neox-20b,64,1,702.0,1.0,1,H100,1697105064486,1697105065188,120,31.0,1.0,"[12, 690]","[1697105064498, 1697105065188]"
1828,778,18,[],200,EleutherAI/gpt-neox-20b,64,1,6283.0,1.0,1,H100,1697105051810,1697105058093,120,16.0,50.0,"[5, 884, 74, 57, 57, 56, 67, 394, 71, 66, 63, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 72, 54, 68, 66, 65, 427]","[1697105051815, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057413, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093]"
1829,587,31,[],200,EleutherAI/gpt-neox-20b,64,1,620.0,1.0,1,H100,1697105058286,1697105058906,120,13.0,1.0,"[5, 615]","[1697105058291, 1697105058906]"
1830,335,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058907,1697105063190,120,,,"[89, 1152, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105058996, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1831,67,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105065190,1697105070950,120,,,"[11, 645, 727, 68, 66, 65, 61, 59, 534, 68, 62, 60, 55, 406, 60, 59, 58, 47, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105065201, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067556, 1697105067616, 1697105067671, 1697105068077, 1697105068137, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1832,255,14,[],200,EleutherAI/gpt-neox-20b,64,1,15979.0,1.0,1,H100,1697105037425,1697105053404,120,216.0,119.0,"[25, 537, 653, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 64, 49, 60, 53, 311, 46, 46, 56, 55, 274, 54, 43, 43, 53, 467, 62, 58, 56, 46, 54, 537, 73, 55, 63, 60, 58, 520, 72, 65, 62, 59, 508, 70, 62, 58, 457, 74, 70, 55, 66, 65, 49, 451, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437, 69, 68, 68, 52, 62, 664, 130, 73, 54, 55, 55, 70, 353, 75, 73, 71, 54, 55, 64, 618, 237, 74, 72, 70, 64, 311, 76, 74, 71, 56, 70, 67, 204, 70, 69, 69, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394]","[1697105037450, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040424, 1697105040473, 1697105040533, 1697105040586, 1697105040897, 1697105040943, 1697105040989, 1697105041045, 1697105041100, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567, 1697105042034, 1697105042096, 1697105042154, 1697105042210, 1697105042256, 1697105042310, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043748, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045419, 1697105045468, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717, 1697105047785, 1697105047837, 1697105047899, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049428, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050816, 1697105050880, 1697105051191, 1697105051267, 1697105051341, 1697105051412, 1697105051468, 1697105051538, 1697105051605, 1697105051809, 1697105051879, 1697105051948, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404]"
1833,214,37,[],200,EleutherAI/gpt-neox-20b,64,1,2469.0,1.0,1,H100,1697105076077,1697105078546,120,52.0,20.0,"[10, 744, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 57]","[1697105076087, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546]"
1834,915,38,[],200,EleutherAI/gpt-neox-20b,64,1,1550.0,1.0,1,H100,1697105078547,1697105080097,120,182.0,1.0,"[9, 1541]","[1697105078556, 1697105080097]"
1835,592,33,[],200,EleutherAI/gpt-neox-20b,64,1,6641.0,1.0,1,H100,1697105070953,1697105077594,120,15.0,50.0,"[29, 1051, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 60, 47, 234, 52, 64, 62, 49, 44, 481, 64, 50, 61, 60, 53, 476, 66, 65, 52, 64, 63, 59, 357, 60, 49, 59, 58, 55, 266, 45, 53, 51, 342, 63, 56, 55, 55, 534]","[1697105070982, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075902, 1697105075961, 1697105076019, 1697105076074, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594]"
1836,569,39,[],200,EleutherAI/gpt-neox-20b,64,1,715.0,1.0,1,H100,1697105080098,1697105080813,120,16.0,1.0,"[17, 698]","[1697105080115, 1697105080813]"
1837,140,19,[],200,EleutherAI/gpt-neox-20b,64,1,2845.0,1.0,1,H100,1697105048694,1697105051539,120,96.0,20.0,"[8, 652, 75, 73, 70, 55, 54, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71]","[1697105048702, 1697105049354, 1697105049429, 1697105049502, 1697105049572, 1697105049627, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539]"
1838,340,40,[],200,EleutherAI/gpt-neox-20b,64,1,3599.0,1.0,1,H100,1697105080814,1697105084413,120,85.0,20.0,"[5, 1159, 72, 71, 68, 65, 435, 77, 73, 70, 54, 53, 69, 414, 71, 69, 51, 67, 64, 592]","[1697105080819, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082689, 1697105082766, 1697105082839, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083639, 1697105083690, 1697105083757, 1697105083821, 1697105084413]"
1839,595,33,[],200,EleutherAI/gpt-neox-20b,64,1,453.0,1.0,1,H100,1697105073780,1697105074233,120,8.0,1.0,"[28, 425]","[1697105073808, 1697105074233]"
1840,611,25,[],200,EleutherAI/gpt-neox-20b,64,1,491.0,1.0,1,H100,1697105054172,1697105054663,120,14.0,1.0,"[4, 487]","[1697105054176, 1697105054663]"
1841,526,23,[],200,EleutherAI/gpt-neox-20b,64,1,3197.0,1.0,1,H100,1697105049746,1697105052943,120,89.0,20.0,"[4, 1441, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56]","[1697105049750, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943]"
1842,268,26,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105054664,1697105055224,120,19.0,1.0,"[8, 552]","[1697105054672, 1697105055224]"
1843,40,27,[],200,EleutherAI/gpt-neox-20b,64,1,2441.0,1.0,1,H100,1697105055225,1697105057666,120,86.0,20.0,"[13, 771, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 52, 63, 348, 72, 53, 68, 66, 65]","[1697105055238, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057342, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666]"
1844,186,26,[],200,EleutherAI/gpt-neox-20b,64,1,2606.0,1.0,1,H100,1697105073779,1697105076385,120,123.0,22.0,"[17, 508, 63, 50, 61, 60, 53, 476, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60, 57, 56, 265, 45]","[1697105073796, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385]"
1845,845,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105053011,1697105058839,120,,,"[4, 1044, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105053015, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1846,562,29,[],200,EleutherAI/gpt-neox-20b,64,1,5066.0,1.0,1,H100,1697105070953,1697105076019,120,67.0,39.0,"[9, 1071, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 62, 49, 44, 481, 64, 50, 61, 60, 53, 476, 66, 65, 52, 64, 63, 59, 357, 60, 49, 60, 57]","[1697105070962, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019]"
1847,622,28,[],200,EleutherAI/gpt-neox-20b,64,1,984.0,1.0,1,H100,1697105057667,1697105058651,120,20.0,1.0,"[17, 967]","[1697105057684, 1697105058651]"
1848,279,32,[],200,EleutherAI/gpt-neox-20b,64,1,3089.0,1.0,1,H100,1697105065212,1697105068301,120,67.0,18.0,"[25, 609, 727, 68, 66, 65, 61, 59, 534, 68, 62, 60, 55, 406, 61, 58, 58, 47]","[1697105065237, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067556, 1697105067616, 1697105067671, 1697105068077, 1697105068138, 1697105068196, 1697105068254, 1697105068301]"
1849,399,29,[],200,EleutherAI/gpt-neox-20b,64,1,3542.0,1.0,1,H100,1697105058652,1697105062194,120,87.0,20.0,"[4, 811, 681, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 397, 75, 121, 125, 70]","[1697105058656, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061803, 1697105061878, 1697105061999, 1697105062124, 1697105062194]"
1850,861,33,[],200,EleutherAI/gpt-neox-20b,64,1,337.0,1.0,1,H100,1697105068302,1697105068639,120,10.0,1.0,"[9, 328]","[1697105068311, 1697105068639]"
1851,639,34,[],200,EleutherAI/gpt-neox-20b,64,1,896.0,1.0,1,H100,1697105068639,1697105069535,120,100.0,6.0,"[5, 655, 61, 60, 59, 56]","[1697105068644, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069535]"
1852,899,26,[],200,EleutherAI/gpt-neox-20b,64,1,2757.0,1.0,1,H100,1697105073318,1697105076075,120,100.0,20.0,"[5, 980, 64, 50, 61, 60, 53, 476, 67, 65, 51, 64, 63, 60, 356, 60, 49, 60, 57, 56]","[1697105073323, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075314, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
1853,293,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069535,1697105070950,120,,,[5],[1697105069540]
1854,66,36,[],200,EleutherAI/gpt-neox-20b,64,1,2827.0,1.0,1,H100,1697105070952,1697105073779,120,84.0,20.0,"[30, 1051, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 60, 47, 234, 52, 64, 62, 49]","[1697105070982, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778]"
1855,885,27,[],200,EleutherAI/gpt-neox-20b,64,1,5868.0,1.0,1,H100,1697105076386,1697105082254,120,84.0,43.0,"[8, 437, 63, 56, 56, 54, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 58, 916, 72, 68, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 141, 75, 70, 68, 67, 424, 72, 71, 68, 65]","[1697105076394, 1697105076831, 1697105076894, 1697105076950, 1697105077006, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078547, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254]"
1856,921,33,[],200,EleutherAI/gpt-neox-20b,64,1,816.0,1.0,1,H100,1697105063193,1697105064009,120,31.0,1.0,"[33, 783]","[1697105063226, 1697105064009]"
1857,693,34,[],200,EleutherAI/gpt-neox-20b,64,1,541.0,1.0,1,H100,1697105064011,1697105064552,120,67.0,2.0,"[32, 509]","[1697105064043, 1697105064552]"
1858,50,30,[],200,EleutherAI/gpt-neox-20b,64,1,524.0,1.0,1,H100,1697105062195,1697105062719,120,90.0,4.0,"[9, 394, 61, 60]","[1697105062204, 1697105062598, 1697105062659, 1697105062719]"
1859,353,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064553,1697105065210,120,,,[8],[1697105064561]
1860,123,36,[],200,EleutherAI/gpt-neox-20b,64,1,1133.0,1.0,1,H100,1697105065212,1697105066345,120,14.0,1.0,"[170, 963]","[1697105065382, 1697105066345]"
1861,138,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058223,1697105058840,120,,,[4],[1697105058227]
1862,307,34,[],200,EleutherAI/gpt-neox-20b,64,1,1201.0,1.0,1,H100,1697105073780,1697105074981,120,26.0,1.0,"[49, 1152]","[1697105073829, 1697105074981]"
1863,559,27,[],200,EleutherAI/gpt-neox-20b,64,1,2470.0,1.0,1,H100,1697105076077,1697105078547,120,86.0,20.0,"[13, 741, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 57]","[1697105076090, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546]"
1864,719,25,[],200,EleutherAI/gpt-neox-20b,64,1,1468.0,1.0,1,H100,1697105058842,1697105060310,120,182.0,6.0,"[5, 620, 681, 59, 57, 45]","[1697105058847, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060309]"
1865,632,28,[],200,EleutherAI/gpt-neox-20b,64,1,3650.0,1.0,1,H100,1697105082255,1697105085905,120,91.0,20.0,"[4, 1240, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562]","[1697105082259, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905]"
1866,327,28,[],200,EleutherAI/gpt-neox-20b,64,1,2803.0,1.0,1,H100,1697105078547,1697105081350,120,563.0,10.0,"[13, 1610, 70, 71, 69, 66, 51, 636, 142, 75]","[1697105078560, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081350]"
1867,631,25,[],200,EleutherAI/gpt-neox-20b,64,1,6456.0,1.0,1,H100,1697105047086,1697105053542,120,216.0,50.0,"[12, 483, 68, 69, 67, 52, 62, 665, 129, 73, 54, 55, 55, 70, 354, 75, 72, 71, 55, 54, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67]","[1697105047098, 1697105047581, 1697105047649, 1697105047718, 1697105047785, 1697105047837, 1697105047899, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049354, 1697105049429, 1697105049501, 1697105049572, 1697105049627, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542]"
1868,891,29,[],200,EleutherAI/gpt-neox-20b,64,1,627.0,1.0,1,H100,1697105081352,1697105081979,120,52.0,2.0,"[8, 618]","[1697105081360, 1697105081978]"
1869,661,30,[],200,EleutherAI/gpt-neox-20b,64,1,1592.0,1.0,1,H100,1697105081979,1697105083571,120,161.0,10.0,"[5, 704, 78, 72, 71, 54, 53, 69, 414, 72]","[1697105081984, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083571]"
1870,549,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058094,1697105058840,120,,,[4],[1697105058098]
1871,210,20,[],200,EleutherAI/gpt-neox-20b,64,1,624.0,1.0,1,H100,1697105058843,1697105059467,120,140.0,2.0,"[53, 571]","[1697105058896, 1697105059467]"
1872,492,26,[],200,EleutherAI/gpt-neox-20b,64,1,2514.0,1.0,1,H100,1697105060310,1697105062824,120,47.0,20.0,"[5, 449, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060315, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1873,908,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105059468,1697105063190,120,,,"[5, 1291, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 54, 350, 61, 60, 58, 47]","[1697105059473, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062248, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1874,541,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058012,1697105058840,120,,,[5],[1697105058017]
1875,193,21,[],200,EleutherAI/gpt-neox-20b,64,1,3351.0,1.0,1,H100,1697105058843,1697105062194,120,79.0,20.0,"[20, 604, 681, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70]","[1697105058863, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194]"
1876,316,31,[],200,EleutherAI/gpt-neox-20b,64,1,3022.0,1.0,1,H100,1697105083572,1697105086594,120,86.0,20.0,"[8, 833, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562, 73, 69, 68, 67, 63, 349]","[1697105083580, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905, 1697105085978, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594]"
1877,567,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063192,1697105065210,120,,,"[42, 840, 46, 432, 61, 59, 57, 45, 54]","[1697105063234, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1878,661,30,[],200,EleutherAI/gpt-neox-20b,64,1,1633.0,1.0,1,H100,1697105084414,1697105086047,120,161.0,10.0,"[4, 619, 56, 73, 54, 55, 68, 561, 73, 70]","[1697105084418, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047]"
1879,245,37,[],200,EleutherAI/gpt-neox-20b,64,1,2295.0,1.0,1,H100,1697105073780,1697105076075,120,100.0,20.0,"[32, 492, 63, 50, 61, 60, 53, 476, 66, 65, 52, 64, 63, 59, 357, 61, 48, 60, 57, 56]","[1697105073812, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075854, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
1880,898,22,[],200,EleutherAI/gpt-neox-20b,64,1,403.0,1.0,1,H100,1697105062195,1697105062598,120,79.0,2.0,"[13, 390]","[1697105062208, 1697105062598]"
1881,211,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105047497,1697105058841,120,,,"[19, 1048, 129, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105047516, 1697105048564, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1882,244,19,[],200,EleutherAI/gpt-neox-20b,64,1,199.0,1.0,1,H100,1697105051540,1697105051739,120,9.0,1.0,"[8, 191]","[1697105051548, 1697105051739]"
1883,364,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105074235,1697105102062,120,,,"[15, 817, 67, 64, 52, 64, 63, 60, 356, 60, 49, 60, 57, 56, 265, 45, 53, 51, 342, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 141, 75, 69, 69, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 71, 68, 52, 66, 65, 592, 77, 56, 74, 73, 72, 272, 56, 72, 55, 55, 68, 561, 73, 70, 68, 67, 63, 348, 70, 53, 53, 66, 62, 291, 71, 55, 70, 70, 67, 63, 546, 63, 61, 48, 55, 264, 61, 60, 59, 56, 55, 272, 58, 47, 57, 46, 55, 319, 65, 57, 56, 45, 56, 529, 65, 48, 61, 59, 58, 529, 71, 67, 65, 64, 311, 74, 68, 52, 52, 65, 299, 56, 67, 52, 66, 64, 360, 77, 69, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54, 246, 54, 67, 65, 50, 65, 336, 66, 50, 64, 64, 511, 75, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75, 67, 53, 51, 242, 55, 57, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63, 56, 55, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105074250, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081418, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083756, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085165, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089766, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090687, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092046, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098251, 1697105098304, 1697105098355, 1697105098597, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100069, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
1884,475,43,[],200,EleutherAI/gpt-neox-20b,64,1,2750.0,1.0,1,H100,1697105089889,1697105092639,120,89.0,20.0,"[4, 682, 64, 49, 60, 59, 58, 529, 71, 67, 65, 64, 312, 74, 67, 52, 52, 65, 299, 57]","[1697105089893, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639]"
1885,14,20,[],200,EleutherAI/gpt-neox-20b,64,1,2690.0,1.0,1,H100,1697105051740,1697105054430,120,90.0,20.0,"[5, 954, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68]","[1697105051745, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430]"
1886,746,38,[],200,EleutherAI/gpt-neox-20b,64,1,2611.0,1.0,1,H100,1697105083571,1697105086182,120,345.0,18.0,"[5, 837, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562, 73, 69, 68, 67]","[1697105083576, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905, 1697105085978, 1697105086047, 1697105086115, 1697105086182]"
1887,1,41,[],200,EleutherAI/gpt-neox-20b,64,1,5033.0,1.0,1,H100,1697105084414,1697105089447,120,47.0,43.0,"[13, 610, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 349, 69, 55, 52, 65, 62, 291, 71, 55, 71, 69, 67, 63, 546, 63, 61, 48, 55, 264, 61, 60, 59, 56, 55, 272, 58, 47, 57, 46, 55]","[1697105084427, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447]"
1888,848,35,[],200,EleutherAI/gpt-neox-20b,64,1,569.0,1.0,1,H100,1697105062599,1697105063168,120,47.0,1.0,"[4, 565]","[1697105062603, 1697105063168]"
1889,711,37,[],200,EleutherAI/gpt-neox-20b,64,1,1211.0,1.0,1,H100,1697105066346,1697105067557,120,457.0,4.0,"[12, 1068, 68, 63]","[1697105066358, 1697105067426, 1697105067494, 1697105067557]"
1890,80,35,[],200,EleutherAI/gpt-neox-20b,64,1,738.0,1.0,1,H100,1697105074982,1697105075720,120,13.0,1.0,"[13, 725]","[1697105074995, 1697105075720]"
1891,523,39,[],200,EleutherAI/gpt-neox-20b,64,1,1338.0,1.0,1,H100,1697105086183,1697105087521,120,345.0,13.0,"[4, 407, 69, 55, 52, 65, 63, 290, 71, 55, 71, 69, 67]","[1697105086187, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835, 1697105086898, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521]"
1892,480,38,[],200,EleutherAI/gpt-neox-20b,64,1,440.0,1.0,1,H100,1697105067558,1697105067998,120,26.0,1.0,"[4, 436]","[1697105067562, 1697105067998]"
1893,510,33,[],200,EleutherAI/gpt-neox-20b,64,1,478.0,1.0,1,H100,1697105086116,1697105086594,120,79.0,2.0,"[5, 473]","[1697105086121, 1697105086594]"
1894,456,38,[],200,EleutherAI/gpt-neox-20b,64,1,2414.0,1.0,1,H100,1697105086771,1697105089185,120,90.0,20.0,"[4, 413, 72, 54, 71, 69, 67, 63, 546, 63, 61, 48, 55, 265, 61, 59, 59, 56, 56, 272]","[1697105086775, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088622, 1697105088683, 1697105088742, 1697105088801, 1697105088857, 1697105088913, 1697105089185]"
1895,288,34,[],200,EleutherAI/gpt-neox-20b,64,1,2590.0,1.0,1,H100,1697105086595,1697105089185,120,93.0,20.0,"[17, 576, 72, 54, 71, 69, 67, 63, 546, 63, 61, 48, 55, 265, 60, 61, 58, 56, 56, 272]","[1697105086612, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089185]"
1896,141,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105067999,1697105070950,120,,,"[4, 695, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105068003, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
1897,234,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105060310,1697105063190,120,,,"[5, 323, 126, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060315, 1697105060638, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1898,687,36,[],200,EleutherAI/gpt-neox-20b,64,1,3489.0,1.0,1,H100,1697105078490,1697105081979,120,96.0,20.0,"[5, 968, 72, 68, 67, 52, 58, 390, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 425]","[1697105078495, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081979]"
1899,123,27,[],200,EleutherAI/gpt-neox-20b,64,1,548.0,1.0,1,H100,1697105062825,1697105063373,120,14.0,1.0,"[21, 527]","[1697105062846, 1697105063373]"
1900,838,40,[],200,EleutherAI/gpt-neox-20b,64,1,2826.0,1.0,1,H100,1697105070953,1697105073779,120,90.0,20.0,"[25, 1055, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 62, 49]","[1697105070978, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778]"
1901,822,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063374,1697105065210,120,,,"[4, 1174, 61, 59, 57, 45, 54]","[1697105063378, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1902,753,47,[],200,EleutherAI/gpt-neox-20b,64,1,2468.0,1.0,1,H100,1697105097029,1697105099497,120,83.0,20.0,"[5, 356, 74, 70, 68, 67, 440, 75, 68, 52, 51, 243, 55, 56, 69, 61, 394, 72, 126, 66]","[1697105097034, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098598, 1697105098653, 1697105098709, 1697105098778, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497]"
1903,322,31,[],200,EleutherAI/gpt-neox-20b,64,1,2574.0,1.0,1,H100,1697105086048,1697105088622,120,93.0,20.0,"[5, 541, 69, 55, 52, 65, 62, 291, 71, 55, 71, 69, 67, 63, 547, 62, 62, 47, 55, 265]","[1697105086053, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088193, 1697105088255, 1697105088302, 1697105088357, 1697105088622]"
1904,435,37,[],200,EleutherAI/gpt-neox-20b,64,1,3925.0,1.0,1,H100,1697105081980,1697105085905,120,563.0,27.0,"[12, 697, 77, 72, 71, 54, 53, 70, 413, 72, 67, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562]","[1697105081992, 1697105082689, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083086, 1697105083499, 1697105083571, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905]"
1905,409,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099498,1697105102062,120,,,"[16, 323, 63, 51, 62, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 48, 58]","[1697105099514, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101580, 1697105101638]"
1906,178,49,[],200,EleutherAI/gpt-neox-20b,64,1,1194.0,1.0,1,H100,1697105102074,1697105103268,120,11.0,1.0,"[129, 1065]","[1697105102203, 1697105103268]"
1907,739,50,[],200,EleutherAI/gpt-neox-20b,64,1,674.0,1.0,1,H100,1697105103269,1697105103943,120,216.0,1.0,"[8, 666]","[1697105103277, 1697105103943]"
1908,509,51,[],200,EleutherAI/gpt-neox-20b,64,1,1303.0,1.0,1,H100,1697105103944,1697105105247,120,286.0,3.0,"[4, 1218, 81]","[1697105103948, 1697105105166, 1697105105247]"
1909,168,52,[],200,EleutherAI/gpt-neox-20b,64,1,33201.0,1.0,1,H100,1697105105250,1697105138451,120,286.0,381.0,"[4, 889, 111, 58, 44, 350, 59, 56, 55, 44, 287, 60, 58, 56, 54, 53, 260, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 65, 64, 54, 572, 72, 121, 70, 53, 59, 459, 215, 128, 73, 54, 65, 62, 49, 243, 65, 50, 50, 50, 58, 423, 72, 57, 72, 71, 62, 450, 76, 71, 71, 70, 63, 62, 304, 66, 66, 62, 47, 61, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 71, 55, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 328, 75, 71, 65, 330, 73, 71, 72, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 78, 58, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 58, 59, 74, 57, 74, 72, 65, 476, 80, 60, 77, 58, 70, 54, 278, 80, 76, 76, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 76, 72, 69, 67, 52, 395, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 443, 67, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71, 65, 63, 49, 50, 49, 88, 64, 58, 43, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 31, 32, 31, 32, 38, 31, 32, 31, 31, 31, 38, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 32, 31, 31, 31, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 38, 31, 31, 30, 31, 31, 36, 30, 30, 32, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 30, 30, 30, 30, 31, 30, 30, 30, 31, 30, 30, 31, 30, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31, 31, 31, 30, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]","[1697105105254, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106765, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107267, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110070, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111374, 1697105111439, 1697105111501, 1697105111550, 1697105111793, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112561, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113420, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114231, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116020, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118362, 1697105118437, 1697105118508, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125212, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126364, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126922, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128401, 1697105128473, 1697105128542, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130494, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134492, 1697105134524, 1697105134555, 1697105134587, 1697105134625, 1697105134656, 1697105134688, 1697105134719, 1697105134750, 1697105134781, 1697105134819, 1697105134850, 1697105134881, 1697105134912, 1697105134943, 1697105134974, 1697105135005, 1697105135036, 1697105135067, 1697105135098, 1697105135128, 1697105135159, 1697105135190, 1697105135221, 1697105135252, 1697105135283, 1697105135315, 1697105135346, 1697105135377, 1697105135408, 1697105135439, 1697105135470, 1697105135502, 1697105135533, 1697105135564, 1697105135596, 1697105135627, 1697105135658, 1697105135690, 1697105135721, 1697105135752, 1697105135784, 1697105135815, 1697105135853, 1697105135884, 1697105135915, 1697105135945, 1697105135976, 1697105136007, 1697105136043, 1697105136073, 1697105136103, 1697105136135, 1697105136164, 1697105136194, 1697105136224, 1697105136254, 1697105136284, 1697105136314, 1697105136344, 1697105136374, 1697105136404, 1697105136434, 1697105136464, 1697105136494, 1697105136524, 1697105136554, 1697105136584, 1697105136614, 1697105136644, 1697105136674, 1697105136705, 1697105136735, 1697105136765, 1697105136795, 1697105136825, 1697105136856, 1697105136886, 1697105136916, 1697105136946, 1697105136977, 1697105137007, 1697105137037, 1697105137068, 1697105137098, 1697105137128, 1697105137159, 1697105137189, 1697105137220, 1697105137250, 1697105137281, 1697105137311, 1697105137342, 1697105137372, 1697105137403, 1697105137433, 1697105137464, 1697105137495, 1697105137525, 1697105137556, 1697105137586, 1697105137617, 1697105137648, 1697105137679, 1697105137709, 1697105137740, 1697105137771, 1697105137802, 1697105137833, 1697105137863, 1697105137894, 1697105137925, 1697105137956, 1697105137987, 1697105138018, 1697105138048, 1697105138079, 1697105138110, 1697105138141, 1697105138172, 1697105138203, 1697105138234, 1697105138265, 1697105138296, 1697105138327, 1697105138358, 1697105138389, 1697105138420, 1697105138451]"
1910,409,26,[],200,EleutherAI/gpt-neox-20b,64,1,3388.0,1.0,1,H100,1697105053543,1697105056931,120,109.0,30.0,"[4, 512, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 52]","[1697105053547, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056931]"
1911,815,28,[],200,EleutherAI/gpt-neox-20b,64,1,1360.0,1.0,1,H100,1697105063192,1697105064552,120,52.0,4.0,"[30, 852, 46, 432]","[1697105063222, 1697105064074, 1697105064120, 1697105064552]"
1912,492,41,[],200,EleutherAI/gpt-neox-20b,64,1,2295.0,1.0,1,H100,1697105073780,1697105076075,120,47.0,20.0,"[20, 504, 63, 50, 61, 60, 53, 476, 66, 65, 52, 64, 64, 59, 356, 60, 49, 60, 57, 56]","[1697105073800, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250, 1697105075314, 1697105075378, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
1913,835,38,[],200,EleutherAI/gpt-neox-20b,64,1,2469.0,1.0,1,H100,1697105076078,1697105078547,120,87.0,20.0,"[12, 741, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 57]","[1697105076090, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546]"
1914,563,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064553,1697105065210,120,,,[4],[1697105064557]
1915,806,20,[],200,EleutherAI/gpt-neox-20b,64,1,2293.0,1.0,1,H100,1697105052138,1697105054431,120,89.0,20.0,"[8, 553, 74, 57, 57, 56, 67, 395, 70, 66, 63, 62, 60, 333, 56, 55, 70, 69, 53, 68]","[1697105052146, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053405, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430]"
1916,882,34,[],200,EleutherAI/gpt-neox-20b,64,1,2003.0,1.0,1,H100,1697105084767,1697105086770,120,345.0,11.0,"[12, 1126, 72, 70, 68, 67, 63, 348, 70, 53, 54]","[1697105084779, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086770]"
1917,693,16,[],200,EleutherAI/gpt-neox-20b,64,1,530.0,1.0,1,H100,1697105056081,1697105056611,120,67.0,2.0,"[16, 513]","[1697105056097, 1697105056610]"
1918,586,22,[],200,EleutherAI/gpt-neox-20b,64,1,2367.0,1.0,1,H100,1697105055299,1697105057666,120,85.0,20.0,"[17, 693, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 348, 72, 53, 68, 66, 65]","[1697105055316, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057342, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666]"
1919,474,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105054431,1697105058839,120,,,"[13, 854, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 70, 68, 67, 64, 52, 63, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105054444, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1920,455,35,[],200,EleutherAI/gpt-neox-20b,64,1,3342.0,1.0,1,H100,1697105058907,1697105062249,120,91.0,20.0,"[98, 1143, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 55]","[1697105059005, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249]"
1921,462,17,[],200,EleutherAI/gpt-neox-20b,64,1,583.0,1.0,1,H100,1697105056612,1697105057195,120,52.0,1.0,"[4, 579]","[1697105056616, 1697105057195]"
1922,652,35,[],200,EleutherAI/gpt-neox-20b,64,1,341.0,1.0,1,H100,1697105086771,1697105087112,120,14.0,1.0,"[8, 333]","[1697105086779, 1697105087112]"
1923,118,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057196,1697105058840,120,,,"[5, 892, 65, 64, 63]","[1697105057201, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1924,312,36,[],200,EleutherAI/gpt-neox-20b,64,1,934.0,1.0,1,H100,1697105087113,1697105088047,120,23.0,1.0,"[12, 922]","[1697105087125, 1697105088047]"
1925,204,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062250,1697105063190,120,,,[12],[1697105062262]
1926,823,19,[],200,EleutherAI/gpt-neox-20b,64,1,3406.0,1.0,1,H100,1697105058843,1697105062249,120,90.0,20.0,"[73, 1232, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 54]","[1697105058916, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062248]"
1927,80,37,[],200,EleutherAI/gpt-neox-20b,64,1,509.0,1.0,1,H100,1697105088048,1697105088557,120,13.0,1.0,"[13, 496]","[1697105088061, 1697105088557]"
1928,789,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063193,1697105065210,120,,,"[90, 791, 46, 432, 61, 59, 57, 45, 54]","[1697105063283, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1929,670,38,[],200,EleutherAI/gpt-neox-20b,64,1,2249.0,1.0,1,H100,1697105088558,1697105090807,120,67.0,18.0,"[13, 614, 57, 47, 57, 46, 55, 319, 65, 57, 56, 45, 56, 530, 64, 49, 60, 59]","[1697105088571, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089766, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807]"
1930,552,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.06 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.21 GiB is free. Process 1645736 has 68.88 GiB memory in use. Of the allocated memory 52.23 GiB is allocated by PyTorch, and 15.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105035976,1697105037422,120,,,"[10, 704, 71, 68, 66, 58]","[1697105035986, 1697105036690, 1697105036761, 1697105036829, 1697105036895, 1697105036953]"
1931,613,18,[],200,EleutherAI/gpt-neox-20b,64,1,3351.0,1.0,1,H100,1697105058843,1697105062194,120,90.0,20.0,"[52, 572, 681, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70]","[1697105058895, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194]"
1932,205,11,[],200,EleutherAI/gpt-neox-20b,64,1,3109.0,1.0,1,H100,1697105037425,1697105040534,120,87.0,20.0,"[17, 545, 653, 58, 48, 55, 54, 560, 71, 68, 66, 63, 58, 57, 440, 68, 54, 63, 50, 60]","[1697105037442, 1697105037987, 1697105038640, 1697105038698, 1697105038746, 1697105038801, 1697105038855, 1697105039415, 1697105039486, 1697105039554, 1697105039620, 1697105039683, 1697105039741, 1697105039798, 1697105040238, 1697105040306, 1697105040360, 1697105040423, 1697105040473, 1697105040533]"
1933,273,19,[],200,EleutherAI/gpt-neox-20b,64,1,334.0,1.0,1,H100,1697105062196,1697105062530,120,19.0,1.0,"[32, 302]","[1697105062228, 1697105062530]"
1934,43,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062531,1697105063190,120,,,[9],[1697105062540]
1935,627,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063193,1697105065210,120,,,"[86, 795, 46, 432, 61, 59, 57, 45, 54]","[1697105063279, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
1936,246,44,[],200,EleutherAI/gpt-neox-20b,64,1,5716.0,1.0,1,H100,1697105092640,1697105098356,120,58.0,47.0,"[20, 588, 76, 69, 67, 52, 51, 609, 74, 57, 72, 71, 55, 54, 247, 53, 67, 65, 50, 65, 336, 66, 50, 64, 64, 511, 75, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75, 67, 53, 51]","[1697105092660, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093512, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098251, 1697105098304, 1697105098355]"
1937,565,38,[],200,EleutherAI/gpt-neox-20b,64,1,3486.0,1.0,1,H100,1697105065212,1697105068698,120,91.0,20.0,"[81, 1280, 68, 66, 65, 61, 59, 534, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55, 53, 289]","[1697105065293, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698]"
1938,243,28,[],200,EleutherAI/gpt-neox-20b,64,1,1365.0,1.0,1,H100,1697105058842,1697105060207,120,67.0,4.0,"[5, 620, 681, 59]","[1697105058847, 1697105059467, 1697105060148, 1697105060207]"
1939,832,29,[],200,EleutherAI/gpt-neox-20b,64,1,430.0,1.0,1,H100,1697105060208,1697105060638,120,15.0,1.0,"[9, 421]","[1697105060217, 1697105060638]"
1940,218,39,[],200,EleutherAI/gpt-neox-20b,64,1,1511.0,1.0,1,H100,1697105068699,1697105070210,120,109.0,7.0,"[5, 595, 61, 60, 59, 56, 675]","[1697105068704, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069535, 1697105070210]"
1941,923,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070211,1697105070950,120,,,[4],[1697105070215]
1942,601,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105060639,1697105063190,120,,,"[12, 1153, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060651, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1943,579,41,[],200,EleutherAI/gpt-neox-20b,64,1,1007.0,1.0,1,H100,1697105070953,1697105071960,120,19.0,1.0,"[138, 869]","[1697105071091, 1697105071960]"
1944,348,42,[],200,EleutherAI/gpt-neox-20b,64,1,2577.0,1.0,1,H100,1697105071961,1697105074538,120,91.0,20.0,"[13, 924, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60]","[1697105071974, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538]"
1945,262,31,[],200,EleutherAI/gpt-neox-20b,64,1,816.0,1.0,1,H100,1697105063193,1697105064009,120,39.0,1.0,"[105, 711]","[1697105063298, 1697105064009]"
1946,699,42,[],200,EleutherAI/gpt-neox-20b,64,1,979.0,1.0,1,H100,1697105089448,1697105090427,120,39.0,1.0,"[9, 970]","[1697105089457, 1697105090427]"
1947,448,43,[],200,EleutherAI/gpt-neox-20b,64,1,1855.0,1.0,1,H100,1697105090428,1697105092283,120,335.0,12.0,"[4, 962, 71, 67, 65, 64, 312, 74, 67, 52, 52, 65]","[1697105090432, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283]"
1948,100,44,[],200,EleutherAI/gpt-neox-20b,64,1,2271.0,1.0,1,H100,1697105092284,1697105094555,120,732.0,14.0,"[5, 959, 76, 69, 67, 51, 52, 609, 75, 56, 73, 70, 55, 54]","[1697105092289, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094247, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555]"
1949,808,45,[],200,EleutherAI/gpt-neox-20b,64,1,883.0,1.0,1,H100,1697105094556,1697105095439,120,286.0,2.0,"[21, 862]","[1697105094577, 1697105095439]"
1950,462,46,[],200,EleutherAI/gpt-neox-20b,64,1,584.0,1.0,1,H100,1697105095440,1697105096024,120,52.0,1.0,"[12, 572]","[1697105095452, 1697105096024]"
1951,37,32,[],200,EleutherAI/gpt-neox-20b,64,1,474.0,1.0,1,H100,1697105064011,1697105064485,120,20.0,1.0,"[36, 438]","[1697105064047, 1697105064485]"
1952,233,47,[],200,EleutherAI/gpt-neox-20b,64,1,718.0,1.0,1,H100,1697105096025,1697105096743,120,6.0,1.0,"[4, 714]","[1697105096029, 1697105096743]"
1953,620,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064486,1697105065210,120,,,[4],[1697105064490]
1954,824,48,[],200,EleutherAI/gpt-neox-20b,64,1,790.0,1.0,1,H100,1697105096744,1697105097534,120,58.0,4.0,"[13, 633, 74, 70]","[1697105096757, 1697105097390, 1697105097464, 1697105097534]"
1955,593,49,[],200,EleutherAI/gpt-neox-20b,64,1,1174.0,1.0,1,H100,1697105097535,1697105098709,120,335.0,9.0,"[4, 571, 74, 68, 52, 52, 242, 54, 57]","[1697105097539, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098652, 1697105098709]"
1956,252,50,[],200,EleutherAI/gpt-neox-20b,64,1,2870.0,1.0,1,H100,1697105098710,1697105101580,120,182.0,22.0,"[4, 519, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 57, 61, 59, 463, 67, 66, 59, 47]","[1697105098714, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100757, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579]"
1957,10,43,[],200,EleutherAI/gpt-neox-20b,64,1,1254.0,1.0,1,H100,1697105074539,1697105075793,120,563.0,9.0,"[16, 512, 67, 65, 51, 65, 62, 60, 356]","[1697105074555, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793]"
1958,367,34,[],200,EleutherAI/gpt-neox-20b,64,1,1560.0,1.0,1,H100,1697105065212,1697105066772,120,92.0,6.0,"[61, 573, 727, 68, 66, 65]","[1697105065273, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772]"
1959,137,35,[],200,EleutherAI/gpt-neox-20b,64,1,2706.0,1.0,1,H100,1697105066773,1697105069479,120,86.0,20.0,"[5, 648, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59]","[1697105066778, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479]"
1960,702,44,[],200,EleutherAI/gpt-neox-20b,64,1,2514.0,1.0,1,H100,1697105075794,1697105078308,120,89.0,20.0,"[5, 541, 45, 53, 52, 341, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56]","[1697105075799, 1697105076340, 1697105076385, 1697105076438, 1697105076490, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308]"
1961,537,40,[],200,EleutherAI/gpt-neox-20b,64,1,3044.0,1.0,1,H100,1697105079723,1697105082767,120,83.0,20.0,"[12, 435, 70, 71, 69, 66, 51, 636, 141, 76, 69, 68, 67, 425, 71, 71, 68, 65, 434, 79]","[1697105079735, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081350, 1697105081419, 1697105081487, 1697105081554, 1697105081979, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082767]"
1962,842,24,[],200,EleutherAI/gpt-neox-20b,64,1,2078.0,1.0,1,H100,1697105054171,1697105056249,120,161.0,16.0,"[5, 636, 70, 67, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62, 59, 48]","[1697105054176, 1697105054812, 1697105054882, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249]"
1963,766,26,[],200,EleutherAI/gpt-neox-20b,64,1,702.0,1.0,1,H100,1697105064486,1697105065188,120,11.0,1.0,"[16, 686]","[1697105064502, 1697105065188]"
1964,422,27,[],200,EleutherAI/gpt-neox-20b,64,1,101.0,1.0,1,H100,1697105065189,1697105065290,120,26.0,1.0,"[8, 93]","[1697105065197, 1697105065290]"
1965,566,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057667,1697105058840,120,,,[13],[1697105057680]
1966,217,23,[],200,EleutherAI/gpt-neox-20b,64,1,3351.0,1.0,1,H100,1697105058843,1697105062194,120,85.0,20.0,"[8, 616, 681, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 397, 75, 122, 124, 70]","[1697105058851, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061803, 1697105061878, 1697105062000, 1697105062124, 1697105062194]"
1967,611,25,[],200,EleutherAI/gpt-neox-20b,64,1,290.0,1.0,1,H100,1697105056250,1697105056540,120,14.0,1.0,"[8, 282]","[1697105056258, 1697105056540]"
1968,268,26,[],200,EleutherAI/gpt-neox-20b,64,1,654.0,1.0,1,H100,1697105056541,1697105057195,120,19.0,1.0,"[20, 634]","[1697105056561, 1697105057195]"
1969,36,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057196,1697105058840,120,,,"[13, 884, 65, 64, 63]","[1697105057209, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1970,187,28,[],200,EleutherAI/gpt-neox-20b,64,1,1543.0,1.0,1,H100,1697105065291,1697105066834,120,161.0,6.0,"[107, 1175, 68, 66, 65, 62]","[1697105065398, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834]"
1971,665,36,[],200,EleutherAI/gpt-neox-20b,64,1,2587.0,1.0,1,H100,1697105075721,1697105078308,120,90.0,20.0,"[4, 615, 45, 53, 51, 342, 64, 55, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56]","[1697105075725, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076895, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308]"
1972,626,28,[],200,EleutherAI/gpt-neox-20b,64,1,1136.0,1.0,1,H100,1697105058843,1697105059979,120,10.0,1.0,"[81, 1055]","[1697105058924, 1697105059979]"
1973,396,29,[],200,EleutherAI/gpt-neox-20b,64,1,2844.0,1.0,1,H100,1697105059980,1697105062824,120,89.0,20.0,"[12, 772, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105059992, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
1974,442,37,[],200,EleutherAI/gpt-neox-20b,64,1,3812.0,1.0,1,H100,1697105078309,1697105082121,120,39.0,22.0,"[5, 1149, 72, 68, 67, 52, 58, 390, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 424, 72, 71]","[1697105078314, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121]"
1975,924,24,[],200,EleutherAI/gpt-neox-20b,64,1,335.0,1.0,1,H100,1697105062195,1697105062530,120,9.0,1.0,"[21, 314]","[1697105062216, 1697105062530]"
1976,577,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062531,1697105063190,120,,,[5],[1697105062536]
1977,323,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063193,1697105065210,120,,,"[113, 768, 46, 432, 61, 60, 56, 45, 54]","[1697105063306, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064673, 1697105064729, 1697105064774, 1697105064828]"
1978,284,29,[],200,EleutherAI/gpt-neox-20b,64,1,3541.0,1.0,1,H100,1697105085906,1697105089447,120,90.0,31.0,"[8, 680, 69, 55, 52, 65, 62, 291, 72, 54, 70, 70, 67, 63, 547, 62, 62, 47, 55, 264, 61, 61, 58, 56, 55, 272, 58, 47, 57, 46, 55]","[1697105085914, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088193, 1697105088255, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447]"
1979,333,23,[],200,EleutherAI/gpt-neox-20b,64,1,2404.0,1.0,1,H100,1697105065212,1697105067616,120,563.0,11.0,"[157, 1204, 68, 66, 65, 61, 59, 534, 68, 63, 59]","[1697105065369, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616]"
1980,92,27,[],200,EleutherAI/gpt-neox-20b,64,1,3486.0,1.0,1,H100,1697105065212,1697105068698,120,85.0,20.0,"[182, 1179, 68, 66, 65, 62, 58, 534, 68, 62, 60, 55, 407, 60, 58, 59, 46, 55, 53, 289]","[1697105065394, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067426, 1697105067494, 1697105067556, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698]"
1981,649,26,[],200,EleutherAI/gpt-neox-20b,64,1,2827.0,1.0,1,H100,1697105070952,1697105073779,120,244.0,20.0,"[58, 1023, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 61, 51]","[1697105071010, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073779]"
1982,19,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064674,1697105065210,120,,,[8],[1697105064682]
1983,696,27,[],200,EleutherAI/gpt-neox-20b,64,1,3197.0,1.0,1,H100,1697105065212,1697105068409,120,83.0,20.0,"[78, 557, 726, 68, 66, 65, 61, 60, 533, 68, 63, 59, 55, 407, 60, 58, 58, 47, 55, 53]","[1697105065290, 1697105065847, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066893, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409]"
1984,923,24,[],200,EleutherAI/gpt-neox-20b,64,1,684.0,1.0,1,H100,1697105067617,1697105068301,120,140.0,6.0,"[5, 456, 60, 58, 59, 46]","[1697105067622, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301]"
1985,351,28,[],200,EleutherAI/gpt-neox-20b,64,1,1124.0,1.0,1,H100,1697105068410,1697105069534,120,216.0,6.0,"[13, 876, 61, 60, 59, 55]","[1697105068423, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534]"
1986,125,29,[],200,EleutherAI/gpt-neox-20b,64,1,1393.0,1.0,1,H100,1697105069536,1697105070929,120,13.0,1.0,"[4, 1389]","[1697105069540, 1697105070929]"
1987,694,25,[],200,EleutherAI/gpt-neox-20b,64,1,2113.0,1.0,1,H100,1697105068303,1697105070416,120,161.0,13.0,"[8, 387, 51, 51, 499, 61, 60, 59, 55, 676, 70, 69, 67]","[1697105068311, 1697105068698, 1697105068749, 1697105068800, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
1988,715,30,[],200,EleutherAI/gpt-neox-20b,64,1,222.0,1.0,1,H100,1697105070930,1697105071152,120,20.0,1.0,"[17, 205]","[1697105070947, 1697105071152]"
1989,239,42,[],200,EleutherAI/gpt-neox-20b,64,1,4092.0,1.0,1,H100,1697105076078,1697105080170,120,39.0,27.0,"[24, 729, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 58, 916, 71, 69, 67, 51, 59, 390]","[1697105076102, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078547, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170]"
1990,610,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105055529,1697105058839,120,,,"[5, 1076, 70, 68, 67, 64, 51, 64, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105055534, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
1991,550,21,[],200,EleutherAI/gpt-neox-20b,64,1,2455.0,1.0,1,H100,1697105049682,1697105052137,120,91.0,20.0,"[8, 673, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 69, 69, 68, 52]","[1697105049690, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051948, 1697105052017, 1697105052085, 1697105052137]"
1992,469,34,[],200,EleutherAI/gpt-neox-20b,64,1,1008.0,1.0,1,H100,1697105070952,1697105071960,120,17.0,1.0,"[42, 966]","[1697105070994, 1697105071960]"
1993,477,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062250,1697105063190,120,,,[4],[1697105062254]
1994,227,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063192,1697105065210,120,,,"[42, 840, 46, 432, 61, 60, 56, 45, 54]","[1697105063234, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064673, 1697105064729, 1697105064774, 1697105064828]"
1995,130,35,[],200,EleutherAI/gpt-neox-20b,64,1,697.0,1.0,1,H100,1697105071961,1697105072658,120,14.0,1.0,"[29, 668]","[1697105071990, 1697105072658]"
1996,809,22,[],200,EleutherAI/gpt-neox-20b,64,1,581.0,1.0,1,H100,1697105065212,1697105065793,120,16.0,1.0,"[37, 544]","[1697105065249, 1697105065793]"
1997,586,23,[],200,EleutherAI/gpt-neox-20b,64,1,2905.0,1.0,1,H100,1697105065794,1697105068699,120,85.0,20.0,"[13, 766, 68, 66, 65, 62, 59, 533, 68, 63, 59, 55, 407, 59, 59, 59, 46, 55, 53, 289]","[1697105065807, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066893, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068137, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698]"
1998,7,43,[],200,EleutherAI/gpt-neox-20b,64,1,2018.0,1.0,1,H100,1697105080171,1697105082189,120,345.0,11.0,"[5, 957, 142, 74, 70, 68, 67, 424, 72, 71, 68]","[1697105080176, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189]"
1999,727,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062720,1697105063191,120,,,[4],[1697105062724]
2000,497,32,[],200,EleutherAI/gpt-neox-20b,64,1,881.0,1.0,1,H100,1697105063193,1697105064074,120,67.0,2.0,"[102, 779]","[1697105063295, 1697105064074]"
2001,377,17,[],200,EleutherAI/gpt-neox-20b,64,1,1136.0,1.0,1,H100,1697105058843,1697105059979,120,13.0,1.0,"[129, 1007]","[1697105058972, 1697105059979]"
2002,744,37,[],200,EleutherAI/gpt-neox-20b,64,1,758.0,1.0,1,H100,1697105073780,1697105074538,120,161.0,6.0,"[28, 496, 63, 50, 61, 60]","[1697105073808, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538]"
2003,153,33,[],200,EleutherAI/gpt-neox-20b,64,1,598.0,1.0,1,H100,1697105064075,1697105064673,120,335.0,4.0,"[4, 473, 61, 59]","[1697105064079, 1697105064552, 1697105064613, 1697105064672]"
2004,836,40,[],200,EleutherAI/gpt-neox-20b,64,1,672.0,1.0,1,H100,1697105076077,1697105076749,120,11.0,1.0,"[5, 667]","[1697105076082, 1697105076749]"
2005,387,39,[],200,EleutherAI/gpt-neox-20b,64,1,3271.0,1.0,1,H100,1697105088194,1697105091465,120,39.0,27.0,"[5, 423, 60, 61, 58, 56, 56, 271, 58, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530, 64, 49, 60, 59, 58, 529, 71]","[1697105088199, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465]"
2006,214,22,[],200,EleutherAI/gpt-neox-20b,64,1,2292.0,1.0,1,H100,1697105052138,1697105054430,120,52.0,20.0,"[12, 549, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68]","[1697105052150, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430]"
2007,38,18,[],200,EleutherAI/gpt-neox-20b,64,1,2844.0,1.0,1,H100,1697105059980,1697105062824,120,88.0,20.0,"[17, 767, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 54, 350, 61, 60, 58, 47]","[1697105059997, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062248, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
2008,862,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064673,1697105065210,120,,,[5],[1697105064678]
2009,515,35,[],200,EleutherAI/gpt-neox-20b,64,1,1132.0,1.0,1,H100,1697105065213,1697105066345,120,11.0,1.0,"[165, 967]","[1697105065378, 1697105066345]"
2010,467,41,[],200,EleutherAI/gpt-neox-20b,64,1,2972.0,1.0,1,H100,1697105076750,1697105079722,120,93.0,20.0,"[4, 840, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 58, 916, 72, 68, 67, 52]","[1697105076754, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078547, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722]"
2011,306,41,[],200,EleutherAI/gpt-neox-20b,64,1,990.0,1.0,1,H100,1697105082767,1697105083757,120,140.0,6.0,"[5, 727, 72, 67, 52, 67]","[1697105082772, 1697105083499, 1697105083571, 1697105083638, 1697105083690, 1697105083757]"
2012,290,36,[],200,EleutherAI/gpt-neox-20b,64,1,928.0,1.0,1,H100,1697105066346,1697105067274,120,14.0,1.0,"[16, 912]","[1697105066362, 1697105067274]"
2013,872,37,[],200,EleutherAI/gpt-neox-20b,64,1,3141.0,1.0,1,H100,1697105067275,1697105070416,120,91.0,20.0,"[4, 799, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67]","[1697105067279, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
2014,865,29,[],200,EleutherAI/gpt-neox-20b,64,1,439.0,1.0,1,H100,1697105066835,1697105067274,120,9.0,1.0,"[4, 435]","[1697105066839, 1697105067274]"
2015,54,42,[],200,EleutherAI/gpt-neox-20b,64,1,2836.0,1.0,1,H100,1697105083758,1697105086594,120,87.0,20.0,"[4, 651, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 561, 74, 69, 68, 67, 63, 349]","[1697105083762, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085978, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594]"
2016,518,30,[],200,EleutherAI/gpt-neox-20b,64,1,723.0,1.0,1,H100,1697105067275,1697105067998,120,23.0,1.0,"[17, 706]","[1697105067292, 1697105067998]"
2017,293,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105067999,1697105070950,120,,,"[8, 691, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105068007, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
2018,51,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062825,1697105065209,120,,,"[13, 1236, 46, 432, 61, 59, 57, 45, 54]","[1697105062838, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2019,177,40,[],200,EleutherAI/gpt-neox-20b,64,1,524.0,1.0,1,H100,1697105087523,1697105088047,120,14.0,1.0,"[4, 520]","[1697105087527, 1697105088047]"
2020,737,19,[],200,EleutherAI/gpt-neox-20b,64,1,1249.0,1.0,1,H100,1697105062825,1697105064074,120,216.0,2.0,"[4, 544, 701]","[1697105062829, 1697105063373, 1697105064074]"
2021,395,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064075,1697105065209,120,,,"[12, 465, 62, 58, 57, 45, 54]","[1697105064087, 1697105064552, 1697105064614, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2022,877,41,[],200,EleutherAI/gpt-neox-20b,64,1,2527.0,1.0,1,H100,1697105088048,1697105090575,120,85.0,20.0,"[4, 570, 60, 61, 58, 56, 56, 271, 59, 46, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530]","[1697105088052, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089184, 1697105089243, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575]"
2023,512,36,[],200,EleutherAI/gpt-neox-20b,64,1,559.0,1.0,1,H100,1697105088558,1697105089117,120,11.0,1.0,"[21, 538]","[1697105088579, 1697105089117]"
2024,638,43,[],200,EleutherAI/gpt-neox-20b,64,1,2590.0,1.0,1,H100,1697105086595,1697105089185,120,88.0,20.0,"[17, 576, 72, 54, 71, 69, 67, 63, 546, 63, 61, 48, 55, 265, 60, 61, 58, 56, 56, 272]","[1697105086612, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089185]"
2025,165,21,[],200,EleutherAI/gpt-neox-20b,64,1,3197.0,1.0,1,H100,1697105065212,1697105068409,120,83.0,20.0,"[5, 629, 727, 68, 66, 65, 61, 59, 533, 69, 62, 60, 55, 406, 60, 59, 58, 47, 55, 53]","[1697105065217, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067425, 1697105067494, 1697105067556, 1697105067616, 1697105067671, 1697105068077, 1697105068137, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409]"
2026,163,37,[],200,EleutherAI/gpt-neox-20b,64,1,871.0,1.0,1,H100,1697105089118,1697105089989,120,67.0,6.0,"[5, 644, 64, 57, 56, 45]","[1697105089123, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989]"
2027,755,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068410,1697105070950,120,,,"[5, 884, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105068415, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
2028,538,42,[],200,EleutherAI/gpt-neox-20b,64,1,2749.0,1.0,1,H100,1697105090575,1697105093324,120,89.0,20.0,"[9, 810, 71, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 57, 66, 52, 66, 64, 361, 76]","[1697105090584, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324]"
2029,527,23,[],200,EleutherAI/gpt-neox-20b,64,1,6641.0,1.0,1,H100,1697105070953,1697105077594,120,732.0,50.0,"[142, 938, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60, 53, 476, 66, 65, 52, 64, 63, 59, 357, 60, 48, 61, 57, 56, 265, 45, 53, 51, 342, 63, 56, 55, 55, 534]","[1697105071095, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075901, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594]"
2030,408,44,[],200,EleutherAI/gpt-neox-20b,64,1,512.0,1.0,1,H100,1697105089186,1697105089698,120,16.0,1.0,"[20, 492]","[1697105089206, 1697105089698]"
2031,69,45,[],200,EleutherAI/gpt-neox-20b,64,1,2940.0,1.0,1,H100,1697105089699,1697105092639,120,85.0,20.0,"[13, 862, 65, 49, 60, 59, 58, 529, 71, 67, 65, 64, 312, 74, 67, 52, 52, 65, 299, 57]","[1697105089712, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639]"
2032,879,14,[],200,EleutherAI/gpt-neox-20b,64,1,7230.0,1.0,1,H100,1697105047582,1697105054812,120,39.0,55.0,"[4, 977, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76, 74, 72, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382]","[1697105047586, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051341, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812]"
2033,333,30,[],200,EleutherAI/gpt-neox-20b,64,1,2404.0,1.0,1,H100,1697105065212,1697105067616,120,563.0,11.0,"[162, 1199, 68, 66, 65, 62, 58, 533, 69, 63, 59]","[1697105065374, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067425, 1697105067494, 1697105067557, 1697105067616]"
2034,682,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068700,1697105070951,120,,,"[16, 583, 61, 60, 59, 56, 675, 70, 69, 67, 56]","[1697105068716, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069535, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
2035,11,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068302,1697105070950,120,,,"[5, 391, 51, 51, 499, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105068307, 1697105068698, 1697105068749, 1697105068800, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
2036,168,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105049684,1697105058841,120,,,"[18, 661, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52, 59, 503, 74, 57, 57, 56, 67, 394, 71, 67, 62, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 480, 72, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 72, 54, 68, 66, 65, 427, 65, 64, 63]","[1697105049702, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137, 1697105052196, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056008, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057413, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
2037,454,29,[],200,EleutherAI/gpt-neox-20b,64,1,1286.0,1.0,1,H100,1697105070953,1697105072239,120,182.0,6.0,"[141, 939, 57, 47, 46, 56]","[1697105071094, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239]"
2038,110,30,[],200,EleutherAI/gpt-neox-20b,64,1,800.0,1.0,1,H100,1697105072240,1697105073040,120,96.0,4.0,"[5, 653, 73, 68]","[1697105072245, 1697105072898, 1697105072971, 1697105073039]"
2039,817,31,[],200,EleutherAI/gpt-neox-20b,64,1,2397.0,1.0,1,H100,1697105073040,1697105075437,120,86.0,20.0,"[9, 502, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60, 53, 476, 67, 64, 52, 64, 63, 60]","[1697105073049, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075437]"
2040,366,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057667,1697105058839,120,,,[9],[1697105057676]
2041,712,39,[],200,EleutherAI/gpt-neox-20b,64,1,2827.0,1.0,1,H100,1697105070952,1697105073779,120,88.0,20.0,"[46, 1035, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 62, 50]","[1697105070998, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073779]"
2042,830,20,[],200,EleutherAI/gpt-neox-20b,64,1,1342.0,1.0,1,H100,1697105056816,1697105058158,120,140.0,9.0,"[4, 522, 72, 53, 68, 66, 65, 427, 65]","[1697105056820, 1697105057342, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158]"
2043,22,24,[],200,EleutherAI/gpt-neox-20b,64,1,1136.0,1.0,1,H100,1697105058843,1697105059979,120,16.0,1.0,"[85, 1051]","[1697105058928, 1697105059979]"
2044,720,25,[],200,EleutherAI/gpt-neox-20b,64,1,1239.0,1.0,1,H100,1697105059980,1697105061219,120,286.0,6.0,"[5, 779, 64, 169, 111, 111]","[1697105059985, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219]"
2045,468,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105061220,1697105063190,120,,,"[5, 385, 194, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105061225, 1697105061610, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
2046,600,21,[],200,EleutherAI/gpt-neox-20b,64,1,492.0,1.0,1,H100,1697105058159,1697105058651,120,23.0,1.0,"[4, 488]","[1697105058163, 1697105058651]"
2047,597,44,[],200,EleutherAI/gpt-neox-20b,64,1,337.0,1.0,1,H100,1697105082190,1697105082527,120,39.0,1.0,"[5, 332]","[1697105082195, 1697105082527]"
2048,367,45,[],200,EleutherAI/gpt-neox-20b,64,1,1230.0,1.0,1,H100,1697105082527,1697105083757,120,92.0,6.0,"[5, 967, 72, 67, 52, 67]","[1697105082532, 1697105083499, 1697105083571, 1697105083638, 1697105083690, 1697105083757]"
2049,264,22,[],200,EleutherAI/gpt-neox-20b,64,1,3542.0,1.0,1,H100,1697105058652,1697105062194,120,86.0,20.0,"[8, 807, 681, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 397, 75, 121, 125, 70]","[1697105058660, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061803, 1697105061878, 1697105061999, 1697105062124, 1697105062194]"
2050,22,46,[],200,EleutherAI/gpt-neox-20b,64,1,404.0,1.0,1,H100,1697105083758,1697105084162,120,16.0,1.0,"[4, 399]","[1697105083762, 1697105084161]"
2051,731,47,[],200,EleutherAI/gpt-neox-20b,64,1,3026.0,1.0,1,H100,1697105084162,1697105087188,120,89.0,20.0,"[5, 870, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 349, 69, 53, 54, 65, 62, 291]","[1697105084167, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086716, 1697105086770, 1697105086835, 1697105086897, 1697105087188]"
2052,21,51,[],200,EleutherAI/gpt-neox-20b,64,1,413.0,1.0,1,H100,1697105101581,1697105101994,120,15.0,1.0,"[4, 409]","[1697105101585, 1697105101994]"
2053,33,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062195,1697105063191,120,,,"[4, 399, 61, 60, 58, 47]","[1697105062199, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
2054,578,52,[],200,EleutherAI/gpt-neox-20b,64,1,175.0,1.0,1,H100,1697105101995,1697105102170,120,31.0,1.0,"[16, 159]","[1697105102011, 1697105102170]"
2055,622,24,[],200,EleutherAI/gpt-neox-20b,64,1,815.0,1.0,1,H100,1697105063194,1697105064009,120,20.0,1.0,"[105, 710]","[1697105063299, 1697105064009]"
2056,392,25,[],200,EleutherAI/gpt-neox-20b,64,1,474.0,1.0,1,H100,1697105064011,1697105064485,120,20.0,1.0,"[28, 446]","[1697105064039, 1697105064485]"
2057,350,53,[],200,EleutherAI/gpt-neox-20b,64,1,1097.0,1.0,1,H100,1697105102171,1697105103268,120,216.0,1.0,"[97, 1000]","[1697105102268, 1697105103268]"
2058,875,32,[],200,EleutherAI/gpt-neox-20b,64,1,4298.0,1.0,1,H100,1697105070952,1697105075250,120,31.0,31.0,"[50, 958, 73, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 62, 50, 43, 481, 64, 50, 61, 60, 53, 476, 66, 65, 52]","[1697105071002, 1697105071960, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073779, 1697105073822, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250]"
2059,47,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064486,1697105065210,120,,,[8],[1697105064494]
2060,723,27,[],200,EleutherAI/gpt-neox-20b,64,1,581.0,1.0,1,H100,1697105065212,1697105065793,120,14.0,1.0,"[25, 556]","[1697105065237, 1697105065793]"
2061,93,32,[],200,EleutherAI/gpt-neox-20b,64,1,2590.0,1.0,1,H100,1697105086595,1697105089185,120,88.0,20.0,"[5, 588, 71, 55, 71, 69, 67, 63, 547, 62, 61, 48, 55, 265, 60, 61, 58, 56, 55, 273]","[1697105086600, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088912, 1697105089185]"
2062,911,12,[],200,EleutherAI/gpt-neox-20b,64,1,1033.0,1.0,1,H100,1697105040534,1697105041567,120,335.0,11.0,"[5, 359, 45, 46, 56, 56, 273, 54, 43, 43, 53]","[1697105040539, 1697105040898, 1697105040943, 1697105040989, 1697105041045, 1697105041101, 1697105041374, 1697105041428, 1697105041471, 1697105041514, 1697105041567]"
2063,3,54,[],200,EleutherAI/gpt-neox-20b,64,1,3437.0,1.0,1,H100,1697105103269,1697105106706,120,89.0,20.0,"[16, 869, 72, 71, 69, 64, 62, 62, 612, 80, 76, 59, 71, 66, 53, 572, 111, 58, 44, 350]","[1697105103285, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104554, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706]"
2064,562,13,[],200,EleutherAI/gpt-neox-20b,64,1,6150.0,1.0,1,H100,1697105041568,1697105047718,120,67.0,39.0,"[5, 1274, 73, 55, 63, 60, 58, 520, 73, 64, 62, 59, 508, 71, 61, 58, 457, 74, 70, 55, 66, 65, 50, 450, 133, 76, 70, 53, 65, 50, 464, 75, 56, 55, 69, 58, 437, 69, 68]","[1697105041573, 1697105042847, 1697105042920, 1697105042975, 1697105043038, 1697105043098, 1697105043156, 1697105043676, 1697105043749, 1697105043813, 1697105043875, 1697105043934, 1697105044442, 1697105044513, 1697105044574, 1697105044632, 1697105045089, 1697105045163, 1697105045233, 1697105045288, 1697105045354, 1697105045419, 1697105045469, 1697105045919, 1697105046052, 1697105046128, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830, 1697105046905, 1697105046961, 1697105047016, 1697105047085, 1697105047143, 1697105047580, 1697105047649, 1697105047717]"
2065,493,28,[],200,EleutherAI/gpt-neox-20b,64,1,2904.0,1.0,1,H100,1697105065794,1697105068698,120,83.0,20.0,"[9, 770, 68, 66, 65, 62, 59, 533, 68, 62, 60, 55, 407, 60, 58, 59, 46, 55, 53, 289]","[1697105065803, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066893, 1697105067426, 1697105067494, 1697105067556, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698]"
2066,552,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062599,1697105063190,120,,,[4],[1697105062603]
2067,652,33,[],200,EleutherAI/gpt-neox-20b,64,1,469.0,1.0,1,H100,1697105075251,1697105075720,120,14.0,1.0,"[5, 464]","[1697105075256, 1697105075720]"
2068,728,31,[],200,EleutherAI/gpt-neox-20b,64,1,581.0,1.0,1,H100,1697105065212,1697105065793,120,20.0,1.0,"[21, 560]","[1697105065233, 1697105065793]"
2069,381,32,[],200,EleutherAI/gpt-neox-20b,64,1,780.0,1.0,1,H100,1697105065794,1697105066574,120,140.0,2.0,"[5, 774]","[1697105065799, 1697105066573]"
2070,161,33,[],200,EleutherAI/gpt-neox-20b,64,1,1504.0,1.0,1,H100,1697105066574,1697105068078,120,109.0,7.0,"[5, 847, 68, 63, 59, 56, 406]","[1697105066579, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067672, 1697105068078]"
2071,320,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063193,1697105065210,120,,,"[129, 752, 46, 432, 61, 59, 57, 45, 54]","[1697105063322, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2072,156,47,[],200,EleutherAI/gpt-neox-20b,64,1,2745.0,1.0,1,H100,1697105095439,1697105098184,120,86.0,20.0,"[5, 750, 74, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 441, 74]","[1697105095444, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184]"
2073,603,39,[],200,EleutherAI/gpt-neox-20b,64,1,1549.0,1.0,1,H100,1697105078548,1697105080097,120,9.0,1.0,"[16, 1533]","[1697105078564, 1697105080097]"
2074,880,25,[],200,EleutherAI/gpt-neox-20b,64,1,1361.0,1.0,1,H100,1697105065212,1697105066573,120,84.0,2.0,"[86, 1275]","[1697105065298, 1697105066573]"
2075,148,29,[],200,EleutherAI/gpt-neox-20b,64,1,520.0,1.0,1,H100,1697105068700,1697105069220,120,16.0,1.0,"[12, 507]","[1697105068712, 1697105069219]"
2076,234,40,[],200,EleutherAI/gpt-neox-20b,64,1,3723.0,1.0,1,H100,1697105080098,1697105083821,120,457.0,25.0,"[4, 711, 320, 141, 75, 70, 68, 67, 424, 72, 71, 68, 65, 434, 79, 71, 71, 54, 53, 69, 414, 71, 68, 52, 67, 64]","[1697105080102, 1697105080813, 1697105081133, 1697105081274, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082767, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821]"
2077,650,26,[],200,EleutherAI/gpt-neox-20b,64,1,699.0,1.0,1,H100,1697105066575,1697105067274,120,13.0,1.0,"[4, 695]","[1697105066579, 1697105067274]"
2078,309,27,[],200,EleutherAI/gpt-neox-20b,64,1,3141.0,1.0,1,H100,1697105067275,1697105070416,120,52.0,20.0,"[12, 791, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67]","[1697105067287, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
2079,170,30,[],200,EleutherAI/gpt-neox-20b,64,1,2926.0,1.0,1,H100,1697105065212,1697105068138,120,335.0,15.0,"[69, 565, 727, 68, 66, 65, 62, 58, 534, 68, 63, 59, 55, 407, 60]","[1697105065281, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138]"
2080,739,48,[],200,EleutherAI/gpt-neox-20b,64,1,340.0,1.0,1,H100,1697105098185,1697105098525,120,216.0,1.0,"[5, 335]","[1697105098190, 1697105098525]"
2081,199,32,[],200,EleutherAI/gpt-neox-20b,64,1,489.0,1.0,1,H100,1697105069480,1697105069969,120,13.0,1.0,"[8, 481]","[1697105069488, 1697105069969]"
2082,482,49,[],200,EleutherAI/gpt-neox-20b,64,1,2947.0,1.0,1,H100,1697105098526,1697105101473,120,91.0,20.0,"[9, 698, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 57, 61, 59, 463, 67, 66]","[1697105098535, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100757, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473]"
2083,847,30,[],200,EleutherAI/gpt-neox-20b,64,1,748.0,1.0,1,H100,1697105069221,1697105069969,120,10.0,1.0,"[16, 732]","[1697105069237, 1697105069969]"
2084,508,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069970,1697105070950,120,,,[8],[1697105069978]
2085,280,32,[],200,EleutherAI/gpt-neox-20b,64,1,2827.0,1.0,1,H100,1697105070952,1697105073779,120,91.0,20.0,"[62, 1019, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 61, 51]","[1697105071014, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073779]"
2086,896,33,[],200,EleutherAI/gpt-neox-20b,64,1,959.0,1.0,1,H100,1697105069970,1697105070929,120,15.0,1.0,"[12, 947]","[1697105069982, 1697105070929]"
2087,323,35,[],200,EleutherAI/gpt-neox-20b,64,1,2818.0,1.0,1,H100,1697105084766,1697105087584,120,84.0,20.0,"[17, 1122, 72, 70, 68, 67, 63, 348, 70, 53, 54, 65, 62, 291, 71, 55, 71, 69, 67, 63]","[1697105084783, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584]"
2088,550,34,[],200,EleutherAI/gpt-neox-20b,64,1,2848.0,1.0,1,H100,1697105070930,1697105073778,120,91.0,20.0,"[5, 1098, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 233, 53, 64, 61, 50]","[1697105070935, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073550, 1697105073603, 1697105073667, 1697105073728, 1697105073778]"
2089,78,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070417,1697105070951,120,,,[13],[1697105070430]
2090,135,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101474,1697105102062,120,,,[4],[1697105101478]
2091,841,51,[],200,EleutherAI/gpt-neox-20b,64,1,3172.0,1.0,1,H100,1697105102075,1697105105247,120,123.0,15.0,"[132, 1308, 66, 51, 50, 64, 407, 73, 71, 69, 64, 63, 60, 613, 81]","[1697105102207, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104493, 1697105104553, 1697105105166, 1697105105247]"
2092,582,21,[],200,EleutherAI/gpt-neox-20b,64,1,792.0,1.0,1,H100,1697105054432,1697105055224,120,19.0,1.0,"[16, 776]","[1697105054448, 1697105055224]"
2093,231,22,[],200,EleutherAI/gpt-neox-20b,64,1,692.0,1.0,1,H100,1697105055225,1697105055917,120,13.0,1.0,"[21, 671]","[1697105055246, 1697105055917]"
2094,907,36,[],200,EleutherAI/gpt-neox-20b,64,1,971.0,1.0,1,H100,1697105087586,1697105088557,120,10.0,1.0,"[12, 959]","[1697105087598, 1697105088557]"
2095,8,23,[],200,EleutherAI/gpt-neox-20b,64,1,762.0,1.0,1,H100,1697105055918,1697105056680,120,39.0,3.0,"[5, 687, 70]","[1697105055923, 1697105056610, 1697105056680]"
2096,592,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105056681,1697105058840,120,,,"[4, 656, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105056685, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
2097,684,37,[],200,EleutherAI/gpt-neox-20b,64,1,2836.0,1.0,1,H100,1697105088558,1697105091394,120,100.0,20.0,"[8, 619, 57, 47, 57, 46, 55, 319, 65, 57, 56, 45, 56, 530, 64, 49, 60, 59, 58, 529]","[1697105088566, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089766, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394]"
2098,668,29,[],200,EleutherAI/gpt-neox-20b,64,1,1286.0,1.0,1,H100,1697105070953,1697105072239,120,109.0,6.0,"[82, 998, 57, 47, 46, 56]","[1697105071035, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239]"
2099,440,30,[],200,EleutherAI/gpt-neox-20b,64,1,2298.0,1.0,1,H100,1697105072240,1697105074538,120,84.0,20.0,"[9, 649, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60]","[1697105072249, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538]"
2100,496,52,[],200,EleutherAI/gpt-neox-20b,64,1,1959.0,1.0,1,H100,1697105105248,1697105107207,120,335.0,11.0,"[4, 891, 111, 58, 44, 350, 58, 57, 55, 44, 287]","[1697105105252, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207]"
2101,527,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062250,1697105063190,120,,,[8],[1697105062258]
2102,171,14,[],200,EleutherAI/gpt-neox-20b,64,1,421.0,1.0,1,H100,1697105043099,1697105043520,120,6.0,1.0,"[5, 416]","[1697105043104, 1697105043520]"
2103,362,25,[],200,EleutherAI/gpt-neox-20b,64,1,1136.0,1.0,1,H100,1697105058843,1697105059979,120,14.0,1.0,"[69, 1067]","[1697105058912, 1697105059979]"
2104,23,26,[],200,EleutherAI/gpt-neox-20b,64,1,658.0,1.0,1,H100,1697105059980,1697105060638,120,26.0,1.0,"[21, 637]","[1697105060001, 1697105060638]"
2105,266,53,[],200,EleutherAI/gpt-neox-20b,64,1,476.0,1.0,1,H100,1697105107208,1697105107684,120,9.0,1.0,"[8, 468]","[1697105107216, 1697105107684]"
2106,298,22,[],200,EleutherAI/gpt-neox-20b,64,1,817.0,1.0,1,H100,1697105063192,1697105064009,120,17.0,1.0,"[6, 811]","[1697105063198, 1697105064009]"
2107,855,54,[],200,EleutherAI/gpt-neox-20b,64,1,3690.0,1.0,1,H100,1697105107685,1697105111375,120,83.0,20.0,"[13, 745, 57, 50, 48, 717, 65, 64, 54, 573, 71, 121, 74, 49, 60, 458, 215, 128, 73, 55]","[1697105107698, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375]"
2108,881,23,[],200,EleutherAI/gpt-neox-20b,64,1,764.0,1.0,1,H100,1697105064010,1697105064774,120,58.0,6.0,"[5, 537, 61, 59, 57, 45]","[1697105064015, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774]"
2109,719,27,[],200,EleutherAI/gpt-neox-20b,64,1,1555.0,1.0,1,H100,1697105060639,1697105062194,120,182.0,6.0,"[8, 1157, 74, 122, 124, 70]","[1697105060647, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194]"
2110,628,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064775,1697105065210,120,,,[9],[1697105064784]
2111,276,25,[],200,EleutherAI/gpt-neox-20b,64,1,2866.0,1.0,1,H100,1697105065212,1697105068078,120,732.0,13.0,"[166, 1195, 68, 66, 65, 62, 58, 534, 68, 63, 59, 55, 407]","[1697105065378, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078]"
2112,53,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068079,1697105070950,120,,,"[12, 607, 51, 51, 499, 61, 60, 59, 55, 676, 71, 68, 67, 56]","[1697105068091, 1697105068698, 1697105068749, 1697105068800, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070281, 1697105070349, 1697105070416, 1697105070472]"
2113,337,38,[],200,EleutherAI/gpt-neox-20b,64,1,495.0,1.0,1,H100,1697105091400,1697105091895,120,12.0,1.0,"[18, 477]","[1697105091418, 1697105091895]"
2114,263,42,[],200,EleutherAI/gpt-neox-20b,64,1,548.0,1.0,1,H100,1697105096195,1697105096743,120,15.0,1.0,"[16, 532]","[1697105096211, 1697105096743]"
2115,114,39,[],200,EleutherAI/gpt-neox-20b,64,1,2659.0,1.0,1,H100,1697105091896,1697105094555,120,88.0,20.0,"[5, 681, 56, 67, 52, 66, 64, 360, 78, 68, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54]","[1697105091901, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093325, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555]"
2116,40,43,[],200,EleutherAI/gpt-neox-20b,64,1,2753.0,1.0,1,H100,1697105096744,1697105099497,120,86.0,20.0,"[9, 637, 74, 70, 68, 67, 440, 75, 68, 52, 51, 243, 55, 56, 69, 61, 394, 72, 126, 66]","[1697105096753, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098598, 1697105098653, 1697105098709, 1697105098778, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497]"
2117,467,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062195,1697105063190,120,,,"[25, 378, 61, 60, 58, 47]","[1697105062220, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
2118,124,29,[],200,EleutherAI/gpt-neox-20b,64,1,882.0,1.0,1,H100,1697105063192,1697105064074,120,83.0,2.0,"[26, 856]","[1697105063218, 1697105064074]"
2119,825,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064075,1697105065209,120,,,"[8, 469, 63, 57, 57, 45, 54]","[1697105064083, 1697105064552, 1697105064615, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2120,636,27,[],200,EleutherAI/gpt-neox-20b,64,1,1007.0,1.0,1,H100,1697105070953,1697105071960,120,31.0,1.0,"[73, 934]","[1697105071026, 1697105071960]"
2121,485,31,[],200,EleutherAI/gpt-neox-20b,64,1,1429.0,1.0,1,H100,1697105065212,1697105066641,120,67.0,3.0,"[102, 1259, 68]","[1697105065314, 1697105066573, 1697105066641]"
2122,255,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105066643,1697105070950,120,,,"[4, 779, 68, 63, 59, 56, 406, 59, 59, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105066647, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067672, 1697105068078, 1697105068137, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
2123,395,38,[],200,EleutherAI/gpt-neox-20b,64,1,2356.0,1.0,1,H100,1697105074539,1697105076895,120,88.0,20.0,"[8, 520, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60, 57, 56, 265, 45, 53, 51, 342, 64]","[1697105074547, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076895]"
2124,172,39,[],200,EleutherAI/gpt-neox-20b,64,1,603.0,1.0,1,H100,1697105076896,1697105077499,120,19.0,1.0,"[20, 583]","[1697105076916, 1697105077499]"
2125,754,40,[],200,EleutherAI/gpt-neox-20b,64,1,989.0,1.0,1,H100,1697105077500,1697105078489,120,88.0,7.0,"[5, 692, 56, 55, 62, 60, 59]","[1697105077505, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489]"
2126,363,34,[],200,EleutherAI/gpt-neox-20b,64,1,3680.0,1.0,1,H100,1697105077595,1697105081275,120,286.0,22.0,"[8, 594, 56, 55, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 142]","[1697105077603, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275]"
2127,159,40,[],200,EleutherAI/gpt-neox-20b,64,1,430.0,1.0,1,H100,1697105091466,1697105091896,120,31.0,1.0,"[5, 424]","[1697105091471, 1697105091895]"
2128,741,41,[],200,EleutherAI/gpt-neox-20b,64,1,1429.0,1.0,1,H100,1697105091896,1697105093325,120,364.0,9.0,"[5, 681, 56, 67, 52, 66, 64, 360, 78]","[1697105091901, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093325]"
2129,216,30,[],200,EleutherAI/gpt-neox-20b,64,1,2288.0,1.0,1,H100,1697105076020,1697105078308,120,91.0,20.0,"[5, 315, 45, 53, 52, 341, 63, 56, 56, 54, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55]","[1697105076025, 1697105076340, 1697105076385, 1697105076438, 1697105076490, 1697105076831, 1697105076894, 1697105076950, 1697105077006, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308]"
2130,236,42,[],200,EleutherAI/gpt-neox-20b,64,1,374.0,1.0,1,H100,1697105079723,1697105080097,120,8.0,1.0,"[20, 354]","[1697105079743, 1697105080097]"
2131,826,43,[],200,EleutherAI/gpt-neox-20b,64,1,3401.0,1.0,1,H100,1697105080098,1697105083499,120,87.0,20.0,"[21, 1014, 142, 74, 70, 68, 67, 424, 72, 71, 68, 65, 434, 79, 71, 71, 54, 53, 69, 414]","[1697105080119, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082767, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499]"
2132,887,36,[],200,EleutherAI/gpt-neox-20b,64,1,7086.0,1.0,1,H100,1697105080498,1697105087584,120,244.0,50.0,"[8, 1472, 72, 71, 68, 65, 435, 78, 72, 70, 54, 53, 69, 414, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 348, 70, 53, 53, 66, 62, 291, 71, 55, 70, 70, 67, 63]","[1697105080506, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082689, 1697105082767, 1697105082839, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584]"
2133,518,42,[],200,EleutherAI/gpt-neox-20b,64,1,747.0,1.0,1,H100,1697105093326,1697105094073,120,23.0,1.0,"[20, 727]","[1697105093346, 1697105094073]"
2134,597,44,[],200,EleutherAI/gpt-neox-20b,64,1,662.0,1.0,1,H100,1697105083500,1697105084162,120,39.0,1.0,"[17, 644]","[1697105083517, 1697105084161]"
2135,709,55,[],200,EleutherAI/gpt-neox-20b,64,1,32016.0,1.0,1,H100,1697105106711,1697105138727,120,457.0,381.0,"[22, 474, 61, 58, 55, 54, 53, 260, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 66, 63, 54, 572, 72, 121, 70, 53, 59, 459, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 72, 57, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 47, 61, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 328, 75, 71, 65, 330, 73, 71, 72, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 78, 58, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 58, 59, 74, 57, 74, 72, 65, 476, 80, 60, 77, 58, 70, 54, 278, 80, 76, 76, 66, 66, 476, 60, 79, 79, 77, 72, 68, 284, 76, 72, 69, 67, 52, 395, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 443, 67, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71, 65, 63, 49, 50, 49, 88, 64, 57, 44, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 31, 32, 31, 32, 38, 31, 32, 31, 31, 31, 38, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 32, 31, 31, 31, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 38, 31, 31, 30, 31, 31, 36, 30, 30, 32, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 30, 30, 30, 30, 31, 30, 30, 30, 31, 30, 30, 31, 30, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 31, 30, 31, 31, 31, 30, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 37, 30, 29, 30, 30, 30, 30, 30, 30]","[1697105106733, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110070, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112561, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114231, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118362, 1697105118437, 1697105118508, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125212, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126364, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126922, 1697105126998, 1697105127064, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128401, 1697105128473, 1697105128542, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130494, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133520, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134492, 1697105134524, 1697105134555, 1697105134587, 1697105134625, 1697105134656, 1697105134688, 1697105134719, 1697105134750, 1697105134781, 1697105134819, 1697105134850, 1697105134881, 1697105134912, 1697105134943, 1697105134974, 1697105135005, 1697105135036, 1697105135067, 1697105135098, 1697105135128, 1697105135159, 1697105135190, 1697105135221, 1697105135252, 1697105135283, 1697105135315, 1697105135346, 1697105135377, 1697105135408, 1697105135439, 1697105135470, 1697105135502, 1697105135533, 1697105135564, 1697105135596, 1697105135627, 1697105135658, 1697105135690, 1697105135721, 1697105135752, 1697105135784, 1697105135815, 1697105135853, 1697105135884, 1697105135915, 1697105135945, 1697105135976, 1697105136007, 1697105136043, 1697105136073, 1697105136103, 1697105136135, 1697105136164, 1697105136194, 1697105136224, 1697105136254, 1697105136284, 1697105136314, 1697105136344, 1697105136374, 1697105136404, 1697105136434, 1697105136464, 1697105136494, 1697105136524, 1697105136554, 1697105136584, 1697105136614, 1697105136644, 1697105136674, 1697105136705, 1697105136735, 1697105136765, 1697105136795, 1697105136825, 1697105136856, 1697105136886, 1697105136916, 1697105136946, 1697105136977, 1697105137007, 1697105137037, 1697105137068, 1697105137098, 1697105137128, 1697105137159, 1697105137189, 1697105137220, 1697105137250, 1697105137281, 1697105137311, 1697105137342, 1697105137372, 1697105137403, 1697105137433, 1697105137464, 1697105137494, 1697105137525, 1697105137556, 1697105137586, 1697105137617, 1697105137648, 1697105137679, 1697105137709, 1697105137740, 1697105137771, 1697105137802, 1697105137833, 1697105137863, 1697105137894, 1697105137925, 1697105137956, 1697105137987, 1697105138018, 1697105138048, 1697105138079, 1697105138110, 1697105138141, 1697105138172, 1697105138203, 1697105138234, 1697105138265, 1697105138296, 1697105138327, 1697105138358, 1697105138389, 1697105138420, 1697105138451, 1697105138488, 1697105138518, 1697105138547, 1697105138577, 1697105138607, 1697105138637, 1697105138667, 1697105138697, 1697105138727]"
2136,892,31,[],200,EleutherAI/gpt-neox-20b,64,1,3670.0,1.0,1,H100,1697105078309,1697105081979,120,87.0,20.0,"[13, 1141, 72, 68, 67, 52, 58, 390, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 424]","[1697105078322, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978]"
2137,250,45,[],200,EleutherAI/gpt-neox-20b,64,1,798.0,1.0,1,H100,1697105084163,1697105084961,120,31.0,1.0,"[8, 790]","[1697105084171, 1697105084961]"
2138,169,43,[],200,EleutherAI/gpt-neox-20b,64,1,655.0,1.0,1,H100,1697105094074,1697105094729,120,10.0,1.0,"[13, 642]","[1697105094087, 1697105094729]"
2139,847,44,[],200,EleutherAI/gpt-neox-20b,64,1,635.0,1.0,1,H100,1697105094730,1697105095365,120,10.0,1.0,"[13, 622]","[1697105094743, 1697105095365]"
2140,28,46,[],200,EleutherAI/gpt-neox-20b,64,1,2622.0,1.0,1,H100,1697105084962,1697105087584,120,86.0,20.0,"[8, 935, 72, 70, 68, 67, 63, 349, 69, 53, 54, 65, 62, 291, 72, 54, 71, 69, 67, 63]","[1697105084970, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086716, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584]"
2141,482,29,[],200,EleutherAI/gpt-neox-20b,64,1,3197.0,1.0,1,H100,1697105065212,1697105068409,120,91.0,20.0,"[57, 577, 727, 68, 66, 65, 62, 58, 534, 68, 63, 59, 55, 407, 60, 58, 58, 47, 55, 53]","[1697105065269, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409]"
2142,496,45,[],200,EleutherAI/gpt-neox-20b,64,1,1595.0,1.0,1,H100,1697105095366,1697105096961,120,335.0,11.0,"[8, 820, 74, 56, 74, 55, 72, 54, 244, 70, 68]","[1697105095374, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961]"
2143,916,31,[],200,EleutherAI/gpt-neox-20b,64,1,381.0,1.0,1,H100,1697105067617,1697105067998,120,8.0,1.0,"[9, 372]","[1697105067626, 1697105067998]"
2144,482,31,[],200,EleutherAI/gpt-neox-20b,64,1,3385.0,1.0,1,H100,1697105071153,1697105074538,120,91.0,20.0,"[4, 1741, 73, 68, 63, 60, 48, 60, 47, 234, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60]","[1697105071157, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538]"
2145,694,32,[],200,EleutherAI/gpt-neox-20b,64,1,2418.0,1.0,1,H100,1697105067999,1697105070417,120,161.0,13.0,"[12, 687, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67]","[1697105068011, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
2146,267,46,[],200,EleutherAI/gpt-neox-20b,64,1,2535.0,1.0,1,H100,1697105096962,1697105099497,120,83.0,20.0,"[5, 423, 74, 70, 68, 67, 440, 75, 68, 52, 51, 243, 55, 56, 69, 61, 394, 72, 126, 66]","[1697105096967, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098598, 1697105098653, 1697105098709, 1697105098778, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497]"
2147,722,20,[],200,EleutherAI/gpt-neox-20b,64,1,199.0,1.0,1,H100,1697105051540,1697105051739,120,39.0,1.0,"[12, 187]","[1697105051552, 1697105051739]"
2148,611,47,[],200,EleutherAI/gpt-neox-20b,64,1,971.0,1.0,1,H100,1697105087586,1697105088557,120,14.0,1.0,"[20, 951]","[1697105087606, 1697105088557]"
2149,207,38,[],200,EleutherAI/gpt-neox-20b,64,1,609.0,1.0,1,H100,1697105085906,1697105086515,120,10.0,1.0,"[16, 593]","[1697105085922, 1697105086515]"
2150,389,48,[],200,EleutherAI/gpt-neox-20b,64,1,559.0,1.0,1,H100,1697105088558,1697105089117,120,8.0,1.0,"[17, 542]","[1697105088575, 1697105089117]"
2151,789,39,[],200,EleutherAI/gpt-neox-20b,64,1,6122.0,1.0,1,H100,1697105086516,1697105092638,120,6.0,50.0,"[10, 662, 71, 55, 71, 69, 67, 63, 547, 62, 61, 48, 55, 265, 60, 61, 58, 56, 55, 273, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 529, 65, 49, 60, 59, 58, 529, 71, 67, 65, 64, 311, 74, 68, 52, 52, 65, 299, 56]","[1697105086526, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088912, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092046, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638]"
2152,42,49,[],200,EleutherAI/gpt-neox-20b,64,1,580.0,1.0,1,H100,1697105089118,1697105089698,120,10.0,1.0,"[13, 567]","[1697105089131, 1697105089698]"
2153,743,50,[],200,EleutherAI/gpt-neox-20b,64,1,1108.0,1.0,1,H100,1697105089699,1697105090807,120,123.0,6.0,"[5, 870, 65, 49, 60, 59]","[1697105089704, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807]"
2154,347,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070418,1697105070950,120,,,[24],[1697105070442]
2155,490,51,[],200,EleutherAI/gpt-neox-20b,64,1,789.0,1.0,1,H100,1697105090808,1697105091597,120,11.0,5.0,"[5, 582, 70, 67, 65]","[1697105090813, 1697105091395, 1697105091465, 1697105091532, 1697105091597]"
2156,118,34,[],200,EleutherAI/gpt-neox-20b,64,1,2826.0,1.0,1,H100,1697105070952,1697105073778,120,85.0,20.0,"[22, 1059, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 62, 49]","[1697105070974, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778]"
2157,144,52,[],200,EleutherAI/gpt-neox-20b,64,1,2574.0,1.0,1,H100,1697105091598,1697105094172,120,96.0,20.0,"[5, 370, 74, 67, 52, 52, 65, 299, 56, 67, 52, 66, 64, 360, 78, 68, 67, 51, 52, 609]","[1697105091603, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093325, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172]"
2158,494,21,[],200,EleutherAI/gpt-neox-20b,64,1,1802.0,1.0,1,H100,1697105051740,1697105053542,120,6.0,10.0,"[9, 950, 74, 57, 57, 56, 67, 394, 71, 67]","[1697105051749, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053542]"
2159,119,27,[],200,EleutherAI/gpt-neox-20b,64,1,815.0,1.0,1,H100,1697105063194,1697105064009,120,31.0,1.0,"[100, 715]","[1697105063294, 1697105064009]"
2160,407,28,[],200,EleutherAI/gpt-neox-20b,64,1,697.0,1.0,1,H100,1697105071961,1697105072658,120,16.0,1.0,"[21, 676]","[1697105071982, 1697105072658]"
2161,65,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105056932,1697105058840,120,,,"[4, 406, 72, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105056936, 1697105057342, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
2162,183,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105052945,1697105058841,120,,,"[16, 443, 71, 67, 62, 62, 60, 333, 56, 55, 69, 70, 53, 68, 382, 70, 67, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 73, 53, 68, 66, 65, 427, 65, 64, 63]","[1697105052961, 1697105053404, 1697105053475, 1697105053542, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054239, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054882, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
2163,826,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064010,1697105065209,120,,,"[17, 525, 61, 59, 57, 45, 54]","[1697105064027, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2164,625,55,[],200,EleutherAI/gpt-neox-20b,64,1,418.0,1.0,1,H100,1697105111376,1697105111794,120,364.0,2.0,"[8, 410]","[1697105111384, 1697105111794]"
2165,68,29,[],200,EleutherAI/gpt-neox-20b,64,1,826.0,1.0,1,H100,1697105072659,1697105073485,120,12.0,1.0,"[9, 816]","[1697105072668, 1697105073484]"
2166,288,56,[],200,EleutherAI/gpt-neox-20b,64,1,2498.0,1.0,1,H100,1697105111795,1697105114293,120,93.0,20.0,"[16, 678, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 61]","[1697105111811, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114293]"
2167,400,22,[],200,EleutherAI/gpt-neox-20b,64,1,1680.0,1.0,1,H100,1697105065212,1697105066892,120,123.0,7.0,"[82, 1279, 68, 66, 65, 61, 59]","[1697105065294, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892]"
2168,767,30,[],200,EleutherAI/gpt-neox-20b,64,1,747.0,1.0,1,H100,1697105073486,1697105074233,120,11.0,1.0,"[16, 731]","[1697105073502, 1697105074233]"
2169,425,31,[],200,EleutherAI/gpt-neox-20b,64,1,2660.0,1.0,1,H100,1697105074234,1697105076894,120,88.0,20.0,"[8, 825, 67, 64, 52, 64, 63, 59, 357, 60, 49, 60, 57, 56, 265, 45, 53, 51, 342, 63]","[1697105074242, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894]"
2170,54,23,[],200,EleutherAI/gpt-neox-20b,64,1,3523.0,1.0,1,H100,1697105066893,1697105070416,120,87.0,20.0,"[5, 1180, 60, 58, 58, 47, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67]","[1697105066898, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
2171,924,35,[],200,EleutherAI/gpt-neox-20b,64,1,609.0,1.0,1,H100,1697105081276,1697105081885,120,9.0,1.0,"[4, 605]","[1697105081280, 1697105081885]"
2172,676,33,[],200,EleutherAI/gpt-neox-20b,64,1,512.0,1.0,1,H100,1697105089186,1697105089698,120,19.0,1.0,"[12, 500]","[1697105089198, 1697105089698]"
2173,450,34,[],200,EleutherAI/gpt-neox-20b,64,1,2940.0,1.0,1,H100,1697105089699,1697105092639,120,91.0,20.0,"[5, 870, 65, 49, 60, 59, 58, 530, 70, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 57]","[1697105089704, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639]"
2174,64,30,[],200,EleutherAI/gpt-neox-20b,64,1,3191.0,1.0,1,H100,1697105089448,1697105092639,120,89.0,20.0,"[5, 1121, 65, 49, 60, 59, 58, 530, 70, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 56]","[1697105089453, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638]"
2175,758,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070417,1697105070950,120,,,[9],[1697105070426]
2176,412,25,[],200,EleutherAI/gpt-neox-20b,64,1,2020.0,1.0,1,H100,1697105070952,1697105072972,120,244.0,9.0,"[66, 1015, 57, 47, 46, 56, 55, 604, 73]","[1697105071018, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971]"
2177,693,36,[],200,EleutherAI/gpt-neox-20b,64,1,802.0,1.0,1,H100,1697105081887,1697105082689,120,67.0,2.0,"[8, 793]","[1697105081895, 1697105082688]"
2178,195,32,[],200,EleutherAI/gpt-neox-20b,64,1,9009.0,1.0,1,H100,1697105076896,1697105085905,120,286.0,64.0,"[20, 678, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 72, 68, 67, 52, 58, 390, 70, 71, 69, 66, 51, 636, 141, 75, 70, 68, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 71, 68, 52, 66, 65, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 561]","[1697105076916, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083756, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904]"
2179,647,31,[],200,EleutherAI/gpt-neox-20b,64,1,2462.0,1.0,1,H100,1697105092640,1697105095102,120,83.0,20.0,"[8, 600, 76, 69, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54, 246, 54, 67, 65, 50, 65]","[1697105092648, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102]"
2180,104,35,[],200,EleutherAI/gpt-neox-20b,64,1,2462.0,1.0,1,H100,1697105092640,1697105095102,120,93.0,20.0,"[8, 600, 76, 69, 67, 51, 52, 609, 74, 57, 73, 70, 55, 54, 246, 54, 67, 65, 50, 65]","[1697105092648, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102]"
2181,45,20,[],200,EleutherAI/gpt-neox-20b,64,1,817.0,1.0,1,H100,1697105063192,1697105064009,120,19.0,1.0,"[10, 807]","[1697105063202, 1697105064009]"
2182,743,21,[],200,EleutherAI/gpt-neox-20b,64,1,764.0,1.0,1,H100,1697105064010,1697105064774,120,123.0,6.0,"[8, 534, 61, 59, 57, 45]","[1697105064018, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774]"
2183,352,37,[],200,EleutherAI/gpt-neox-20b,64,1,881.0,1.0,1,H100,1697105082690,1697105083571,120,11.0,3.0,"[4, 712, 93, 72]","[1697105082694, 1697105083406, 1697105083499, 1697105083571]"
2184,404,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064775,1697105065210,120,,,[5],[1697105064780]
2185,172,23,[],200,EleutherAI/gpt-neox-20b,64,1,581.0,1.0,1,H100,1697105065212,1697105065793,120,19.0,1.0,"[69, 512]","[1697105065281, 1697105065793]"
2186,754,24,[],200,EleutherAI/gpt-neox-20b,64,1,1099.0,1.0,1,H100,1697105065794,1697105066893,120,88.0,7.0,"[5, 774, 68, 66, 65, 62, 59]","[1697105065799, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066893]"
2187,122,38,[],200,EleutherAI/gpt-neox-20b,64,1,3023.0,1.0,1,H100,1697105083571,1697105086594,120,88.0,20.0,"[5, 837, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562, 73, 69, 68, 67, 63, 349]","[1697105083576, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905, 1697105085978, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594]"
2188,531,25,[],200,EleutherAI/gpt-neox-20b,64,1,3523.0,1.0,1,H100,1697105066893,1697105070416,120,52.0,20.0,"[5, 1180, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67]","[1697105066898, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
2189,806,36,[],200,EleutherAI/gpt-neox-20b,64,1,3081.0,1.0,1,H100,1697105095103,1697105098184,120,89.0,20.0,"[8, 1082, 75, 56, 74, 56, 71, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75]","[1697105095111, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096454, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184]"
2190,159,26,[],200,EleutherAI/gpt-neox-20b,64,1,511.0,1.0,1,H100,1697105072973,1697105073484,120,31.0,1.0,"[4, 507]","[1697105072977, 1697105073484]"
2191,860,27,[],200,EleutherAI/gpt-neox-20b,64,1,2590.0,1.0,1,H100,1697105073485,1697105076075,120,85.0,20.0,"[5, 813, 64, 50, 61, 60, 53, 476, 67, 64, 52, 64, 63, 60, 356, 60, 49, 60, 57, 56]","[1697105073490, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
2192,753,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068139,1697105070950,120,,,"[4, 555, 51, 51, 499, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105068143, 1697105068698, 1697105068749, 1697105068800, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
2193,96,31,[],200,EleutherAI/gpt-neox-20b,64,1,442.0,1.0,1,H100,1697105074539,1697105074981,120,31.0,1.0,"[20, 422]","[1697105074559, 1697105074981]"
2194,803,32,[],200,EleutherAI/gpt-neox-20b,64,1,737.0,1.0,1,H100,1697105074983,1697105075720,120,20.0,1.0,"[16, 721]","[1697105074999, 1697105075720]"
2195,155,22,[],200,EleutherAI/gpt-neox-20b,64,1,2537.0,1.0,1,H100,1697105053543,1697105056080,120,90.0,20.0,"[4, 512, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71]","[1697105053547, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080]"
2196,456,33,[],200,EleutherAI/gpt-neox-20b,64,1,2587.0,1.0,1,H100,1697105075721,1697105078308,120,90.0,20.0,"[12, 607, 45, 53, 52, 341, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56]","[1697105075733, 1697105076340, 1697105076385, 1697105076438, 1697105076490, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308]"
2197,753,15,[],200,EleutherAI/gpt-neox-20b,64,1,3309.0,1.0,1,H100,1697105043521,1697105046830,120,83.0,20.0,"[9, 912, 70, 62, 58, 458, 73, 71, 54, 66, 64, 51, 450, 134, 76, 69, 53, 65, 50, 464]","[1697105043530, 1697105044442, 1697105044512, 1697105044574, 1697105044632, 1697105045090, 1697105045163, 1697105045234, 1697105045288, 1697105045354, 1697105045418, 1697105045469, 1697105045919, 1697105046053, 1697105046129, 1697105046198, 1697105046251, 1697105046316, 1697105046366, 1697105046830]"
2198,385,48,[],200,EleutherAI/gpt-neox-20b,64,1,5449.0,1.0,1,H100,1697105087189,1697105092638,120,52.0,43.0,"[5, 937, 62, 62, 47, 55, 264, 61, 60, 59, 56, 55, 273, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 529, 65, 49, 60, 59, 58, 529, 71, 67, 65, 64, 311, 74, 68, 52, 52, 65, 299, 56]","[1697105087194, 1697105088131, 1697105088193, 1697105088255, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092046, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638]"
2199,699,40,[],200,EleutherAI/gpt-neox-20b,64,1,809.0,1.0,1,H100,1697105094556,1697105095365,120,39.0,1.0,"[5, 804]","[1697105094561, 1697105095365]"
2200,742,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068079,1697105070950,120,,,"[4, 615, 51, 51, 499, 61, 60, 59, 55, 676, 71, 68, 67, 56]","[1697105068083, 1697105068698, 1697105068749, 1697105068800, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070281, 1697105070349, 1697105070416, 1697105070472]"
2201,308,34,[],200,EleutherAI/gpt-neox-20b,64,1,2587.0,1.0,1,H100,1697105075721,1697105078308,120,87.0,20.0,"[8, 611, 45, 53, 51, 342, 64, 55, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56]","[1697105075729, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076895, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308]"
2202,865,38,[],200,EleutherAI/gpt-neox-20b,64,1,436.0,1.0,1,H100,1697105089991,1697105090427,120,9.0,1.0,"[4, 432]","[1697105089995, 1697105090427]"
2203,58,57,[],200,EleutherAI/gpt-neox-20b,64,1,1300.0,1.0,1,H100,1697105114294,1697105115594,120,15.0,1.0,"[20, 1279]","[1697105114314, 1697105115593]"
2204,734,58,[],200,EleutherAI/gpt-neox-20b,64,1,1461.0,1.0,1,H100,1697105115594,1697105117055,120,100.0,6.0,"[5, 1170, 80, 76, 74, 56]","[1697105115599, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055]"
2205,519,39,[],200,EleutherAI/gpt-neox-20b,64,1,6025.0,1.0,1,H100,1697105090428,1697105096453,120,58.0,47.0,"[5, 961, 71, 67, 65, 64, 312, 73, 68, 52, 52, 65, 299, 57, 66, 52, 66, 64, 361, 76, 69, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54, 246, 54, 67, 65, 50, 65, 336, 66, 50, 64, 64, 511, 75, 56, 74, 55]","[1697105090433, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091973, 1697105092046, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453]"
2206,81,35,[],200,EleutherAI/gpt-neox-20b,64,1,2188.0,1.0,1,H100,1697105078309,1697105080497,120,732.0,13.0,"[9, 1145, 72, 68, 67, 52, 58, 390, 70, 71, 69, 66, 51]","[1697105078318, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497]"
2207,389,59,[],200,EleutherAI/gpt-neox-20b,64,1,435.0,1.0,1,H100,1697105117056,1697105117491,120,8.0,1.0,"[16, 419]","[1697105117072, 1697105117491]"
2208,333,14,[],200,EleutherAI/gpt-neox-20b,64,1,1783.0,1.0,1,H100,1697105047718,1697105049501,120,563.0,11.0,"[5, 840, 130, 73, 54, 55, 55, 70, 353, 76, 72]","[1697105047723, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501]"
2209,671,36,[],200,EleutherAI/gpt-neox-20b,64,1,1387.0,1.0,1,H100,1697105080498,1697105081885,120,12.0,1.0,"[4, 1383]","[1697105080502, 1697105081885]"
2210,159,60,[],200,EleutherAI/gpt-neox-20b,64,1,793.0,1.0,1,H100,1697105117492,1697105118285,120,31.0,1.0,"[12, 781]","[1697105117504, 1697105118285]"
2211,748,61,[],200,EleutherAI/gpt-neox-20b,64,1,1712.0,1.0,1,H100,1697105118286,1697105119998,120,182.0,14.0,"[4, 613, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55]","[1697105118290, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998]"
2212,519,35,[],200,EleutherAI/gpt-neox-20b,64,1,5996.0,1.0,1,H100,1697105070954,1697105076950,120,58.0,47.0,"[129, 950, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 233, 53, 64, 62, 49, 44, 482, 63, 50, 61, 60, 53, 476, 66, 65, 52, 64, 63, 59, 357, 60, 48, 60, 58, 56, 265, 45, 53, 51, 342, 63, 56]","[1697105071083, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073550, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075901, 1697105075961, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894, 1697105076950]"
2213,462,37,[],200,EleutherAI/gpt-neox-20b,64,1,340.0,1.0,1,H100,1697105098185,1697105098525,120,52.0,1.0,"[9, 331]","[1697105098194, 1697105098525]"
2214,231,38,[],200,EleutherAI/gpt-neox-20b,64,1,620.0,1.0,1,H100,1697105098526,1697105099146,120,13.0,1.0,"[21, 599]","[1697105098547, 1697105099146]"
2215,908,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099148,1697105102062,120,,,"[12, 677, 63, 51, 62, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105099160, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2216,561,40,[],200,EleutherAI/gpt-neox-20b,64,1,3453.0,1.0,1,H100,1697105102065,1697105105518,120,87.0,20.0,"[62, 681, 707, 67, 50, 50, 64, 408, 72, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66]","[1697105102127, 1697105102808, 1697105103515, 1697105103582, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518]"
2217,326,35,[],200,EleutherAI/gpt-neox-20b,64,1,1536.0,1.0,1,H100,1697105073779,1697105075315,120,345.0,12.0,"[5, 519, 64, 50, 61, 60, 53, 476, 67, 65, 51, 65]","[1697105073784, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315]"
2218,906,36,[],200,EleutherAI/gpt-neox-20b,64,1,2451.0,1.0,1,H100,1697105075316,1697105077767,120,86.0,20.0,"[4, 473, 61, 48, 60, 57, 56, 265, 45, 53, 51, 342, 64, 55, 55, 55, 534, 63, 49, 61]","[1697105075320, 1697105075793, 1697105075854, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076895, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767]"
2219,853,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105056081,1697105058839,120,,,"[4, 525, 70, 68, 67, 64, 52, 63, 347, 72, 54, 68, 66, 65, 427, 65, 64, 63]","[1697105056085, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341, 1697105057413, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
2220,862,33,[],200,EleutherAI/gpt-neox-20b,64,1,524.0,1.0,1,H100,1697105073780,1697105074304,120,216.0,2.0,"[41, 483]","[1697105073821, 1697105074304]"
2221,638,34,[],200,EleutherAI/gpt-neox-20b,64,1,2590.0,1.0,1,H100,1697105074305,1697105076895,120,88.0,20.0,"[4, 758, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60, 57, 56, 265, 45, 53, 51, 342, 63]","[1697105074309, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894]"
2222,848,53,[],200,EleutherAI/gpt-neox-20b,64,1,556.0,1.0,1,H100,1697105094173,1697105094729,120,47.0,1.0,"[13, 543]","[1697105094186, 1697105094729]"
2223,497,54,[],200,EleutherAI/gpt-neox-20b,64,1,709.0,1.0,1,H100,1697105094730,1697105095439,120,67.0,2.0,"[5, 704]","[1697105094735, 1697105095439]"
2224,684,37,[],200,EleutherAI/gpt-neox-20b,64,1,2729.0,1.0,1,H100,1697105077768,1697105080497,120,100.0,20.0,"[5, 424, 56, 55, 62, 60, 59, 57, 917, 72, 68, 67, 51, 59, 390, 70, 71, 69, 66, 51]","[1697105077773, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497]"
2225,241,24,[],200,EleutherAI/gpt-neox-20b,64,1,520.0,1.0,1,H100,1697105068700,1697105069220,120,19.0,1.0,"[16, 504]","[1697105068716, 1697105069220]"
2226,11,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069221,1697105070950,120,,,"[12, 977, 70, 69, 67, 56]","[1697105069233, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
2227,275,55,[],200,EleutherAI/gpt-neox-20b,64,1,886.0,1.0,1,H100,1697105095439,1697105096325,120,161.0,4.0,"[5, 750, 74, 57]","[1697105095444, 1697105096194, 1697105096268, 1697105096325]"
2228,859,56,[],200,EleutherAI/gpt-neox-20b,64,1,417.0,1.0,1,H100,1697105096326,1697105096743,120,23.0,1.0,"[4, 413]","[1697105096330, 1697105096743]"
2229,629,57,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105096745,1697105102062,120,,,"[16, 629, 74, 70, 68, 67, 440, 75, 68, 52, 51, 243, 55, 56, 69, 61, 394, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105096761, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098598, 1697105098653, 1697105098709, 1697105098778, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2230,795,23,[],200,EleutherAI/gpt-neox-20b,64,1,1136.0,1.0,1,H100,1697105058843,1697105059979,120,12.0,1.0,"[158, 978]","[1697105059001, 1697105059979]"
2231,600,26,[],200,EleutherAI/gpt-neox-20b,64,1,1007.0,1.0,1,H100,1697105070953,1697105071960,120,23.0,1.0,"[41, 966]","[1697105070994, 1697105071960]"
2232,370,27,[],200,EleutherAI/gpt-neox-20b,64,1,697.0,1.0,1,H100,1697105071961,1697105072658,120,31.0,1.0,"[20, 677]","[1697105071981, 1697105072658]"
2233,439,39,[],200,EleutherAI/gpt-neox-20b,64,1,724.0,1.0,1,H100,1697105090808,1697105091532,120,13.0,4.0,"[9, 578, 70, 67]","[1697105090817, 1697105091395, 1697105091465, 1697105091532]"
2234,725,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069480,1697105070950,120,,,"[4, 726, 71, 68, 67, 56]","[1697105069484, 1697105070210, 1697105070281, 1697105070349, 1697105070416, 1697105070472]"
2235,451,45,[],200,EleutherAI/gpt-neox-20b,64,1,883.0,1.0,1,H100,1697105078309,1697105079192,120,286.0,1.0,"[9, 874]","[1697105078318, 1697105079192]"
2236,25,28,[],200,EleutherAI/gpt-neox-20b,64,1,825.0,1.0,1,H100,1697105072659,1697105073484,120,12.0,1.0,"[5, 820]","[1697105072664, 1697105073484]"
2237,572,24,[],200,EleutherAI/gpt-neox-20b,64,1,658.0,1.0,1,H100,1697105059980,1697105060638,120,16.0,1.0,"[17, 641]","[1697105059997, 1697105060638]"
2238,769,28,[],200,EleutherAI/gpt-neox-20b,64,1,3351.0,1.0,1,H100,1697105058843,1697105062194,120,47.0,20.0,"[16, 608, 681, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 62, 397, 74, 121, 125, 70]","[1697105058859, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105061999, 1697105062124, 1697105062194]"
2239,225,25,[],200,EleutherAI/gpt-neox-20b,64,1,971.0,1.0,1,H100,1697105060639,1697105061610,120,23.0,1.0,"[16, 955]","[1697105060655, 1697105061610]"
2240,104,46,[],200,EleutherAI/gpt-neox-20b,64,1,3574.0,1.0,1,H100,1697105079193,1697105082767,120,93.0,20.0,"[5, 972, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 425, 71, 71, 68, 65, 434, 78]","[1697105079198, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081979, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766]"
2241,724,29,[],200,EleutherAI/gpt-neox-20b,64,1,747.0,1.0,1,H100,1697105073486,1697105074233,120,11.0,1.0,"[12, 735]","[1697105073498, 1697105074233]"
2242,494,37,[],200,EleutherAI/gpt-neox-20b,64,1,2086.0,1.0,1,H100,1697105070953,1697105073039,120,6.0,10.0,"[17, 1063, 57, 47, 46, 56, 55, 603, 74, 68]","[1697105070970, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039]"
2243,95,40,[],200,EleutherAI/gpt-neox-20b,64,1,362.0,1.0,1,H100,1697105091534,1697105091896,120,12.0,1.0,"[8, 353]","[1697105091542, 1697105091895]"
2244,472,30,[],200,EleutherAI/gpt-neox-20b,64,1,2660.0,1.0,1,H100,1697105074234,1697105076894,120,85.0,20.0,"[8, 825, 67, 64, 52, 64, 63, 59, 357, 60, 49, 60, 57, 56, 265, 45, 53, 51, 342, 63]","[1697105074242, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894]"
2245,1,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105061611,1697105063190,120,,,"[9, 978, 61, 60, 58, 47]","[1697105061620, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
2246,108,39,[],200,EleutherAI/gpt-neox-20b,64,1,581.0,1.0,1,H100,1697105089186,1697105089767,120,182.0,2.0,"[12, 569]","[1697105089198, 1697105089767]"
2247,95,38,[],200,EleutherAI/gpt-neox-20b,64,1,404.0,1.0,1,H100,1697105082123,1697105082527,120,12.0,1.0,"[8, 396]","[1697105082131, 1697105082527]"
2248,771,39,[],200,EleutherAI/gpt-neox-20b,64,1,3377.0,1.0,1,H100,1697105082528,1697105085905,120,47.0,20.0,"[8, 963, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562]","[1697105082536, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905]"
2249,866,35,[],200,EleutherAI/gpt-neox-20b,64,1,2861.0,1.0,1,H100,1697105089186,1697105092047,120,93.0,20.0,"[16, 565, 64, 57, 56, 45, 56, 529, 65, 49, 60, 59, 58, 530, 70, 67, 65, 64, 311, 75]","[1697105089202, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047]"
2250,540,40,[],200,EleutherAI/gpt-neox-20b,64,1,864.0,1.0,1,H100,1697105085906,1697105086770,120,140.0,5.0,"[12, 676, 69, 55, 52]","[1697105085918, 1697105086594, 1697105086663, 1697105086718, 1697105086770]"
2251,715,39,[],200,EleutherAI/gpt-neox-20b,64,1,517.0,1.0,1,H100,1697105086595,1697105087112,120,20.0,1.0,"[13, 504]","[1697105086608, 1697105087112]"
2252,517,62,[],200,EleutherAI/gpt-neox-20b,64,1,1015.0,1.0,1,H100,1697105120000,1697105121015,120,15.0,1.0,"[4, 1011]","[1697105120004, 1697105121015]"
2253,487,40,[],200,EleutherAI/gpt-neox-20b,64,1,2279.0,1.0,1,H100,1697105087113,1697105089392,120,123.0,17.0,"[4, 1014, 62, 61, 48, 55, 264, 61, 60, 59, 56, 55, 273, 57, 47, 57, 46]","[1697105087117, 1697105088131, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392]"
2254,174,63,[],200,EleutherAI/gpt-neox-20b,64,1,3081.0,1.0,1,H100,1697105121016,1697105124097,120,87.0,20.0,"[9, 903, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58]","[1697105121025, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097]"
2255,200,41,[],200,EleutherAI/gpt-neox-20b,64,1,1361.0,1.0,1,H100,1697105086770,1697105088131,120,6.0,9.0,"[5, 413, 72, 54, 71, 69, 67, 63, 547]","[1697105086775, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088131]"
2256,143,32,[],200,EleutherAI/gpt-neox-20b,64,1,1423.0,1.0,1,H100,1697105074539,1697105075962,120,6.0,12.0,"[4, 524, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60]","[1697105074543, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962]"
2257,300,50,[],200,EleutherAI/gpt-neox-20b,64,1,874.0,1.0,1,H100,1697105084766,1697105085640,120,9.0,1.0,"[9, 865]","[1697105084775, 1697105085640]"
2258,603,21,[],200,EleutherAI/gpt-neox-20b,64,1,793.0,1.0,1,H100,1697105054431,1697105055224,120,9.0,1.0,"[9, 784]","[1697105054440, 1697105055224]"
2259,336,41,[],200,EleutherAI/gpt-neox-20b,64,1,1245.0,1.0,1,H100,1697105105519,1697105106764,120,58.0,7.0,"[12, 612, 111, 58, 44, 350, 58]","[1697105105531, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764]"
2260,622,44,[],200,EleutherAI/gpt-neox-20b,64,1,272.0,1.0,1,H100,1697105099498,1697105099770,120,20.0,1.0,"[12, 260]","[1697105099510, 1697105099770]"
2261,394,45,[],200,EleutherAI/gpt-neox-20b,64,1,730.0,1.0,1,H100,1697105099771,1697105100501,120,11.0,1.0,"[16, 714]","[1697105099787, 1697105100501]"
2262,55,46,[],200,EleutherAI/gpt-neox-20b,64,1,763.0,1.0,1,H100,1697105100502,1697105101265,120,12.0,1.0,"[13, 750]","[1697105100515, 1697105101265]"
2263,374,22,[],200,EleutherAI/gpt-neox-20b,64,1,2441.0,1.0,1,H100,1697105055225,1697105057666,120,85.0,20.0,"[5, 779, 71, 62, 59, 48, 57, 305, 69, 68, 67, 64, 52, 63, 348, 72, 53, 68, 66, 65]","[1697105055230, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056611, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057342, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666]"
2264,527,41,[],200,EleutherAI/gpt-neox-20b,64,1,7415.0,1.0,1,H100,1697105078490,1697105085905,120,732.0,50.0,"[5, 968, 72, 68, 67, 52, 58, 390, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 425, 71, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 561]","[1697105078495, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081979, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904]"
2265,650,38,[],200,EleutherAI/gpt-neox-20b,64,1,512.0,1.0,1,H100,1697105070417,1697105070929,120,13.0,1.0,"[21, 491]","[1697105070438, 1697105070929]"
2266,305,39,[],200,EleutherAI/gpt-neox-20b,64,1,2848.0,1.0,1,H100,1697105070930,1697105073778,120,86.0,20.0,"[9, 1094, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 233, 53, 64, 61, 50]","[1697105070939, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073550, 1697105073603, 1697105073667, 1697105073728, 1697105073778]"
2267,754,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101266,1697105102062,120,,,[13],[1697105101279]
2268,845,33,[],200,EleutherAI/gpt-neox-20b,64,1,6642.0,1.0,1,H100,1697105070952,1697105077594,120,244.0,50.0,"[26, 1055, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 60, 47, 234, 52, 64, 62, 49, 44, 481, 64, 50, 61, 60, 53, 476, 66, 65, 52, 64, 63, 59, 357, 60, 49, 59, 58, 55, 266, 45, 53, 51, 342, 63, 56, 55, 55, 534]","[1697105070978, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075902, 1697105075961, 1697105076019, 1697105076074, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594]"
2269,409,48,[],200,EleutherAI/gpt-neox-20b,64,1,4855.0,1.0,1,H100,1697105102065,1697105106920,120,109.0,30.0,"[11, 731, 708, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66, 52, 573, 111, 58, 44, 350, 58, 57, 55, 44]","[1697105102076, 1697105102807, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105570, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920]"
2270,919,42,[],200,EleutherAI/gpt-neox-20b,64,1,375.0,1.0,1,H100,1697105106765,1697105107140,120,14.0,1.0,"[5, 370]","[1697105106770, 1697105107140]"
2271,35,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057667,1697105058839,120,,,[5],[1697105057672]
2272,699,43,[],200,EleutherAI/gpt-neox-20b,64,1,543.0,1.0,1,H100,1697105107141,1697105107684,120,39.0,1.0,"[12, 531]","[1697105107153, 1697105107684]"
2273,438,37,[],200,EleutherAI/gpt-neox-20b,64,1,640.0,1.0,1,H100,1697105081887,1697105082527,120,9.0,1.0,"[12, 628]","[1697105081899, 1697105082527]"
2274,70,38,[],200,EleutherAI/gpt-neox-20b,64,1,878.0,1.0,1,H100,1697105082528,1697105083406,120,39.0,1.0,"[8, 870]","[1697105082536, 1697105083406]"
2275,768,39,[],200,EleutherAI/gpt-neox-20b,64,1,1286.0,1.0,1,H100,1697105083407,1697105084693,120,47.0,6.0,"[4, 1002, 77, 56, 74, 73]","[1697105083411, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693]"
2276,127,31,[],200,EleutherAI/gpt-neox-20b,64,1,872.0,1.0,1,H100,1697105076895,1697105077767,120,100.0,5.0,"[5, 694, 63, 49, 61]","[1697105076900, 1697105077594, 1697105077657, 1697105077706, 1697105077767]"
2277,424,40,[],200,EleutherAI/gpt-neox-20b,64,1,2494.0,1.0,1,H100,1697105084694,1697105087188,120,88.0,20.0,"[4, 339, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 349, 69, 55, 52, 65, 62, 291]","[1697105084698, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835, 1697105086897, 1697105087188]"
2278,421,27,[],200,EleutherAI/gpt-neox-20b,64,1,2295.0,1.0,1,H100,1697105073780,1697105076075,120,85.0,20.0,"[36, 488, 63, 50, 61, 60, 53, 476, 67, 64, 52, 64, 63, 59, 357, 61, 48, 60, 57, 56]","[1697105073816, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075854, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
2279,184,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070417,1697105070951,120,,,[13],[1697105070430]
2280,860,27,[],200,EleutherAI/gpt-neox-20b,64,1,2825.0,1.0,1,H100,1697105070953,1697105073778,120,85.0,20.0,"[121, 959, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 58, 48, 234, 52, 64, 61, 50]","[1697105071074, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073778]"
2281,858,47,[],200,EleutherAI/gpt-neox-20b,64,1,1379.0,1.0,1,H100,1697105099498,1697105100877,120,182.0,12.0,"[20, 319, 63, 51, 62, 57, 54, 460, 116, 56, 62, 59]","[1697105099518, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877]"
2282,513,28,[],200,EleutherAI/gpt-neox-20b,64,1,2469.0,1.0,1,H100,1697105076077,1697105078546,120,83.0,20.0,"[10, 744, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 57]","[1697105076087, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546]"
2283,468,32,[],200,EleutherAI/gpt-neox-20b,64,1,2870.0,1.0,1,H100,1697105075438,1697105078308,120,31.0,20.0,"[4, 838, 60, 45, 53, 51, 342, 64, 55, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56]","[1697105075442, 1697105076280, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076895, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308]"
2284,183,24,[],200,EleutherAI/gpt-neox-20b,64,1,7170.0,1.0,1,H100,1697105077595,1697105084765,120,17.0,50.0,"[13, 589, 56, 55, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72]","[1697105077608, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765]"
2285,626,48,[],200,EleutherAI/gpt-neox-20b,64,1,1116.0,1.0,1,H100,1697105100878,1697105101994,120,10.0,1.0,"[4, 1112]","[1697105100882, 1697105101994]"
2286,288,49,[],200,EleutherAI/gpt-neox-20b,64,1,3523.0,1.0,1,H100,1697105101995,1697105105518,120,93.0,20.0,"[8, 804, 707, 67, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 65]","[1697105102003, 1697105102807, 1697105103514, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105517]"
2287,140,41,[],200,EleutherAI/gpt-neox-20b,64,1,2654.0,1.0,1,H100,1697105089393,1697105092047,120,96.0,20.0,"[4, 370, 64, 57, 56, 45, 56, 529, 65, 49, 60, 59, 58, 530, 70, 67, 65, 64, 311, 75]","[1697105089397, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047]"
2288,55,50,[],200,EleutherAI/gpt-neox-20b,64,1,545.0,1.0,1,H100,1697105105519,1697105106064,120,12.0,1.0,"[8, 537]","[1697105105527, 1697105106064]"
2289,645,51,[],200,EleutherAI/gpt-neox-20b,64,1,2436.0,1.0,1,H100,1697105106065,1697105108501,120,86.0,20.0,"[17, 624, 58, 57, 55, 44, 287, 60, 58, 56, 54, 53, 260, 61, 61, 50, 55, 45, 423, 57]","[1697105106082, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107267, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107920, 1697105107975, 1697105108020, 1697105108443, 1697105108500]"
2290,418,52,[],200,EleutherAI/gpt-neox-20b,64,1,880.0,1.0,1,H100,1697105108501,1697105109381,120,286.0,3.0,"[9, 805, 66]","[1697105108510, 1697105109315, 1697105109381]"
2291,75,51,[],200,EleutherAI/gpt-neox-20b,64,1,2661.0,1.0,1,H100,1697105085641,1697105088302,120,345.0,18.0,"[5, 948, 69, 53, 54, 65, 62, 291, 72, 54, 71, 69, 67, 63, 547, 62, 61, 48]","[1697105085646, 1697105086594, 1697105086663, 1697105086716, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088193, 1697105088254, 1697105088302]"
2292,873,64,[],200,EleutherAI/gpt-neox-20b,64,1,335.0,1.0,1,H100,1697105124098,1697105124433,120,6.0,1.0,"[17, 318]","[1697105124115, 1697105124433]"
2293,528,65,[],200,EleutherAI/gpt-neox-20b,64,1,2564.0,1.0,1,H100,1697105124434,1697105126998,120,52.0,20.0,"[12, 708, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 70, 53, 279, 79, 77, 75]","[1697105124446, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998]"
2294,658,52,[],200,EleutherAI/gpt-neox-20b,64,1,253.0,1.0,1,H100,1697105088304,1697105088557,120,11.0,1.0,"[4, 249]","[1697105088308, 1697105088557]"
2295,654,15,[],200,EleutherAI/gpt-neox-20b,64,1,596.0,1.0,1,H100,1697105054813,1697105055409,120,47.0,4.0,"[4, 481, 62, 48]","[1697105054817, 1697105055298, 1697105055360, 1697105055408]"
2296,434,53,[],200,EleutherAI/gpt-neox-20b,64,1,2837.0,1.0,1,H100,1697105088558,1697105091395,120,85.0,20.0,"[13, 614, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530, 64, 49, 60, 59, 58, 529]","[1697105088571, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394]"
2297,304,66,[],200,EleutherAI/gpt-neox-20b,64,1,2444.0,1.0,1,H100,1697105126999,1697105129443,120,86.0,20.0,"[8, 599, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 67, 51, 395, 72, 126, 57, 69, 63]","[1697105127007, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128610, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129443]"
2298,887,67,[],200,EleutherAI/gpt-neox-20b,64,1,4759.0,1.0,1,H100,1697105129444,1697105134203,120,244.0,50.0,"[12, 1039, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70, 65, 63, 50, 49, 49, 88, 64, 58, 43, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34]","[1697105129456, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203]"
2299,500,32,[],200,EleutherAI/gpt-neox-20b,64,1,2150.0,1.0,1,H100,1697105070952,1697105073102,120,335.0,11.0,"[38, 1043, 57, 47, 46, 56, 55, 604, 73, 68, 63]","[1697105070990, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102]"
2300,152,33,[],200,EleutherAI/gpt-neox-20b,64,1,2334.0,1.0,1,H100,1697105073103,1697105075437,120,87.0,20.0,"[5, 443, 52, 64, 62, 50, 43, 482, 63, 50, 61, 60, 53, 476, 67, 64, 52, 64, 63, 60]","[1697105073108, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073779, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075437]"
2301,423,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062195,1697105063191,120,,,"[9, 394, 61, 60, 58, 47]","[1697105062204, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
2302,155,38,[],200,EleutherAI/gpt-neox-20b,64,1,2397.0,1.0,1,H100,1697105073040,1697105075437,120,90.0,20.0,"[5, 506, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60, 53, 476, 67, 64, 52, 64, 63, 60]","[1697105073045, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075437]"
2303,187,42,[],200,EleutherAI/gpt-neox-20b,64,1,929.0,1.0,1,H100,1697105085906,1697105086835,120,161.0,6.0,"[4, 684, 69, 54, 53, 65]","[1697105085910, 1697105086594, 1697105086663, 1697105086717, 1697105086770, 1697105086835]"
2304,886,43,[],200,EleutherAI/gpt-neox-20b,64,1,276.0,1.0,1,H100,1697105086836,1697105087112,120,17.0,1.0,"[5, 271]","[1697105086841, 1697105087112]"
2305,542,44,[],200,EleutherAI/gpt-neox-20b,64,1,934.0,1.0,1,H100,1697105087113,1697105088047,120,11.0,1.0,"[8, 926]","[1697105087121, 1697105088047]"
2306,310,45,[],200,EleutherAI/gpt-neox-20b,64,1,509.0,1.0,1,H100,1697105088048,1697105088557,120,26.0,1.0,"[12, 497]","[1697105088060, 1697105088557]"
2307,875,33,[],200,EleutherAI/gpt-neox-20b,64,1,3542.0,1.0,1,H100,1697105085905,1697105089447,120,31.0,31.0,"[5, 605, 79, 69, 53, 53, 66, 62, 291, 72, 54, 70, 70, 67, 63, 547, 62, 62, 47, 55, 264, 61, 61, 58, 56, 55, 272, 58, 47, 57, 46, 55]","[1697105085910, 1697105086515, 1697105086594, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088193, 1697105088255, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447]"
2308,871,46,[],200,EleutherAI/gpt-neox-20b,64,1,834.0,1.0,1,H100,1697105088558,1697105089392,120,123.0,6.0,"[4, 623, 57, 47, 57, 46]","[1697105088562, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392]"
2309,640,47,[],200,EleutherAI/gpt-neox-20b,64,1,305.0,1.0,1,H100,1697105089393,1697105089698,120,15.0,1.0,"[8, 297]","[1697105089401, 1697105089698]"
2310,831,36,[],200,EleutherAI/gpt-neox-20b,64,1,825.0,1.0,1,H100,1697105072659,1697105073484,120,11.0,1.0,"[5, 820]","[1697105072664, 1697105073484]"
2311,73,53,[],200,EleutherAI/gpt-neox-20b,64,1,535.0,1.0,1,H100,1697105109382,1697105109917,120,9.0,1.0,"[4, 531]","[1697105109386, 1697105109917]"
2312,734,24,[],200,EleutherAI/gpt-neox-20b,64,1,1468.0,1.0,1,H100,1697105058842,1697105060310,120,100.0,6.0,"[49, 576, 681, 59, 57, 46]","[1697105058891, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310]"
2313,90,32,[],200,EleutherAI/gpt-neox-20b,64,1,494.0,1.0,1,H100,1697105088623,1697105089117,120,19.0,1.0,"[8, 486]","[1697105088631, 1697105089117]"
2314,680,33,[],200,EleutherAI/gpt-neox-20b,64,1,1630.0,1.0,1,H100,1697105089118,1697105090748,120,123.0,11.0,"[9, 640, 64, 57, 56, 45, 56, 529, 65, 48, 61]","[1697105089127, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090687, 1697105090748]"
2315,293,29,[],200,EleutherAI/gpt-neox-20b,64,1,4220.0,1.0,1,H100,1697105078547,1697105082767,120,91.0,20.0,"[13, 1610, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 425, 71, 71, 68, 65, 434, 78]","[1697105078560, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081979, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766]"
2316,449,34,[],200,EleutherAI/gpt-neox-20b,64,1,2576.0,1.0,1,H100,1697105090749,1697105093325,120,86.0,20.0,"[4, 642, 70, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 56, 67, 52, 66, 64, 361, 77]","[1697105090753, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093325]"
2317,105,35,[],200,EleutherAI/gpt-neox-20b,64,1,4343.0,1.0,1,H100,1697105093326,1697105097669,120,364.0,36.0,"[12, 834, 75, 56, 73, 70, 55, 54, 246, 54, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512, 74, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67]","[1697105093338, 1697105094172, 1697105094247, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669]"
2318,90,54,[],200,EleutherAI/gpt-neox-20b,64,1,497.0,1.0,1,H100,1697105091398,1697105091895,120,19.0,1.0,"[12, 485]","[1697105091410, 1697105091895]"
2319,826,32,[],200,EleutherAI/gpt-neox-20b,64,1,2729.0,1.0,1,H100,1697105077768,1697105080497,120,87.0,20.0,"[8, 421, 56, 55, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51]","[1697105077776, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497]"
2320,788,55,[],200,EleutherAI/gpt-neox-20b,64,1,610.0,1.0,1,H100,1697105091896,1697105092506,120,31.0,1.0,"[13, 597]","[1697105091909, 1697105092506]"
2321,453,56,[],200,EleutherAI/gpt-neox-20b,64,1,660.0,1.0,1,H100,1697105092507,1697105093167,120,26.0,1.0,"[8, 652]","[1697105092515, 1697105093167]"
2322,805,47,[],200,EleutherAI/gpt-neox-20b,64,1,6416.0,1.0,1,H100,1697105082768,1697105089184,120,286.0,50.0,"[4, 727, 72, 68, 51, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562, 72, 70, 68, 67, 63, 348, 70, 53, 54, 65, 62, 291, 71, 55, 70, 70, 67, 63, 546, 63, 61, 48, 55, 264, 61, 60, 59, 56, 55, 272]","[1697105082772, 1697105083499, 1697105083571, 1697105083639, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089184]"
2323,223,57,[],200,EleutherAI/gpt-neox-20b,64,1,905.0,1.0,1,H100,1697105093168,1697105094073,120,16.0,1.0,"[9, 896]","[1697105093177, 1697105094073]"
2324,486,33,[],200,EleutherAI/gpt-neox-20b,64,1,3915.0,1.0,1,H100,1697105080498,1697105084413,120,14.0,20.0,"[12, 1468, 72, 71, 68, 65, 435, 77, 73, 70, 54, 53, 70, 413, 71, 68, 52, 67, 64, 592]","[1697105080510, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082689, 1697105082766, 1697105082839, 1697105082909, 1697105082963, 1697105083016, 1697105083086, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413]"
2325,897,58,[],200,EleutherAI/gpt-neox-20b,64,1,655.0,1.0,1,H100,1697105094074,1697105094729,120,9.0,1.0,"[13, 642]","[1697105094087, 1697105094729]"
2326,551,59,[],200,EleutherAI/gpt-neox-20b,64,1,2734.0,1.0,1,H100,1697105094730,1697105097464,120,90.0,20.0,"[9, 700, 65, 50, 64, 64, 511, 75, 57, 73, 55, 72, 54, 245, 69, 68, 67, 64, 298, 74]","[1697105094739, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096325, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096824, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464]"
2327,170,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063193,1697105065210,120,,,"[89, 792, 46, 432, 61, 59, 57, 45, 54]","[1697105063282, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2328,419,32,[],200,EleutherAI/gpt-neox-20b,64,1,3081.0,1.0,1,H100,1697105095103,1697105098184,120,88.0,20.0,"[12, 1079, 74, 56, 74, 56, 71, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75]","[1697105095115, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096454, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184]"
2329,373,40,[],200,EleutherAI/gpt-neox-20b,64,1,1201.0,1.0,1,H100,1697105073780,1697105074981,120,15.0,1.0,"[48, 1153]","[1697105073828, 1697105074981]"
2330,254,34,[],200,EleutherAI/gpt-neox-20b,64,1,547.0,1.0,1,H100,1697105084414,1697105084961,120,58.0,1.0,"[17, 530]","[1697105084431, 1697105084961]"
2331,847,35,[],200,EleutherAI/gpt-neox-20b,64,1,678.0,1.0,1,H100,1697105084962,1697105085640,120,10.0,1.0,"[12, 666]","[1697105084974, 1697105085640]"
2332,145,41,[],200,EleutherAI/gpt-neox-20b,64,1,1403.0,1.0,1,H100,1697105074982,1697105076385,120,161.0,9.0,"[5, 806, 60, 49, 60, 57, 56, 265, 45]","[1697105074987, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385]"
2333,617,36,[],200,EleutherAI/gpt-neox-20b,64,1,2981.0,1.0,1,H100,1697105085641,1697105088622,120,87.0,20.0,"[5, 948, 69, 53, 54, 65, 62, 291, 72, 54, 70, 70, 67, 63, 547, 62, 62, 47, 55, 264]","[1697105085646, 1697105086594, 1697105086663, 1697105086716, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088193, 1697105088255, 1697105088302, 1697105088357, 1697105088621]"
2334,272,37,[],200,EleutherAI/gpt-neox-20b,64,1,2773.0,1.0,1,H100,1697105088622,1697105091395,120,86.0,20.0,"[5, 558, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530, 64, 49, 60, 59, 58, 530]","[1697105088627, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091395]"
2335,328,60,[],200,EleutherAI/gpt-neox-20b,64,1,891.0,1.0,1,H100,1697105097465,1697105098356,120,109.0,6.0,"[8, 637, 74, 68, 52, 52]","[1697105097473, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356]"
2336,912,61,[],200,EleutherAI/gpt-neox-20b,64,1,3116.0,1.0,1,H100,1697105098357,1697105101473,120,92.0,20.0,"[4, 872, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66]","[1697105098361, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473]"
2337,727,42,[],200,EleutherAI/gpt-neox-20b,64,1,620.0,1.0,1,H100,1697105076386,1697105077006,120,58.0,5.0,"[4, 441, 63, 56, 56]","[1697105076390, 1697105076831, 1697105076894, 1697105076950, 1697105077006]"
2338,292,35,[],200,EleutherAI/gpt-neox-20b,64,1,604.0,1.0,1,H100,1697105076895,1697105077499,120,286.0,1.0,"[9, 595]","[1697105076904, 1697105077499]"
2339,41,36,[],200,EleutherAI/gpt-neox-20b,64,1,6257.0,1.0,1,H100,1697105077500,1697105083757,120,39.0,43.0,"[9, 688, 56, 55, 62, 60, 59, 57, 917, 72, 68, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 141, 75, 70, 68, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 71, 68, 52, 67]","[1697105077509, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757]"
2340,846,42,[],200,EleutherAI/gpt-neox-20b,64,1,775.0,1.0,1,H100,1697105092048,1697105092823,120,140.0,6.0,"[12, 522, 56, 68, 51, 66]","[1697105092060, 1697105092582, 1697105092638, 1697105092706, 1697105092757, 1697105092823]"
2341,475,43,[],200,EleutherAI/gpt-neox-20b,64,1,2716.0,1.0,1,H100,1697105077006,1697105079722,120,89.0,20.0,"[5, 583, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 57, 917, 72, 68, 67, 52]","[1697105077011, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722]"
2342,841,33,[],200,EleutherAI/gpt-neox-20b,64,1,1864.0,1.0,1,H100,1697105075962,1697105077826,120,123.0,15.0,"[5, 373, 45, 53, 52, 341, 63, 56, 55, 55, 534, 63, 49, 61, 59]","[1697105075967, 1697105076340, 1697105076385, 1697105076438, 1697105076490, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826]"
2343,244,44,[],200,EleutherAI/gpt-neox-20b,64,1,374.0,1.0,1,H100,1697105079723,1697105080097,120,9.0,1.0,"[16, 358]","[1697105079739, 1697105080097]"
2344,833,45,[],200,EleutherAI/gpt-neox-20b,64,1,1880.0,1.0,1,H100,1697105080098,1697105081978,120,563.0,8.0,"[13, 1022, 142, 74, 70, 68, 67, 424]","[1697105080111, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978]"
2345,500,43,[],200,EleutherAI/gpt-neox-20b,64,1,1552.0,1.0,1,H100,1697105092824,1697105094376,120,335.0,11.0,"[4, 420, 76, 69, 67, 51, 52, 609, 74, 57, 72]","[1697105092828, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375]"
2346,598,46,[],200,EleutherAI/gpt-neox-20b,64,1,1711.0,1.0,1,H100,1697105081979,1697105083690,120,345.0,12.0,"[5, 705, 77, 72, 71, 54, 53, 69, 415, 71, 67, 52]","[1697105081984, 1697105082689, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083500, 1697105083571, 1697105083638, 1697105083690]"
2347,259,47,[],200,EleutherAI/gpt-neox-20b,64,1,2903.0,1.0,1,H100,1697105083691,1697105086594,120,87.0,20.0,"[5, 717, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562, 73, 69, 68, 67, 63, 349]","[1697105083696, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905, 1697105085978, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594]"
2348,496,34,[],200,EleutherAI/gpt-neox-20b,64,1,1776.0,1.0,1,H100,1697105077827,1697105079603,120,335.0,11.0,"[5, 365, 56, 55, 62, 60, 59, 57, 917, 71, 69]","[1697105077832, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603]"
2349,274,35,[],200,EleutherAI/gpt-neox-20b,64,1,1815.0,1.0,1,H100,1697105079604,1697105081419,120,364.0,11.0,"[4, 562, 70, 71, 69, 66, 51, 636, 142, 74, 70]","[1697105079608, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419]"
2350,245,44,[],200,EleutherAI/gpt-neox-20b,64,1,2448.0,1.0,1,H100,1697105094376,1697105096824,120,100.0,20.0,"[5, 421, 53, 67, 65, 50, 65, 336, 66, 50, 64, 64, 512, 74, 56, 74, 55, 72, 54, 244]","[1697105094381, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823]"
2351,860,25,[],200,EleutherAI/gpt-neox-20b,64,1,2818.0,1.0,1,H100,1697105084766,1697105087584,120,85.0,20.0,"[9, 1129, 73, 70, 68, 67, 63, 348, 70, 56, 51, 65, 62, 291, 71, 55, 71, 69, 67, 63]","[1697105084775, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086719, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584]"
2352,912,23,[],200,EleutherAI/gpt-neox-20b,64,1,2911.0,1.0,1,H100,1697105054431,1697105057342,120,92.0,20.0,"[5, 862, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 70, 68, 67, 64, 52, 63, 347]","[1697105054436, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057341]"
2353,751,54,[],200,EleutherAI/gpt-neox-20b,64,1,6931.0,1.0,1,H100,1697105109918,1697105116849,120,216.0,50.0,"[12, 975, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80]","[1697105109930, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849]"
2354,479,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105065212,1697105070950,120,,,"[158, 1203, 68, 66, 65, 62, 58, 534, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105065370, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
2355,852,36,[],200,EleutherAI/gpt-neox-20b,64,1,2993.0,1.0,1,H100,1697105081420,1697105084413,120,100.0,20.0,"[4, 554, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 415, 71, 68, 51, 67, 64, 592]","[1697105081424, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083500, 1697105083571, 1697105083639, 1697105083690, 1697105083757, 1697105083821, 1697105084413]"
2356,390,25,[],200,EleutherAI/gpt-neox-20b,64,1,2513.0,1.0,1,H100,1697105060311,1697105062824,120,84.0,20.0,"[8, 445, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060319, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
2357,307,43,[],200,EleutherAI/gpt-neox-20b,64,1,747.0,1.0,1,H100,1697105093326,1697105094073,120,26.0,1.0,"[16, 731]","[1697105093342, 1697105094073]"
2358,866,44,[],200,EleutherAI/gpt-neox-20b,64,1,2749.0,1.0,1,H100,1697105094074,1697105096823,120,93.0,20.0,"[5, 722, 54, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512, 74, 56, 74, 55, 72, 54, 244]","[1697105094079, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823]"
2359,178,49,[],200,EleutherAI/gpt-neox-20b,64,1,762.0,1.0,1,H100,1697105106922,1697105107684,120,11.0,1.0,"[8, 754]","[1697105106930, 1697105107684]"
2360,855,50,[],200,EleutherAI/gpt-neox-20b,64,1,3690.0,1.0,1,H100,1697105107685,1697105111375,120,83.0,20.0,"[8, 750, 57, 50, 48, 717, 66, 63, 54, 573, 71, 121, 74, 49, 60, 458, 215, 128, 73, 55]","[1697105107693, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375]"
2361,599,37,[],200,EleutherAI/gpt-neox-20b,64,1,6451.0,1.0,1,H100,1697105084414,1697105090865,120,58.0,55.0,"[17, 606, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 349, 69, 55, 52, 65, 62, 291, 71, 55, 71, 69, 67, 63, 546, 63, 61, 48, 55, 264, 61, 60, 59, 56, 55, 272, 58, 47, 57, 46, 55, 319, 65, 57, 56, 45, 56, 529, 65, 48, 61, 59, 58]","[1697105084431, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089766, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090687, 1697105090748, 1697105090807, 1697105090865]"
2362,639,45,[],200,EleutherAI/gpt-neox-20b,64,1,845.0,1.0,1,H100,1697105096824,1697105097669,120,100.0,6.0,"[5, 561, 74, 70, 68, 67]","[1697105096829, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669]"
2363,510,51,[],200,EleutherAI/gpt-neox-20b,64,1,418.0,1.0,1,H100,1697105111376,1697105111794,120,79.0,2.0,"[4, 414]","[1697105111380, 1697105111794]"
2364,300,46,[],200,EleutherAI/gpt-neox-20b,64,1,855.0,1.0,1,H100,1697105097670,1697105098525,120,9.0,1.0,"[8, 847]","[1697105097678, 1697105098525]"
2365,72,47,[],200,EleutherAI/gpt-neox-20b,64,1,2947.0,1.0,1,H100,1697105098526,1697105101473,120,84.0,20.0,"[4, 703, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66]","[1697105098530, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473]"
2366,763,46,[],200,EleutherAI/gpt-neox-20b,64,1,527.0,1.0,1,H100,1697105092640,1697105093167,120,20.0,1.0,"[16, 511]","[1697105092656, 1697105093167]"
2367,861,34,[],200,EleutherAI/gpt-neox-20b,64,1,842.0,1.0,1,H100,1697105075438,1697105076280,120,10.0,1.0,"[8, 834]","[1697105075446, 1697105076280]"
2368,82,28,[],200,EleutherAI/gpt-neox-20b,64,1,2469.0,1.0,1,H100,1697105076077,1697105078546,120,67.0,20.0,"[5, 749, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 57]","[1697105076082, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546]"
2369,515,35,[],200,EleutherAI/gpt-neox-20b,64,1,467.0,1.0,1,H100,1697105076282,1697105076749,120,11.0,1.0,"[8, 459]","[1697105076290, 1697105076749]"
2370,502,36,[],200,EleutherAI/gpt-neox-20b,64,1,204.0,1.0,1,H100,1697105063169,1697105063373,120,19.0,1.0,"[5, 199]","[1697105063174, 1697105063373]"
2371,286,36,[],200,EleutherAI/gpt-neox-20b,64,1,1620.0,1.0,1,H100,1697105076750,1697105078370,120,161.0,12.0,"[16, 828, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62]","[1697105076766, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370]"
2372,423,47,[],200,EleutherAI/gpt-neox-20b,64,1,3026.0,1.0,1,H100,1697105093168,1697105096194,120,84.0,20.0,"[9, 995, 74, 57, 72, 71, 55, 54, 247, 53, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512]","[1697105093177, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194]"
2373,531,34,[],200,EleutherAI/gpt-neox-20b,64,1,3191.0,1.0,1,H100,1697105089448,1697105092639,120,52.0,20.0,"[5, 1121, 65, 49, 60, 59, 58, 530, 70, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 56]","[1697105089453, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638]"
2374,899,42,[],200,EleutherAI/gpt-neox-20b,64,1,2444.0,1.0,1,H100,1697105088131,1697105090575,120,100.0,20.0,"[5, 486, 60, 61, 58, 56, 56, 271, 58, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530]","[1697105088136, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575]"
2375,278,37,[],200,EleutherAI/gpt-neox-20b,64,1,1111.0,1.0,1,H100,1697105063374,1697105064485,120,13.0,1.0,"[16, 1095]","[1697105063390, 1697105064485]"
2376,859,38,[],200,EleutherAI/gpt-neox-20b,64,1,702.0,1.0,1,H100,1697105064486,1697105065188,120,23.0,1.0,"[16, 686]","[1697105064502, 1697105065188]"
2377,202,34,[],200,EleutherAI/gpt-neox-20b,64,1,9993.0,1.0,1,H100,1697105078309,1697105088302,120,874.0,72.0,"[21, 1133, 72, 68, 67, 52, 58, 390, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 348, 70, 53, 53, 66, 62, 291, 71, 55, 70, 70, 67, 63, 546, 63, 61, 48]","[1697105078330, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302]"
2378,607,39,[],200,EleutherAI/gpt-neox-20b,64,1,2305.0,1.0,1,H100,1697105065189,1697105067494,120,6.0,10.0,"[4, 653, 727, 68, 66, 65, 61, 59, 534, 68]","[1697105065193, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494]"
2379,300,35,[],200,EleutherAI/gpt-neox-20b,64,1,527.0,1.0,1,H100,1697105092640,1697105093167,120,9.0,1.0,"[12, 515]","[1697105092652, 1697105093167]"
2380,376,40,[],200,EleutherAI/gpt-neox-20b,64,1,2921.0,1.0,1,H100,1697105067495,1697105070416,120,87.0,20.0,"[5, 578, 60, 58, 58, 47, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67]","[1697105067500, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
2381,890,36,[],200,EleutherAI/gpt-neox-20b,64,1,3026.0,1.0,1,H100,1697105093168,1697105096194,120,93.0,20.0,"[5, 999, 74, 57, 72, 71, 55, 54, 247, 53, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512]","[1697105093173, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194]"
2382,245,33,[],200,EleutherAI/gpt-neox-20b,64,1,3669.0,1.0,1,H100,1697105078309,1697105081978,120,100.0,20.0,"[5, 1149, 72, 68, 67, 52, 58, 390, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 424]","[1697105078314, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978]"
2383,447,41,[],200,EleutherAI/gpt-neox-20b,64,1,1727.0,1.0,1,H100,1697105095366,1697105097093,120,161.0,13.0,"[17, 811, 74, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64]","[1697105095383, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092]"
2384,173,36,[],200,EleutherAI/gpt-neox-20b,64,1,2771.0,1.0,1,H100,1697105076951,1697105079722,120,96.0,20.0,"[5, 638, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 72, 68, 67, 52]","[1697105076956, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722]"
2385,520,55,[],200,EleutherAI/gpt-neox-20b,64,1,640.0,1.0,1,H100,1697105116851,1697105117491,120,11.0,1.0,"[4, 636]","[1697105116855, 1697105117491]"
2386,16,45,[],200,EleutherAI/gpt-neox-20b,64,1,488.0,1.0,1,H100,1697105096825,1697105097313,120,9.0,1.0,"[20, 468]","[1697105096845, 1697105097313]"
2387,247,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070954,1697105102061,120,,,"[144, 935, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60, 53, 476, 67, 64, 52, 64, 63, 59, 357, 60, 49, 59, 58, 56, 265, 45, 53, 51, 342, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 141, 75, 69, 69, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 71, 68, 52, 66, 65, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 348, 70, 53, 53, 66, 62, 291, 71, 55, 70, 70, 67, 63, 546, 63, 61, 48, 55, 264, 61, 60, 59, 56, 55, 272, 58, 47, 57, 46, 55, 319, 65, 57, 56, 45, 56, 529, 65, 48, 61, 59, 58, 529, 71, 67, 65, 64, 311, 74, 68, 52, 52, 65, 299, 56, 67, 52, 66, 64, 360, 77, 69, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54, 246, 54, 67, 65, 50, 65, 336, 66, 50, 64, 64, 511, 75, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75, 67, 53, 51, 242, 55, 57, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63, 56, 55, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105071098, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075902, 1697105075961, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081418, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083756, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089766, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090687, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092046, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098251, 1697105098304, 1697105098355, 1697105098597, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100069, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2388,601,46,[],200,EleutherAI/gpt-neox-20b,64,1,2699.0,1.0,1,H100,1697105097314,1697105100013,120,83.0,20.0,"[13, 783, 74, 68, 52, 52, 241, 56, 56, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63]","[1697105097327, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098597, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013]"
2389,32,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070418,1697105070951,120,,,[20],[1697105070438]
2390,734,42,[],200,EleutherAI/gpt-neox-20b,64,1,1286.0,1.0,1,H100,1697105070953,1697105072239,120,100.0,6.0,"[13, 1067, 57, 47, 46, 56]","[1697105070966, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239]"
2391,388,43,[],200,EleutherAI/gpt-neox-20b,64,1,2298.0,1.0,1,H100,1697105072240,1697105074538,120,87.0,20.0,"[5, 653, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60]","[1697105072245, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538]"
2392,922,15,[],200,EleutherAI/gpt-neox-20b,64,1,2635.0,1.0,1,H100,1697105049502,1697105052137,120,91.0,20.0,"[5, 856, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70, 68, 68, 52]","[1697105049507, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949, 1697105052017, 1697105052085, 1697105052137]"
2393,642,36,[],200,EleutherAI/gpt-neox-20b,64,1,2507.0,1.0,1,H100,1697105092048,1697105094555,120,89.0,20.0,"[8, 526, 56, 67, 52, 66, 64, 361, 76, 69, 67, 52, 51, 609, 74, 57, 72, 71, 55, 54]","[1697105092056, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093512, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555]"
2394,373,47,[],200,EleutherAI/gpt-neox-20b,64,1,486.0,1.0,1,H100,1697105100015,1697105100501,120,15.0,1.0,"[16, 470]","[1697105100031, 1697105100501]"
2395,26,48,[],200,EleutherAI/gpt-neox-20b,64,1,763.0,1.0,1,H100,1697105100502,1697105101265,120,18.0,1.0,"[17, 746]","[1697105100519, 1697105101265]"
2396,732,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101266,1697105102062,120,,,[17],[1697105101283]
2397,384,50,[],200,EleutherAI/gpt-neox-20b,64,1,3453.0,1.0,1,H100,1697105102065,1697105105518,120,92.0,20.0,"[34, 709, 707, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66]","[1697105102099, 1697105102808, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518]"
2398,295,37,[],200,EleutherAI/gpt-neox-20b,64,1,883.0,1.0,1,H100,1697105094556,1697105095439,120,52.0,2.0,"[5, 878]","[1697105094561, 1697105095439]"
2399,42,38,[],200,EleutherAI/gpt-neox-20b,64,1,584.0,1.0,1,H100,1697105095440,1697105096024,120,10.0,1.0,"[8, 576]","[1697105095448, 1697105096024]"
2400,627,39,[],200,EleutherAI/gpt-neox-20b,64,1,2753.0,1.0,1,H100,1697105096025,1697105098778,120,93.0,20.0,"[4, 794, 70, 68, 67, 64, 298, 74, 70, 68, 67, 441, 74, 68, 52, 52, 242, 54, 57, 69]","[1697105096029, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098652, 1697105098709, 1697105098778]"
2401,49,38,[],200,EleutherAI/gpt-neox-20b,64,1,650.0,1.0,1,H100,1697105091397,1697105092047,120,109.0,3.0,"[10, 566, 74]","[1697105091407, 1697105091973, 1697105092047]"
2402,690,16,[],200,EleutherAI/gpt-neox-20b,64,1,467.0,1.0,1,H100,1697105052138,1697105052605,120,39.0,1.0,"[16, 451]","[1697105052154, 1697105052605]"
2403,4,41,[],200,EleutherAI/gpt-neox-20b,64,1,3366.0,1.0,1,H100,1697105083822,1697105087188,120,89.0,20.0,"[5, 1210, 56, 73, 54, 55, 68, 561, 74, 69, 68, 67, 63, 349, 69, 53, 54, 65, 62, 291]","[1697105083827, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085978, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086716, 1697105086770, 1697105086835, 1697105086897, 1697105087188]"
2404,350,17,[],200,EleutherAI/gpt-neox-20b,64,1,710.0,1.0,1,H100,1697105052606,1697105053316,120,216.0,1.0,"[5, 705]","[1697105052611, 1697105053316]"
2405,120,18,[],200,EleutherAI/gpt-neox-20b,64,1,668.0,1.0,1,H100,1697105053317,1697105053985,120,17.0,1.0,"[11, 657]","[1697105053328, 1697105053985]"
2406,632,39,[],200,EleutherAI/gpt-neox-20b,64,1,2507.0,1.0,1,H100,1697105092048,1697105094555,120,91.0,20.0,"[8, 526, 56, 67, 52, 66, 64, 361, 76, 69, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54]","[1697105092056, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555]"
2407,483,24,[],200,EleutherAI/gpt-neox-20b,64,1,3351.0,1.0,1,H100,1697105058843,1697105062194,120,84.0,20.0,"[36, 588, 681, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70]","[1697105058879, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194]"
2408,296,48,[],200,EleutherAI/gpt-neox-20b,64,1,728.0,1.0,1,H100,1697105089699,1697105090427,120,6.0,1.0,"[13, 715]","[1697105089712, 1697105090427]"
2409,396,40,[],200,EleutherAI/gpt-neox-20b,64,1,2693.0,1.0,1,H100,1697105098780,1697105101473,120,89.0,20.0,"[6, 447, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 57, 61, 59, 463, 67, 66]","[1697105098786, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100757, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473]"
2410,781,29,[],200,EleutherAI/gpt-neox-20b,64,1,2803.0,1.0,1,H100,1697105078547,1697105081350,120,335.0,10.0,"[5, 1618, 70, 71, 69, 66, 51, 636, 142, 74]","[1697105078552, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349]"
2411,73,49,[],200,EleutherAI/gpt-neox-20b,64,1,807.0,1.0,1,H100,1697105090428,1697105091235,120,9.0,1.0,"[9, 798]","[1697105090437, 1697105091235]"
2412,655,50,[],200,EleutherAI/gpt-neox-20b,64,1,1521.0,1.0,1,H100,1697105091236,1697105092757,120,335.0,11.0,"[4, 733, 74, 67, 52, 52, 65, 299, 56, 68, 51]","[1697105091240, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092706, 1697105092757]"
2413,434,51,[],200,EleutherAI/gpt-neox-20b,64,1,2344.0,1.0,1,H100,1697105092758,1697105095102,120,85.0,20.0,"[5, 485, 76, 69, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54, 247, 53, 67, 65, 50, 65]","[1697105092763, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102]"
2414,704,19,[],200,EleutherAI/gpt-neox-20b,64,1,676.0,1.0,1,H100,1697105053987,1697105054663,120,14.0,1.0,"[12, 664]","[1697105053999, 1697105054663]"
2415,456,20,[],200,EleutherAI/gpt-neox-20b,64,1,2678.0,1.0,1,H100,1697105054664,1697105057342,120,90.0,20.0,"[4, 630, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 70, 68, 67, 64, 52, 63, 348]","[1697105054668, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056931, 1697105056994, 1697105057342]"
2416,439,30,[],200,EleutherAI/gpt-neox-20b,64,1,769.0,1.0,1,H100,1697105081352,1697105082121,120,13.0,4.0,"[9, 617, 72, 71]","[1697105081361, 1697105081978, 1697105082050, 1697105082121]"
2417,31,48,[],200,EleutherAI/gpt-neox-20b,64,1,2590.0,1.0,1,H100,1697105086595,1697105089185,120,84.0,20.0,"[21, 572, 72, 54, 71, 69, 67, 63, 546, 63, 61, 48, 55, 265, 61, 60, 58, 56, 56, 272]","[1697105086616, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088622, 1697105088683, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089185]"
2418,860,25,[],200,EleutherAI/gpt-neox-20b,64,1,3405.0,1.0,1,H100,1697105058844,1697105062249,120,85.0,20.0,"[136, 1168, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 55]","[1697105058980, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249]"
2419,631,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062250,1697105063190,120,,,[16],[1697105062266]
2420,355,44,[],200,EleutherAI/gpt-neox-20b,64,1,3690.0,1.0,1,H100,1697105107685,1697105111375,120,90.0,20.0,"[4, 754, 57, 50, 48, 717, 66, 63, 54, 573, 71, 121, 74, 49, 59, 459, 215, 128, 73, 55]","[1697105107689, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263, 1697105110337, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375]"
2421,285,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063193,1697105065210,120,,,"[109, 772, 46, 432, 61, 59, 57, 45, 54]","[1697105063302, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2422,124,45,[],200,EleutherAI/gpt-neox-20b,64,1,418.0,1.0,1,H100,1697105111376,1697105111794,120,83.0,2.0,"[4, 414]","[1697105111380, 1697105111794]"
2423,109,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057342,1697105058841,120,,,"[7, 744, 65, 64, 63]","[1697105057349, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
2424,714,46,[],200,EleutherAI/gpt-neox-20b,64,1,2498.0,1.0,1,H100,1697105111795,1697105114293,120,83.0,20.0,"[12, 682, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 61]","[1697105111807, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114293]"
2425,280,52,[],200,EleutherAI/gpt-neox-20b,64,1,2498.0,1.0,1,H100,1697105111795,1697105114293,120,91.0,20.0,"[16, 678, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 61]","[1697105111811, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114293]"
2426,682,62,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101474,1697105102062,120,,,[12],[1697105101486]
2427,592,42,[],200,EleutherAI/gpt-neox-20b,64,1,6204.0,1.0,1,H100,1697105087189,1697105093393,120,15.0,50.0,"[13, 929, 62, 62, 47, 56, 263, 61, 60, 59, 56, 55, 273, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 529, 65, 49, 60, 59, 58, 529, 71, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 56, 67, 52, 66, 64, 360, 77, 69]","[1697105087202, 1697105088131, 1697105088193, 1697105088255, 1697105088302, 1697105088358, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093324, 1697105093393]"
2428,242,26,[],200,EleutherAI/gpt-neox-20b,64,1,2007.0,1.0,1,H100,1697105083086,1697105085093,120,345.0,9.0,"[5, 1322, 77, 56, 74, 73, 72, 272, 56]","[1697105083091, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093]"
2429,654,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101474,1697105102062,120,,,[4],[1697105101478]
2430,431,49,[],200,EleutherAI/gpt-neox-20b,64,1,4181.0,1.0,1,H100,1697105102073,1697105106254,120,732.0,22.0,"[114, 1328, 66, 51, 50, 64, 408, 73, 70, 69, 64, 62, 61, 613, 81, 76, 58, 71, 66, 53, 572, 111]","[1697105102187, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104227, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254]"
2431,513,28,[],200,EleutherAI/gpt-neox-20b,64,1,2296.0,1.0,1,H100,1697105073779,1697105076075,120,83.0,20.0,"[9, 515, 64, 50, 61, 60, 53, 476, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60, 57, 56]","[1697105073788, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
2432,566,40,[],200,EleutherAI/gpt-neox-20b,64,1,4389.0,1.0,1,H100,1697105092639,1697105097028,120,109.0,36.0,"[5, 604, 76, 69, 67, 51, 52, 609, 74, 57, 73, 70, 55, 54, 246, 54, 67, 65, 50, 65, 336, 66, 50, 64, 64, 511, 75, 56, 74, 55, 71, 55, 244, 70, 68, 67]","[1697105092644, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096524, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028]"
2433,290,29,[],200,EleutherAI/gpt-neox-20b,64,1,671.0,1.0,1,H100,1697105076078,1697105076749,120,14.0,1.0,"[28, 643]","[1697105076106, 1697105076749]"
2434,878,30,[],200,EleutherAI/gpt-neox-20b,64,1,2972.0,1.0,1,H100,1697105076750,1697105079722,120,83.0,20.0,"[8, 836, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 57, 917, 72, 68, 67, 52]","[1697105076758, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722]"
2435,19,27,[],200,EleutherAI/gpt-neox-20b,64,1,8469.0,1.0,1,H100,1697105085094,1697105093563,120,563.0,72.0,"[5, 806, 73, 69, 68, 67, 63, 349, 69, 53, 53, 66, 62, 291, 72, 54, 71, 69, 67, 63, 547, 62, 61, 48, 55, 264, 61, 60, 59, 56, 55, 272, 58, 47, 57, 46, 55, 319, 65, 57, 56, 45, 56, 529, 65, 48, 61, 59, 58, 529, 71, 67, 65, 64, 311, 74, 68, 52, 52, 65, 299, 56, 67, 52, 66, 64, 360, 77, 69, 67, 51, 52]","[1697105085099, 1697105085905, 1697105085978, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089766, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090687, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092046, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563]"
2436,775,41,[],200,EleutherAI/gpt-neox-20b,64,1,609.0,1.0,1,H100,1697105091897,1697105092506,120,17.0,1.0,"[16, 593]","[1697105091913, 1697105092506]"
2437,429,42,[],200,EleutherAI/gpt-neox-20b,64,1,6202.0,1.0,1,H100,1697105092507,1697105098709,120,244.0,50.0,"[13, 728, 76, 69, 67, 51, 52, 609, 74, 57, 73, 70, 55, 54, 246, 54, 67, 65, 50, 65, 336, 66, 50, 64, 64, 511, 75, 56, 74, 55, 71, 55, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75, 67, 53, 51, 242, 55, 57]","[1697105092520, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096524, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098251, 1697105098304, 1697105098355, 1697105098597, 1697105098652, 1697105098709]"
2438,219,41,[],200,EleutherAI/gpt-neox-20b,64,1,2468.0,1.0,1,H100,1697105097029,1697105099497,120,90.0,20.0,"[5, 356, 74, 70, 68, 67, 440, 75, 68, 52, 52, 242, 55, 56, 68, 62, 394, 72, 126, 66]","[1697105097034, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497]"
2439,612,34,[],200,EleutherAI/gpt-neox-20b,64,1,2902.0,1.0,1,H100,1697105077595,1697105080497,120,93.0,20.0,"[4, 598, 56, 55, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51]","[1697105077599, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497]"
2440,75,40,[],200,EleutherAI/gpt-neox-20b,64,1,2183.0,1.0,1,H100,1697105073779,1697105075962,120,345.0,18.0,"[13, 512, 63, 50, 61, 60, 53, 476, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60]","[1697105073792, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962]"
2441,647,31,[],200,EleutherAI/gpt-neox-20b,64,1,3044.0,1.0,1,H100,1697105079723,1697105082767,120,83.0,20.0,"[8, 439, 70, 71, 69, 66, 51, 636, 141, 75, 70, 68, 67, 425, 71, 71, 68, 65, 434, 79]","[1697105079731, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081979, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082767]"
2442,337,38,[],200,EleutherAI/gpt-neox-20b,64,1,1387.0,1.0,1,H100,1697105080498,1697105081885,120,12.0,1.0,"[8, 1379]","[1697105080506, 1697105081885]"
2443,273,35,[],200,EleutherAI/gpt-neox-20b,64,1,1387.0,1.0,1,H100,1697105080498,1697105081885,120,19.0,1.0,"[12, 1375]","[1697105080510, 1697105081885]"
2444,39,36,[],200,EleutherAI/gpt-neox-20b,64,1,641.0,1.0,1,H100,1697105081886,1697105082527,120,8.0,1.0,"[9, 631]","[1697105081895, 1697105082526]"
2445,308,32,[],200,EleutherAI/gpt-neox-20b,64,1,3137.0,1.0,1,H100,1697105082768,1697105085905,120,87.0,20.0,"[12, 719, 72, 68, 51, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562]","[1697105082780, 1697105083499, 1697105083571, 1697105083639, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905]"
2446,623,37,[],200,EleutherAI/gpt-neox-20b,64,1,1044.0,1.0,1,H100,1697105082527,1697105083571,120,140.0,3.0,"[5, 967, 72]","[1697105082532, 1697105083499, 1697105083571]"
2447,371,38,[],200,EleutherAI/gpt-neox-20b,64,1,590.0,1.0,1,H100,1697105083572,1697105084162,120,13.0,1.0,"[8, 582]","[1697105083580, 1697105084162]"
2448,25,39,[],200,EleutherAI/gpt-neox-20b,64,1,798.0,1.0,1,H100,1697105084163,1697105084961,120,12.0,1.0,"[8, 790]","[1697105084171, 1697105084961]"
2449,729,40,[],200,EleutherAI/gpt-neox-20b,64,1,944.0,1.0,1,H100,1697105084961,1697105085905,120,874.0,2.0,"[5, 939]","[1697105084966, 1697105085905]"
2450,873,37,[],200,EleutherAI/gpt-neox-20b,64,1,374.0,1.0,1,H100,1697105079723,1697105080097,120,6.0,1.0,"[20, 354]","[1697105079743, 1697105080097]"
2451,179,56,[],200,EleutherAI/gpt-neox-20b,64,1,1017.0,1.0,1,H100,1697105117492,1697105118509,120,161.0,4.0,"[8, 865, 72, 72]","[1697105117500, 1697105118365, 1697105118437, 1697105118509]"
2452,534,38,[],200,EleutherAI/gpt-neox-20b,64,1,3401.0,1.0,1,H100,1697105080098,1697105083499,120,96.0,20.0,"[16, 1019, 142, 74, 70, 68, 67, 424, 72, 71, 68, 65, 434, 79, 71, 71, 54, 53, 69, 414]","[1697105080114, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082767, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499]"
2453,877,57,[],200,EleutherAI/gpt-neox-20b,64,1,2081.0,1.0,1,H100,1697105118510,1697105120591,120,85.0,20.0,"[4, 389, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68]","[1697105118514, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590]"
2454,166,26,[],200,EleutherAI/gpt-neox-20b,64,1,547.0,1.0,1,H100,1697105062826,1697105063373,120,14.0,1.0,"[24, 523]","[1697105062850, 1697105063373]"
2455,749,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063374,1697105065209,120,,,"[8, 1170, 61, 59, 57, 45, 54]","[1697105063382, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2456,494,28,[],200,EleutherAI/gpt-neox-20b,64,1,2282.0,1.0,1,H100,1697105065212,1697105067494,120,6.0,10.0,"[13, 621, 727, 68, 66, 65, 61, 59, 533, 69]","[1697105065225, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067425, 1697105067494]"
2457,613,49,[],200,EleutherAI/gpt-neox-20b,64,1,2861.0,1.0,1,H100,1697105089186,1697105092047,120,90.0,20.0,"[16, 565, 64, 57, 56, 45, 56, 529, 65, 49, 60, 59, 58, 530, 70, 67, 65, 64, 311, 75]","[1697105089202, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047]"
2458,260,29,[],200,EleutherAI/gpt-neox-20b,64,1,2921.0,1.0,1,H100,1697105067495,1697105070416,120,86.0,20.0,"[9, 574, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67]","[1697105067504, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
2459,289,58,[],200,EleutherAI/gpt-neox-20b,64,1,3497.0,1.0,1,H100,1697105102074,1697105105571,120,89.0,20.0,"[121, 1320, 66, 51, 50, 64, 408, 73, 70, 69, 64, 63, 60, 613, 81, 76, 58, 71, 66, 53]","[1697105102195, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104227, 1697105104297, 1697105104366, 1697105104430, 1697105104493, 1697105104553, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571]"
2460,548,37,[],200,EleutherAI/gpt-neox-20b,64,1,2989.0,1.0,1,H100,1697105087585,1697105090574,120,86.0,20.0,"[9, 1027, 61, 60, 59, 56, 55, 272, 58, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 529]","[1697105087594, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574]"
2461,482,47,[],200,EleutherAI/gpt-neox-20b,64,1,3741.0,1.0,1,H100,1697105114294,1697105118035,120,91.0,20.0,"[12, 1456, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56, 65, 626, 81, 59, 78, 71]","[1697105114306, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035]"
2462,539,58,[],200,EleutherAI/gpt-neox-20b,64,1,3506.0,1.0,1,H100,1697105120591,1697105124097,120,83.0,20.0,"[13, 1324, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58]","[1697105120604, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097]"
2463,306,59,[],200,EleutherAI/gpt-neox-20b,64,1,710.0,1.0,1,H100,1697105124098,1697105124808,120,140.0,6.0,"[9, 412, 79, 78, 59, 73]","[1697105124107, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808]"
2464,889,60,[],200,EleutherAI/gpt-neox-20b,64,1,2189.0,1.0,1,H100,1697105124809,1697105126998,120,86.0,20.0,"[8, 337, 59, 58, 74, 57, 74, 72, 66, 475, 80, 60, 77, 59, 69, 54, 279, 79, 77, 75]","[1697105124817, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125614, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998]"
2465,162,44,[],200,EleutherAI/gpt-neox-20b,64,1,2356.0,1.0,1,H100,1697105074539,1697105076895,120,90.0,20.0,"[12, 516, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60, 57, 56, 265, 45, 53, 52, 341, 64]","[1697105074551, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076490, 1697105076831, 1697105076895]"
2466,666,61,[],200,EleutherAI/gpt-neox-20b,64,1,2444.0,1.0,1,H100,1697105126999,1697105129443,120,84.0,20.0,"[12, 595, 61, 78, 79, 77, 72, 68, 284, 77, 71, 70, 67, 52, 394, 72, 126, 57, 69, 63]","[1697105127011, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128610, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129443]"
2467,701,35,[],200,EleutherAI/gpt-neox-20b,64,1,4766.0,1.0,1,H100,1697105073780,1697105078546,120,58.0,43.0,"[36, 488, 63, 50, 61, 60, 53, 476, 67, 64, 52, 64, 63, 59, 357, 61, 48, 60, 57, 56, 265, 45, 53, 51, 342, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57]","[1697105073816, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075854, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546]"
2468,850,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070417,1697105070951,120,,,[17],[1697105070434]
2469,290,40,[],200,EleutherAI/gpt-neox-20b,64,1,289.0,1.0,1,H100,1697105096454,1697105096743,120,14.0,1.0,"[5, 284]","[1697105096459, 1697105096743]"
2470,829,45,[],200,EleutherAI/gpt-neox-20b,64,1,789.0,1.0,1,H100,1697105098357,1697105099146,120,20.0,1.0,"[4, 785]","[1697105098361, 1697105099146]"
2471,880,41,[],200,EleutherAI/gpt-neox-20b,64,1,646.0,1.0,1,H100,1697105096744,1697105097390,120,84.0,2.0,"[8, 638]","[1697105096752, 1697105097390]"
2472,649,42,[],200,EleutherAI/gpt-neox-20b,64,1,2622.0,1.0,1,H100,1697105097391,1697105100013,120,244.0,20.0,"[4, 715, 74, 68, 52, 52, 241, 55, 57, 68, 62, 394, 72, 126, 66, 55, 285, 63, 51, 62]","[1697105097395, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098597, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013]"
2473,621,31,[],200,EleutherAI/gpt-neox-20b,64,1,2825.0,1.0,1,H100,1697105070953,1697105073778,120,88.0,20.0,"[129, 951, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 62, 49]","[1697105071082, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778]"
2474,876,37,[],200,EleutherAI/gpt-neox-20b,64,1,821.0,1.0,1,H100,1697105078371,1697105079192,120,11.0,1.0,"[5, 816]","[1697105078376, 1697105079192]"
2475,601,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099147,1697105102062,120,,,"[9, 681, 63, 51, 62, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105099156, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2476,275,32,[],200,EleutherAI/gpt-neox-20b,64,1,638.0,1.0,1,H100,1697105073779,1697105074417,120,161.0,4.0,"[9, 515, 64, 50]","[1697105073788, 1697105074303, 1697105074367, 1697105074417]"
2477,52,33,[],200,EleutherAI/gpt-neox-20b,64,1,897.0,1.0,1,H100,1697105074418,1697105075315,120,58.0,6.0,"[4, 645, 67, 65, 51, 65]","[1697105074422, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315]"
2478,308,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100014,1697105102062,120,,,"[9, 561, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105100023, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2479,254,47,[],200,EleutherAI/gpt-neox-20b,64,1,1195.0,1.0,1,H100,1697105102073,1697105103268,120,58.0,1.0,"[106, 1089]","[1697105102179, 1697105103268]"
2480,635,34,[],200,EleutherAI/gpt-neox-20b,64,1,404.0,1.0,1,H100,1697105075316,1697105075720,120,23.0,1.0,"[4, 400]","[1697105075320, 1697105075720]"
2481,30,48,[],200,EleutherAI/gpt-neox-20b,64,1,3437.0,1.0,1,H100,1697105103269,1697105106706,120,93.0,20.0,"[20, 865, 72, 71, 69, 64, 62, 62, 612, 80, 76, 59, 71, 66, 53, 572, 111, 58, 44, 350]","[1697105103289, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104554, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706]"
2482,409,35,[],200,EleutherAI/gpt-neox-20b,64,1,4059.0,1.0,1,H100,1697105075721,1697105079780,120,109.0,30.0,"[12, 607, 45, 53, 52, 341, 64, 55, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59]","[1697105075733, 1697105076340, 1697105076385, 1697105076438, 1697105076490, 1697105076831, 1697105076895, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780]"
2483,612,49,[],200,EleutherAI/gpt-neox-20b,64,1,2736.0,1.0,1,H100,1697105106708,1697105109444,120,93.0,20.0,"[6, 493, 61, 57, 56, 54, 53, 259, 62, 61, 49, 56, 45, 423, 58, 49, 48, 717, 65, 64]","[1697105106714, 1697105107207, 1697105107268, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107747, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108501, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
2484,624,37,[],200,EleutherAI/gpt-neox-20b,64,1,13911.0,1.0,1,H100,1697105083758,1697105097669,120,563.0,119.0,"[8, 647, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 561, 74, 69, 68, 67, 63, 349, 69, 53, 53, 66, 62, 291, 71, 55, 71, 69, 67, 63, 546, 63, 61, 48, 55, 264, 61, 60, 59, 56, 55, 272, 58, 47, 57, 46, 55, 319, 65, 57, 56, 45, 56, 529, 65, 48, 61, 59, 58, 529, 70, 68, 65, 64, 311, 74, 68, 52, 52, 65, 299, 56, 67, 52, 66, 64, 360, 77, 69, 66, 52, 52, 609, 74, 57, 72, 71, 55, 54, 246, 54, 67, 65, 50, 65, 336, 66, 50, 64, 64, 511, 75, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67]","[1697105083766, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085978, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089766, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090687, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091464, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092046, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093324, 1697105093393, 1697105093459, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669]"
2485,87,52,[],200,EleutherAI/gpt-neox-20b,64,1,3007.0,1.0,1,H100,1697105095103,1697105098110,120,335.0,19.0,"[16, 1075, 74, 56, 74, 56, 71, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 441]","[1697105095119, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096454, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110]"
2486,512,26,[],200,EleutherAI/gpt-neox-20b,64,1,971.0,1.0,1,H100,1697105087586,1697105088557,120,11.0,1.0,"[16, 955]","[1697105087602, 1697105088557]"
2487,288,27,[],200,EleutherAI/gpt-neox-20b,64,1,2837.0,1.0,1,H100,1697105088558,1697105091395,120,93.0,20.0,"[9, 618, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530, 64, 49, 60, 59, 58, 529]","[1697105088567, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394]"
2488,924,42,[],200,EleutherAI/gpt-neox-20b,64,1,272.0,1.0,1,H100,1697105099498,1697105099770,120,9.0,1.0,"[16, 256]","[1697105099514, 1697105099770]"
2489,84,50,[],200,EleutherAI/gpt-neox-20b,64,1,319.0,1.0,1,H100,1697105106255,1697105106574,120,26.0,1.0,"[5, 314]","[1697105106260, 1697105106574]"
2490,581,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099771,1697105102062,120,,,"[4, 809, 116, 56, 62, 59, 463, 67, 66, 59, 48, 58]","[1697105099775, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101580, 1697105101638]"
2491,789,51,[],200,EleutherAI/gpt-neox-20b,64,1,6846.0,1.0,1,H100,1697105106575,1697105113421,120,6.0,50.0,"[17, 615, 61, 57, 56, 54, 53, 259, 62, 61, 50, 55, 45, 423, 58, 49, 48, 717, 65, 64, 54, 572, 72, 121, 70, 53, 59, 459, 215, 128, 73, 54, 65, 62, 49, 244, 64, 50, 50, 50, 58, 423, 72, 57, 72, 71, 62, 450, 76, 72]","[1697105106592, 1697105107207, 1697105107268, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107747, 1697105107809, 1697105107870, 1697105107920, 1697105107975, 1697105108020, 1697105108443, 1697105108501, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110070, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111374, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112561, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421]"
2492,583,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063193,1697105065209,120,,,"[37, 844, 46, 432, 61, 59, 57, 45, 54]","[1697105063230, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2493,304,16,[],200,EleutherAI/gpt-neox-20b,64,1,2257.0,1.0,1,H100,1697105055409,1697105057666,120,86.0,20.0,"[5, 595, 71, 62, 59, 48, 57, 304, 70, 68, 67, 64, 51, 64, 347, 73, 53, 68, 66, 65]","[1697105055414, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056680, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666]"
2494,75,33,[],200,EleutherAI/gpt-neox-20b,64,1,2396.0,1.0,1,H100,1697105085906,1697105088302,120,345.0,18.0,"[16, 672, 69, 55, 52, 65, 63, 290, 71, 55, 71, 69, 67, 63, 547, 62, 62, 47]","[1697105085922, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835, 1697105086898, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088193, 1697105088255, 1697105088302]"
2495,579,37,[],200,EleutherAI/gpt-neox-20b,64,1,747.0,1.0,1,H100,1697105073486,1697105074233,120,19.0,1.0,"[8, 739]","[1697105073494, 1697105074233]"
2496,232,38,[],200,EleutherAI/gpt-neox-20b,64,1,2660.0,1.0,1,H100,1697105074234,1697105076894,120,93.0,20.0,"[4, 829, 67, 64, 52, 64, 63, 59, 357, 61, 48, 60, 57, 56, 265, 45, 53, 51, 342, 63]","[1697105074238, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075854, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894]"
2497,201,41,[],200,EleutherAI/gpt-neox-20b,64,1,2642.0,1.0,1,H100,1697105087189,1697105089831,120,67.0,20.0,"[9, 933, 62, 62, 47, 55, 264, 61, 60, 59, 56, 55, 273, 57, 47, 57, 46, 55, 320, 64]","[1697105087198, 1697105088131, 1697105088193, 1697105088255, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831]"
2498,160,51,[],200,EleutherAI/gpt-neox-20b,64,1,545.0,1.0,1,H100,1697105105519,1697105106064,120,13.0,1.0,"[20, 525]","[1697105105539, 1697105106064]"
2499,660,37,[],200,EleutherAI/gpt-neox-20b,64,1,3302.0,1.0,1,H100,1697105096195,1697105099497,120,732.0,25.0,"[4, 624, 70, 68, 67, 65, 297, 74, 70, 68, 67, 441, 74, 68, 52, 52, 242, 54, 57, 68, 62, 394, 72, 126, 66]","[1697105096199, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097093, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497]"
2500,743,52,[],200,EleutherAI/gpt-neox-20b,64,1,855.0,1.0,1,H100,1697105106065,1697105106920,120,123.0,6.0,"[21, 620, 58, 57, 55, 44]","[1697105106086, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920]"
2501,518,53,[],200,EleutherAI/gpt-neox-20b,64,1,762.0,1.0,1,H100,1697105106922,1697105107684,120,23.0,1.0,"[8, 754]","[1697105106930, 1697105107684]"
2502,836,15,[],200,EleutherAI/gpt-neox-20b,64,1,581.0,1.0,1,H100,1697105053405,1697105053986,120,11.0,1.0,"[5, 575]","[1697105053410, 1697105053985]"
2503,150,54,[],200,EleutherAI/gpt-neox-20b,64,1,758.0,1.0,1,H100,1697105107685,1697105108443,120,216.0,2.0,"[4, 754]","[1697105107689, 1697105108443]"
2504,848,55,[],200,EleutherAI/gpt-neox-20b,64,1,647.0,1.0,1,H100,1697105108444,1697105109091,120,47.0,1.0,"[5, 642]","[1697105108449, 1697105109091]"
2505,646,38,[],200,EleutherAI/gpt-neox-20b,64,1,904.0,1.0,1,H100,1697105079193,1697105080097,120,14.0,1.0,"[9, 895]","[1697105079202, 1697105080097]"
2506,584,16,[],200,EleutherAI/gpt-neox-20b,64,1,676.0,1.0,1,H100,1697105053987,1697105054663,120,10.0,1.0,"[8, 668]","[1697105053995, 1697105054663]"
2507,241,17,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105054664,1697105055224,120,19.0,1.0,"[12, 548]","[1697105054676, 1697105055224]"
2508,508,56,[],200,EleutherAI/gpt-neox-20b,64,1,2916.0,1.0,1,H100,1697105109092,1697105112008,120,86.0,20.0,"[4, 975, 71, 121, 70, 53, 59, 459, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50]","[1697105109096, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008]"
2509,12,18,[],200,EleutherAI/gpt-neox-20b,64,1,692.0,1.0,1,H100,1697105055225,1697105055917,120,11.0,1.0,"[17, 675]","[1697105055242, 1697105055917]"
2510,779,42,[],200,EleutherAI/gpt-neox-20b,64,1,1700.0,1.0,1,H100,1697105089832,1697105091532,120,563.0,10.0,"[9, 734, 64, 49, 60, 59, 58, 529, 71, 67]","[1697105089841, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532]"
2511,602,19,[],200,EleutherAI/gpt-neox-20b,64,1,622.0,1.0,1,H100,1697105055918,1697105056540,120,15.0,1.0,"[9, 613]","[1697105055927, 1697105056540]"
2512,192,48,[],200,EleutherAI/gpt-neox-20b,64,1,2583.0,1.0,1,H100,1697105096195,1697105098778,120,93.0,20.0,"[4, 624, 70, 68, 67, 65, 297, 74, 70, 68, 67, 441, 74, 68, 52, 52, 242, 54, 57, 69]","[1697105096199, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097093, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098652, 1697105098709, 1697105098778]"
2513,348,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105070417,1697105070951,120,,,[5],[1697105070422]
2514,555,43,[],200,EleutherAI/gpt-neox-20b,64,1,362.0,1.0,1,H100,1697105091533,1697105091895,120,11.0,1.0,"[5, 357]","[1697105091538, 1697105091895]"
2515,208,44,[],200,EleutherAI/gpt-neox-20b,64,1,2659.0,1.0,1,H100,1697105091896,1697105094555,120,96.0,20.0,"[9, 677, 56, 67, 52, 66, 64, 360, 78, 68, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54]","[1697105091905, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093325, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555]"
2516,96,27,[],200,EleutherAI/gpt-neox-20b,64,1,1007.0,1.0,1,H100,1697105070953,1697105071960,120,31.0,1.0,"[122, 885]","[1697105071075, 1697105071960]"
2517,371,20,[],200,EleutherAI/gpt-neox-20b,64,1,654.0,1.0,1,H100,1697105056541,1697105057195,120,13.0,1.0,"[24, 630]","[1697105056565, 1697105057195]"
2518,57,41,[],200,EleutherAI/gpt-neox-20b,64,1,520.0,1.0,1,H100,1697105101474,1697105101994,120,13.0,1.0,"[16, 504]","[1697105101490, 1697105101994]"
2519,33,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057196,1697105058840,120,,,"[9, 888, 65, 64, 63]","[1697105057205, 1697105058093, 1697105058158, 1697105058222, 1697105058285]"
2520,678,28,[],200,EleutherAI/gpt-neox-20b,64,1,2456.0,1.0,1,H100,1697105071961,1697105074417,120,244.0,18.0,"[12, 925, 73, 68, 63, 60, 48, 60, 47, 234, 52, 64, 62, 49, 44, 482, 63, 50]","[1697105071973, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417]"
2521,212,31,[],200,EleutherAI/gpt-neox-20b,64,1,405.0,1.0,1,H100,1697105082122,1697105082527,120,31.0,1.0,"[9, 396]","[1697105082131, 1697105082527]"
2522,872,28,[],200,EleutherAI/gpt-neox-20b,64,1,2775.0,1.0,1,H100,1697105091397,1697105094172,120,91.0,20.0,"[16, 560, 74, 67, 52, 52, 65, 299, 56, 68, 51, 66, 64, 360, 78, 68, 67, 51, 52, 609]","[1697105091413, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092706, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093325, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172]"
2523,378,40,[],200,EleutherAI/gpt-neox-20b,64,1,6200.0,1.0,1,H100,1697105094556,1697105100756,120,93.0,47.0,"[13, 870, 65, 50, 64, 64, 511, 75, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75, 68, 52, 51, 242, 55, 57, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 56]","[1697105094569, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098597, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756]"
2524,758,42,[],200,EleutherAI/gpt-neox-20b,64,1,3523.0,1.0,1,H100,1697105101995,1697105105518,120,84.0,20.0,"[4, 808, 707, 67, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 65]","[1697105101999, 1697105102807, 1697105103514, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105517]"
2525,803,32,[],200,EleutherAI/gpt-neox-20b,64,1,878.0,1.0,1,H100,1697105082528,1697105083406,120,20.0,1.0,"[12, 866]","[1697105082540, 1697105083406]"
2526,574,33,[],200,EleutherAI/gpt-neox-20b,64,1,1006.0,1.0,1,H100,1697105083407,1697105084413,120,364.0,2.0,"[4, 1002]","[1697105083411, 1697105084413]"
2527,62,28,[],200,EleutherAI/gpt-neox-20b,64,1,3486.0,1.0,1,H100,1697105065212,1697105068698,120,91.0,20.0,"[89, 1272, 68, 66, 65, 61, 59, 534, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55, 53, 289]","[1697105065301, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698]"
2528,852,39,[],200,EleutherAI/gpt-neox-20b,64,1,2871.0,1.0,1,H100,1697105075437,1697105078308,120,100.0,20.0,"[5, 898, 45, 53, 51, 342, 64, 55, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56]","[1697105075442, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076895, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308]"
2529,100,42,[],200,EleutherAI/gpt-neox-20b,64,1,2338.0,1.0,1,H100,1697105097093,1697105099431,120,732.0,14.0,"[5, 1011, 75, 68, 52, 51, 243, 55, 56, 68, 62, 394, 72, 126]","[1697105097098, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098598, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431]"
2530,506,40,[],200,EleutherAI/gpt-neox-20b,64,1,882.0,1.0,1,H100,1697105078310,1697105079192,120,16.0,1.0,"[20, 862]","[1697105078330, 1697105079192]"
2531,282,41,[],200,EleutherAI/gpt-neox-20b,64,1,3574.0,1.0,1,H100,1697105079193,1697105082767,120,87.0,20.0,"[5, 972, 70, 71, 69, 66, 51, 636, 142, 75, 69, 68, 67, 425, 71, 71, 68, 65, 434, 78]","[1697105079198, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081350, 1697105081419, 1697105081487, 1697105081554, 1697105081979, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766]"
2532,863,42,[],200,EleutherAI/gpt-neox-20b,64,1,638.0,1.0,1,H100,1697105082768,1697105083406,120,10.0,1.0,"[8, 630]","[1697105082776, 1697105083406]"
2533,611,43,[],200,EleutherAI/gpt-neox-20b,64,1,755.0,1.0,1,H100,1697105083407,1697105084162,120,14.0,1.0,"[8, 746]","[1697105083415, 1697105084161]"
2534,264,44,[],200,EleutherAI/gpt-neox-20b,64,1,3026.0,1.0,1,H100,1697105084162,1697105087188,120,86.0,20.0,"[5, 870, 56, 73, 54, 55, 68, 561, 74, 69, 68, 67, 63, 349, 69, 53, 53, 66, 62, 291]","[1697105084167, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085978, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188]"
2535,205,43,[],200,EleutherAI/gpt-neox-20b,64,1,2763.0,1.0,1,H100,1697105098710,1697105101473,120,87.0,20.0,"[4, 519, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 57, 61, 59, 463, 67, 66]","[1697105098714, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100757, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473]"
2536,36,45,[],200,EleutherAI/gpt-neox-20b,64,1,2642.0,1.0,1,H100,1697105087189,1697105089831,120,457.0,20.0,"[5, 937, 62, 61, 48, 55, 264, 61, 60, 59, 56, 55, 273, 57, 47, 57, 46, 55, 320, 64]","[1697105087194, 1697105088131, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831]"
2537,800,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099432,1697105102062,120,,,"[4, 401, 63, 51, 62, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105099436, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2538,629,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105089833,1697105102061,120,,,"[16, 726, 64, 49, 60, 59, 58, 529, 71, 67, 65, 64, 312, 74, 67, 52, 52, 65, 299, 57, 66, 52, 66, 64, 361, 76, 69, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54, 246, 54, 67, 65, 50, 65, 336, 66, 50, 64, 64, 511, 75, 56, 74, 55, 71, 55, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75, 68, 52, 51, 242, 55, 57, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63, 56, 55, 460, 116, 56, 61, 60, 463, 67, 66, 59, 47, 59]","[1697105089849, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096524, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098597, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100069, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100817, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2539,660,32,[],200,EleutherAI/gpt-neox-20b,64,1,3296.0,1.0,1,H100,1697105081979,1697105085275,120,732.0,25.0,"[9, 700, 78, 72, 71, 54, 53, 69, 414, 72, 67, 52, 66, 65, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55]","[1697105081988, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083571, 1697105083638, 1697105083690, 1697105083756, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275]"
2540,744,45,[],200,EleutherAI/gpt-neox-20b,64,1,932.0,1.0,1,H100,1697105076895,1697105077827,120,161.0,6.0,"[17, 682, 63, 49, 61, 59]","[1697105076912, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826]"
2541,343,63,[],200,EleutherAI/gpt-neox-20b,64,1,3501.0,1.0,1,H100,1697105102070,1697105105571,120,84.0,20.0,"[113, 1332, 66, 51, 50, 64, 408, 73, 70, 69, 64, 62, 61, 613, 81, 76, 58, 71, 66, 53]","[1697105102183, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104227, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571]"
2542,368,38,[],200,EleutherAI/gpt-neox-20b,64,1,3306.0,1.0,1,H100,1697105090866,1697105094172,120,88.0,20.0,"[4, 1102, 75, 67, 52, 52, 65, 299, 56, 68, 51, 66, 64, 361, 76, 69, 67, 51, 52, 609]","[1697105090870, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092706, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172]"
2543,384,50,[],200,EleutherAI/gpt-neox-20b,64,1,2563.0,1.0,1,H100,1697105109445,1697105112008,120,92.0,20.0,"[13, 613, 72, 120, 73, 50, 60, 459, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50]","[1697105109458, 1697105110071, 1697105110143, 1697105110263, 1697105110336, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008]"
2544,755,31,[],200,EleutherAI/gpt-neox-20b,64,1,4208.0,1.0,1,H100,1697105065212,1697105069420,120,286.0,25.0,"[106, 1255, 68, 66, 65, 61, 59, 534, 68, 63, 59, 55, 407, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60]","[1697105065318, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420]"
2545,79,44,[],200,EleutherAI/gpt-neox-20b,64,1,1194.0,1.0,1,H100,1697105102074,1697105103268,120,12.0,1.0,"[117, 1077]","[1697105102191, 1697105103268]"
2546,44,51,[],200,EleutherAI/gpt-neox-20b,64,1,391.0,1.0,1,H100,1697105112010,1697105112401,120,12.0,1.0,"[16, 375]","[1697105112026, 1697105112401]"
2547,744,52,[],200,EleutherAI/gpt-neox-20b,64,1,1159.0,1.0,1,H100,1697105112402,1697105113561,120,161.0,6.0,"[8, 863, 76, 72, 70, 70]","[1697105112410, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561]"
2548,729,22,[],200,EleutherAI/gpt-neox-20b,64,1,1305.0,1.0,1,H100,1697105058843,1697105060148,120,874.0,2.0,"[73, 1232]","[1697105058916, 1697105060148]"
2549,390,23,[],200,EleutherAI/gpt-neox-20b,64,1,2675.0,1.0,1,H100,1697105060149,1697105062824,120,84.0,20.0,"[5, 610, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060154, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
2550,227,34,[],200,EleutherAI/gpt-neox-20b,64,1,3107.0,1.0,1,H100,1697105084414,1697105087521,120,364.0,25.0,"[9, 614, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 349, 69, 55, 52, 65, 62, 291, 71, 55, 71, 69, 67]","[1697105084423, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521]"
2551,29,39,[],200,EleutherAI/gpt-neox-20b,64,1,865.0,1.0,1,H100,1697105094173,1697105095038,120,161.0,6.0,"[4, 625, 53, 67, 65, 50]","[1697105094177, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037]"
2552,59,59,[],200,EleutherAI/gpt-neox-20b,64,1,2928.0,1.0,1,H100,1697105105572,1697105108500,120,91.0,20.0,"[12, 1122, 58, 57, 55, 44, 287, 61, 58, 55, 54, 53, 260, 61, 61, 49, 56, 45, 423, 57]","[1697105105584, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500]"
2553,729,40,[],200,EleutherAI/gpt-neox-20b,64,1,401.0,1.0,1,H100,1697105095038,1697105095439,120,874.0,2.0,"[5, 396]","[1697105095043, 1697105095439]"
2554,156,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062825,1697105065209,120,,,"[9, 1240, 46, 432, 61, 59, 57, 45, 54]","[1697105062834, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2555,666,34,[],200,EleutherAI/gpt-neox-20b,64,1,2272.0,1.0,1,H100,1697105088303,1697105090575,120,84.0,20.0,"[5, 314, 60, 61, 58, 56, 56, 271, 58, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530]","[1697105088308, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575]"
2556,461,44,[],200,EleutherAI/gpt-neox-20b,64,1,4855.0,1.0,1,H100,1697105102065,1697105106920,120,216.0,30.0,"[30, 712, 708, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66, 52, 573, 111, 58, 44, 350, 58, 57, 55, 44]","[1697105102095, 1697105102807, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105570, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920]"
2557,107,64,[],200,EleutherAI/gpt-neox-20b,64,1,1135.0,1.0,1,H100,1697105105571,1697105106706,120,216.0,2.0,"[9, 1126]","[1697105105580, 1697105106706]"
2558,696,65,[],200,EleutherAI/gpt-neox-20b,64,1,2735.0,1.0,1,H100,1697105106709,1697105109444,120,83.0,20.0,"[9, 489, 61, 57, 56, 54, 53, 260, 61, 61, 49, 56, 45, 423, 58, 49, 48, 717, 65, 64]","[1697105106718, 1697105107207, 1697105107268, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108501, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
2559,109,39,[],200,EleutherAI/gpt-neox-20b,64,1,2879.0,1.0,1,H100,1697105081886,1697105084765,120,90.0,20.0,"[5, 797, 78, 72, 71, 54, 53, 69, 415, 71, 67, 52, 67, 64, 592, 77, 56, 74, 73, 72]","[1697105081891, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083500, 1697105083571, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765]"
2560,817,40,[],200,EleutherAI/gpt-neox-20b,64,1,2871.0,1.0,1,H100,1697105089768,1697105092639,120,86.0,20.0,"[4, 802, 65, 49, 60, 59, 58, 529, 71, 67, 65, 64, 312, 74, 67, 52, 52, 65, 299, 57]","[1697105089772, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639]"
2561,642,45,[],200,EleutherAI/gpt-neox-20b,64,1,3437.0,1.0,1,H100,1697105103269,1697105106706,120,89.0,20.0,"[12, 873, 72, 71, 69, 64, 62, 62, 612, 80, 76, 59, 71, 66, 53, 572, 111, 58, 44, 350]","[1697105103281, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104554, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706]"
2562,383,41,[],200,EleutherAI/gpt-neox-20b,64,1,609.0,1.0,1,H100,1697105085906,1697105086515,120,15.0,1.0,"[20, 589]","[1697105085926, 1697105086515]"
2563,159,42,[],200,EleutherAI/gpt-neox-20b,64,1,596.0,1.0,1,H100,1697105086516,1697105087112,120,31.0,1.0,"[13, 583]","[1697105086529, 1697105087112]"
2564,747,43,[],200,EleutherAI/gpt-neox-20b,64,1,4859.0,1.0,1,H100,1697105087113,1697105091972,120,140.0,36.0,"[4, 1014, 62, 61, 48, 55, 264, 61, 60, 59, 56, 55, 273, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 529, 65, 49, 60, 59, 58, 529, 71, 67, 65, 64, 311]","[1697105087117, 1697105088131, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972]"
2565,875,30,[],200,EleutherAI/gpt-neox-20b,64,1,4129.0,1.0,1,H100,1697105082768,1697105086897,120,31.0,31.0,"[8, 630, 93, 72, 68, 51, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562, 72, 70, 68, 67, 63, 348, 70, 53, 54, 65, 62]","[1697105082776, 1697105083406, 1697105083499, 1697105083571, 1697105083639, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086770, 1697105086835, 1697105086897]"
2566,904,35,[],200,EleutherAI/gpt-neox-20b,64,1,609.0,1.0,1,H100,1697105087522,1697105088131,120,563.0,2.0,"[5, 604]","[1697105087527, 1697105088131]"
2567,556,36,[],200,EleutherAI/gpt-neox-20b,64,1,425.0,1.0,1,H100,1697105088132,1697105088557,120,9.0,1.0,"[8, 417]","[1697105088140, 1697105088557]"
2568,79,33,[],200,EleutherAI/gpt-neox-20b,64,1,340.0,1.0,1,H100,1697105098185,1697105098525,120,12.0,1.0,"[17, 323]","[1697105098202, 1697105098525]"
2569,778,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105098526,1697105102061,120,,,"[13, 694, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 57, 61, 59, 463, 67, 66, 59, 47, 59]","[1697105098539, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100757, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2570,529,16,[],200,EleutherAI/gpt-neox-20b,64,1,664.0,1.0,1,H100,1697105046831,1697105047495,120,10.0,1.0,"[13, 651]","[1697105046844, 1697105047495]"
2571,551,43,[],200,EleutherAI/gpt-neox-20b,64,1,2749.0,1.0,1,H100,1697105090576,1697105093325,120,90.0,20.0,"[16, 802, 71, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 56, 67, 52, 66, 64, 361, 76]","[1697105090592, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324]"
2572,182,17,[],200,EleutherAI/gpt-neox-20b,64,1,4453.0,1.0,1,H100,1697105047496,1697105051949,120,47.0,31.0,"[12, 1055, 130, 73, 54, 55, 55, 70, 353, 76, 72, 71, 54, 55, 64, 618, 237, 74, 72, 71, 63, 311, 76, 75, 71, 55, 71, 66, 204, 70, 70]","[1697105047508, 1697105048563, 1697105048693, 1697105048766, 1697105048820, 1697105048875, 1697105048930, 1697105049000, 1697105049353, 1697105049429, 1697105049501, 1697105049572, 1697105049626, 1697105049681, 1697105049745, 1697105050363, 1697105050600, 1697105050674, 1697105050746, 1697105050817, 1697105050880, 1697105051191, 1697105051267, 1697105051342, 1697105051413, 1697105051468, 1697105051539, 1697105051605, 1697105051809, 1697105051879, 1697105051949]"
2573,702,40,[],200,EleutherAI/gpt-neox-20b,64,1,2818.0,1.0,1,H100,1697105084766,1697105087584,120,89.0,20.0,"[5, 1133, 73, 70, 68, 67, 63, 349, 69, 56, 51, 65, 62, 291, 71, 55, 71, 69, 67, 63]","[1697105084771, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086719, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584]"
2574,3,39,[],200,EleutherAI/gpt-neox-20b,64,1,2827.0,1.0,1,H100,1697105076895,1697105079722,120,89.0,20.0,"[5, 694, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 72, 68, 67, 52]","[1697105076900, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722]"
2575,740,25,[],200,EleutherAI/gpt-neox-20b,64,1,2866.0,1.0,1,H100,1697105065212,1697105068078,120,563.0,14.0,"[57, 577, 727, 68, 66, 65, 62, 58, 534, 68, 63, 59, 55, 407]","[1697105065269, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078]"
2576,304,39,[],200,EleutherAI/gpt-neox-20b,64,1,3401.0,1.0,1,H100,1697105080098,1697105083499,120,86.0,20.0,"[8, 1027, 141, 75, 70, 68, 67, 424, 72, 71, 68, 65, 434, 79, 71, 71, 54, 53, 69, 414]","[1697105080106, 1697105081133, 1697105081274, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082767, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499]"
2577,642,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068699,1697105070951,120,,,"[9, 591, 61, 60, 59, 56, 675, 70, 69, 67, 56]","[1697105068708, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069535, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
2578,465,48,[],200,EleutherAI/gpt-neox-20b,64,1,646.0,1.0,1,H100,1697105089185,1697105089831,120,364.0,3.0,"[5, 577, 64]","[1697105089190, 1697105089767, 1697105089831]"
2579,234,49,[],200,EleutherAI/gpt-neox-20b,64,1,3416.0,1.0,1,H100,1697105089832,1697105093248,120,457.0,25.0,"[4, 591, 147, 65, 49, 60, 59, 58, 529, 71, 67, 65, 64, 312, 74, 67, 52, 52, 65, 299, 57, 66, 52, 66, 64, 361]","[1697105089836, 1697105090427, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248]"
2580,914,45,[],200,EleutherAI/gpt-neox-20b,64,1,2908.0,1.0,1,H100,1697105094556,1697105097464,120,84.0,20.0,"[9, 874, 65, 50, 64, 64, 511, 75, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74]","[1697105094565, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464]"
2581,74,40,[],200,EleutherAI/gpt-neox-20b,64,1,3094.0,1.0,1,H100,1697105083500,1697105086594,120,88.0,20.0,"[5, 908, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562, 72, 70, 68, 67, 63, 349]","[1697105083505, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594]"
2582,524,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069421,1697105070950,120,,,"[5, 784, 71, 68, 67, 56]","[1697105069426, 1697105070210, 1697105070281, 1697105070349, 1697105070416, 1697105070472]"
2583,569,46,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105097465,1697105098025,120,16.0,1.0,"[12, 548]","[1697105097477, 1697105098025]"
2584,823,50,[],200,EleutherAI/gpt-neox-20b,64,1,2946.0,1.0,1,H100,1697105093248,1697105096194,120,90.0,20.0,"[5, 919, 74, 57, 72, 71, 55, 54, 247, 53, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512]","[1697105093253, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194]"
2585,339,47,[],200,EleutherAI/gpt-neox-20b,64,1,2730.0,1.0,1,H100,1697105098026,1697105100756,120,87.0,20.0,"[5, 567, 55, 56, 68, 62, 394, 72, 126, 66, 55, 285, 63, 51, 62, 57, 54, 460, 116, 56]","[1697105098031, 1697105098598, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756]"
2586,186,33,[],200,EleutherAI/gpt-neox-20b,64,1,3351.0,1.0,1,H100,1697105070953,1697105074304,120,123.0,22.0,"[118, 962, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 58, 48, 234, 52, 64, 62, 50, 43, 482]","[1697105071071, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073779, 1697105073822, 1697105074304]"
2587,747,41,[],200,EleutherAI/gpt-neox-20b,64,1,4212.0,1.0,1,H100,1697105086595,1697105090807,120,140.0,36.0,"[9, 584, 72, 54, 71, 69, 67, 63, 546, 63, 61, 48, 55, 265, 60, 61, 58, 56, 55, 273, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 529, 65, 49, 60, 59]","[1697105086604, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088912, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807]"
2588,277,57,[],200,EleutherAI/gpt-neox-20b,64,1,391.0,1.0,1,H100,1697105112010,1697105112401,120,18.0,1.0,"[12, 379]","[1697105112022, 1697105112401]"
2589,466,66,[],200,EleutherAI/gpt-neox-20b,64,1,2563.0,1.0,1,H100,1697105109445,1697105112008,120,457.0,20.0,"[9, 617, 72, 120, 74, 49, 60, 459, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50]","[1697105109454, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008]"
2590,414,46,[],200,EleutherAI/gpt-neox-20b,64,1,2735.0,1.0,1,H100,1697105106709,1697105109444,120,87.0,20.0,"[16, 482, 61, 58, 55, 54, 53, 260, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 65, 64]","[1697105106725, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
2591,866,58,[],200,EleutherAI/gpt-neox-20b,64,1,2822.0,1.0,1,H100,1697105112402,1697105115224,120,93.0,20.0,"[4, 867, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 47, 61, 520, 140, 70, 68, 68, 66]","[1697105112406, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114231, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224]"
2592,86,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100757,1697105102061,120,,,"[5, 578, 67, 66, 59, 47, 59]","[1697105100762, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2593,672,49,[],200,EleutherAI/gpt-neox-20b,64,1,3452.0,1.0,1,H100,1697105102066,1697105105518,120,93.0,20.0,"[41, 701, 707, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66]","[1697105102107, 1697105102808, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518]"
2594,303,39,[],200,EleutherAI/gpt-neox-20b,64,1,3094.0,1.0,1,H100,1697105083500,1697105086594,120,88.0,20.0,"[5, 908, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562, 72, 70, 68, 67, 63, 349]","[1697105083505, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594]"
2595,382,41,[],200,EleutherAI/gpt-neox-20b,64,1,2744.0,1.0,1,H100,1697105095440,1697105098184,120,47.0,20.0,"[8, 746, 74, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 441, 74]","[1697105095448, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184]"
2596,390,50,[],200,EleutherAI/gpt-neox-20b,64,1,2507.0,1.0,1,H100,1697105092048,1697105094555,120,84.0,20.0,"[12, 522, 56, 67, 52, 66, 64, 361, 76, 69, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54]","[1697105092060, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555]"
2597,67,47,[],200,EleutherAI/gpt-neox-20b,64,1,29854.0,1.0,1,H100,1697105109447,1697105139301,120,140.0,381.0,"[27, 597, 72, 120, 74, 50, 59, 459, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 328, 75, 72, 64, 330, 73, 71, 72, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 154, 81, 77, 406, 79, 77, 77, 74, 57, 71, 359, 78, 58, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 58, 59, 74, 57, 74, 72, 65, 476, 80, 60, 77, 58, 70, 54, 278, 80, 76, 76, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 76, 72, 70, 66, 52, 395, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71, 65, 63, 49, 50, 49, 88, 64, 57, 44, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 31, 32, 31, 32, 38, 31, 32, 31, 31, 31, 38, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 32, 31, 31, 31, 31, 32, 31, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 38, 31, 31, 30, 31, 31, 36, 30, 30, 32, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 30, 30, 30, 30, 31, 30, 30, 30, 31, 30, 30, 31, 30, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31, 31, 31, 30, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 37, 30, 29, 30, 30, 30, 30, 30, 30, 34, 28, 29, 28, 28, 29, 28, 28, 29, 28, 28, 29, 28, 29, 28, 29, 28, 29, 28, 29]","[1697105109474, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110387, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118362, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122487, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125212, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126364, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126922, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128401, 1697105128473, 1697105128543, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133520, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134492, 1697105134524, 1697105134555, 1697105134587, 1697105134625, 1697105134656, 1697105134688, 1697105134719, 1697105134750, 1697105134781, 1697105134819, 1697105134850, 1697105134881, 1697105134912, 1697105134943, 1697105134974, 1697105135005, 1697105135036, 1697105135067, 1697105135098, 1697105135128, 1697105135159, 1697105135190, 1697105135221, 1697105135252, 1697105135283, 1697105135315, 1697105135346, 1697105135377, 1697105135408, 1697105135439, 1697105135471, 1697105135502, 1697105135533, 1697105135564, 1697105135596, 1697105135627, 1697105135658, 1697105135690, 1697105135721, 1697105135752, 1697105135784, 1697105135815, 1697105135853, 1697105135884, 1697105135915, 1697105135945, 1697105135976, 1697105136007, 1697105136043, 1697105136073, 1697105136103, 1697105136135, 1697105136164, 1697105136194, 1697105136224, 1697105136254, 1697105136284, 1697105136314, 1697105136344, 1697105136374, 1697105136404, 1697105136434, 1697105136464, 1697105136494, 1697105136524, 1697105136554, 1697105136584, 1697105136614, 1697105136644, 1697105136674, 1697105136705, 1697105136735, 1697105136765, 1697105136795, 1697105136825, 1697105136856, 1697105136886, 1697105136916, 1697105136946, 1697105136977, 1697105137007, 1697105137037, 1697105137068, 1697105137098, 1697105137128, 1697105137159, 1697105137189, 1697105137220, 1697105137250, 1697105137281, 1697105137311, 1697105137342, 1697105137372, 1697105137403, 1697105137433, 1697105137464, 1697105137495, 1697105137525, 1697105137556, 1697105137586, 1697105137617, 1697105137648, 1697105137679, 1697105137709, 1697105137740, 1697105137771, 1697105137802, 1697105137832, 1697105137863, 1697105137894, 1697105137925, 1697105137956, 1697105137987, 1697105138018, 1697105138048, 1697105138079, 1697105138110, 1697105138141, 1697105138172, 1697105138203, 1697105138234, 1697105138265, 1697105138296, 1697105138327, 1697105138358, 1697105138389, 1697105138420, 1697105138451, 1697105138488, 1697105138518, 1697105138547, 1697105138577, 1697105138607, 1697105138637, 1697105138667, 1697105138697, 1697105138727, 1697105138761, 1697105138789, 1697105138818, 1697105138846, 1697105138874, 1697105138903, 1697105138931, 1697105138959, 1697105138988, 1697105139016, 1697105139044, 1697105139073, 1697105139101, 1697105139130, 1697105139158, 1697105139187, 1697105139215, 1697105139244, 1697105139272, 1697105139301]"
2598,41,51,[],200,EleutherAI/gpt-neox-20b,64,1,5514.0,1.0,1,H100,1697105094556,1697105100070,120,39.0,43.0,"[17, 866, 65, 50, 64, 64, 511, 75, 57, 73, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75, 68, 52, 51, 242, 55, 57, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63, 57]","[1697105094573, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096325, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098597, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070]"
2599,50,40,[],200,EleutherAI/gpt-neox-20b,64,1,719.0,1.0,1,H100,1697105086595,1697105087314,120,90.0,4.0,"[9, 584, 71, 55]","[1697105086604, 1697105087188, 1697105087259, 1697105087314]"
2600,628,41,[],200,EleutherAI/gpt-neox-20b,64,1,1486.0,1.0,1,H100,1697105087315,1697105088801,120,732.0,10.0,"[5, 811, 62, 62, 47, 56, 263, 61, 60, 59]","[1697105087320, 1697105088131, 1697105088193, 1697105088255, 1697105088302, 1697105088358, 1697105088621, 1697105088682, 1697105088742, 1697105088801]"
2601,160,42,[],200,EleutherAI/gpt-neox-20b,64,1,340.0,1.0,1,H100,1697105098185,1697105098525,120,13.0,1.0,"[13, 327]","[1697105098198, 1697105098525]"
2602,745,43,[],200,EleutherAI/gpt-neox-20b,64,1,620.0,1.0,1,H100,1697105098526,1697105099146,120,17.0,1.0,"[17, 603]","[1697105098543, 1697105099146]"
2603,521,44,[],200,EleutherAI/gpt-neox-20b,64,1,622.0,1.0,1,H100,1697105099148,1697105099770,120,18.0,1.0,"[16, 606]","[1697105099164, 1697105099770]"
2604,317,38,[],200,EleutherAI/gpt-neox-20b,64,1,6247.0,1.0,1,H100,1697105090576,1697105096823,120,244.0,50.0,"[12, 806, 71, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 57, 66, 52, 66, 64, 361, 76, 69, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54, 246, 54, 67, 65, 50, 65, 336, 66, 50, 64, 64, 511, 75, 56, 74, 55, 71, 55, 244]","[1697105090588, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096524, 1697105096579, 1697105096823]"
2605,174,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099771,1697105102062,120,,,"[8, 805, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105099779, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2606,113,48,[],200,EleutherAI/gpt-neox-20b,64,1,791.0,1.0,1,H100,1697105118036,1697105118827,120,13.0,1.0,"[20, 771]","[1697105118056, 1697105118827]"
2607,522,46,[],200,EleutherAI/gpt-neox-20b,64,1,297.0,1.0,1,H100,1697105077828,1697105078125,120,20.0,1.0,"[8, 289]","[1697105077836, 1697105078125]"
2608,851,46,[],200,EleutherAI/gpt-neox-20b,64,1,1194.0,1.0,1,H100,1697105102074,1697105103268,120,23.0,1.0,"[109, 1085]","[1697105102183, 1697105103268]"
2609,176,47,[],200,EleutherAI/gpt-neox-20b,64,1,1337.0,1.0,1,H100,1697105078126,1697105079463,120,216.0,2.0,"[5, 1332]","[1697105078131, 1697105079463]"
2610,876,48,[],200,EleutherAI/gpt-neox-20b,64,1,633.0,1.0,1,H100,1697105079464,1697105080097,120,11.0,1.0,"[5, 628]","[1697105079469, 1697105080097]"
2611,511,49,[],200,EleutherAI/gpt-neox-20b,64,1,8645.0,1.0,1,H100,1697105080098,1697105088743,120,364.0,64.0,"[25, 1010, 142, 74, 70, 68, 67, 424, 72, 71, 68, 65, 434, 79, 71, 71, 54, 53, 69, 414, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 348, 70, 53, 53, 66, 62, 291, 71, 55, 70, 70, 67, 63, 546, 63, 61, 48, 55, 264, 61, 60]","[1697105080123, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082767, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742]"
2612,506,47,[],200,EleutherAI/gpt-neox-20b,64,1,674.0,1.0,1,H100,1697105103269,1697105103943,120,16.0,1.0,"[37, 637]","[1697105103306, 1697105103943]"
2613,276,48,[],200,EleutherAI/gpt-neox-20b,64,1,2763.0,1.0,1,H100,1697105103944,1697105106707,120,732.0,13.0,"[16, 1206, 81, 76, 58, 71, 66, 53, 572, 111, 58, 44, 350]","[1697105103960, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706]"
2614,318,38,[],200,EleutherAI/gpt-neox-20b,64,1,572.0,1.0,1,H100,1697105099498,1697105100070,120,6.0,6.0,"[4, 335, 63, 51, 62, 57]","[1697105099502, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070]"
2615,632,41,[],200,EleutherAI/gpt-neox-20b,64,1,2345.0,1.0,1,H100,1697105075963,1697105078308,120,91.0,20.0,"[4, 373, 45, 53, 52, 341, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55]","[1697105075967, 1697105076340, 1697105076385, 1697105076438, 1697105076490, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308]"
2616,88,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100071,1697105102062,120,,,"[8, 505, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105100079, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2617,572,24,[],200,EleutherAI/gpt-neox-20b,64,1,669.0,1.0,1,H100,1697105057343,1697105058012,120,16.0,1.0,"[5, 663]","[1697105057348, 1697105058011]"
2618,866,49,[],200,EleutherAI/gpt-neox-20b,64,1,2735.0,1.0,1,H100,1697105106709,1697105109444,120,93.0,20.0,"[12, 486, 61, 57, 56, 54, 53, 259, 62, 61, 49, 56, 45, 423, 58, 49, 48, 717, 65, 64]","[1697105106721, 1697105107207, 1697105107268, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107747, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108501, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
2619,24,35,[],200,EleutherAI/gpt-neox-20b,64,1,2232.0,1.0,1,H100,1697105102065,1697105104297,120,79.0,9.0,"[130, 1320, 66, 51, 50, 64, 408, 72, 71]","[1697105102195, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104226, 1697105104297]"
2620,674,40,[],200,EleutherAI/gpt-neox-20b,64,1,16901.0,1.0,1,H100,1697105102075,1697105118976,120,161.0,119.0,"[129, 1311, 66, 51, 50, 64, 407, 73, 71, 69, 64, 63, 60, 613, 81, 76, 58, 71, 66, 53, 572, 111, 58, 44, 350, 58, 57, 55, 44, 286, 61, 58, 56, 53, 54, 259, 62, 61, 49, 56, 44, 424, 57, 50, 48, 717, 65, 64, 54, 572, 72, 121, 70, 53, 59, 459, 215, 128, 73, 54, 65, 62, 49, 243, 65, 50, 50, 50, 58, 423, 72, 57, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 47, 61, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 71, 55, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 328, 75, 71, 65, 330, 73]","[1697105102204, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104493, 1697105104553, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325, 1697105107381, 1697105107434, 1697105107488, 1697105107747, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108019, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110070, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111374, 1697105111439, 1697105111501, 1697105111550, 1697105111793, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112561, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114231, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116020, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118362, 1697105118437, 1697105118508, 1697105118573, 1697105118903, 1697105118976]"
2621,341,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058013,1697105058840,120,,,[16],[1697105058029]
2622,926,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058843,1697105063190,120,,,"[24, 600, 681, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 54, 350, 61, 60, 58, 47]","[1697105058867, 1697105059467, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062248, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
2623,725,36,[],200,EleutherAI/gpt-neox-20b,64,1,3028.0,1.0,1,H100,1697105104298,1697105107326,120,90.0,20.0,"[5, 863, 81, 76, 58, 71, 66, 53, 572, 111, 58, 44, 350, 58, 57, 55, 44, 286, 61, 58]","[1697105104303, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325]"
2624,702,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105063192,1697105065209,120,,,"[22, 860, 46, 432, 61, 59, 57, 45, 54]","[1697105063214, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2625,32,41,[],200,EleutherAI/gpt-neox-20b,64,1,822.0,1.0,1,H100,1697105100758,1697105101580,120,140.0,6.0,"[16, 566, 67, 66, 59, 48]","[1697105100774, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101580]"
2626,595,51,[],200,EleutherAI/gpt-neox-20b,64,1,548.0,1.0,1,H100,1697105096195,1697105096743,120,8.0,1.0,"[16, 532]","[1697105096211, 1697105096743]"
2627,331,28,[],200,EleutherAI/gpt-neox-20b,64,1,1133.0,1.0,1,H100,1697105065212,1697105066345,120,26.0,1.0,"[178, 955]","[1697105065390, 1697105066345]"
2628,733,42,[],200,EleutherAI/gpt-neox-20b,64,1,413.0,1.0,1,H100,1697105101581,1697105101994,120,31.0,1.0,"[4, 409]","[1697105101585, 1697105101994]"
2629,257,52,[],200,EleutherAI/gpt-neox-20b,64,1,569.0,1.0,1,H100,1697105096744,1697105097313,120,14.0,1.0,"[17, 552]","[1697105096761, 1697105097313]"
2630,357,44,[],200,EleutherAI/gpt-neox-20b,64,1,5310.0,1.0,1,H100,1697105102071,1697105107381,120,52.0,33.0,"[104, 1340, 66, 51, 50, 64, 408, 72, 71, 69, 64, 62, 61, 613, 81, 76, 58, 71, 66, 52, 573, 111, 58, 44, 350, 58, 56, 56, 44, 286, 61, 58, 56]","[1697105102175, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105570, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106820, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325, 1697105107381]"
2631,161,49,[],200,EleutherAI/gpt-neox-20b,64,1,924.0,1.0,1,H100,1697105092639,1697105093563,120,109.0,7.0,"[5, 604, 76, 69, 67, 51, 52]","[1697105092644, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563]"
2632,651,31,[],200,EleutherAI/gpt-neox-20b,64,1,1232.0,1.0,1,H100,1697105086899,1697105088131,120,457.0,2.0,"[4, 1228]","[1697105086903, 1697105088131]"
2633,742,50,[],200,EleutherAI/gpt-neox-20b,64,1,3259.0,1.0,1,H100,1697105093564,1697105096823,120,89.0,20.0,"[5, 1232, 54, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512, 74, 56, 74, 55, 72, 54, 244]","[1697105093569, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823]"
2634,403,42,[],200,EleutherAI/gpt-neox-20b,64,1,587.0,1.0,1,H100,1697105090808,1697105091395,120,874.0,2.0,"[5, 582]","[1697105090813, 1697105091395]"
2635,172,43,[],200,EleutherAI/gpt-neox-20b,64,1,496.0,1.0,1,H100,1697105091399,1697105091895,120,19.0,1.0,"[18, 478]","[1697105091417, 1697105091895]"
2636,305,32,[],200,EleutherAI/gpt-neox-20b,64,1,2443.0,1.0,1,H100,1697105088132,1697105090575,120,86.0,20.0,"[4, 486, 60, 61, 58, 56, 56, 271, 58, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530]","[1697105088136, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575]"
2637,99,29,[],200,EleutherAI/gpt-neox-20b,64,1,928.0,1.0,1,H100,1697105066346,1697105067274,120,10.0,1.0,"[20, 908]","[1697105066366, 1697105067274]"
2638,762,44,[],200,EleutherAI/gpt-neox-20b,64,1,2659.0,1.0,1,H100,1697105091896,1697105094555,120,92.0,20.0,"[13, 673, 56, 67, 52, 66, 64, 360, 77, 69, 67, 52, 51, 609, 74, 57, 72, 71, 55, 54]","[1697105091909, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093324, 1697105093393, 1697105093460, 1697105093512, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555]"
2639,517,44,[],200,EleutherAI/gpt-neox-20b,64,1,532.0,1.0,1,H100,1697105091974,1697105092506,120,15.0,1.0,"[4, 528]","[1697105091978, 1697105092506]"
2640,682,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105067275,1697105070950,120,,,"[13, 790, 60, 58, 59, 46, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105067288, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
2641,807,36,[],200,EleutherAI/gpt-neox-20b,64,1,3086.0,1.0,1,H100,1697105097670,1697105100756,120,90.0,20.0,"[4, 924, 55, 56, 68, 62, 394, 72, 126, 66, 55, 285, 63, 51, 62, 57, 54, 460, 116, 56]","[1697105097674, 1697105098598, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756]"
2642,460,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100757,1697105102061,120,,,"[9, 574, 67, 66, 59, 47, 59]","[1697105100766, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2643,488,26,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105068079,1697105068639,120,6.0,1.0,"[4, 556]","[1697105068083, 1697105068639]"
2644,257,27,[],200,EleutherAI/gpt-neox-20b,64,1,580.0,1.0,1,H100,1697105068640,1697105069220,120,14.0,1.0,"[12, 567]","[1697105068652, 1697105069219]"
2645,435,35,[],200,EleutherAI/gpt-neox-20b,64,1,3727.0,1.0,1,H100,1697105090576,1697105094303,120,563.0,27.0,"[20, 798, 71, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 56, 68, 51, 66, 64, 361, 76, 69, 67, 51, 52, 609, 74, 57]","[1697105090596, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092706, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303]"
2646,846,28,[],200,EleutherAI/gpt-neox-20b,64,1,1252.0,1.0,1,H100,1697105069220,1697105070472,120,140.0,6.0,"[5, 985, 70, 69, 67, 56]","[1697105069225, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
2647,793,53,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105098111,1697105102061,120,,,"[4, 483, 55, 56, 69, 61, 394, 72, 126, 66, 55, 285, 63, 51, 62, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105098115, 1697105098598, 1697105098653, 1697105098709, 1697105098778, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2648,615,29,[],200,EleutherAI/gpt-neox-20b,64,1,3305.0,1.0,1,H100,1697105070473,1697105073778,120,93.0,20.0,"[5, 1555, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 233, 53, 64, 61, 50]","[1697105070478, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073550, 1697105073603, 1697105073667, 1697105073728, 1697105073778]"
2649,593,40,[],200,EleutherAI/gpt-neox-20b,64,1,1552.0,1.0,1,H100,1697105079723,1697105081275,120,335.0,9.0,"[4, 443, 70, 71, 69, 66, 51, 636, 142]","[1697105079727, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275]"
2650,91,36,[],200,EleutherAI/gpt-neox-20b,64,1,426.0,1.0,1,H100,1697105094304,1697105094730,120,23.0,1.0,"[5, 420]","[1697105094309, 1697105094729]"
2651,764,37,[],200,EleutherAI/gpt-neox-20b,64,1,634.0,1.0,1,H100,1697105094731,1697105095365,120,39.0,1.0,"[12, 622]","[1697105094743, 1697105095365]"
2652,534,38,[],200,EleutherAI/gpt-neox-20b,64,1,2818.0,1.0,1,H100,1697105095366,1697105098184,120,96.0,20.0,"[9, 819, 74, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 441, 74]","[1697105095375, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184]"
2653,206,38,[],200,EleutherAI/gpt-neox-20b,64,1,690.0,1.0,1,H100,1697105102065,1697105102755,120,16.0,1.0,"[46, 644]","[1697105102111, 1697105102755]"
2654,788,39,[],200,EleutherAI/gpt-neox-20b,64,1,512.0,1.0,1,H100,1697105102756,1697105103268,120,31.0,1.0,"[17, 495]","[1697105102773, 1697105103268]"
2655,276,30,[],200,EleutherAI/gpt-neox-20b,64,1,1598.0,1.0,1,H100,1697105073780,1697105075378,120,732.0,13.0,"[16, 508, 63, 50, 61, 60, 53, 476, 67, 65, 51, 65, 63]","[1697105073796, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075378]"
2656,566,40,[],200,EleutherAI/gpt-neox-20b,64,1,4751.0,1.0,1,H100,1697105103269,1697105108020,120,109.0,36.0,"[28, 857, 72, 71, 69, 64, 63, 61, 612, 81, 75, 59, 71, 66, 53, 572, 111, 58, 44, 350, 58, 57, 55, 44, 286, 61, 58, 56, 54, 53, 259, 62, 61, 49, 56, 44]","[1697105103297, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104493, 1697105104554, 1697105105166, 1697105105247, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107747, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108019]"
2657,192,39,[],200,EleutherAI/gpt-neox-20b,64,1,2572.0,1.0,1,H100,1697105098185,1697105100757,120,93.0,20.0,"[8, 405, 55, 56, 69, 61, 394, 72, 126, 66, 55, 285, 63, 51, 62, 57, 54, 460, 116, 56]","[1697105098193, 1697105098598, 1697105098653, 1697105098709, 1697105098778, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756]"
2658,230,45,[],200,EleutherAI/gpt-neox-20b,64,1,998.0,1.0,1,H100,1697105106921,1697105107919,120,86.0,5.0,"[5, 822, 61, 61, 49]","[1697105106926, 1697105107748, 1697105107809, 1697105107870, 1697105107919]"
2659,819,46,[],200,EleutherAI/gpt-neox-20b,64,1,453.0,1.0,1,H100,1697105107921,1697105108374,120,13.0,1.0,"[12, 441]","[1697105107933, 1697105108374]"
2660,584,47,[],200,EleutherAI/gpt-neox-20b,64,1,716.0,1.0,1,H100,1697105108375,1697105109091,120,10.0,1.0,"[13, 703]","[1697105108388, 1697105109091]"
2661,51,31,[],200,EleutherAI/gpt-neox-20b,64,1,4401.0,1.0,1,H100,1697105075379,1697105079780,120,364.0,36.0,"[8, 406, 61, 48, 60, 57, 56, 265, 45, 53, 51, 342, 64, 55, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59]","[1697105075387, 1697105075793, 1697105075854, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076895, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780]"
2662,246,48,[],200,EleutherAI/gpt-neox-20b,64,1,6670.0,1.0,1,H100,1697105109092,1697105115762,120,58.0,47.0,"[8, 971, 71, 121, 70, 53, 60, 458, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538]","[1697105109100, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762]"
2663,256,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062195,1697105063190,120,,,"[29, 374, 61, 60, 58, 47]","[1697105062224, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
2664,356,28,[],200,EleutherAI/gpt-neox-20b,64,1,634.0,1.0,1,H100,1697105065212,1697105065846,120,874.0,2.0,"[17, 617]","[1697105065229, 1697105065846]"
2665,98,29,[],200,EleutherAI/gpt-neox-20b,64,1,1427.0,1.0,1,H100,1697105065847,1697105067274,120,14.0,1.0,"[5, 1422]","[1697105065852, 1697105067274]"
2666,681,30,[],200,EleutherAI/gpt-neox-20b,64,1,722.0,1.0,1,H100,1697105067276,1697105067998,120,23.0,1.0,"[20, 702]","[1697105067296, 1697105067998]"
2667,458,31,[],200,EleutherAI/gpt-neox-20b,64,1,640.0,1.0,1,H100,1697105067999,1697105068639,120,11.0,1.0,"[12, 628]","[1697105068011, 1697105068639]"
2668,113,32,[],200,EleutherAI/gpt-neox-20b,64,1,579.0,1.0,1,H100,1697105068640,1697105069219,120,13.0,1.0,"[12, 567]","[1697105068652, 1697105069219]"
2669,812,33,[],200,EleutherAI/gpt-neox-20b,64,1,748.0,1.0,1,H100,1697105069221,1697105069969,120,16.0,1.0,"[20, 728]","[1697105069241, 1697105069969]"
2670,472,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069970,1697105070950,120,,,[4],[1697105069974]
2671,240,35,[],200,EleutherAI/gpt-neox-20b,64,1,2826.0,1.0,1,H100,1697105070953,1697105073779,120,83.0,20.0,"[37, 1043, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 62, 50]","[1697105070990, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073779]"
2672,405,42,[],200,EleutherAI/gpt-neox-20b,64,1,2593.0,1.0,1,H100,1697105088802,1697105091395,120,87.0,20.0,"[5, 378, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 529, 65, 49, 60, 59, 58, 530]","[1697105088807, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091395]"
2673,76,33,[],200,EleutherAI/gpt-neox-20b,64,1,1707.0,1.0,1,H100,1697105090576,1697105092283,120,364.0,12.0,"[8, 810, 71, 67, 65, 64, 311, 74, 68, 52, 52, 65]","[1697105090584, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092046, 1697105092114, 1697105092166, 1697105092218, 1697105092283]"
2674,901,39,[],200,EleutherAI/gpt-neox-20b,64,1,488.0,1.0,1,H100,1697105096825,1697105097313,120,17.0,1.0,"[16, 472]","[1697105096841, 1697105097313]"
2675,671,40,[],200,EleutherAI/gpt-neox-20b,64,1,711.0,1.0,1,H100,1697105097314,1697105098025,120,12.0,1.0,"[13, 698]","[1697105097327, 1697105098025]"
2676,52,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105057667,1697105058840,120,,,[21],[1697105057688]
2677,303,41,[],200,EleutherAI/gpt-neox-20b,64,1,2731.0,1.0,1,H100,1697105098026,1697105100757,120,88.0,20.0,"[9, 563, 55, 56, 68, 62, 394, 72, 126, 66, 55, 285, 63, 51, 62, 57, 54, 460, 116, 56]","[1697105098035, 1697105098598, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756]"
2678,752,18,[],200,EleutherAI/gpt-neox-20b,64,1,1364.0,1.0,1,H100,1697105058843,1697105060207,120,39.0,3.0,"[141, 1164, 59]","[1697105058984, 1697105060148, 1697105060207]"
2679,405,19,[],200,EleutherAI/gpt-neox-20b,64,1,2616.0,1.0,1,H100,1697105060208,1697105062824,120,87.0,20.0,"[5, 551, 64, 169, 111, 111, 64, 62, 62, 397, 74, 122, 124, 70, 55, 349, 61, 60, 58, 47]","[1697105060213, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
2680,419,30,[],200,EleutherAI/gpt-neox-20b,64,1,2826.0,1.0,1,H100,1697105070953,1697105073779,120,88.0,20.0,"[69, 1011, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 61, 51]","[1697105071022, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073779]"
2681,472,41,[],200,EleutherAI/gpt-neox-20b,64,1,2990.0,1.0,1,H100,1697105087585,1697105090575,120,85.0,20.0,"[9, 1027, 61, 61, 58, 56, 56, 271, 58, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 529]","[1697105087594, 1697105088621, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574]"
2682,279,50,[],200,EleutherAI/gpt-neox-20b,64,1,2065.0,1.0,1,H100,1697105088743,1697105090808,120,67.0,18.0,"[5, 437, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530, 64, 49, 60, 59]","[1697105088748, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807]"
2683,217,42,[],200,EleutherAI/gpt-neox-20b,64,1,2749.0,1.0,1,H100,1697105090575,1697105093324,120,85.0,20.0,"[5, 814, 71, 67, 65, 64, 312, 73, 68, 52, 52, 65, 299, 57, 66, 52, 66, 64, 361, 76]","[1697105090580, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091973, 1697105092046, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324]"
2684,453,29,[],200,EleutherAI/gpt-neox-20b,64,1,563.0,1.0,1,H100,1697105074418,1697105074981,120,26.0,1.0,"[4, 559]","[1697105074422, 1697105074981]"
2685,183,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062825,1697105065209,120,,,"[17, 1232, 46, 432, 61, 59, 57, 45, 54]","[1697105062842, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2686,74,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100757,1697105102062,120,,,"[13, 570, 67, 66, 59, 47, 59]","[1697105100770, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2687,763,21,[],200,EleutherAI/gpt-neox-20b,64,1,1133.0,1.0,1,H100,1697105065212,1697105066345,120,20.0,1.0,"[174, 959]","[1697105065386, 1697105066345]"
2688,110,30,[],200,EleutherAI/gpt-neox-20b,64,1,920.0,1.0,1,H100,1697105074982,1697105075902,120,96.0,4.0,"[5, 806, 60, 49]","[1697105074987, 1697105075793, 1697105075853, 1697105075902]"
2689,816,31,[],200,EleutherAI/gpt-neox-20b,64,1,535.0,1.0,1,H100,1697105075903,1697105076438,120,182.0,4.0,"[4, 433, 45, 53]","[1697105075907, 1697105076340, 1697105076385, 1697105076438]"
2690,471,32,[],200,EleutherAI/gpt-neox-20b,64,1,2108.0,1.0,1,H100,1697105076439,1697105078547,120,86.0,20.0,"[4, 388, 63, 56, 56, 54, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 58]","[1697105076443, 1697105076831, 1697105076894, 1697105076950, 1697105077006, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078547]"
2691,28,53,[],200,EleutherAI/gpt-neox-20b,64,1,2699.0,1.0,1,H100,1697105097314,1697105100013,120,86.0,20.0,"[9, 787, 74, 68, 52, 52, 241, 56, 56, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63]","[1697105097323, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098597, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013]"
2692,389,43,[],200,EleutherAI/gpt-neox-20b,64,1,175.0,1.0,1,H100,1697105101995,1697105102170,120,8.0,1.0,"[12, 163]","[1697105102007, 1697105102170]"
2693,158,44,[],200,EleutherAI/gpt-neox-20b,64,1,3400.0,1.0,1,H100,1697105102171,1697105105571,120,85.0,20.0,"[101, 1243, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 62, 612, 81, 76, 58, 71, 66, 53]","[1697105102272, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104554, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571]"
2694,657,43,[],200,EleutherAI/gpt-neox-20b,64,1,1202.0,1.0,1,H100,1697105102066,1697105103268,120,10.0,1.0,"[105, 1097]","[1697105102171, 1697105103268]"
2695,332,37,[],200,EleutherAI/gpt-neox-20b,64,1,559.0,1.0,1,H100,1697105088558,1697105089117,120,39.0,1.0,"[25, 534]","[1697105088583, 1697105089117]"
2696,618,60,[],200,EleutherAI/gpt-neox-20b,64,1,589.0,1.0,1,H100,1697105108502,1697105109091,120,9.0,1.0,"[16, 573]","[1697105108518, 1697105109091]"
2697,391,61,[],200,EleutherAI/gpt-neox-20b,64,1,2916.0,1.0,1,H100,1697105109092,1697105112008,120,79.0,20.0,"[12, 967, 71, 121, 70, 53, 60, 459, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50]","[1697105109104, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008]"
2698,915,38,[],200,EleutherAI/gpt-neox-20b,64,1,580.0,1.0,1,H100,1697105089118,1697105089698,120,182.0,1.0,"[9, 571]","[1697105089127, 1697105089698]"
2699,241,33,[],200,EleutherAI/gpt-neox-20b,64,1,1549.0,1.0,1,H100,1697105078548,1697105080097,120,19.0,1.0,"[16, 1533]","[1697105078564, 1697105080097]"
2700,831,34,[],200,EleutherAI/gpt-neox-20b,64,1,715.0,1.0,1,H100,1697105080098,1697105080813,120,11.0,1.0,"[21, 694]","[1697105080119, 1697105080813]"
2701,683,39,[],200,EleutherAI/gpt-neox-20b,64,1,876.0,1.0,1,H100,1697105089699,1697105090575,120,874.0,2.0,"[9, 866]","[1697105089708, 1697105090574]"
2702,251,30,[],200,EleutherAI/gpt-neox-20b,64,1,809.0,1.0,1,H100,1697105068410,1697105069219,120,31.0,1.0,"[17, 792]","[1697105068427, 1697105069219]"
2703,321,33,[],200,EleutherAI/gpt-neox-20b,64,1,771.0,1.0,1,H100,1697105085276,1697105086047,120,182.0,4.0,"[5, 624, 73, 69]","[1697105085281, 1697105085905, 1697105085978, 1697105086047]"
2704,836,31,[],200,EleutherAI/gpt-neox-20b,64,1,748.0,1.0,1,H100,1697105069221,1697105069969,120,11.0,1.0,"[16, 732]","[1697105069237, 1697105069969]"
2705,94,34,[],200,EleutherAI/gpt-neox-20b,64,1,2574.0,1.0,1,H100,1697105086048,1697105088622,120,86.0,20.0,"[5, 541, 69, 55, 52, 65, 63, 290, 71, 55, 71, 69, 67, 63, 547, 62, 62, 47, 55, 265]","[1697105086053, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835, 1697105086898, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088193, 1697105088255, 1697105088302, 1697105088357, 1697105088622]"
2706,438,35,[],200,EleutherAI/gpt-neox-20b,64,1,690.0,1.0,1,H100,1697105102065,1697105102755,120,9.0,1.0,"[21, 669]","[1697105102086, 1697105102755]"
2707,207,36,[],200,EleutherAI/gpt-neox-20b,64,1,512.0,1.0,1,H100,1697105102756,1697105103268,120,10.0,1.0,"[12, 500]","[1697105102768, 1697105103268]"
2708,763,37,[],200,EleutherAI/gpt-neox-20b,64,1,674.0,1.0,1,H100,1697105103269,1697105103943,120,20.0,1.0,"[24, 650]","[1697105103293, 1697105103943]"
2709,533,38,[],200,EleutherAI/gpt-neox-20b,64,1,1222.0,1.0,1,H100,1697105103944,1697105105166,120,216.0,2.0,"[8, 1214]","[1697105103952, 1697105105166]"
2710,490,51,[],200,EleutherAI/gpt-neox-20b,64,1,778.0,1.0,1,H100,1697105096824,1697105097602,120,11.0,5.0,"[5, 561, 74, 70, 68]","[1697105096829, 1697105097390, 1697105097464, 1697105097534, 1697105097602]"
2711,188,39,[],200,EleutherAI/gpt-neox-20b,64,1,2752.0,1.0,1,H100,1697105105167,1697105107919,120,85.0,20.0,"[5, 971, 111, 58, 44, 350, 58, 57, 55, 44, 287, 60, 58, 55, 55, 53, 259, 62, 61, 49]","[1697105105172, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107267, 1697105107325, 1697105107380, 1697105107435, 1697105107488, 1697105107747, 1697105107809, 1697105107870, 1697105107919]"
2712,144,52,[],200,EleutherAI/gpt-neox-20b,64,1,2410.0,1.0,1,H100,1697105097603,1697105100013,120,96.0,20.0,"[5, 502, 74, 68, 52, 52, 242, 54, 57, 68, 62, 394, 72, 126, 66, 55, 285, 63, 51, 62]","[1697105097608, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013]"
2713,827,34,[],200,EleutherAI/gpt-neox-20b,64,1,2786.0,1.0,1,H100,1697105081979,1697105084765,120,96.0,20.0,"[9, 700, 78, 72, 71, 54, 53, 70, 413, 72, 67, 52, 66, 65, 592, 77, 56, 74, 73, 72]","[1697105081988, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083086, 1697105083499, 1697105083571, 1697105083638, 1697105083690, 1697105083756, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765]"
2714,605,32,[],200,EleutherAI/gpt-neox-20b,64,1,959.0,1.0,1,H100,1697105069970,1697105070929,120,8.0,1.0,"[12, 947]","[1697105069982, 1697105070929]"
2715,261,33,[],200,EleutherAI/gpt-neox-20b,64,1,1103.0,1.0,1,H100,1697105070930,1697105072033,120,874.0,2.0,"[5, 1098]","[1697105070935, 1697105072033]"
2716,63,36,[],200,EleutherAI/gpt-neox-20b,64,1,1032.0,1.0,1,H100,1697105079781,1697105080813,120,39.0,1.0,"[5, 1027]","[1697105079786, 1697105080813]"
2717,419,43,[],200,EleutherAI/gpt-neox-20b,64,1,2400.0,1.0,1,H100,1697105105519,1697105107919,120,88.0,20.0,"[4, 620, 111, 58, 44, 350, 58, 57, 55, 44, 287, 61, 58, 55, 54, 53, 260, 61, 61, 49]","[1697105105523, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919]"
2718,38,34,[],200,EleutherAI/gpt-neox-20b,64,1,2504.0,1.0,1,H100,1697105072034,1697105074538,120,88.0,20.0,"[5, 859, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60]","[1697105072039, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538]"
2719,868,51,[],200,EleutherAI/gpt-neox-20b,64,1,2517.0,1.0,1,H100,1697105090808,1697105093325,120,85.0,20.0,"[9, 578, 70, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 56, 68, 51, 66, 64, 361, 77]","[1697105090817, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092706, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093325]"
2720,643,29,[],200,EleutherAI/gpt-neox-20b,64,1,556.0,1.0,1,H100,1697105094173,1697105094729,120,18.0,1.0,"[13, 543]","[1697105094186, 1697105094729]"
2721,846,26,[],200,EleutherAI/gpt-neox-20b,64,1,1480.0,1.0,1,H100,1697105063192,1697105064672,120,140.0,6.0,"[26, 856, 46, 432, 61, 59]","[1697105063218, 1697105064074, 1697105064120, 1697105064552, 1697105064613, 1697105064672]"
2722,880,34,[],200,EleutherAI/gpt-neox-20b,64,1,763.0,1.0,1,H100,1697105074304,1697105075067,120,84.0,2.0,"[5, 758]","[1697105074309, 1697105075067]"
2723,617,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064673,1697105065210,120,,,[5],[1697105064678]
2724,540,35,[],200,EleutherAI/gpt-neox-20b,64,1,894.0,1.0,1,H100,1697105075068,1697105075962,120,140.0,5.0,"[4, 721, 60, 49, 60]","[1697105075072, 1697105075793, 1697105075853, 1697105075902, 1697105075962]"
2725,603,28,[],200,EleutherAI/gpt-neox-20b,64,1,1165.0,1.0,1,H100,1697105093564,1697105094729,120,9.0,1.0,"[5, 1160]","[1697105093569, 1697105094729]"
2726,177,45,[],200,EleutherAI/gpt-neox-20b,64,1,660.0,1.0,1,H100,1697105092507,1697105093167,120,14.0,1.0,"[17, 643]","[1697105092524, 1697105093167]"
2727,271,28,[],200,EleutherAI/gpt-neox-20b,64,1,3486.0,1.0,1,H100,1697105065212,1697105068698,120,87.0,20.0,"[101, 1260, 68, 66, 65, 61, 59, 534, 68, 63, 59, 55, 407, 60, 58, 58, 47, 55, 53, 289]","[1697105065313, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066833, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409, 1697105068698]"
2728,309,36,[],200,EleutherAI/gpt-neox-20b,64,1,2345.0,1.0,1,H100,1697105075963,1697105078308,120,52.0,20.0,"[8, 369, 45, 53, 52, 341, 63, 56, 56, 54, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55]","[1697105075971, 1697105076340, 1697105076385, 1697105076438, 1697105076490, 1697105076831, 1697105076894, 1697105076950, 1697105077006, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308]"
2729,327,44,[],200,EleutherAI/gpt-neox-20b,64,1,1530.0,1.0,1,H100,1697105093325,1697105094855,120,563.0,10.0,"[9, 838, 75, 56, 73, 70, 55, 54, 246, 54]","[1697105093334, 1697105094172, 1697105094247, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855]"
2730,317,62,[],200,EleutherAI/gpt-neox-20b,64,1,4759.0,1.0,1,H100,1697105129444,1697105134203,120,244.0,50.0,"[16, 1035, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70, 65, 63, 50, 49, 49, 88, 64, 58, 43, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34]","[1697105129460, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203]"
2731,874,46,[],200,EleutherAI/gpt-neox-20b,64,1,6329.0,1.0,1,H100,1697105093168,1697105099497,120,140.0,50.0,"[13, 991, 74, 57, 72, 71, 55, 54, 247, 53, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512, 74, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75, 68, 52, 51, 242, 55, 57, 68, 62, 394, 72, 126, 66]","[1697105093181, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098597, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497]"
2732,374,29,[],200,EleutherAI/gpt-neox-20b,64,1,2734.0,1.0,1,H100,1697105094730,1697105097464,120,85.0,20.0,"[9, 700, 65, 50, 64, 64, 511, 75, 56, 74, 55, 72, 54, 245, 69, 68, 67, 64, 298, 74]","[1697105094739, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096824, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464]"
2733,833,36,[],200,EleutherAI/gpt-neox-20b,64,1,1287.0,1.0,1,H100,1697105073780,1697105075067,120,563.0,8.0,"[32, 492, 63, 50, 61, 60, 53, 476]","[1697105073812, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067]"
2734,887,18,[],200,EleutherAI/gpt-neox-20b,64,1,6143.0,1.0,1,H100,1697105051950,1697105058093,120,244.0,50.0,"[4, 745, 74, 57, 57, 56, 67, 394, 71, 66, 63, 62, 60, 333, 56, 55, 70, 69, 53, 68, 382, 69, 68, 65, 51, 233, 62, 48, 62, 58, 481, 71, 62, 59, 48, 57, 304, 69, 69, 67, 64, 51, 64, 347, 73, 53, 68, 66, 65, 427]","[1697105051954, 1697105052699, 1697105052773, 1697105052830, 1697105052887, 1697105052943, 1697105053010, 1697105053404, 1697105053475, 1697105053541, 1697105053604, 1697105053666, 1697105053726, 1697105054059, 1697105054115, 1697105054170, 1697105054240, 1697105054309, 1697105054362, 1697105054430, 1697105054812, 1697105054881, 1697105054949, 1697105055014, 1697105055065, 1697105055298, 1697105055360, 1697105055408, 1697105055470, 1697105055528, 1697105056009, 1697105056080, 1697105056142, 1697105056201, 1697105056249, 1697105056306, 1697105056610, 1697105056679, 1697105056748, 1697105056815, 1697105056879, 1697105056930, 1697105056994, 1697105057341, 1697105057414, 1697105057467, 1697105057535, 1697105057601, 1697105057666, 1697105058093]"
2735,910,45,[],200,EleutherAI/gpt-neox-20b,64,1,509.0,1.0,1,H100,1697105094856,1697105095365,120,8.0,1.0,"[4, 505]","[1697105094860, 1697105095365]"
2736,478,36,[],200,EleutherAI/gpt-neox-20b,64,1,9037.0,1.0,1,H100,1697105078547,1697105087584,120,161.0,62.0,"[9, 1614, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 348, 70, 53, 53, 66, 62, 291, 71, 55, 70, 70, 67, 63]","[1697105078556, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584]"
2737,685,46,[],200,EleutherAI/gpt-neox-20b,64,1,828.0,1.0,1,H100,1697105095366,1697105096194,120,364.0,2.0,"[4, 824]","[1697105095370, 1697105096194]"
2738,362,41,[],200,EleutherAI/gpt-neox-20b,64,1,609.0,1.0,1,H100,1697105081276,1697105081885,120,14.0,1.0,"[4, 605]","[1697105081280, 1697105081885]"
2739,339,47,[],200,EleutherAI/gpt-neox-20b,64,1,2583.0,1.0,1,H100,1697105096195,1697105098778,120,87.0,20.0,"[8, 620, 70, 68, 67, 65, 297, 74, 70, 68, 67, 441, 74, 68, 52, 52, 242, 54, 57, 69]","[1697105096203, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097093, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098652, 1697105098709, 1697105098778]"
2740,666,34,[],200,EleutherAI/gpt-neox-20b,64,1,2818.0,1.0,1,H100,1697105092284,1697105095102,120,84.0,20.0,"[5, 959, 76, 69, 67, 51, 52, 609, 75, 56, 72, 71, 55, 54, 246, 54, 67, 65, 50, 65]","[1697105092289, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094247, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102]"
2741,61,43,[],200,EleutherAI/gpt-neox-20b,64,1,489.0,1.0,1,H100,1697105091406,1697105091895,120,9.0,1.0,"[15, 474]","[1697105091421, 1697105091895]"
2742,24,42,[],200,EleutherAI/gpt-neox-20b,64,1,1614.0,1.0,1,H100,1697105081886,1697105083500,120,79.0,9.0,"[5, 797, 78, 72, 71, 54, 53, 69, 415]","[1697105081891, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083500]"
2743,759,44,[],200,EleutherAI/gpt-neox-20b,64,1,2659.0,1.0,1,H100,1697105091896,1697105094555,120,92.0,20.0,"[9, 677, 56, 67, 52, 66, 64, 360, 77, 69, 67, 52, 51, 609, 74, 57, 72, 71, 55, 54]","[1697105091905, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093324, 1697105093393, 1697105093460, 1697105093512, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555]"
2744,717,43,[],200,EleutherAI/gpt-neox-20b,64,1,3093.0,1.0,1,H100,1697105083501,1697105086594,120,89.0,20.0,"[16, 896, 77, 56, 74, 73, 72, 272, 56, 73, 54, 56, 67, 562, 72, 70, 68, 67, 63, 349]","[1697105083517, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085276, 1697105085343, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594]"
2745,540,22,[],200,EleutherAI/gpt-neox-20b,64,1,1270.0,1.0,1,H100,1697105066346,1697105067616,120,140.0,5.0,"[12, 1068, 68, 63, 59]","[1697105066358, 1697105067426, 1697105067494, 1697105067557, 1697105067616]"
2746,435,35,[],200,EleutherAI/gpt-neox-20b,64,1,3675.0,1.0,1,H100,1697105095103,1697105098778,120,563.0,27.0,"[4, 1086, 75, 57, 73, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 441, 74, 68, 51, 53, 241, 55, 57, 68]","[1697105095107, 1697105096193, 1697105096268, 1697105096325, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184, 1697105098252, 1697105098303, 1697105098356, 1697105098597, 1697105098652, 1697105098709, 1697105098777]"
2747,621,35,[],200,EleutherAI/gpt-neox-20b,64,1,2356.0,1.0,1,H100,1697105074539,1697105076895,120,88.0,20.0,"[16, 512, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60, 57, 56, 265, 45, 53, 52, 341, 64]","[1697105074555, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076490, 1697105076831, 1697105076895]"
2748,304,30,[],200,EleutherAI/gpt-neox-20b,64,1,2734.0,1.0,1,H100,1697105094730,1697105097464,120,86.0,20.0,"[5, 704, 65, 50, 64, 64, 511, 75, 56, 74, 55, 72, 54, 245, 69, 68, 67, 64, 298, 74]","[1697105094735, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096824, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464]"
2749,634,32,[],200,EleutherAI/gpt-neox-20b,64,1,1032.0,1.0,1,H100,1697105079781,1697105080813,120,13.0,1.0,"[4, 1028]","[1697105079785, 1697105080813]"
2750,188,44,[],200,EleutherAI/gpt-neox-20b,64,1,3455.0,1.0,1,H100,1697105107920,1697105111375,120,85.0,20.0,"[9, 514, 57, 50, 49, 716, 65, 64, 54, 573, 72, 120, 74, 49, 60, 458, 215, 128, 73, 55]","[1697105107929, 1697105108443, 1697105108500, 1697105108550, 1697105108599, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375]"
2751,67,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105098789,1697105102062,120,,,"[5, 439, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105098794, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2752,773,45,[],200,EleutherAI/gpt-neox-20b,64,1,2310.0,1.0,1,H100,1697105111376,1697105113686,120,90.0,20.0,"[8, 410, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62]","[1697105111384, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686]"
2753,69,31,[],200,EleutherAI/gpt-neox-20b,64,1,2548.0,1.0,1,H100,1697105097465,1697105100013,120,85.0,20.0,"[8, 637, 74, 68, 52, 52, 242, 54, 57, 68, 62, 394, 73, 125, 66, 55, 285, 63, 51, 62]","[1697105097473, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099306, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013]"
2754,549,46,[],200,EleutherAI/gpt-neox-20b,64,1,3368.0,1.0,1,H100,1697105113687,1697105117055,120,93.0,20.0,"[17, 1108, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56]","[1697105113704, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055]"
2755,410,33,[],200,EleutherAI/gpt-neox-20b,64,1,2202.0,1.0,1,H100,1697105080814,1697105083016,120,364.0,12.0,"[9, 1155, 72, 71, 68, 65, 435, 77, 73, 70, 54, 53]","[1697105080823, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082689, 1697105082766, 1697105082839, 1697105082909, 1697105082963, 1697105083016]"
2756,638,52,[],200,EleutherAI/gpt-neox-20b,64,1,2869.0,1.0,1,H100,1697105093325,1697105096194,120,88.0,20.0,"[9, 838, 75, 56, 73, 70, 55, 54, 246, 54, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512]","[1697105093334, 1697105094172, 1697105094247, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194]"
2757,659,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100016,1697105102062,120,,,"[15, 553, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105100031, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2758,202,47,[],200,EleutherAI/gpt-neox-20b,64,1,9173.0,1.0,1,H100,1697105117056,1697105126229,120,874.0,72.0,"[8, 682, 81, 59, 78, 71, 328, 74, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 78, 58, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60]","[1697105117064, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118363, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229]"
2759,366,36,[],200,EleutherAI/gpt-neox-20b,64,1,931.0,1.0,1,H100,1697105076895,1697105077826,120,85.0,6.0,"[17, 682, 63, 49, 61, 59]","[1697105076912, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826]"
2760,434,44,[],200,EleutherAI/gpt-neox-20b,64,1,3437.0,1.0,1,H100,1697105103269,1697105106706,120,85.0,20.0,"[4, 881, 72, 71, 69, 64, 62, 62, 612, 81, 76, 58, 71, 66, 53, 572, 111, 58, 44, 350]","[1697105103273, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104554, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706]"
2761,638,59,[],200,EleutherAI/gpt-neox-20b,64,1,3894.0,1.0,1,H100,1697105115225,1697105119119,120,88.0,20.0,"[5, 1539, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 332, 71, 72, 64, 330, 73, 71, 72]","[1697105115230, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118366, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119]"
2762,448,50,[],200,EleutherAI/gpt-neox-20b,64,1,1749.0,1.0,1,H100,1697105105519,1697105107268,120,335.0,12.0,"[8, 616, 111, 58, 44, 350, 58, 57, 55, 44, 287, 61]","[1697105105527, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268]"
2763,812,49,[],200,EleutherAI/gpt-neox-20b,64,1,622.0,1.0,1,H100,1697105118828,1697105119450,120,16.0,1.0,"[16, 606]","[1697105118844, 1697105119450]"
2764,464,50,[],200,EleutherAI/gpt-neox-20b,64,1,701.0,1.0,1,H100,1697105119452,1697105120153,120,12.0,1.0,"[16, 685]","[1697105119468, 1697105120153]"
2765,288,60,[],200,EleutherAI/gpt-neox-20b,64,1,2228.0,1.0,1,H100,1697105119120,1697105121348,120,93.0,20.0,"[4, 408, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55]","[1697105119124, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348]"
2766,136,37,[],200,EleutherAI/gpt-neox-20b,64,1,297.0,1.0,1,H100,1697105077828,1697105078125,120,31.0,1.0,"[8, 289]","[1697105077836, 1697105078125]"
2767,603,37,[],200,EleutherAI/gpt-neox-20b,64,1,652.0,1.0,1,H100,1697105075068,1697105075720,120,9.0,1.0,"[4, 648]","[1697105075072, 1697105075720]"
2768,258,38,[],200,EleutherAI/gpt-neox-20b,64,1,7117.0,1.0,1,H100,1697105075721,1697105082838,120,244.0,50.0,"[8, 611, 45, 53, 51, 342, 64, 55, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 141, 75, 70, 68, 67, 424, 72, 71, 68, 65, 434, 78, 72]","[1697105075729, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076895, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838]"
2769,241,51,[],200,EleutherAI/gpt-neox-20b,64,1,861.0,1.0,1,H100,1697105120154,1697105121015,120,19.0,1.0,"[4, 857]","[1697105120158, 1697105121015]"
2770,823,52,[],200,EleutherAI/gpt-neox-20b,64,1,3081.0,1.0,1,H100,1697105121016,1697105124097,120,90.0,20.0,"[9, 903, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58]","[1697105121025, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097]"
2771,48,29,[],200,EleutherAI/gpt-neox-20b,64,1,836.0,1.0,1,H100,1697105068699,1697105069535,120,6.0,6.0,"[5, 595, 61, 60, 59, 56]","[1697105068704, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069535]"
2772,894,40,[],200,EleutherAI/gpt-neox-20b,64,1,454.0,1.0,1,H100,1697105107920,1697105108374,120,14.0,1.0,"[13, 441]","[1697105107933, 1697105108374]"
2773,628,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069536,1697105070950,120,,,[8],[1697105069544]
2774,405,31,[],200,EleutherAI/gpt-neox-20b,64,1,2827.0,1.0,1,H100,1697105070952,1697105073779,120,87.0,20.0,"[54, 1027, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 61, 51]","[1697105071006, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073779]"
2775,548,41,[],200,EleutherAI/gpt-neox-20b,64,1,3419.0,1.0,1,H100,1697105108375,1697105111794,120,86.0,20.0,"[5, 935, 65, 64, 54, 573, 72, 120, 74, 49, 60, 458, 215, 128, 73, 55, 64, 62, 49, 244]","[1697105108380, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794]"
2776,679,35,[],200,EleutherAI/gpt-neox-20b,64,1,494.0,1.0,1,H100,1697105088623,1697105089117,120,15.0,1.0,"[8, 486]","[1697105088631, 1697105089117]"
2777,455,36,[],200,EleutherAI/gpt-neox-20b,64,1,2929.0,1.0,1,H100,1697105089118,1697105092047,120,91.0,20.0,"[5, 644, 64, 57, 56, 45, 56, 529, 65, 48, 61, 59, 58, 530, 70, 67, 65, 64, 311, 75]","[1697105089123, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090687, 1697105090748, 1697105090807, 1697105090865, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047]"
2778,401,42,[],200,EleutherAI/gpt-neox-20b,64,1,3670.0,1.0,1,H100,1697105078309,1697105081979,120,84.0,20.0,"[17, 1137, 72, 68, 67, 52, 58, 390, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 424]","[1697105078326, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978]"
2779,55,32,[],200,EleutherAI/gpt-neox-20b,64,1,1201.0,1.0,1,H100,1697105073780,1697105074981,120,12.0,1.0,"[45, 1156]","[1697105073825, 1697105074981]"
2780,782,49,[],200,EleutherAI/gpt-neox-20b,64,1,2693.0,1.0,1,H100,1697105098780,1697105101473,120,90.0,20.0,"[10, 443, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 57, 61, 59, 463, 67, 66]","[1697105098790, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100757, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473]"
2781,629,50,[],200,EleutherAI/gpt-neox-20b,64,1,29854.0,1.0,1,H100,1697105109447,1697105139301,120,457.0,381.0,"[27, 597, 72, 120, 70, 53, 60, 459, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 328, 75, 72, 64, 330, 73, 71, 72, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 78, 58, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 58, 59, 74, 57, 74, 72, 65, 476, 80, 60, 77, 58, 70, 54, 278, 80, 76, 76, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 76, 72, 70, 66, 52, 395, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 443, 67, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71, 65, 63, 49, 50, 49, 88, 64, 57, 44, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 31, 32, 31, 32, 38, 31, 32, 31, 31, 31, 38, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 32, 31, 31, 31, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 38, 31, 31, 30, 31, 31, 36, 30, 30, 32, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 30, 30, 30, 30, 31, 30, 30, 30, 31, 30, 30, 31, 30, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31, 31, 31, 30, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 37, 29, 30, 30, 30, 30, 30, 30, 30, 34, 28, 29, 28, 28, 29, 28, 28, 29, 28, 28, 29, 28, 29, 28, 29, 28, 29, 28, 29]","[1697105109474, 1697105110071, 1697105110143, 1697105110263, 1697105110333, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118362, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125212, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126364, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126922, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128401, 1697105128473, 1697105128543, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130494, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133520, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134492, 1697105134524, 1697105134555, 1697105134587, 1697105134625, 1697105134656, 1697105134688, 1697105134719, 1697105134750, 1697105134781, 1697105134819, 1697105134850, 1697105134881, 1697105134912, 1697105134943, 1697105134974, 1697105135005, 1697105135036, 1697105135067, 1697105135098, 1697105135128, 1697105135159, 1697105135190, 1697105135221, 1697105135252, 1697105135283, 1697105135315, 1697105135346, 1697105135377, 1697105135408, 1697105135439, 1697105135470, 1697105135502, 1697105135533, 1697105135564, 1697105135596, 1697105135627, 1697105135658, 1697105135690, 1697105135721, 1697105135752, 1697105135784, 1697105135815, 1697105135853, 1697105135884, 1697105135915, 1697105135945, 1697105135976, 1697105136007, 1697105136043, 1697105136073, 1697105136103, 1697105136135, 1697105136164, 1697105136194, 1697105136224, 1697105136254, 1697105136284, 1697105136314, 1697105136344, 1697105136374, 1697105136404, 1697105136434, 1697105136464, 1697105136494, 1697105136524, 1697105136554, 1697105136584, 1697105136614, 1697105136644, 1697105136674, 1697105136705, 1697105136735, 1697105136765, 1697105136795, 1697105136825, 1697105136856, 1697105136886, 1697105136916, 1697105136946, 1697105136977, 1697105137007, 1697105137037, 1697105137068, 1697105137098, 1697105137128, 1697105137159, 1697105137189, 1697105137220, 1697105137250, 1697105137281, 1697105137311, 1697105137342, 1697105137372, 1697105137403, 1697105137433, 1697105137464, 1697105137495, 1697105137525, 1697105137556, 1697105137586, 1697105137617, 1697105137648, 1697105137679, 1697105137709, 1697105137740, 1697105137771, 1697105137802, 1697105137833, 1697105137863, 1697105137894, 1697105137925, 1697105137956, 1697105137987, 1697105138018, 1697105138048, 1697105138079, 1697105138110, 1697105138141, 1697105138172, 1697105138203, 1697105138234, 1697105138265, 1697105138296, 1697105138327, 1697105138358, 1697105138389, 1697105138420, 1697105138451, 1697105138488, 1697105138517, 1697105138547, 1697105138577, 1697105138607, 1697105138637, 1697105138667, 1697105138697, 1697105138727, 1697105138761, 1697105138789, 1697105138818, 1697105138846, 1697105138874, 1697105138903, 1697105138931, 1697105138959, 1697105138988, 1697105139016, 1697105139044, 1697105139073, 1697105139101, 1697105139130, 1697105139158, 1697105139187, 1697105139215, 1697105139244, 1697105139272, 1697105139301]"
2782,890,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100757,1697105102062,120,,,"[9, 574, 67, 66, 59, 48, 58]","[1697105100766, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101580, 1697105101638]"
2783,109,37,[],200,EleutherAI/gpt-neox-20b,64,1,2507.0,1.0,1,H100,1697105092048,1697105094555,120,90.0,20.0,"[4, 530, 56, 67, 52, 66, 64, 360, 77, 69, 67, 52, 51, 609, 74, 57, 72, 71, 55, 54]","[1697105092052, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093324, 1697105093393, 1697105093460, 1697105093512, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555]"
2784,398,47,[],200,EleutherAI/gpt-neox-20b,64,1,3453.0,1.0,1,H100,1697105102065,1697105105518,120,87.0,20.0,"[11, 731, 708, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66]","[1697105102076, 1697105102807, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518]"
2785,553,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101474,1697105102062,120,,,[8],[1697105101482]
2786,207,51,[],200,EleutherAI/gpt-neox-20b,64,1,1192.0,1.0,1,H100,1697105102076,1697105103268,120,10.0,1.0,"[135, 1057]","[1697105102211, 1697105103268]"
2787,884,52,[],200,EleutherAI/gpt-neox-20b,64,1,3437.0,1.0,1,H100,1697105103269,1697105106706,120,90.0,20.0,"[32, 853, 72, 71, 69, 64, 63, 61, 612, 81, 75, 59, 71, 66, 53, 572, 111, 58, 44, 350]","[1697105103301, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104493, 1697105104554, 1697105105166, 1697105105247, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706]"
2788,438,52,[],200,EleutherAI/gpt-neox-20b,64,1,499.0,1.0,1,H100,1697105113422,1697105113921,120,9.0,1.0,"[4, 495]","[1697105113426, 1697105113921]"
2789,186,53,[],200,EleutherAI/gpt-neox-20b,64,1,3824.0,1.0,1,H100,1697105113922,1697105117746,120,123.0,22.0,"[5, 885, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56, 65, 626]","[1697105113927, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117120, 1697105117746]"
2790,197,23,[],200,EleutherAI/gpt-neox-20b,64,1,792.0,1.0,1,H100,1697105067617,1697105068409,120,6.0,8.0,"[5, 456, 60, 58, 59, 46, 55, 53]","[1697105067622, 1697105068078, 1697105068138, 1697105068196, 1697105068255, 1697105068301, 1697105068356, 1697105068409]"
2791,599,35,[],200,EleutherAI/gpt-neox-20b,64,1,7543.0,1.0,1,H100,1697105080814,1697105088357,120,58.0,55.0,"[17, 1147, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 415, 71, 68, 51, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 348, 70, 53, 53, 66, 62, 291, 71, 55, 70, 70, 67, 63, 546, 63, 61, 48, 55]","[1697105080831, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083500, 1697105083571, 1697105083639, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087384, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357]"
2792,343,40,[],200,EleutherAI/gpt-neox-20b,64,1,2748.0,1.0,1,H100,1697105090576,1697105093324,120,84.0,20.0,"[16, 802, 71, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 57, 66, 52, 66, 64, 361, 76]","[1697105090592, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324]"
2793,886,54,[],200,EleutherAI/gpt-neox-20b,64,1,538.0,1.0,1,H100,1697105117747,1697105118285,120,17.0,1.0,"[5, 533]","[1697105117752, 1697105118285]"
2794,541,55,[],200,EleutherAI/gpt-neox-20b,64,1,2304.0,1.0,1,H100,1697105118286,1697105120590,120,90.0,20.0,"[4, 613, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68]","[1697105118290, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590]"
2795,611,54,[],200,EleutherAI/gpt-neox-20b,64,1,487.0,1.0,1,H100,1697105100014,1697105100501,120,14.0,1.0,"[9, 478]","[1697105100023, 1697105100501]"
2796,358,55,[],200,EleutherAI/gpt-neox-20b,64,1,905.0,1.0,1,H100,1697105100502,1697105101407,120,216.0,3.0,"[13, 825, 67]","[1697105100515, 1697105101340, 1697105101407]"
2797,750,20,[],200,EleutherAI/gpt-neox-20b,64,1,3405.0,1.0,1,H100,1697105058844,1697105062249,120,88.0,20.0,"[124, 1180, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 55]","[1697105058968, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249]"
2798,495,53,[],200,EleutherAI/gpt-neox-20b,64,1,359.0,1.0,1,H100,1697105113562,1697105113921,120,13.0,1.0,"[8, 351]","[1697105113570, 1697105113921]"
2799,147,54,[],200,EleutherAI/gpt-neox-20b,64,1,674.0,1.0,1,H100,1697105113922,1697105114596,120,182.0,1.0,"[5, 669]","[1697105113927, 1697105114596]"
2800,852,55,[],200,EleutherAI/gpt-neox-20b,64,1,3438.0,1.0,1,H100,1697105114597,1697105118035,120,100.0,20.0,"[8, 1157, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 71]","[1697105114605, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035]"
2801,121,67,[],200,EleutherAI/gpt-neox-20b,64,1,392.0,1.0,1,H100,1697105112009,1697105112401,120,13.0,1.0,"[9, 383]","[1697105112018, 1697105112401]"
2802,798,68,[],200,EleutherAI/gpt-neox-20b,64,1,1159.0,1.0,1,H100,1697105112402,1697105113561,120,79.0,6.0,"[4, 867, 76, 72, 70, 70]","[1697105112406, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561]"
2803,60,43,[],200,EleutherAI/gpt-neox-20b,64,1,4790.0,1.0,1,H100,1697105081980,1697105086770,120,93.0,36.0,"[12, 697, 77, 72, 71, 54, 53, 70, 413, 72, 67, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562, 72, 70, 68, 67, 63, 348, 70, 53, 54]","[1697105081992, 1697105082689, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083086, 1697105083499, 1697105083571, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086770]"
2804,743,37,[],200,EleutherAI/gpt-neox-20b,64,1,1440.0,1.0,1,H100,1697105080814,1697105082254,120,123.0,6.0,"[4, 1160, 72, 71, 68, 65]","[1697105080818, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254]"
2805,451,69,[],200,EleutherAI/gpt-neox-20b,64,1,359.0,1.0,1,H100,1697105113562,1697105113921,120,286.0,1.0,"[4, 355]","[1697105113566, 1697105113921]"
2806,227,70,[],200,EleutherAI/gpt-neox-20b,64,1,4042.0,1.0,1,H100,1697105113922,1697105117964,120,364.0,25.0,"[13, 877, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56, 65, 626, 81, 59, 78]","[1697105113935, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964]"
2807,472,41,[],200,EleutherAI/gpt-neox-20b,64,1,2462.0,1.0,1,H100,1697105092640,1697105095102,120,85.0,20.0,"[12, 596, 76, 69, 67, 52, 51, 609, 74, 57, 72, 71, 55, 54, 247, 53, 67, 65, 50, 65]","[1697105092652, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093512, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102]"
2808,538,53,[],200,EleutherAI/gpt-neox-20b,64,1,2735.0,1.0,1,H100,1697105106709,1697105109444,120,89.0,20.0,"[20, 478, 61, 58, 55, 54, 53, 260, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 65, 64]","[1697105106729, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
2809,813,38,[],200,EleutherAI/gpt-neox-20b,64,1,2908.0,1.0,1,H100,1697105094556,1697105097464,120,85.0,20.0,"[13, 870, 65, 50, 64, 64, 511, 75, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74]","[1697105094569, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464]"
2810,398,38,[],200,EleutherAI/gpt-neox-20b,64,1,3650.0,1.0,1,H100,1697105082255,1697105085905,120,87.0,20.0,"[5, 1239, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562]","[1697105082260, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905]"
2811,898,24,[],200,EleutherAI/gpt-neox-20b,64,1,889.0,1.0,1,H100,1697105068410,1697105069299,120,79.0,2.0,"[9, 880]","[1697105068419, 1697105069299]"
2812,760,37,[],200,EleutherAI/gpt-neox-20b,64,1,9485.0,1.0,1,H100,1697105102065,1697105111550,120,335.0,64.0,"[50, 693, 707, 66, 51, 50, 64, 408, 72, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66, 52, 573, 111, 58, 44, 350, 58, 57, 55, 44, 286, 61, 58, 55, 55, 53, 259, 62, 61, 49, 56, 44, 424, 57, 50, 48, 717, 65, 64, 53, 573, 72, 121, 70, 53, 59, 459, 215, 128, 73, 54, 65, 62, 49]","[1697105102115, 1697105102808, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105570, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325, 1697105107380, 1697105107435, 1697105107488, 1697105107747, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108019, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109497, 1697105110070, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111374, 1697105111439, 1697105111501, 1697105111550]"
2813,558,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069300,1697105070950,120,,,"[4, 906, 71, 68, 67, 56]","[1697105069304, 1697105070210, 1697105070281, 1697105070349, 1697105070416, 1697105070472]"
2814,242,42,[],200,EleutherAI/gpt-neox-20b,64,1,1720.0,1.0,1,H100,1697105095103,1697105096823,120,345.0,9.0,"[8, 1082, 75, 56, 74, 56, 71, 54, 244]","[1697105095111, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096454, 1697105096525, 1697105096579, 1697105096823]"
2815,85,45,[],200,EleutherAI/gpt-neox-20b,64,1,2735.0,1.0,1,H100,1697105106709,1697105109444,120,88.0,20.0,"[5, 493, 61, 57, 56, 54, 53, 259, 62, 61, 49, 56, 44, 424, 58, 49, 48, 717, 65, 64]","[1697105106714, 1697105107207, 1697105107268, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107747, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108019, 1697105108443, 1697105108501, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
2816,11,56,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101408,1697105102062,120,,,[4],[1697105101412]
2817,716,57,[],200,EleutherAI/gpt-neox-20b,64,1,5128.0,1.0,1,H100,1697105102079,1697105107207,120,79.0,30.0,"[188, 1248, 66, 51, 50, 64, 407, 73, 71, 69, 64, 63, 60, 613, 81, 76, 58, 71, 66, 52, 573, 111, 58, 44, 350, 58, 57, 55, 44, 286]","[1697105102267, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104493, 1697105104553, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105570, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206]"
2818,786,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101474,1697105102062,120,,,[8],[1697105101482]
2819,221,41,[],200,EleutherAI/gpt-neox-20b,64,1,5540.0,1.0,1,H100,1697105108021,1697105113561,120,364.0,36.0,"[4, 1290, 65, 64, 54, 573, 72, 120, 74, 49, 60, 458, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70]","[1697105108025, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561]"
2820,563,45,[],200,EleutherAI/gpt-neox-20b,64,1,3381.0,1.0,1,H100,1697105102071,1697105105452,120,874.0,18.0,"[120, 1324, 66, 51, 50, 64, 408, 73, 70, 69, 64, 62, 61, 613, 81, 76, 58, 71]","[1697105102191, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104227, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452]"
2821,458,31,[],200,EleutherAI/gpt-neox-20b,64,1,1007.0,1.0,1,H100,1697105070953,1697105071960,120,11.0,1.0,"[137, 870]","[1697105071090, 1697105071960]"
2822,551,41,[],200,EleutherAI/gpt-neox-20b,64,1,3452.0,1.0,1,H100,1697105102066,1697105105518,120,90.0,20.0,"[28, 713, 708, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66]","[1697105102094, 1697105102807, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518]"
2823,527,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062250,1697105063190,120,,,[8],[1697105062258]
2824,178,22,[],200,EleutherAI/gpt-neox-20b,64,1,816.0,1.0,1,H100,1697105063193,1697105064009,120,11.0,1.0,"[122, 694]","[1697105063315, 1697105064009]"
2825,856,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064010,1697105065209,120,,,"[29, 513, 61, 59, 57, 45, 54]","[1697105064039, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
2826,365,58,[],200,EleutherAI/gpt-neox-20b,64,1,476.0,1.0,1,H100,1697105107208,1697105107684,120,23.0,1.0,"[4, 472]","[1697105107212, 1697105107684]"
2827,920,42,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105113562,1697105114122,120,96.0,4.0,"[4, 424, 67, 65]","[1697105113566, 1697105113990, 1697105114057, 1697105114122]"
2828,143,59,[],200,EleutherAI/gpt-neox-20b,64,1,2578.0,1.0,1,H100,1697105107685,1697105110263,120,6.0,12.0,"[9, 749, 57, 50, 48, 717, 65, 64, 54, 573, 71, 121]","[1697105107694, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263]"
2829,625,24,[],200,EleutherAI/gpt-neox-20b,64,1,635.0,1.0,1,H100,1697105065212,1697105065847,120,364.0,2.0,"[29, 606]","[1697105065241, 1697105065847]"
2830,726,60,[],200,EleutherAI/gpt-neox-20b,64,1,5866.0,1.0,1,H100,1697105110264,1697105116130,120,67.0,47.0,"[5, 636, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55]","[1697105110269, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130]"
2831,277,25,[],200,EleutherAI/gpt-neox-20b,64,1,1426.0,1.0,1,H100,1697105065848,1697105067274,120,18.0,1.0,"[4, 1422]","[1697105065852, 1697105067274]"
2832,54,26,[],200,EleutherAI/gpt-neox-20b,64,1,3141.0,1.0,1,H100,1697105067275,1697105070416,120,87.0,20.0,"[4, 799, 60, 58, 58, 47, 55, 53, 289, 51, 50, 500, 61, 60, 59, 55, 676, 70, 69, 67]","[1697105067279, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301, 1697105068356, 1697105068409, 1697105068698, 1697105068749, 1697105068799, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416]"
2833,583,43,[],200,EleutherAI/gpt-neox-20b,64,1,2932.0,1.0,1,H100,1697105114123,1697105117055,120,96.0,20.0,"[5, 684, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56]","[1697105114128, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055]"
2834,7,39,[],200,EleutherAI/gpt-neox-20b,64,1,1781.0,1.0,1,H100,1697105082839,1697105084620,120,345.0,11.0,"[5, 655, 72, 68, 51, 67, 64, 592, 77, 56, 74]","[1697105082844, 1697105083499, 1697105083571, 1697105083639, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620]"
2835,705,40,[],200,EleutherAI/gpt-neox-20b,64,1,3510.0,1.0,1,H100,1697105084621,1697105088131,120,79.0,27.0,"[5, 411, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 349, 69, 55, 52, 65, 62, 291, 71, 55, 71, 69, 67, 63, 546]","[1697105084626, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130]"
2836,365,41,[],200,EleutherAI/gpt-neox-20b,64,1,425.0,1.0,1,H100,1697105088132,1697105088557,120,23.0,1.0,"[8, 417]","[1697105088140, 1697105088557]"
2837,134,42,[],200,EleutherAI/gpt-neox-20b,64,1,2837.0,1.0,1,H100,1697105088558,1697105091395,120,86.0,20.0,"[17, 610, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530, 64, 49, 60, 59, 58, 530]","[1697105088575, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091395]"
2838,506,56,[],200,EleutherAI/gpt-neox-20b,64,1,791.0,1.0,1,H100,1697105118036,1697105118827,120,16.0,1.0,"[20, 771]","[1697105118056, 1697105118827]"
2839,719,43,[],200,EleutherAI/gpt-neox-20b,64,1,821.0,1.0,1,H100,1697105091397,1697105092218,120,182.0,6.0,"[12, 564, 74, 67, 52, 52]","[1697105091409, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218]"
2840,869,53,[],200,EleutherAI/gpt-neox-20b,64,1,2632.0,1.0,1,H100,1697105114293,1697105116925,120,244.0,12.0,"[13, 1456, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76]","[1697105114306, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925]"
2841,427,33,[],200,EleutherAI/gpt-neox-20b,64,1,1611.0,1.0,1,H100,1697105102071,1697105103682,120,58.0,5.0,"[128, 1316, 66, 51, 50]","[1697105102199, 1697105103515, 1697105103581, 1697105103632, 1697105103682]"
2842,277,57,[],200,EleutherAI/gpt-neox-20b,64,1,623.0,1.0,1,H100,1697105118828,1697105119451,120,18.0,1.0,"[16, 606]","[1697105118844, 1697105119450]"
2843,175,34,[],200,EleutherAI/gpt-neox-20b,64,1,871.0,1.0,1,H100,1697105103683,1697105104554,120,140.0,8.0,"[5, 466, 72, 71, 69, 64, 63, 61]","[1697105103688, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104493, 1697105104554]"
2844,489,44,[],200,EleutherAI/gpt-neox-20b,64,1,3399.0,1.0,1,H100,1697105092219,1697105095618,120,79.0,30.0,"[5, 358, 57, 67, 51, 66, 64, 361, 76, 69, 67, 51, 52, 609, 75, 56, 72, 71, 55, 54, 246, 54, 67, 65, 50, 65, 336, 66, 50, 64]","[1697105092224, 1697105092582, 1697105092639, 1697105092706, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094247, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618]"
2845,467,39,[],200,EleutherAI/gpt-neox-20b,64,1,2548.0,1.0,1,H100,1697105097465,1697105100013,120,93.0,20.0,"[4, 641, 74, 68, 52, 52, 242, 54, 57, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63]","[1697105097469, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013]"
2846,101,51,[],200,EleutherAI/gpt-neox-20b,64,1,415.0,1.0,1,H100,1697105107269,1697105107684,120,13.0,1.0,"[4, 411]","[1697105107273, 1697105107684]"
2847,62,34,[],200,EleutherAI/gpt-neox-20b,64,1,2888.0,1.0,1,H100,1697105083017,1697105085905,120,91.0,20.0,"[5, 477, 72, 68, 51, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562]","[1697105083022, 1697105083499, 1697105083571, 1697105083639, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905]"
2848,807,52,[],200,EleutherAI/gpt-neox-20b,64,1,3690.0,1.0,1,H100,1697105107685,1697105111375,120,90.0,20.0,"[17, 741, 57, 50, 49, 716, 65, 64, 54, 573, 72, 120, 70, 53, 60, 458, 215, 128, 73, 55]","[1697105107702, 1697105108443, 1697105108500, 1697105108550, 1697105108599, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110333, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375]"
2849,34,30,[],200,EleutherAI/gpt-neox-20b,64,1,560.0,1.0,1,H100,1697105097465,1697105098025,120,12.0,1.0,"[12, 548]","[1697105097477, 1697105098025]"
2850,327,26,[],200,EleutherAI/gpt-neox-20b,64,1,2087.0,1.0,1,H100,1697105070953,1697105073040,120,563.0,10.0,"[69, 1011, 57, 47, 46, 56, 55, 604, 73, 68]","[1697105071022, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039]"
2851,530,45,[],200,EleutherAI/gpt-neox-20b,64,1,809.0,1.0,1,H100,1697105094556,1697105095365,120,26.0,1.0,"[17, 792]","[1697105094573, 1697105095365]"
2852,889,27,[],200,EleutherAI/gpt-neox-20b,64,1,2397.0,1.0,1,H100,1697105073040,1697105075437,120,86.0,20.0,"[5, 506, 52, 64, 62, 49, 44, 482, 63, 50, 61, 60, 53, 476, 67, 64, 52, 64, 63, 60]","[1697105073045, 1697105073551, 1697105073603, 1697105073667, 1697105073729, 1697105073778, 1697105073822, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075437]"
2853,190,46,[],200,EleutherAI/gpt-neox-20b,64,1,1527.0,1.0,1,H100,1697105095366,1697105096893,120,335.0,10.0,"[4, 824, 74, 56, 74, 55, 72, 54, 244, 70]","[1697105095370, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893]"
2854,599,53,[],200,EleutherAI/gpt-neox-20b,64,1,7259.0,1.0,1,H100,1697105124098,1697105131357,120,58.0,55.0,"[21, 1035, 59, 58, 74, 57, 74, 72, 65, 476, 80, 63, 74, 59, 70, 53, 279, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 66, 52, 395, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75]","[1697105124119, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126232, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357]"
2855,756,33,[],200,EleutherAI/gpt-neox-20b,64,1,738.0,1.0,1,H100,1697105074982,1697105075720,120,19.0,1.0,"[9, 729]","[1697105074991, 1697105075720]"
2856,462,53,[],200,EleutherAI/gpt-neox-20b,64,1,345.0,1.0,1,H100,1697105111376,1697105111721,120,52.0,1.0,"[12, 333]","[1697105111388, 1697105111721]"
2857,232,54,[],200,EleutherAI/gpt-neox-20b,64,1,2570.0,1.0,1,H100,1697105111722,1697105114292,120,93.0,20.0,"[5, 762, 73, 56, 72, 71, 62, 451, 75, 72, 70, 70, 63, 62, 304, 66, 66, 62, 48, 60]","[1697105111727, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113274, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292]"
2858,889,47,[],200,EleutherAI/gpt-neox-20b,64,1,2603.0,1.0,1,H100,1697105096894,1697105099497,120,86.0,20.0,"[5, 491, 74, 70, 68, 67, 440, 75, 68, 52, 51, 243, 55, 56, 69, 61, 394, 72, 126, 66]","[1697105096899, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098598, 1697105098653, 1697105098709, 1697105098778, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497]"
2859,822,55,[],200,EleutherAI/gpt-neox-20b,64,1,3742.0,1.0,1,H100,1697105114293,1697105118035,120,88.0,20.0,"[5, 1464, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56, 65, 626, 81, 59, 78, 71]","[1697105114298, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035]"
2860,420,45,[],200,EleutherAI/gpt-neox-20b,64,1,2908.0,1.0,1,H100,1697105094556,1697105097464,120,52.0,20.0,"[9, 874, 65, 50, 64, 64, 512, 74, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74]","[1697105094565, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464]"
2861,317,56,[],200,EleutherAI/gpt-neox-20b,64,1,7076.0,1.0,1,H100,1697105120591,1697105127667,120,244.0,50.0,"[13, 1324, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 279, 79, 77, 75, 67, 65, 476, 60]","[1697105120604, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666]"
2862,322,42,[],200,EleutherAI/gpt-neox-20b,64,1,2497.0,1.0,1,H100,1697105111795,1697105114292,120,93.0,20.0,"[8, 686, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 60]","[1697105111803, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292]"
2863,86,48,[],200,EleutherAI/gpt-neox-20b,64,1,2098.0,1.0,1,H100,1697105098779,1697105100877,120,335.0,17.0,"[6, 448, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 57, 61, 59]","[1697105098785, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100757, 1697105100818, 1697105100877]"
2864,590,56,[],200,EleutherAI/gpt-neox-20b,64,1,2554.0,1.0,1,H100,1697105118036,1697105120590,120,88.0,20.0,"[8, 859, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68]","[1697105118044, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590]"
2865,378,37,[],200,EleutherAI/gpt-neox-20b,64,1,6297.0,1.0,1,H100,1697105107327,1697105113624,120,93.0,47.0,"[8, 413, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 66, 63, 54, 573, 71, 121, 70, 53, 59, 459, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63]","[1697105107335, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624]"
2866,671,49,[],200,EleutherAI/gpt-neox-20b,64,1,1116.0,1.0,1,H100,1697105100878,1697105101994,120,12.0,1.0,"[8, 1108]","[1697105100886, 1697105101994]"
2867,444,50,[],200,EleutherAI/gpt-neox-20b,64,1,1688.0,1.0,1,H100,1697105101994,1697105103682,120,457.0,6.0,"[5, 808, 707, 67, 51, 50]","[1697105101999, 1697105102807, 1697105103514, 1697105103581, 1697105103632, 1697105103682]"
2868,104,51,[],200,EleutherAI/gpt-neox-20b,64,1,3023.0,1.0,1,H100,1697105103683,1697105106706,120,93.0,20.0,"[5, 466, 72, 71, 69, 64, 63, 61, 612, 81, 75, 59, 71, 66, 53, 572, 111, 58, 44, 350]","[1697105103688, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104493, 1697105104554, 1697105105166, 1697105105247, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706]"
2869,905,43,[],200,EleutherAI/gpt-neox-20b,64,1,1300.0,1.0,1,H100,1697105114294,1697105115594,120,11.0,1.0,"[16, 1284]","[1697105114310, 1697105115594]"
2870,813,22,[],200,EleutherAI/gpt-neox-20b,64,1,3406.0,1.0,1,H100,1697105058843,1697105062249,120,85.0,20.0,"[149, 1156, 59, 57, 45, 54, 401, 64, 169, 111, 111, 64, 62, 61, 398, 74, 122, 124, 70, 55]","[1697105058992, 1697105060148, 1697105060207, 1697105060264, 1697105060309, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061406, 1697105061804, 1697105061878, 1697105062000, 1697105062124, 1697105062194, 1697105062249]"
2871,685,44,[],200,EleutherAI/gpt-neox-20b,64,1,1174.0,1.0,1,H100,1697105115595,1697105116769,120,364.0,2.0,"[4, 1170]","[1697105115599, 1697105116769]"
2872,638,54,[],200,EleutherAI/gpt-neox-20b,64,1,2889.0,1.0,1,H100,1697105116926,1697105119815,120,88.0,20.0,"[4, 816, 81, 59, 78, 71, 331, 71, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56]","[1697105116930, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118366, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815]"
2873,729,38,[],200,EleutherAI/gpt-neox-20b,64,1,1337.0,1.0,1,H100,1697105078126,1697105079463,120,874.0,2.0,"[9, 1328]","[1697105078135, 1697105079463]"
2874,501,39,[],200,EleutherAI/gpt-neox-20b,64,1,633.0,1.0,1,H100,1697105079464,1697105080097,120,19.0,1.0,"[9, 624]","[1697105079473, 1697105080097]"
2875,739,35,[],200,EleutherAI/gpt-neox-20b,64,1,609.0,1.0,1,H100,1697105085906,1697105086515,120,216.0,1.0,"[8, 601]","[1697105085914, 1697105086515]"
2876,867,58,[],200,EleutherAI/gpt-neox-20b,64,1,3194.0,1.0,1,H100,1697105119451,1697105122645,120,91.0,20.0,"[9, 772, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77]","[1697105119460, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645]"
2877,503,34,[],200,EleutherAI/gpt-neox-20b,64,1,2587.0,1.0,1,H100,1697105075721,1697105078308,120,109.0,20.0,"[4, 615, 45, 53, 51, 342, 64, 55, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56]","[1697105075725, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076895, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308]"
2878,844,53,[],200,EleutherAI/gpt-neox-20b,64,1,486.0,1.0,1,H100,1697105100015,1697105100501,120,10.0,1.0,"[12, 474]","[1697105100027, 1697105100501]"
2879,501,54,[],200,EleutherAI/gpt-neox-20b,64,1,763.0,1.0,1,H100,1697105100502,1697105101265,120,19.0,1.0,"[9, 754]","[1697105100511, 1697105101265]"
2880,271,55,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101266,1697105102062,120,,,[17],[1697105101283]
2881,113,41,[],200,EleutherAI/gpt-neox-20b,64,1,747.0,1.0,1,H100,1697105093326,1697105094073,120,13.0,1.0,"[16, 731]","[1697105093342, 1697105094073]"
2882,702,42,[],200,EleutherAI/gpt-neox-20b,64,1,2749.0,1.0,1,H100,1697105094074,1697105096823,120,89.0,20.0,"[9, 719, 53, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512, 74, 56, 74, 55, 72, 54, 244]","[1697105094083, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823]"
2883,901,35,[],200,EleutherAI/gpt-neox-20b,64,1,253.0,1.0,1,H100,1697105088304,1697105088557,120,17.0,1.0,"[8, 245]","[1697105088312, 1697105088557]"
2884,52,62,[],200,EleutherAI/gpt-neox-20b,64,1,752.0,1.0,1,H100,1697105112009,1697105112761,120,58.0,6.0,"[5, 475, 73, 56, 72, 71]","[1697105112014, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761]"
2885,752,63,[],200,EleutherAI/gpt-neox-20b,64,1,587.0,1.0,1,H100,1697105112762,1697105113349,120,39.0,3.0,"[9, 502, 76]","[1697105112771, 1697105113273, 1697105113349]"
2886,406,64,[],200,EleutherAI/gpt-neox-20b,64,1,772.0,1.0,1,H100,1697105113350,1697105114122,120,244.0,4.0,"[4, 567, 69, 67, 65]","[1697105113354, 1697105113921, 1697105113990, 1697105114057, 1697105114122]"
2887,182,65,[],200,EleutherAI/gpt-neox-20b,64,1,4780.0,1.0,1,H100,1697105114123,1697105118903,120,47.0,31.0,"[4, 685, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56, 65, 626, 81, 59, 78, 71, 328, 74, 72, 64, 330]","[1697105114127, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118363, 1697105118437, 1697105118509, 1697105118573, 1697105118903]"
2888,562,36,[],200,EleutherAI/gpt-neox-20b,64,1,4835.0,1.0,1,H100,1697105088558,1697105093393,120,67.0,39.0,"[21, 606, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530, 64, 49, 60, 59, 58, 529, 71, 67, 65, 64, 311, 75, 67, 52, 52, 65, 299, 56, 67, 52, 66, 64, 360, 77, 69]","[1697105088579, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093324, 1697105093393]"
2889,156,38,[],200,EleutherAI/gpt-neox-20b,64,1,2505.0,1.0,1,H100,1697105113625,1697105116130,120,86.0,20.0,"[5, 360, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55]","[1697105113630, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130]"
2890,316,54,[],200,EleutherAI/gpt-neox-20b,64,1,2563.0,1.0,1,H100,1697105109445,1697105112008,120,86.0,20.0,"[21, 605, 72, 120, 74, 49, 60, 459, 214, 128, 73, 55, 64, 62, 49, 243, 65, 50, 50, 50]","[1697105109466, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111793, 1697105111858, 1697105111908, 1697105111958, 1697105112008]"
2891,733,31,[],200,EleutherAI/gpt-neox-20b,64,1,499.0,1.0,1,H100,1697105098026,1697105098525,120,31.0,1.0,"[13, 486]","[1697105098039, 1697105098525]"
2892,362,32,[],200,EleutherAI/gpt-neox-20b,64,1,621.0,1.0,1,H100,1697105098526,1697105099147,120,14.0,1.0,"[21, 599]","[1697105098547, 1697105099146]"
2893,144,45,[],200,EleutherAI/gpt-neox-20b,64,1,2559.0,1.0,1,H100,1697105095625,1697105098184,120,96.0,20.0,"[11, 558, 74, 57, 73, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 441, 74]","[1697105095636, 1697105096194, 1697105096268, 1697105096325, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184]"
2894,471,43,[],200,EleutherAI/gpt-neox-20b,64,1,2673.0,1.0,1,H100,1697105096824,1697105099497,120,86.0,20.0,"[9, 557, 74, 70, 68, 67, 440, 75, 68, 52, 51, 243, 55, 56, 69, 61, 394, 72, 126, 66]","[1697105096833, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098598, 1697105098653, 1697105098709, 1697105098778, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497]"
2895,333,37,[],200,EleutherAI/gpt-neox-20b,64,1,1528.0,1.0,1,H100,1697105093394,1697105094922,120,563.0,11.0,"[4, 774, 75, 56, 73, 70, 55, 54, 246, 54, 67]","[1697105093398, 1697105094172, 1697105094247, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922]"
2896,128,33,[],200,EleutherAI/gpt-neox-20b,64,1,622.0,1.0,1,H100,1697105099148,1697105099770,120,9.0,1.0,"[16, 606]","[1697105099164, 1697105099770]"
2897,718,34,[],200,EleutherAI/gpt-neox-20b,64,1,730.0,1.0,1,H100,1697105099771,1697105100501,120,13.0,1.0,"[8, 722]","[1697105099779, 1697105100501]"
2898,489,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100502,1697105102061,120,,,"[5, 833, 67, 66, 59, 47, 59]","[1697105100507, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2899,911,38,[],200,EleutherAI/gpt-neox-20b,64,1,1531.0,1.0,1,H100,1697105094923,1697105096454,120,335.0,11.0,"[4, 512, 65, 50, 64, 64, 511, 75, 57, 73, 56]","[1697105094927, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096325, 1697105096398, 1697105096454]"
2900,143,36,[],200,EleutherAI/gpt-neox-20b,64,1,2365.0,1.0,1,H100,1697105102065,1697105104430,120,6.0,12.0,"[22, 720, 708, 66, 51, 50, 64, 407, 73, 71, 69, 64]","[1697105102087, 1697105102807, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430]"
2901,689,39,[],200,EleutherAI/gpt-neox-20b,64,1,288.0,1.0,1,H100,1697105096455,1697105096743,120,15.0,1.0,"[5, 283]","[1697105096460, 1697105096743]"
2902,496,61,[],200,EleutherAI/gpt-neox-20b,64,1,1833.0,1.0,1,H100,1697105116131,1697105117964,120,335.0,11.0,"[5, 633, 80, 76, 74, 56, 66, 625, 81, 59, 78]","[1697105116136, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117121, 1697105117746, 1697105117827, 1697105117886, 1697105117964]"
2903,340,40,[],200,EleutherAI/gpt-neox-20b,64,1,2753.0,1.0,1,H100,1697105096744,1697105099497,120,85.0,20.0,"[5, 641, 74, 70, 68, 67, 440, 75, 68, 52, 51, 243, 55, 56, 69, 61, 394, 72, 126, 66]","[1697105096749, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098598, 1697105098653, 1697105098709, 1697105098778, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497]"
2904,849,37,[],200,EleutherAI/gpt-neox-20b,64,1,506.0,1.0,1,H100,1697105104432,1697105104938,120,10.0,1.0,"[6, 500]","[1697105104438, 1697105104938]"
2905,851,46,[],200,EleutherAI/gpt-neox-20b,64,1,340.0,1.0,1,H100,1697105098185,1697105098525,120,23.0,1.0,"[21, 319]","[1697105098206, 1697105098525]"
2906,501,38,[],200,EleutherAI/gpt-neox-20b,64,1,1125.0,1.0,1,H100,1697105104939,1697105106064,120,19.0,1.0,"[5, 1120]","[1697105104944, 1697105106064]"
2907,277,39,[],200,EleutherAI/gpt-neox-20b,64,1,509.0,1.0,1,H100,1697105106065,1697105106574,120,18.0,1.0,"[21, 488]","[1697105106086, 1697105106574]"
2908,504,47,[],200,EleutherAI/gpt-neox-20b,64,1,2947.0,1.0,1,H100,1697105098526,1697105101473,120,58.0,20.0,"[9, 698, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66]","[1697105098535, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473]"
2909,117,41,[],200,EleutherAI/gpt-neox-20b,64,1,339.0,1.0,1,H100,1697105099498,1697105099837,120,364.0,2.0,"[4, 335]","[1697105099502, 1697105099837]"
2910,699,42,[],200,EleutherAI/gpt-neox-20b,64,1,663.0,1.0,1,H100,1697105099838,1697105100501,120,39.0,1.0,"[5, 658]","[1697105099843, 1697105100501]"
2911,471,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100502,1697105102061,120,,,"[5, 833, 67, 66, 59, 47, 59]","[1697105100507, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2912,392,36,[],200,EleutherAI/gpt-neox-20b,64,1,596.0,1.0,1,H100,1697105086516,1697105087112,120,20.0,1.0,"[14, 582]","[1697105086530, 1697105087112]"
2913,111,32,[],200,EleutherAI/gpt-neox-20b,64,1,1141.0,1.0,1,H100,1697105071961,1697105073102,120,79.0,5.0,"[16, 921, 73, 68, 63]","[1697105071977, 1697105072898, 1697105072971, 1697105073039, 1697105073102]"
2914,164,37,[],200,EleutherAI/gpt-neox-20b,64,1,933.0,1.0,1,H100,1697105087113,1697105088046,120,15.0,1.0,"[8, 925]","[1697105087121, 1697105088046]"
2915,750,38,[],200,EleutherAI/gpt-neox-20b,64,1,2528.0,1.0,1,H100,1697105088047,1697105090575,120,88.0,20.0,"[5, 570, 60, 61, 58, 56, 56, 271, 59, 46, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530]","[1697105088052, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089184, 1697105089243, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575]"
2916,858,56,[],200,EleutherAI/gpt-neox-20b,64,1,2418.0,1.0,1,H100,1697105102075,1697105104493,120,182.0,12.0,"[140, 1300, 67, 50, 50, 64, 407, 73, 71, 69, 64, 63]","[1697105102215, 1697105103515, 1697105103582, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104493]"
2917,548,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099498,1697105102062,120,,,"[8, 331, 63, 51, 62, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 48, 58]","[1697105099506, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101580, 1697105101638]"
2918,636,27,[],200,EleutherAI/gpt-neox-20b,64,1,511.0,1.0,1,H100,1697105070418,1697105070929,120,31.0,1.0,"[24, 487]","[1697105070442, 1697105070929]"
2919,218,57,[],200,EleutherAI/gpt-neox-20b,64,1,1977.0,1.0,1,H100,1697105120591,1697105122568,120,109.0,7.0,"[5, 1332, 83, 151, 171, 155, 80]","[1697105120596, 1697105121928, 1697105122011, 1697105122162, 1697105122333, 1697105122488, 1697105122568]"
2920,413,28,[],200,EleutherAI/gpt-neox-20b,64,1,6664.0,1.0,1,H100,1697105070930,1697105077594,120,244.0,50.0,"[17, 1086, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 48, 59, 48, 234, 52, 64, 61, 50, 44, 481, 64, 50, 61, 60, 53, 476, 66, 65, 52, 64, 63, 59, 357, 60, 49, 59, 58, 55, 266, 45, 53, 51, 342, 63, 56, 55, 55, 534]","[1697105070947, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073210, 1697105073269, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073778, 1697105073822, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075133, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075853, 1697105075902, 1697105075961, 1697105076019, 1697105076074, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594]"
2921,916,58,[],200,EleutherAI/gpt-neox-20b,64,1,385.0,1.0,1,H100,1697105122569,1697105122954,120,8.0,1.0,"[5, 380]","[1697105122574, 1697105122954]"
2922,341,45,[],200,EleutherAI/gpt-neox-20b,64,1,3045.0,1.0,1,H100,1697105116770,1697105119815,120,87.0,20.0,"[5, 971, 81, 59, 78, 71, 331, 71, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56]","[1697105116775, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118366, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815]"
2923,293,53,[],200,EleutherAI/gpt-neox-20b,64,1,2583.0,1.0,1,H100,1697105096195,1697105098778,120,91.0,20.0,"[12, 616, 70, 68, 67, 65, 297, 74, 70, 68, 67, 441, 74, 68, 52, 52, 242, 55, 56, 69]","[1697105096207, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097093, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098653, 1697105098709, 1697105098778]"
2924,757,35,[],200,EleutherAI/gpt-neox-20b,64,1,1509.0,1.0,1,H100,1697105104555,1697105106064,120,20.0,1.0,"[4, 1505]","[1697105104559, 1697105106064]"
2925,467,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105062250,1697105063190,120,,,[4],[1697105062254]
2926,528,36,[],200,EleutherAI/gpt-neox-20b,64,1,2435.0,1.0,1,H100,1697105106065,1697105108500,120,52.0,20.0,"[9, 632, 58, 57, 55, 44, 286, 61, 59, 55, 54, 53, 260, 61, 61, 50, 55, 45, 423, 57]","[1697105106074, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107920, 1697105107975, 1697105108020, 1697105108443, 1697105108500]"
2927,243,24,[],200,EleutherAI/gpt-neox-20b,64,1,1360.0,1.0,1,H100,1697105063192,1697105064552,120,67.0,4.0,"[78, 804, 46, 432]","[1697105063270, 1697105064074, 1697105064120, 1697105064552]"
2928,828,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064553,1697105065210,120,,,[4],[1697105064557]
2929,597,26,[],200,EleutherAI/gpt-neox-20b,64,1,581.0,1.0,1,H100,1697105065212,1697105065793,120,39.0,1.0,"[73, 508]","[1697105065285, 1697105065793]"
2930,254,27,[],200,EleutherAI/gpt-neox-20b,64,1,551.0,1.0,1,H100,1697105065794,1697105066345,120,58.0,1.0,"[13, 538]","[1697105065807, 1697105066345]"
2931,24,28,[],200,EleutherAI/gpt-neox-20b,64,1,1850.0,1.0,1,H100,1697105066346,1697105068196,120,79.0,9.0,"[4, 1076, 68, 63, 59, 55, 407, 59, 59]","[1697105066350, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068137, 1697105068196]"
2932,733,39,[],200,EleutherAI/gpt-neox-20b,64,1,427.0,1.0,1,H100,1697105116131,1697105116558,120,31.0,1.0,"[13, 414]","[1697105116144, 1697105116558]"
2933,510,40,[],200,EleutherAI/gpt-neox-20b,64,1,1187.0,1.0,1,H100,1697105116559,1697105117746,120,79.0,2.0,"[5, 1182]","[1697105116564, 1697105117746]"
2934,162,41,[],200,EleutherAI/gpt-neox-20b,64,1,2562.0,1.0,1,H100,1697105117747,1697105120309,120,90.0,20.0,"[4, 615, 71, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77]","[1697105117751, 1697105118366, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309]"
2935,699,29,[],200,EleutherAI/gpt-neox-20b,64,1,441.0,1.0,1,H100,1697105068198,1697105068639,120,39.0,1.0,"[5, 436]","[1697105068203, 1697105068639]"
2936,353,30,[],200,EleutherAI/gpt-neox-20b,64,1,781.0,1.0,1,H100,1697105068639,1697105069420,120,52.0,4.0,"[5, 655, 61, 60]","[1697105068644, 1697105069299, 1697105069360, 1697105069420]"
2937,860,40,[],200,EleutherAI/gpt-neox-20b,64,1,2869.0,1.0,1,H100,1697105106575,1697105109444,120,85.0,20.0,"[9, 623, 60, 58, 56, 54, 53, 260, 61, 61, 50, 55, 45, 423, 57, 50, 48, 717, 65, 64]","[1697105106584, 1697105107207, 1697105107267, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107920, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
2938,129,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105069423,1697105070950,120,,,"[7, 780, 71, 68, 67, 56]","[1697105069430, 1697105070210, 1697105070281, 1697105070349, 1697105070416, 1697105070472]"
2939,714,32,[],200,EleutherAI/gpt-neox-20b,64,1,2825.0,1.0,1,H100,1697105070953,1697105073778,120,83.0,20.0,"[125, 955, 57, 47, 46, 56, 55, 603, 74, 68, 63, 60, 49, 59, 47, 234, 52, 64, 61, 50]","[1697105071078, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072897, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073778]"
2940,898,37,[],200,EleutherAI/gpt-neox-20b,64,1,1154.0,1.0,1,H100,1697105078309,1697105079463,120,79.0,2.0,"[13, 1141]","[1697105078322, 1697105079463]"
2941,719,52,[],200,EleutherAI/gpt-neox-20b,64,1,807.0,1.0,1,H100,1697105100070,1697105100877,120,182.0,6.0,"[5, 509, 116, 56, 62, 59]","[1697105100075, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877]"
2942,758,44,[],200,EleutherAI/gpt-neox-20b,64,1,2414.0,1.0,1,H100,1697105086771,1697105089185,120,84.0,20.0,"[8, 409, 72, 54, 71, 69, 67, 63, 546, 63, 61, 48, 55, 265, 60, 60, 59, 56, 55, 273]","[1697105086779, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088622, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089185]"
2943,804,43,[],200,EleutherAI/gpt-neox-20b,64,1,489.0,1.0,1,H100,1697105096824,1697105097313,120,20.0,1.0,"[9, 480]","[1697105096833, 1697105097313]"
2944,375,53,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100878,1697105102062,120,,,[4],[1697105100882]
2945,147,54,[],200,EleutherAI/gpt-neox-20b,64,1,690.0,1.0,1,H100,1697105102065,1697105102755,120,182.0,1.0,"[18, 672]","[1697105102083, 1697105102755]"
2946,737,55,[],200,EleutherAI/gpt-neox-20b,64,1,759.0,1.0,1,H100,1697105102756,1697105103515,120,216.0,2.0,"[4, 508, 247]","[1697105102760, 1697105103268, 1697105103515]"
2947,506,56,[],200,EleutherAI/gpt-neox-20b,64,1,427.0,1.0,1,H100,1697105103516,1697105103943,120,16.0,1.0,"[5, 422]","[1697105103521, 1697105103943]"
2948,165,57,[],200,EleutherAI/gpt-neox-20b,64,1,3381.0,1.0,1,H100,1697105103944,1697105107325,120,83.0,20.0,"[4, 1218, 81, 75, 59, 71, 66, 53, 572, 111, 58, 44, 350, 58, 57, 55, 44, 286, 61, 58]","[1697105103948, 1697105105166, 1697105105247, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325]"
2949,670,38,[],200,EleutherAI/gpt-neox-20b,64,1,2790.0,1.0,1,H100,1697105079464,1697105082254,120,67.0,18.0,"[5, 701, 70, 71, 69, 66, 51, 636, 142, 74, 69, 69, 67, 425, 71, 71, 68, 65]","[1697105079469, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081418, 1697105081487, 1697105081554, 1697105081979, 1697105082050, 1697105082121, 1697105082189, 1697105082254]"
2950,573,44,[],200,EleutherAI/gpt-neox-20b,64,1,796.0,1.0,1,H100,1697105097314,1697105098110,120,874.0,2.0,"[9, 787]","[1697105097323, 1697105098110]"
2951,378,44,[],200,EleutherAI/gpt-neox-20b,64,1,5623.0,1.0,1,H100,1697105086595,1697105092218,120,93.0,47.0,"[4, 589, 71, 55, 71, 69, 67, 63, 547, 62, 61, 48, 56, 264, 60, 61, 58, 56, 55, 273, 57, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 529, 65, 49, 60, 59, 58, 529, 71, 67, 65, 64, 311, 74, 68, 52, 52]","[1697105086599, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088193, 1697105088254, 1697105088302, 1697105088358, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088912, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092046, 1697105092114, 1697105092166, 1697105092218]"
2952,231,45,[],200,EleutherAI/gpt-neox-20b,64,1,414.0,1.0,1,H100,1697105098111,1697105098525,120,13.0,1.0,"[8, 406]","[1697105098119, 1697105098525]"
2953,1,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105098526,1697105102061,120,,,"[13, 694, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 57, 61, 59, 463, 67, 66, 59, 47, 59]","[1697105098539, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100757, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2954,816,33,[],200,EleutherAI/gpt-neox-20b,64,1,564.0,1.0,1,H100,1697105073103,1697105073667,120,182.0,4.0,"[8, 440, 52, 64]","[1697105073111, 1697105073551, 1697105073603, 1697105073667]"
2955,368,43,[],200,EleutherAI/gpt-neox-20b,64,1,2800.0,1.0,1,H100,1697105093394,1697105096194,120,88.0,20.0,"[4, 774, 75, 56, 73, 70, 55, 54, 246, 54, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512]","[1697105093398, 1697105094172, 1697105094247, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194]"
2956,469,34,[],200,EleutherAI/gpt-neox-20b,64,1,565.0,1.0,1,H100,1697105073668,1697105074233,120,17.0,1.0,"[5, 560]","[1697105073673, 1697105074233]"
2957,800,43,[],200,EleutherAI/gpt-neox-20b,64,1,2869.0,1.0,1,H100,1697105093325,1697105096194,120,140.0,20.0,"[5, 842, 75, 56, 73, 70, 55, 54, 246, 54, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512]","[1697105093330, 1697105094172, 1697105094247, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194]"
2958,246,35,[],200,EleutherAI/gpt-neox-20b,64,1,6146.0,1.0,1,H100,1697105074234,1697105080380,120,58.0,47.0,"[12, 821, 67, 64, 52, 64, 63, 60, 356, 60, 49, 60, 57, 56, 265, 45, 53, 51, 342, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 55, 56, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69]","[1697105074246, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078252, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380]"
2959,57,48,[],200,EleutherAI/gpt-neox-20b,64,1,545.0,1.0,1,H100,1697105105519,1697105106064,120,13.0,1.0,"[28, 517]","[1697105105547, 1697105106064]"
2960,756,49,[],200,EleutherAI/gpt-neox-20b,64,1,509.0,1.0,1,H100,1697105106065,1697105106574,120,19.0,1.0,"[25, 484]","[1697105106090, 1697105106574]"
2961,417,50,[],200,EleutherAI/gpt-neox-20b,64,1,565.0,1.0,1,H100,1697105106575,1697105107140,120,17.0,1.0,"[13, 552]","[1697105106588, 1697105107140]"
2962,189,51,[],200,EleutherAI/gpt-neox-20b,64,1,3245.0,1.0,1,H100,1697105107141,1697105110386,120,88.0,20.0,"[8, 599, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 66, 63, 54, 573, 71, 121, 70, 53]","[1697105107149, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386]"
2963,771,52,[],200,EleutherAI/gpt-neox-20b,64,1,2374.0,1.0,1,H100,1697105110387,1697105112761,120,47.0,20.0,"[13, 505, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 72, 57, 72, 71]","[1697105110400, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112561, 1697105112618, 1697105112690, 1697105112761]"
2964,134,37,[],200,EleutherAI/gpt-neox-20b,64,1,2989.0,1.0,1,H100,1697105087585,1697105090574,120,86.0,20.0,"[5, 1031, 61, 60, 59, 56, 55, 272, 58, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 529]","[1697105087590, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574]"
2965,628,57,[],200,EleutherAI/gpt-neox-20b,64,1,1760.0,1.0,1,H100,1697105104494,1697105106254,120,732.0,10.0,"[4, 668, 81, 76, 58, 71, 66, 53, 572, 111]","[1697105104498, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254]"
2966,325,49,[],200,EleutherAI/gpt-neox-20b,64,1,3453.0,1.0,1,H100,1697105102065,1697105105518,120,85.0,20.0,"[14, 729, 707, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66]","[1697105102079, 1697105102808, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518]"
2967,907,50,[],200,EleutherAI/gpt-neox-20b,64,1,545.0,1.0,1,H100,1697105105519,1697105106064,120,10.0,1.0,"[12, 533]","[1697105105531, 1697105106064]"
2968,154,40,[],200,EleutherAI/gpt-neox-20b,64,1,715.0,1.0,1,H100,1697105080098,1697105080813,120,13.0,1.0,"[29, 686]","[1697105080127, 1697105080813]"
2969,169,39,[],200,EleutherAI/gpt-neox-20b,64,1,609.0,1.0,1,H100,1697105085906,1697105086515,120,10.0,1.0,"[20, 589]","[1697105085926, 1697105086515]"
2970,70,54,[],200,EleutherAI/gpt-neox-20b,64,1,366.0,1.0,1,H100,1697105098781,1697105099147,120,39.0,1.0,"[12, 353]","[1697105098793, 1697105099146]"
2971,860,41,[],200,EleutherAI/gpt-neox-20b,64,1,3599.0,1.0,1,H100,1697105080814,1697105084413,120,85.0,20.0,"[13, 1151, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 72, 68, 51, 67, 64, 592]","[1697105080827, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083571, 1697105083639, 1697105083690, 1697105083757, 1697105083821, 1697105084413]"
2972,758,40,[],200,EleutherAI/gpt-neox-20b,64,1,2669.0,1.0,1,H100,1697105086516,1697105089185,120,84.0,20.0,"[4, 668, 71, 55, 71, 69, 67, 63, 547, 63, 60, 48, 55, 265, 60, 61, 58, 56, 55, 273]","[1697105086520, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088194, 1697105088254, 1697105088302, 1697105088357, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088912, 1697105089185]"
2973,346,36,[],200,EleutherAI/gpt-neox-20b,64,1,3036.0,1.0,1,H100,1697105088358,1697105091394,120,85.0,20.0,"[5, 821, 58, 47, 57, 46, 55, 319, 65, 57, 56, 45, 56, 530, 64, 49, 60, 59, 58, 529]","[1697105088363, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089766, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394]"
2974,655,51,[],200,EleutherAI/gpt-neox-20b,64,1,1370.0,1.0,1,H100,1697105106065,1697105107435,120,335.0,11.0,"[5, 636, 58, 57, 55, 44, 286, 61, 59, 55, 54]","[1697105106070, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107326, 1697105107381, 1697105107435]"
2975,653,55,[],200,EleutherAI/gpt-neox-20b,64,1,923.0,1.0,1,H100,1697105099147,1697105100070,120,96.0,6.0,"[5, 685, 63, 50, 63, 57]","[1697105099152, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070]"
2976,218,44,[],200,EleutherAI/gpt-neox-20b,64,1,626.0,1.0,1,H100,1697105099498,1697105100124,120,109.0,7.0,"[12, 327, 63, 51, 62, 57, 54]","[1697105099510, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124]"
2977,425,56,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100071,1697105102062,120,,,"[8, 505, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105100079, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2978,323,39,[],200,EleutherAI/gpt-neox-20b,64,1,3650.0,1.0,1,H100,1697105082255,1697105085905,120,84.0,20.0,"[8, 1236, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562]","[1697105082263, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905]"
2979,801,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100125,1697105102062,120,,,"[5, 1210, 67, 66, 59, 47, 59]","[1697105100130, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
2980,576,46,[],200,EleutherAI/gpt-neox-20b,64,1,1192.0,1.0,1,H100,1697105102076,1697105103268,120,14.0,1.0,"[140, 1052]","[1697105102216, 1697105103268]"
2981,236,47,[],200,EleutherAI/gpt-neox-20b,64,1,674.0,1.0,1,H100,1697105103269,1697105103943,120,8.0,1.0,"[36, 638]","[1697105103305, 1697105103943]"
2982,637,59,[],200,EleutherAI/gpt-neox-20b,64,1,2903.0,1.0,1,H100,1697105122646,1697105125549,120,96.0,20.0,"[9, 1190, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 72]","[1697105122655, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548]"
2983,656,28,[],200,EleutherAI/gpt-neox-20b,64,1,843.0,1.0,1,H100,1697105075438,1697105076281,120,26.0,1.0,"[12, 830]","[1697105075450, 1697105076280]"
2984,71,40,[],200,EleutherAI/gpt-neox-20b,64,1,1479.0,1.0,1,H100,1697105085906,1697105087385,120,364.0,11.0,"[12, 676, 69, 53, 54, 65, 62, 291, 72, 54, 71]","[1697105085918, 1697105086594, 1697105086663, 1697105086716, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087385]"
2985,6,48,[],200,EleutherAI/gpt-neox-20b,64,1,4076.0,1.0,1,H100,1697105103944,1697105108020,120,100.0,29.0,"[12, 1210, 81, 76, 58, 71, 66, 53, 572, 111, 58, 44, 350, 58, 57, 55, 44, 286, 61, 58, 56, 54, 53, 259, 62, 61, 49, 56, 45]","[1697105103956, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107747, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020]"
2986,317,29,[],200,EleutherAI/gpt-neox-20b,64,1,6804.0,1.0,1,H100,1697105076282,1697105083086,120,244.0,50.0,"[8, 541, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 58, 916, 72, 68, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 141, 75, 70, 68, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69]","[1697105076290, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078547, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085]"
2987,771,41,[],200,EleutherAI/gpt-neox-20b,64,1,2446.0,1.0,1,H100,1697105087385,1697105089831,120,47.0,20.0,"[5, 741, 62, 62, 47, 56, 263, 61, 60, 59, 56, 55, 272, 58, 47, 57, 46, 55, 320, 64]","[1697105087390, 1697105088131, 1697105088193, 1697105088255, 1697105088302, 1697105088358, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831]"
2988,102,44,[],200,EleutherAI/gpt-neox-20b,64,1,3505.0,1.0,1,H100,1697105102066,1697105105571,120,84.0,20.0,"[105, 1343, 67, 51, 50, 64, 408, 72, 71, 69, 64, 62, 61, 613, 81, 76, 58, 71, 66, 52]","[1697105102171, 1697105103514, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105570]"
2989,432,42,[],200,EleutherAI/gpt-neox-20b,64,1,595.0,1.0,1,H100,1697105089832,1697105090427,120,13.0,1.0,"[9, 586]","[1697105089841, 1697105090427]"
2990,202,43,[],200,EleutherAI/gpt-neox-20b,64,1,9003.0,1.0,1,H100,1697105090428,1697105099431,120,874.0,72.0,"[12, 954, 71, 67, 65, 64, 312, 74, 67, 52, 52, 65, 299, 57, 66, 52, 66, 64, 361, 76, 69, 67, 51, 52, 609, 74, 57, 72, 71, 55, 54, 246, 54, 67, 65, 50, 65, 336, 66, 50, 64, 64, 511, 75, 56, 74, 55, 71, 55, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75, 67, 53, 51, 242, 55, 57, 68, 62, 394, 72, 126]","[1697105090440, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096524, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098251, 1697105098304, 1697105098355, 1697105098597, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431]"
2991,520,39,[],200,EleutherAI/gpt-neox-20b,64,1,659.0,1.0,1,H100,1697105090576,1697105091235,120,11.0,1.0,"[25, 634]","[1697105090601, 1697105091235]"
2992,180,40,[],200,EleutherAI/gpt-neox-20b,64,1,1587.0,1.0,1,H100,1697105091236,1697105092823,120,123.0,12.0,"[8, 729, 74, 67, 52, 52, 65, 299, 56, 68, 51, 66]","[1697105091244, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092706, 1697105092757, 1697105092823]"
2993,599,35,[],200,EleutherAI/gpt-neox-20b,64,1,7205.0,1.0,1,H100,1697105084767,1697105091972,120,58.0,55.0,"[16, 1122, 72, 70, 68, 67, 63, 348, 70, 53, 54, 65, 62, 291, 72, 54, 71, 69, 67, 63, 546, 63, 61, 48, 55, 264, 61, 61, 58, 56, 55, 272, 58, 47, 57, 46, 55, 319, 65, 57, 56, 45, 56, 529, 65, 48, 61, 59, 58, 529, 70, 68, 65, 64, 311]","[1697105084783, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088912, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089766, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090687, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091464, 1697105091532, 1697105091597, 1697105091661, 1697105091972]"
2994,594,49,[],200,EleutherAI/gpt-neox-20b,64,1,16498.0,1.0,1,H100,1697105108021,1697105124519,120,216.0,119.0,"[8, 1286, 65, 64, 54, 573, 72, 120, 74, 49, 60, 458, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 47, 61, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 328, 75, 72, 64, 330, 73, 71, 72, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 78, 58, 58, 58, 69, 353]","[1697105108029, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114231, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118362, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519]"
2995,572,59,[],200,EleutherAI/gpt-neox-20b,64,1,716.0,1.0,1,H100,1697105122955,1697105123671,120,16.0,1.0,"[12, 704]","[1697105122967, 1697105123671]"
2996,877,41,[],200,EleutherAI/gpt-neox-20b,64,1,2278.0,1.0,1,H100,1697105092824,1697105095102,120,85.0,20.0,"[4, 420, 76, 69, 67, 52, 51, 609, 74, 57, 72, 71, 55, 54, 247, 53, 67, 65, 50, 65]","[1697105092828, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093512, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102]"
2997,813,71,[],200,EleutherAI/gpt-neox-20b,64,1,2344.0,1.0,1,H100,1697105117965,1697105120309,120,85.0,20.0,"[8, 392, 72, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77]","[1697105117973, 1697105118365, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309]"
2998,763,66,[],200,EleutherAI/gpt-neox-20b,64,1,546.0,1.0,1,H100,1697105118904,1697105119450,120,20.0,1.0,"[4, 542]","[1697105118908, 1697105119450]"
2999,540,67,[],200,EleutherAI/gpt-neox-20b,64,1,1002.0,1.0,1,H100,1697105119451,1697105120453,120,140.0,5.0,"[5, 776, 77, 72, 72]","[1697105119456, 1697105120232, 1697105120309, 1697105120381, 1697105120453]"
3000,302,55,[],200,EleutherAI/gpt-neox-20b,64,1,2829.0,1.0,1,H100,1697105119816,1697105122645,120,85.0,20.0,"[5, 412, 76, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 172, 154, 80, 77]","[1697105119821, 1697105120233, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122334, 1697105122488, 1697105122568, 1697105122645]"
3001,194,68,[],200,EleutherAI/gpt-neox-20b,64,1,2676.0,1.0,1,H100,1697105120454,1697105123130,120,335.0,16.0,"[4, 651, 74, 55, 55, 56, 66, 513, 82, 152, 173, 153, 80, 77, 406, 79]","[1697105120458, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121349, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122335, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130]"
3002,831,38,[],200,EleutherAI/gpt-neox-20b,64,1,659.0,1.0,1,H100,1697105090576,1697105091235,120,11.0,1.0,"[21, 638]","[1697105090597, 1697105091235]"
3003,464,39,[],200,EleutherAI/gpt-neox-20b,64,1,659.0,1.0,1,H100,1697105091236,1697105091895,120,12.0,1.0,"[13, 646]","[1697105091249, 1697105091895]"
3004,890,69,[],200,EleutherAI/gpt-neox-20b,64,1,2418.0,1.0,1,H100,1697105123131,1697105125549,120,93.0,20.0,"[5, 709, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 72]","[1697105123136, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548]"
3005,69,31,[],200,EleutherAI/gpt-neox-20b,64,1,3114.0,1.0,1,H100,1697105073780,1697105076894,120,85.0,20.0,"[44, 1243, 67, 64, 52, 64, 63, 59, 357, 61, 48, 60, 57, 56, 265, 45, 53, 51, 342, 63]","[1697105073824, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075854, 1697105075902, 1697105075962, 1697105076019, 1697105076075, 1697105076340, 1697105076385, 1697105076438, 1697105076489, 1697105076831, 1697105076894]"
3006,216,46,[],200,EleutherAI/gpt-neox-20b,64,1,2466.0,1.0,1,H100,1697105105453,1697105107919,120,91.0,20.0,"[5, 685, 111, 58, 44, 350, 58, 57, 55, 44, 287, 61, 57, 56, 54, 53, 260, 61, 61, 49]","[1697105105458, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919]"
3007,157,35,[],200,EleutherAI/gpt-neox-20b,64,1,7936.0,1.0,1,H100,1697105078309,1697105086245,120,563.0,55.0,"[17, 1137, 72, 68, 67, 52, 58, 390, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63]","[1697105078326, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079722, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245]"
3008,526,41,[],200,EleutherAI/gpt-neox-20b,64,1,2861.0,1.0,1,H100,1697105089186,1697105092047,120,89.0,20.0,"[8, 573, 64, 57, 56, 45, 56, 529, 65, 48, 61, 59, 58, 530, 70, 67, 65, 64, 311, 75]","[1697105089194, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090687, 1697105090748, 1697105090807, 1697105090865, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047]"
3009,83,30,[],200,EleutherAI/gpt-neox-20b,64,1,2892.0,1.0,1,H100,1697105083086,1697105085978,120,123.0,15.0,"[5, 1322, 77, 56, 74, 73, 72, 272, 56, 73, 54, 55, 68, 562, 72]","[1697105083091, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085905, 1697105085977]"
3010,419,45,[],200,EleutherAI/gpt-neox-20b,64,1,2861.0,1.0,1,H100,1697105089186,1697105092047,120,88.0,20.0,"[20, 561, 64, 57, 56, 45, 56, 529, 65, 49, 60, 59, 58, 530, 70, 67, 65, 64, 311, 75]","[1697105089206, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047]"
3011,801,45,[],200,EleutherAI/gpt-neox-20b,64,1,2929.0,1.0,1,H100,1697105105571,1697105108500,120,47.0,20.0,"[5, 1130, 58, 57, 55, 44, 287, 61, 58, 55, 54, 53, 260, 61, 61, 49, 56, 45, 423, 57]","[1697105105576, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500]"
3012,668,31,[],200,EleutherAI/gpt-neox-20b,64,1,857.0,1.0,1,H100,1697105085978,1697105086835,120,109.0,6.0,"[5, 611, 69, 55, 52, 65]","[1697105085983, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835]"
3013,444,32,[],200,EleutherAI/gpt-neox-20b,64,1,618.0,1.0,1,H100,1697105086836,1697105087454,120,457.0,6.0,"[5, 347, 72, 54, 71, 69]","[1697105086841, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454]"
3014,917,47,[],200,EleutherAI/gpt-neox-20b,64,1,523.0,1.0,1,H100,1697105107920,1697105108443,120,123.0,2.0,"[5, 518]","[1697105107925, 1697105108443]"
3015,574,48,[],200,EleutherAI/gpt-neox-20b,64,1,871.0,1.0,1,H100,1697105108444,1697105109315,120,364.0,2.0,"[4, 867]","[1697105108448, 1697105109315]"
3016,344,49,[],200,EleutherAI/gpt-neox-20b,64,1,601.0,1.0,1,H100,1697105109316,1697105109917,120,13.0,1.0,"[5, 596]","[1697105109321, 1697105109917]"
3017,89,50,[],200,EleutherAI/gpt-neox-20b,64,1,2843.0,1.0,1,H100,1697105109918,1697105112761,120,52.0,20.0,"[8, 979, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71]","[1697105109926, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761]"
3018,748,45,[],200,EleutherAI/gpt-neox-20b,64,1,2237.0,1.0,1,H100,1697105105572,1697105107809,120,182.0,14.0,"[12, 1122, 58, 57, 55, 44, 287, 61, 58, 55, 54, 53, 260, 61]","[1697105105584, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809]"
3019,17,49,[],200,EleutherAI/gpt-neox-20b,64,1,795.0,1.0,1,H100,1697105115763,1697105116558,120,23.0,1.0,"[5, 790]","[1697105115768, 1697105116558]"
3020,599,50,[],200,EleutherAI/gpt-neox-20b,64,1,7607.0,1.0,1,H100,1697105116559,1697105124166,120,58.0,55.0,"[9, 1178, 81, 59, 78, 71, 331, 71, 71, 65, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 78, 58, 58, 58, 69]","[1697105116568, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118366, 1697105118437, 1697105118508, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166]"
3021,672,51,[],200,EleutherAI/gpt-neox-20b,64,1,2462.0,1.0,1,H100,1697105112762,1697105115224,120,93.0,20.0,"[13, 498, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68, 66]","[1697105112775, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224]"
3022,863,42,[],200,EleutherAI/gpt-neox-20b,64,1,705.0,1.0,1,H100,1697105120310,1697105121015,120,10.0,1.0,"[13, 692]","[1697105120323, 1697105121015]"
3023,351,44,[],200,EleutherAI/gpt-neox-20b,64,1,979.0,1.0,1,H100,1697105117056,1697105118035,120,216.0,6.0,"[8, 682, 81, 59, 78, 71]","[1697105117064, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035]"
3024,611,43,[],200,EleutherAI/gpt-neox-20b,64,1,654.0,1.0,1,H100,1697105121016,1697105121670,120,14.0,1.0,"[13, 641]","[1697105121029, 1697105121670]"
3025,13,45,[],200,EleutherAI/gpt-neox-20b,64,1,3004.0,1.0,1,H100,1697105107382,1697105110386,120,90.0,20.0,"[4, 362, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 66, 63, 54, 573, 71, 121, 70, 53]","[1697105107386, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386]"
3026,12,45,[],200,EleutherAI/gpt-neox-20b,64,1,791.0,1.0,1,H100,1697105118036,1697105118827,120,11.0,1.0,"[16, 775]","[1697105118052, 1697105118827]"
3027,710,46,[],200,EleutherAI/gpt-neox-20b,64,1,623.0,1.0,1,H100,1697105118828,1697105119451,120,14.0,1.0,"[20, 602]","[1697105118848, 1697105119450]"
3028,366,47,[],200,EleutherAI/gpt-neox-20b,64,1,1071.0,1.0,1,H100,1697105119451,1697105120522,120,85.0,6.0,"[13, 769, 76, 72, 72, 69]","[1697105119464, 1697105120233, 1697105120309, 1697105120381, 1697105120453, 1697105120522]"
3029,114,48,[],200,EleutherAI/gpt-neox-20b,64,1,2892.0,1.0,1,H100,1697105120523,1697105123415,120,88.0,20.0,"[4, 582, 74, 55, 55, 56, 66, 513, 83, 151, 173, 153, 80, 77, 406, 79, 77, 77, 74, 57]","[1697105120527, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121349, 1697105121415, 1697105121928, 1697105122011, 1697105122162, 1697105122335, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415]"
3030,711,46,[],200,EleutherAI/gpt-neox-20b,64,1,860.0,1.0,1,H100,1697105110387,1697105111247,120,457.0,4.0,"[17, 500, 215, 128]","[1697105110404, 1697105110904, 1697105111119, 1697105111247]"
3031,813,49,[],200,EleutherAI/gpt-neox-20b,64,1,2949.0,1.0,1,H100,1697105123416,1697105126365,120,85.0,20.0,"[9, 1094, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 73, 64, 476, 80, 60, 77, 59]","[1697105123425, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125549, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365]"
3032,231,40,[],200,EleutherAI/gpt-neox-20b,64,1,609.0,1.0,1,H100,1697105091897,1697105092506,120,13.0,1.0,"[16, 593]","[1697105091913, 1697105092506]"
3033,824,41,[],200,EleutherAI/gpt-neox-20b,64,1,886.0,1.0,1,H100,1697105092507,1697105093393,120,58.0,4.0,"[4, 737, 76, 69]","[1697105092511, 1697105093248, 1697105093324, 1697105093393]"
3034,343,47,[],200,EleutherAI/gpt-neox-20b,64,1,2438.0,1.0,1,H100,1697105111248,1697105113686,120,84.0,20.0,"[6, 540, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62]","[1697105111254, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686]"
3035,483,33,[],200,EleutherAI/gpt-neox-20b,64,1,2296.0,1.0,1,H100,1697105073779,1697105076075,120,84.0,20.0,"[5, 519, 64, 50, 61, 60, 53, 476, 67, 65, 51, 65, 62, 60, 356, 60, 49, 60, 57, 56]","[1697105073784, 1697105074303, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075199, 1697105075250, 1697105075315, 1697105075377, 1697105075437, 1697105075793, 1697105075853, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
3036,78,57,[],200,EleutherAI/gpt-neox-20b,64,1,3503.0,1.0,1,H100,1697105102068,1697105105571,120,84.0,20.0,"[111, 1336, 66, 51, 50, 64, 408, 72, 71, 69, 64, 62, 61, 613, 81, 76, 58, 71, 66, 53]","[1697105102179, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571]"
3037,587,47,[],200,EleutherAI/gpt-neox-20b,64,1,690.0,1.0,1,H100,1697105102065,1697105102755,120,13.0,1.0,"[38, 652]","[1697105102103, 1697105102755]"
3038,541,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 12.41 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.83 GiB is free. Process 1645736 has 68.27 GiB memory in use. Of the allocated memory 53.83 GiB is allocated by PyTorch, and 13.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105058094,1697105058840,120,,,[4],[1697105058098]
3039,315,20,[],200,EleutherAI/gpt-neox-20b,64,1,2564.0,1.0,1,H100,1697105058843,1697105061407,120,335.0,14.0,"[145, 1160, 59, 57, 46, 53, 401, 64, 169, 111, 111, 64, 62, 62]","[1697105058988, 1697105060148, 1697105060207, 1697105060264, 1697105060310, 1697105060363, 1697105060764, 1697105060828, 1697105060997, 1697105061108, 1697105061219, 1697105061283, 1697105061345, 1697105061407]"
3040,577,44,[],200,EleutherAI/gpt-neox-20b,64,1,1339.0,1.0,1,H100,1697105096195,1697105097534,120,93.0,9.0,"[12, 616, 70, 68, 67, 65, 297, 74, 70]","[1697105096207, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097093, 1697105097390, 1697105097464, 1697105097534]"
3041,834,36,[],200,EleutherAI/gpt-neox-20b,64,1,3118.0,1.0,1,H100,1697105080381,1697105083499,120,85.0,20.0,"[4, 748, 142, 74, 70, 68, 67, 424, 72, 71, 68, 65, 435, 78, 72, 70, 54, 53, 69, 414]","[1697105080385, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082689, 1697105082767, 1697105082839, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499]"
3042,801,52,[],200,EleutherAI/gpt-neox-20b,64,1,2735.0,1.0,1,H100,1697105106709,1697105109444,120,47.0,20.0,"[16, 482, 61, 57, 56, 54, 53, 260, 61, 61, 49, 56, 45, 423, 58, 49, 48, 717, 65, 64]","[1697105106725, 1697105107207, 1697105107268, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108501, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
3043,232,45,[],200,EleutherAI/gpt-neox-20b,64,1,2478.0,1.0,1,H100,1697105097535,1697105100013,120,93.0,20.0,"[4, 571, 74, 68, 52, 52, 242, 54, 57, 68, 62, 394, 72, 126, 66, 55, 285, 63, 51, 62]","[1697105097539, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013]"
3044,604,37,[],200,EleutherAI/gpt-neox-20b,64,1,1046.0,1.0,1,H100,1697105083500,1697105084546,120,161.0,4.0,"[9, 904, 77, 56]","[1697105083509, 1697105084413, 1697105084490, 1697105084546]"
3045,897,57,[],200,EleutherAI/gpt-neox-20b,64,1,579.0,1.0,1,H100,1697105127668,1697105128247,120,9.0,1.0,"[4, 575]","[1697105127672, 1697105128247]"
3046,674,58,[],200,EleutherAI/gpt-neox-20b,64,1,7759.0,1.0,1,H100,1697105128248,1697105136007,120,161.0,119.0,"[16, 792, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70, 65, 63, 49, 50, 49, 88, 64, 58, 43, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 31, 32, 32, 31, 38, 31, 32, 31, 31, 31, 38, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 32, 31, 31, 31, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 38, 31, 31, 30, 31, 31]","[1697105128264, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134492, 1697105134524, 1697105134556, 1697105134587, 1697105134625, 1697105134656, 1697105134688, 1697105134719, 1697105134750, 1697105134781, 1697105134819, 1697105134850, 1697105134881, 1697105134912, 1697105134943, 1697105134974, 1697105135005, 1697105135036, 1697105135067, 1697105135098, 1697105135128, 1697105135159, 1697105135190, 1697105135221, 1697105135252, 1697105135283, 1697105135315, 1697105135346, 1697105135377, 1697105135408, 1697105135439, 1697105135470, 1697105135502, 1697105135533, 1697105135564, 1697105135596, 1697105135627, 1697105135658, 1697105135690, 1697105135721, 1697105135752, 1697105135784, 1697105135815, 1697105135853, 1697105135884, 1697105135915, 1697105135945, 1697105135976, 1697105136007]"
3047,157,62,[],200,EleutherAI/gpt-neox-20b,64,1,6843.0,1.0,1,H100,1697105117965,1697105124808,120,563.0,55.0,"[12, 388, 72, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73]","[1697105117977, 1697105118365, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808]"
3048,462,53,[],200,EleutherAI/gpt-neox-20b,64,1,472.0,1.0,1,H100,1697105109445,1697105109917,120,52.0,1.0,"[25, 447]","[1697105109470, 1697105109917]"
3049,232,54,[],200,EleutherAI/gpt-neox-20b,64,1,2843.0,1.0,1,H100,1697105109918,1697105112761,120,93.0,20.0,"[4, 983, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71]","[1697105109922, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761]"
3050,622,47,[],200,EleutherAI/gpt-neox-20b,64,1,272.0,1.0,1,H100,1697105099498,1697105099770,120,20.0,1.0,"[8, 264]","[1697105099506, 1697105099770]"
3051,276,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099771,1697105102062,120,,,"[13, 800, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105099784, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3052,469,50,[],200,EleutherAI/gpt-neox-20b,64,1,319.0,1.0,1,H100,1697105126366,1697105126685,120,17.0,1.0,"[16, 302]","[1697105126382, 1697105126684]"
3053,512,42,[],200,EleutherAI/gpt-neox-20b,64,1,547.0,1.0,1,H100,1697105084414,1697105084961,120,11.0,1.0,"[13, 534]","[1697105084427, 1697105084961]"
3054,47,49,[],200,EleutherAI/gpt-neox-20b,64,1,3453.0,1.0,1,H100,1697105102065,1697105105518,120,90.0,20.0,"[25, 718, 707, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66]","[1697105102090, 1697105102808, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518]"
3055,594,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105093394,1697105102062,120,,,"[8, 770, 75, 56, 73, 70, 55, 54, 246, 54, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512, 74, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75, 67, 53, 51, 242, 55, 57, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63, 56, 55, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105093402, 1697105094172, 1697105094247, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098251, 1697105098304, 1697105098355, 1697105098597, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100069, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3056,288,43,[],200,EleutherAI/gpt-neox-20b,64,1,2622.0,1.0,1,H100,1697105084962,1697105087584,120,93.0,20.0,"[4, 939, 72, 70, 68, 67, 63, 348, 70, 53, 53, 66, 62, 291, 72, 54, 71, 69, 67, 63]","[1697105084966, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086593, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584]"
3057,633,50,[],200,EleutherAI/gpt-neox-20b,64,1,2401.0,1.0,1,H100,1697105105519,1697105107920,120,90.0,20.0,"[16, 608, 111, 58, 44, 350, 58, 57, 55, 44, 287, 61, 58, 55, 54, 53, 260, 61, 61, 49]","[1697105105535, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919]"
3058,359,48,[],200,EleutherAI/gpt-neox-20b,64,1,512.0,1.0,1,H100,1697105102756,1697105103268,120,10.0,1.0,"[21, 491]","[1697105102777, 1697105103268]"
3059,292,60,[],200,EleutherAI/gpt-neox-20b,64,1,460.0,1.0,1,H100,1697105125549,1697105126009,120,286.0,1.0,"[9, 451]","[1697105125558, 1697105126009]"
3060,12,49,[],200,EleutherAI/gpt-neox-20b,64,1,674.0,1.0,1,H100,1697105103269,1697105103943,120,11.0,1.0,"[33, 641]","[1697105103302, 1697105103943]"
3061,871,44,[],200,EleutherAI/gpt-neox-20b,64,1,1272.0,1.0,1,H100,1697105087585,1697105088857,120,123.0,6.0,"[5, 1031, 61, 60, 59, 56]","[1697105087590, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857]"
3062,62,61,[],200,EleutherAI/gpt-neox-20b,64,1,2652.0,1.0,1,H100,1697105126010,1697105128662,120,91.0,20.0,"[9, 748, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 66, 53]","[1697105126019, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662]"
3063,639,45,[],200,EleutherAI/gpt-neox-20b,64,1,534.0,1.0,1,H100,1697105088858,1697105089392,120,100.0,6.0,"[4, 323, 57, 47, 57, 46]","[1697105088862, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392]"
3064,215,40,[],200,EleutherAI/gpt-neox-20b,64,1,487.0,1.0,1,H100,1697105100014,1697105100501,120,12.0,1.0,"[13, 474]","[1697105100027, 1697105100501]"
3065,271,46,[],200,EleutherAI/gpt-neox-20b,64,1,2654.0,1.0,1,H100,1697105089393,1697105092047,120,87.0,20.0,"[4, 370, 64, 57, 56, 45, 56, 529, 65, 49, 60, 59, 58, 530, 70, 67, 65, 64, 311, 75]","[1697105089397, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091395, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092047]"
3066,40,47,[],200,EleutherAI/gpt-neox-20b,64,1,2507.0,1.0,1,H100,1697105092048,1697105094555,120,86.0,20.0,"[17, 517, 57, 67, 51, 66, 64, 361, 76, 69, 67, 51, 52, 609, 75, 56, 72, 71, 55, 54]","[1697105092065, 1697105092582, 1697105092639, 1697105092706, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094247, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555]"
3067,630,48,[],200,EleutherAI/gpt-neox-20b,64,1,808.0,1.0,1,H100,1697105094557,1697105095365,120,6.0,1.0,"[24, 784]","[1697105094581, 1697105095365]"
3068,795,41,[],200,EleutherAI/gpt-neox-20b,64,1,763.0,1.0,1,H100,1697105100502,1697105101265,120,12.0,1.0,"[17, 746]","[1697105100519, 1697105101265]"
3069,565,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101266,1697105102062,120,,,[5],[1697105101271]
3070,622,62,[],200,EleutherAI/gpt-neox-20b,64,1,1030.0,1.0,1,H100,1697105128663,1697105129693,120,20.0,1.0,"[8, 1022]","[1697105128671, 1697105129693]"
3071,98,33,[],200,EleutherAI/gpt-neox-20b,64,1,592.0,1.0,1,H100,1697105087455,1697105088047,120,14.0,1.0,"[5, 586]","[1697105087460, 1697105088046]"
3072,64,29,[],200,EleutherAI/gpt-neox-20b,64,1,2902.0,1.0,1,H100,1697105077595,1697105080497,120,89.0,20.0,"[4, 598, 56, 55, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51]","[1697105077599, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497]"
3073,225,43,[],200,EleutherAI/gpt-neox-20b,64,1,690.0,1.0,1,H100,1697105102065,1697105102755,120,23.0,1.0,"[58, 632]","[1697105102123, 1697105102755]"
3074,924,44,[],200,EleutherAI/gpt-neox-20b,64,1,512.0,1.0,1,H100,1697105102756,1697105103268,120,9.0,1.0,"[20, 492]","[1697105102776, 1697105103268]"
3075,802,34,[],200,EleutherAI/gpt-neox-20b,64,1,509.0,1.0,1,H100,1697105088048,1697105088557,120,9.0,1.0,"[8, 501]","[1697105088056, 1697105088557]"
3076,398,49,[],200,EleutherAI/gpt-neox-20b,64,1,2818.0,1.0,1,H100,1697105095366,1697105098184,120,87.0,20.0,"[12, 816, 74, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 441, 74]","[1697105095378, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184]"
3077,191,46,[],200,EleutherAI/gpt-neox-20b,64,1,2507.0,1.0,1,H100,1697105092048,1697105094555,120,85.0,20.0,"[16, 518, 57, 67, 51, 66, 64, 361, 76, 69, 67, 51, 52, 609, 75, 56, 72, 71, 55, 54]","[1697105092064, 1697105092582, 1697105092639, 1697105092706, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094247, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555]"
3078,264,44,[],200,EleutherAI/gpt-neox-20b,64,1,3206.0,1.0,1,H100,1697105121671,1697105124877,120,86.0,20.0,"[9, 1371, 79, 77, 77, 75, 56, 71, 359, 78, 58, 58, 58, 69, 353, 79, 78, 59, 73, 69]","[1697105121680, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877]"
3079,898,55,[],200,EleutherAI/gpt-neox-20b,64,1,480.0,1.0,1,H100,1697105112009,1697105112489,120,79.0,2.0,"[5, 475]","[1697105112014, 1697105112489]"
3080,670,56,[],200,EleutherAI/gpt-neox-20b,64,1,2600.0,1.0,1,H100,1697105112490,1697105115090,120,67.0,18.0,"[5, 778, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 47, 61, 520, 140, 70, 68]","[1697105112495, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114231, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090]"
3081,323,42,[],200,EleutherAI/gpt-neox-20b,64,1,2401.0,1.0,1,H100,1697105105519,1697105107920,120,84.0,20.0,"[4, 620, 111, 58, 44, 350, 58, 57, 55, 44, 287, 61, 58, 55, 54, 53, 260, 61, 61, 49]","[1697105105523, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919]"
3082,791,46,[],200,EleutherAI/gpt-neox-20b,64,1,8519.0,1.0,1,H100,1697105109445,1697105117964,120,182.0,64.0,"[17, 609, 72, 120, 70, 53, 60, 459, 214, 128, 73, 55, 64, 62, 49, 243, 65, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78]","[1697105109462, 1697105110071, 1697105110143, 1697105110263, 1697105110333, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111793, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964]"
3083,635,41,[],200,EleutherAI/gpt-neox-20b,64,1,472.0,1.0,1,H100,1697105109445,1697105109917,120,23.0,1.0,"[13, 459]","[1697105109458, 1697105109917]"
3084,382,42,[],200,EleutherAI/gpt-neox-20b,64,1,2843.0,1.0,1,H100,1697105109918,1697105112761,120,47.0,20.0,"[12, 975, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71]","[1697105109930, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761]"
3085,774,47,[],200,EleutherAI/gpt-neox-20b,64,1,808.0,1.0,1,H100,1697105094557,1697105095365,120,8.0,1.0,"[20, 788]","[1697105094577, 1697105095365]"
3086,554,48,[],200,EleutherAI/gpt-neox-20b,64,1,658.0,1.0,1,H100,1697105095366,1697105096024,120,26.0,1.0,"[13, 645]","[1697105095379, 1697105096024]"
3087,206,49,[],200,EleutherAI/gpt-neox-20b,64,1,718.0,1.0,1,H100,1697105096025,1697105096743,120,16.0,1.0,"[8, 710]","[1697105096033, 1697105096743]"
3088,882,50,[],200,EleutherAI/gpt-neox-20b,64,1,1612.0,1.0,1,H100,1697105096744,1697105098356,120,345.0,11.0,"[4, 642, 74, 70, 68, 67, 441, 74, 68, 52, 52]","[1697105096748, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356]"
3089,652,51,[],200,EleutherAI/gpt-neox-20b,64,1,790.0,1.0,1,H100,1697105098357,1697105099147,120,14.0,1.0,"[8, 781]","[1697105098365, 1697105099146]"
3090,307,52,[],200,EleutherAI/gpt-neox-20b,64,1,622.0,1.0,1,H100,1697105099148,1697105099770,120,26.0,1.0,"[20, 602]","[1697105099168, 1697105099770]"
3091,84,53,[],200,EleutherAI/gpt-neox-20b,64,1,730.0,1.0,1,H100,1697105099771,1697105100501,120,26.0,1.0,"[17, 713]","[1697105099788, 1697105100501]"
3092,667,54,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100502,1697105102061,120,,,"[9, 829, 67, 66, 59, 47, 59]","[1697105100511, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3093,110,48,[],200,EleutherAI/gpt-neox-20b,64,1,1335.0,1.0,1,H100,1697105113687,1697105115022,120,96.0,4.0,"[9, 1116, 140, 70]","[1697105113696, 1697105114812, 1697105114952, 1697105115022]"
3094,701,49,[],200,EleutherAI/gpt-neox-20b,64,1,5567.0,1.0,1,H100,1697105115023,1697105120590,120,58.0,43.0,"[5, 734, 75, 56, 56, 72, 54, 55, 69, 570, 80, 76, 74, 55, 66, 626, 81, 59, 78, 71, 330, 72, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68]","[1697105115028, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116199, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118365, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590]"
3095,438,55,[],200,EleutherAI/gpt-neox-20b,64,1,690.0,1.0,1,H100,1697105102065,1697105102755,120,9.0,1.0,"[66, 624]","[1697105102131, 1697105102755]"
3096,348,60,[],200,EleutherAI/gpt-neox-20b,64,1,2693.0,1.0,1,H100,1697105123672,1697105126365,120,91.0,20.0,"[12, 835, 79, 78, 59, 73, 69, 277, 58, 59, 74, 57, 74, 73, 64, 476, 80, 60, 77, 59]","[1697105123684, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125212, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125549, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365]"
3097,2,61,[],200,EleutherAI/gpt-neox-20b,64,1,699.0,1.0,1,H100,1697105126366,1697105127065,120,58.0,6.0,"[4, 397, 79, 77, 75, 67]","[1697105126370, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065]"
3098,706,62,[],200,EleutherAI/gpt-neox-20b,64,1,2376.0,1.0,1,H100,1697105127066,1697105129442,120,86.0,20.0,"[4, 536, 61, 78, 79, 77, 72, 68, 284, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62]","[1697105127070, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442]"
3099,583,45,[],200,EleutherAI/gpt-neox-20b,64,1,3437.0,1.0,1,H100,1697105103269,1697105106706,120,96.0,20.0,"[28, 857, 72, 71, 69, 64, 63, 61, 612, 80, 76, 59, 71, 66, 53, 572, 111, 58, 44, 350]","[1697105103297, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104493, 1697105104554, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706]"
3100,190,37,[],200,EleutherAI/gpt-neox-20b,64,1,1885.0,1.0,1,H100,1697105108501,1697105110386,120,335.0,10.0,"[5, 809, 65, 64, 54, 573, 72, 120, 70, 53]","[1697105108506, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110333, 1697105110386]"
3101,71,56,[],200,EleutherAI/gpt-neox-20b,64,1,2089.0,1.0,1,H100,1697105122646,1697105124735,120,364.0,11.0,"[8, 1191, 79, 57, 58, 58, 69, 353, 79, 78, 59]","[1697105122654, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735]"
3102,925,37,[],200,EleutherAI/gpt-neox-20b,64,1,2775.0,1.0,1,H100,1697105091397,1697105094172,120,87.0,20.0,"[10, 566, 74, 67, 52, 52, 65, 299, 56, 68, 51, 66, 64, 361, 77, 68, 67, 51, 52, 609]","[1697105091407, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092706, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093325, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172]"
3103,863,58,[],200,EleutherAI/gpt-neox-20b,64,1,357.0,1.0,1,H100,1697105107327,1697105107684,120,10.0,1.0,"[4, 353]","[1697105107331, 1697105107684]"
3104,187,46,[],200,EleutherAI/gpt-neox-20b,64,1,891.0,1.0,1,H100,1697105097465,1697105098356,120,161.0,6.0,"[4, 641, 74, 68, 52, 52]","[1697105097469, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356]"
3105,524,59,[],200,EleutherAI/gpt-neox-20b,64,1,4804.0,1.0,1,H100,1697105107685,1697105112489,120,100.0,30.0,"[17, 741, 57, 50, 48, 717, 65, 64, 54, 573, 72, 120, 74, 49, 60, 458, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423]","[1697105107702, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489]"
3106,770,32,[],200,EleutherAI/gpt-neox-20b,64,1,604.0,1.0,1,H100,1697105076895,1697105077499,120,13.0,1.0,"[13, 591]","[1697105076908, 1697105077499]"
3107,430,33,[],200,EleutherAI/gpt-neox-20b,64,1,625.0,1.0,1,H100,1697105077500,1697105078125,120,15.0,1.0,"[9, 616]","[1697105077509, 1697105078125]"
3108,145,34,[],200,EleutherAI/gpt-neox-20b,64,1,1629.0,1.0,1,H100,1697105076077,1697105077706,120,161.0,9.0,"[21, 733, 63, 56, 55, 55, 534, 63, 49]","[1697105076098, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706]"
3109,455,35,[],200,EleutherAI/gpt-neox-20b,64,1,2836.0,1.0,1,H100,1697105088558,1697105091394,120,91.0,20.0,"[5, 622, 57, 47, 57, 46, 55, 319, 65, 57, 56, 45, 56, 530, 64, 49, 60, 59, 58, 529]","[1697105088563, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089766, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394]"
3110,200,34,[],200,EleutherAI/gpt-neox-20b,64,1,2114.0,1.0,1,H100,1697105078126,1697105080240,120,6.0,9.0,"[5, 1332, 72, 68, 67, 51, 59, 390, 70]","[1697105078131, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240]"
3111,514,53,[],200,EleutherAI/gpt-neox-20b,64,1,2462.0,1.0,1,H100,1697105112762,1697105115224,120,85.0,20.0,"[17, 494, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68, 66]","[1697105112779, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224]"
3112,403,51,[],200,EleutherAI/gpt-neox-20b,64,1,523.0,1.0,1,H100,1697105107920,1697105108443,120,874.0,2.0,"[5, 518]","[1697105107925, 1697105108443]"
3113,63,52,[],200,EleutherAI/gpt-neox-20b,64,1,647.0,1.0,1,H100,1697105108444,1697105109091,120,39.0,1.0,"[8, 639]","[1697105108452, 1697105109091]"
3114,760,53,[],200,EleutherAI/gpt-neox-20b,64,1,8872.0,1.0,1,H100,1697105109092,1697105117964,120,335.0,64.0,"[16, 963, 71, 121, 74, 49, 60, 459, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 58, 79]","[1697105109108, 1697105110071, 1697105110142, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117885, 1697105117964]"
3115,65,61,[],200,EleutherAI/gpt-neox-20b,64,1,3922.0,1.0,1,H100,1697105121349,1697105125271,120,39.0,30.0,"[5, 574, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58]","[1697105121354, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271]"
3116,203,36,[],200,EleutherAI/gpt-neox-20b,64,1,1490.0,1.0,1,H100,1697105091397,1697105092887,120,364.0,13.0,"[17, 559, 74, 67, 52, 52, 65, 299, 56, 68, 51, 66, 64]","[1697105091414, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092706, 1697105092757, 1697105092823, 1697105092887]"
3117,2,46,[],200,EleutherAI/gpt-neox-20b,64,1,863.0,1.0,1,H100,1697105100014,1697105100877,120,58.0,6.0,"[5, 565, 116, 56, 62, 59]","[1697105100019, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877]"
3118,816,55,[],200,EleutherAI/gpt-neox-20b,64,1,659.0,1.0,1,H100,1697105112762,1697105113421,120,182.0,4.0,"[5, 506, 76, 72]","[1697105112767, 1697105113273, 1697105113349, 1697105113421]"
3119,36,43,[],200,EleutherAI/gpt-neox-20b,64,1,2462.0,1.0,1,H100,1697105112762,1697105115224,120,457.0,20.0,"[9, 502, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 47, 61, 520, 140, 70, 68, 68, 66]","[1697105112771, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114231, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224]"
3120,238,51,[],200,EleutherAI/gpt-neox-20b,64,1,1216.0,1.0,1,H100,1697105126685,1697105127901,120,563.0,6.0,"[5, 916, 60, 79, 79, 77]","[1697105126690, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901]"
3121,827,52,[],200,EleutherAI/gpt-neox-20b,64,1,2593.0,1.0,1,H100,1697105127902,1697105130495,120,96.0,20.0,"[4, 419, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444]","[1697105127906, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495]"
3122,587,47,[],200,EleutherAI/gpt-neox-20b,64,1,1116.0,1.0,1,H100,1697105100878,1697105101994,120,13.0,1.0,"[8, 1108]","[1697105100886, 1697105101994]"
3123,289,58,[],200,EleutherAI/gpt-neox-20b,64,1,2246.0,1.0,1,H100,1697105106255,1697105108501,120,89.0,20.0,"[5, 446, 58, 57, 55, 44, 286, 61, 58, 56, 54, 53, 260, 61, 61, 50, 55, 45, 423, 58]","[1697105106260, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107920, 1697105107975, 1697105108020, 1697105108443, 1697105108501]"
3124,357,48,[],200,EleutherAI/gpt-neox-20b,64,1,5330.0,1.0,1,H100,1697105101995,1697105107325,120,52.0,33.0,"[8, 804, 707, 67, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66, 52, 573, 111, 58, 44, 350, 58, 57, 55, 44, 286, 61, 58]","[1697105102003, 1697105102807, 1697105103514, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105570, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325]"
3125,464,46,[],200,EleutherAI/gpt-neox-20b,64,1,589.0,1.0,1,H100,1697105108502,1697105109091,120,12.0,1.0,"[12, 577]","[1697105108514, 1697105109091]"
3126,233,47,[],200,EleutherAI/gpt-neox-20b,64,1,825.0,1.0,1,H100,1697105109092,1697105109917,120,6.0,1.0,"[8, 817]","[1697105109100, 1697105109917]"
3127,394,63,[],200,EleutherAI/gpt-neox-20b,64,1,715.0,1.0,1,H100,1697105129694,1697105130409,120,11.0,1.0,"[9, 706]","[1697105129703, 1697105130409]"
3128,823,48,[],200,EleutherAI/gpt-neox-20b,64,1,2843.0,1.0,1,H100,1697105109918,1697105112761,120,90.0,20.0,"[8, 979, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71]","[1697105109926, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761]"
3129,754,58,[],200,EleutherAI/gpt-neox-20b,64,1,1636.0,1.0,1,H100,1697105105571,1697105107207,120,88.0,7.0,"[9, 1126, 58, 57, 55, 44, 287]","[1697105105580, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207]"
3130,60,59,[],200,EleutherAI/gpt-neox-20b,64,1,5059.0,1.0,1,H100,1697105108502,1697105113561,120,93.0,36.0,"[16, 797, 66, 63, 54, 573, 71, 121, 74, 49, 59, 459, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70]","[1697105108518, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263, 1697105110337, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561]"
3131,538,42,[],200,EleutherAI/gpt-neox-20b,64,1,3081.0,1.0,1,H100,1697105095103,1697105098184,120,89.0,20.0,"[17, 1074, 74, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 441, 74]","[1697105095120, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184]"
3132,235,38,[],200,EleutherAI/gpt-neox-20b,64,1,1634.0,1.0,1,H100,1697105084548,1697105086182,120,161.0,12.0,"[4, 485, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67]","[1697105084552, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182]"
3133,292,60,[],200,EleutherAI/gpt-neox-20b,64,1,571.0,1.0,1,H100,1697105112490,1697105113061,120,286.0,1.0,"[4, 567]","[1697105112494, 1697105113061]"
3134,759,35,[],200,EleutherAI/gpt-neox-20b,64,1,3258.0,1.0,1,H100,1697105080241,1697105083499,120,92.0,20.0,"[4, 888, 142, 74, 70, 68, 67, 424, 72, 71, 68, 65, 435, 78, 71, 71, 54, 53, 69, 414]","[1697105080245, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082689, 1697105082767, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499]"
3135,875,61,[],200,EleutherAI/gpt-neox-20b,64,1,4902.0,1.0,1,H100,1697105113062,1697105117964,120,31.0,31.0,"[8, 851, 69, 67, 65, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78]","[1697105113070, 1697105113921, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964]"
3136,448,54,[],200,EleutherAI/gpt-neox-20b,64,1,2365.0,1.0,1,H100,1697105102065,1697105104430,120,335.0,12.0,"[11, 731, 708, 66, 51, 50, 64, 407, 73, 71, 69, 64]","[1697105102076, 1697105102807, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430]"
3137,907,37,[],200,EleutherAI/gpt-neox-20b,64,1,1185.0,1.0,1,H100,1697105092888,1697105094073,120,10.0,1.0,"[5, 1180]","[1697105092893, 1697105094073]"
3138,561,38,[],200,EleutherAI/gpt-neox-20b,64,1,2749.0,1.0,1,H100,1697105094074,1697105096823,120,87.0,20.0,"[9, 718, 54, 67, 65, 50, 65, 337, 65, 50, 65, 63, 512, 74, 56, 74, 55, 72, 54, 244]","[1697105094083, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095619, 1697105095682, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823]"
3139,218,55,[],200,EleutherAI/gpt-neox-20b,64,1,1087.0,1.0,1,H100,1697105104431,1697105105518,120,109.0,7.0,"[7, 728, 81, 76, 58, 71, 66]","[1697105104438, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518]"
3140,250,48,[],200,EleutherAI/gpt-neox-20b,64,1,520.0,1.0,1,H100,1697105101474,1697105101994,120,31.0,1.0,"[12, 508]","[1697105101486, 1697105101994]"
3141,832,49,[],200,EleutherAI/gpt-neox-20b,64,1,175.0,1.0,1,H100,1697105101995,1697105102170,120,15.0,1.0,"[12, 163]","[1697105102007, 1697105102170]"
3142,895,56,[],200,EleutherAI/gpt-neox-20b,64,1,545.0,1.0,1,H100,1697105105519,1697105106064,120,15.0,1.0,"[32, 513]","[1697105105551, 1697105106064]"
3143,612,50,[],200,EleutherAI/gpt-neox-20b,64,1,3400.0,1.0,1,H100,1697105102171,1697105105571,120,93.0,20.0,"[100, 1244, 66, 51, 50, 64, 407, 73, 71, 69, 64, 63, 61, 612, 81, 76, 58, 71, 66, 53]","[1697105102271, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104493, 1697105104554, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571]"
3144,891,38,[],200,EleutherAI/gpt-neox-20b,64,1,518.0,1.0,1,H100,1697105110387,1697105110905,120,52.0,2.0,"[5, 513]","[1697105110392, 1697105110905]"
3145,551,39,[],200,EleutherAI/gpt-neox-20b,64,1,2780.0,1.0,1,H100,1697105110906,1697105113686,120,90.0,20.0,"[8, 880, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62]","[1697105110914, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686]"
3146,330,57,[],200,EleutherAI/gpt-neox-20b,64,1,1964.0,1.0,1,H100,1697105115091,1697105117055,120,345.0,14.0,"[5, 666, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55]","[1697105115096, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054]"
3147,92,56,[],200,EleutherAI/gpt-neox-20b,64,1,2815.0,1.0,1,H100,1697105102756,1697105105571,120,85.0,20.0,"[12, 747, 67, 50, 50, 64, 407, 73, 71, 69, 64, 62, 62, 612, 81, 76, 58, 71, 66, 53]","[1697105102768, 1697105103515, 1697105103582, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104554, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571]"
3148,41,45,[],200,EleutherAI/gpt-neox-20b,64,1,5785.0,1.0,1,H100,1697105124878,1697105130663,120,39.0,43.0,"[8, 1203, 80, 60, 77, 59, 69, 54, 279, 79, 77, 75, 67, 65, 476, 61, 78, 79, 77, 72, 68, 284, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51]","[1697105124886, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663]"
3149,718,50,[],200,EleutherAI/gpt-neox-20b,64,1,994.0,1.0,1,H100,1697105103944,1697105104938,120,13.0,1.0,"[12, 982]","[1697105103956, 1697105104938]"
3150,16,49,[],200,EleutherAI/gpt-neox-20b,64,1,357.0,1.0,1,H100,1697105107327,1697105107684,120,9.0,1.0,"[12, 345]","[1697105107339, 1697105107684]"
3151,231,36,[],200,EleutherAI/gpt-neox-20b,64,1,532.0,1.0,1,H100,1697105091974,1697105092506,120,13.0,1.0,"[4, 528]","[1697105091978, 1697105092506]"
3152,421,38,[],200,EleutherAI/gpt-neox-20b,64,1,2741.0,1.0,1,H100,1697105111551,1697105114292,120,85.0,20.0,"[4, 934, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 60]","[1697105111555, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292]"
3153,925,37,[],200,EleutherAI/gpt-neox-20b,64,1,2595.0,1.0,1,H100,1697105092507,1697105095102,120,87.0,20.0,"[4, 737, 76, 69, 67, 51, 52, 609, 75, 56, 73, 70, 55, 54, 246, 54, 67, 65, 50, 65]","[1697105092511, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094247, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102]"
3154,771,30,[],200,EleutherAI/gpt-neox-20b,64,1,3915.0,1.0,1,H100,1697105080498,1697105084413,120,47.0,20.0,"[4, 1476, 72, 71, 68, 65, 435, 78, 72, 70, 54, 53, 69, 414, 71, 68, 52, 67, 64, 592]","[1697105080502, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082689, 1697105082767, 1697105082839, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413]"
3155,307,52,[],200,EleutherAI/gpt-neox-20b,64,1,248.0,1.0,1,H100,1697105107436,1697105107684,120,26.0,1.0,"[5, 243]","[1697105107441, 1697105107684]"
3156,83,53,[],200,EleutherAI/gpt-neox-20b,64,1,2761.0,1.0,1,H100,1697105107685,1697105110446,120,123.0,15.0,"[13, 745, 57, 50, 48, 717, 65, 64, 54, 573, 71, 121, 74, 49, 60]","[1697105107698, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263, 1697105110337, 1697105110386, 1697105110446]"
3157,424,31,[],200,EleutherAI/gpt-neox-20b,64,1,2774.0,1.0,1,H100,1697105084414,1697105087188,120,88.0,20.0,"[5, 618, 56, 73, 54, 55, 68, 561, 73, 70, 68, 67, 63, 349, 69, 53, 54, 65, 62, 291]","[1697105084419, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085275, 1697105085343, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086716, 1697105086770, 1697105086835, 1697105086897, 1697105087188]"
3158,198,32,[],200,EleutherAI/gpt-neox-20b,64,1,2642.0,1.0,1,H100,1697105087189,1697105089831,120,96.0,20.0,"[9, 933, 62, 62, 47, 55, 264, 61, 60, 59, 56, 55, 273, 57, 47, 57, 46, 55, 320, 64]","[1697105087198, 1697105088131, 1697105088193, 1697105088255, 1697105088302, 1697105088357, 1697105088621, 1697105088682, 1697105088742, 1697105088801, 1697105088857, 1697105088912, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831]"
3159,528,36,[],200,EleutherAI/gpt-neox-20b,64,1,3094.0,1.0,1,H100,1697105083500,1697105086594,120,52.0,20.0,"[13, 900, 77, 56, 74, 73, 72, 272, 56, 73, 54, 56, 67, 562, 72, 70, 68, 67, 63, 349]","[1697105083513, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093, 1697105085166, 1697105085220, 1697105085276, 1697105085343, 1697105085905, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594]"
3160,268,51,[],200,EleutherAI/gpt-neox-20b,64,1,1002.0,1.0,1,H100,1697105105572,1697105106574,120,19.0,1.0,"[16, 986]","[1697105105588, 1697105106574]"
3161,740,44,[],200,EleutherAI/gpt-neox-20b,64,1,3212.0,1.0,1,H100,1697105115225,1697105118437,120,563.0,14.0,"[5, 1539, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 331, 72]","[1697105115230, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118365, 1697105118437]"
3162,37,52,[],200,EleutherAI/gpt-neox-20b,64,1,564.0,1.0,1,H100,1697105106576,1697105107140,120,20.0,1.0,"[16, 548]","[1697105106592, 1697105107140]"
3163,760,33,[],200,EleutherAI/gpt-neox-20b,64,1,7770.0,1.0,1,H100,1697105089832,1697105097602,120,335.0,64.0,"[13, 729, 65, 49, 60, 59, 58, 529, 71, 67, 65, 64, 312, 74, 67, 52, 52, 65, 299, 57, 66, 52, 66, 64, 361, 76, 69, 66, 52, 52, 609, 74, 57, 72, 71, 55, 54, 246, 54, 67, 65, 50, 65, 336, 66, 50, 64, 64, 511, 75, 56, 74, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68]","[1697105089845, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093459, 1697105093511, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095438, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602]"
3164,250,43,[],200,EleutherAI/gpt-neox-20b,64,1,1192.0,1.0,1,H100,1697105102076,1697105103268,120,31.0,1.0,"[132, 1060]","[1697105102208, 1697105103268]"
3165,27,44,[],200,EleutherAI/gpt-neox-20b,64,1,7177.0,1.0,1,H100,1697105103269,1697105110446,120,15.0,50.0,"[24, 861, 72, 71, 69, 64, 63, 61, 612, 80, 76, 59, 71, 66, 53, 572, 111, 58, 44, 350, 58, 57, 55, 44, 286, 61, 58, 56, 54, 53, 259, 62, 61, 49, 56, 44, 424, 57, 50, 48, 717, 65, 64, 54, 572, 72, 121, 70, 53, 59]","[1697105103293, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104493, 1697105104554, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107747, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108019, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110070, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110445]"
3166,147,45,[],200,EleutherAI/gpt-neox-20b,64,1,287.0,1.0,1,H100,1697105092219,1697105092506,120,182.0,1.0,"[5, 282]","[1697105092224, 1697105092506]"
3167,731,46,[],200,EleutherAI/gpt-neox-20b,64,1,2595.0,1.0,1,H100,1697105092507,1697105095102,120,89.0,20.0,"[9, 732, 76, 69, 67, 51, 52, 609, 75, 56, 73, 70, 55, 54, 246, 54, 67, 65, 50, 65]","[1697105092516, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172, 1697105094247, 1697105094303, 1697105094376, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102]"
3168,449,52,[],200,EleutherAI/gpt-neox-20b,64,1,3894.0,1.0,1,H100,1697105115225,1697105119119,120,86.0,20.0,"[9, 1535, 80, 76, 74, 56, 65, 626, 81, 59, 78, 70, 332, 71, 72, 64, 330, 73, 71, 72]","[1697105115234, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118366, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119]"
3169,777,47,[],200,EleutherAI/gpt-neox-20b,64,1,790.0,1.0,1,H100,1697105098357,1697105099147,120,9.0,1.0,"[8, 781]","[1697105098365, 1697105099146]"
3170,550,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099147,1697105102062,120,,,"[5, 685, 63, 50, 63, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105099152, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3171,656,57,[],200,EleutherAI/gpt-neox-20b,64,1,342.0,1.0,1,H100,1697105124736,1697105125078,120,26.0,1.0,"[8, 334]","[1697105124744, 1697105125078]"
3172,697,38,[],200,EleutherAI/gpt-neox-20b,64,1,1381.0,1.0,1,H100,1697105094173,1697105095554,120,123.0,10.0,"[9, 620, 53, 67, 65, 50, 65, 337, 65, 50]","[1697105094182, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554]"
3173,524,59,[],200,EleutherAI/gpt-neox-20b,64,1,4586.0,1.0,1,H100,1697105107208,1697105111794,120,100.0,30.0,"[4, 536, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 66, 63, 54, 573, 71, 121, 70, 53, 60, 458, 215, 128, 73, 55, 64, 62, 49, 244]","[1697105107212, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794]"
3174,64,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.26 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.31 GiB is allocated by PyTorch, and 22.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105061408,1697105063190,120,,,"[4, 1186, 61, 60, 58, 47]","[1697105061412, 1697105062598, 1697105062659, 1697105062719, 1697105062777, 1697105062824]"
3175,100,58,[],200,EleutherAI/gpt-neox-20b,64,1,2064.0,1.0,1,H100,1697105117055,1697105119119,120,732.0,14.0,"[5, 686, 81, 59, 78, 71, 331, 71, 72, 64, 330, 73, 72, 71]","[1697105117060, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118366, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119]"
3176,58,50,[],200,EleutherAI/gpt-neox-20b,64,1,340.0,1.0,1,H100,1697105098185,1697105098525,120,15.0,1.0,"[17, 323]","[1697105098202, 1697105098525]"
3177,646,22,[],200,EleutherAI/gpt-neox-20b,64,1,815.0,1.0,1,H100,1697105063194,1697105064009,120,14.0,1.0,"[92, 723]","[1697105063286, 1697105064009]"
3178,757,51,[],200,EleutherAI/gpt-neox-20b,64,1,620.0,1.0,1,H100,1697105098526,1697105099146,120,20.0,1.0,"[17, 603]","[1697105098543, 1697105099146]"
3179,421,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.53 GiB. GPU 0 has a total capacty of 79.11 GiB of which 4.71 GiB is free. Process 1645736 has 74.39 GiB memory in use. Of the allocated memory 51.58 GiB is allocated by PyTorch, and 21.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105064010,1697105065209,120,,,"[17, 525, 61, 59, 57, 45, 54]","[1697105064027, 1697105064552, 1697105064613, 1697105064672, 1697105064729, 1697105064774, 1697105064828]"
3180,596,53,[],200,EleutherAI/gpt-neox-20b,64,1,2540.0,1.0,1,H100,1697105130496,1697105133036,120,87.0,20.0,"[8, 698, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 71]","[1697105130504, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133036]"
3181,370,51,[],200,EleutherAI/gpt-neox-20b,64,1,1125.0,1.0,1,H100,1697105104939,1697105106064,120,31.0,1.0,"[9, 1116]","[1697105104948, 1697105106064]"
3182,75,24,[],200,EleutherAI/gpt-neox-20b,64,1,3089.0,1.0,1,H100,1697105065212,1697105068301,120,345.0,18.0,"[74, 560, 727, 68, 66, 65, 62, 58, 534, 68, 63, 59, 55, 407, 60, 58, 58, 47]","[1697105065286, 1697105065846, 1697105066573, 1697105066641, 1697105066707, 1697105066772, 1697105066834, 1697105066892, 1697105067426, 1697105067494, 1697105067557, 1697105067616, 1697105067671, 1697105068078, 1697105068138, 1697105068196, 1697105068254, 1697105068301]"
3183,110,46,[],200,EleutherAI/gpt-neox-20b,64,1,566.0,1.0,1,H100,1697105119816,1697105120382,120,96.0,4.0,"[5, 412, 76, 72]","[1697105119821, 1697105120233, 1697105120309, 1697105120381]"
3184,787,47,[],200,EleutherAI/gpt-neox-20b,64,1,967.0,1.0,1,H100,1697105120382,1697105121349,120,123.0,6.0,"[5, 722, 74, 55, 55, 56]","[1697105120387, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121349]"
3185,439,48,[],200,EleutherAI/gpt-neox-20b,64,1,813.0,1.0,1,H100,1697105121349,1697105122162,120,13.0,4.0,"[9, 570, 82, 152]","[1697105121358, 1697105121928, 1697105122010, 1697105122162]"
3186,307,43,[],200,EleutherAI/gpt-neox-20b,64,1,340.0,1.0,1,H100,1697105098185,1697105098525,120,26.0,1.0,"[13, 327]","[1697105098198, 1697105098525]"
3187,891,44,[],200,EleutherAI/gpt-neox-20b,64,1,707.0,1.0,1,H100,1697105098526,1697105099233,120,52.0,2.0,"[5, 702]","[1697105098531, 1697105099233]"
3188,643,45,[],200,EleutherAI/gpt-neox-20b,64,1,536.0,1.0,1,H100,1697105099234,1697105099770,120,18.0,1.0,"[5, 531]","[1697105099239, 1697105099770]"
3189,413,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099771,1697105102062,120,,,"[12, 801, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105099783, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3190,215,49,[],200,EleutherAI/gpt-neox-20b,64,1,791.0,1.0,1,H100,1697105122163,1697105122954,120,12.0,1.0,"[5, 786]","[1697105122168, 1697105122954]"
3191,191,39,[],200,EleutherAI/gpt-neox-20b,64,1,3742.0,1.0,1,H100,1697105114293,1697105118035,120,85.0,20.0,"[5, 1464, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56, 65, 626, 81, 59, 78, 71]","[1697105114298, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035]"
3192,798,50,[],200,EleutherAI/gpt-neox-20b,64,1,1143.0,1.0,1,H100,1697105122954,1697105124097,120,79.0,6.0,"[5, 886, 79, 57, 58, 58]","[1697105122959, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097]"
3193,547,57,[],200,EleutherAI/gpt-neox-20b,64,1,509.0,1.0,1,H100,1697105106065,1697105106574,120,12.0,1.0,"[17, 492]","[1697105106082, 1697105106574]"
3194,320,58,[],200,EleutherAI/gpt-neox-20b,64,1,5219.0,1.0,1,H100,1697105106575,1697105111794,120,109.0,36.0,"[5, 627, 60, 58, 56, 54, 53, 260, 61, 61, 50, 55, 45, 423, 57, 50, 48, 717, 65, 64, 54, 572, 72, 121, 70, 53, 59, 459, 215, 128, 73, 54, 65, 62, 49, 244]","[1697105106580, 1697105107207, 1697105107267, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107920, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110070, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111374, 1697105111439, 1697105111501, 1697105111550, 1697105111794]"
3195,566,51,[],200,EleutherAI/gpt-neox-20b,64,1,4227.0,1.0,1,H100,1697105124098,1697105128325,120,109.0,36.0,"[8, 413, 79, 78, 59, 73, 69, 277, 59, 58, 74, 57, 74, 72, 65, 476, 80, 63, 74, 59, 70, 53, 279, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284]","[1697105124106, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126232, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325]"
3196,186,42,[],200,EleutherAI/gpt-neox-20b,64,1,2807.0,1.0,1,H100,1697105092048,1697105094855,120,123.0,22.0,"[4, 530, 56, 67, 52, 66, 64, 361, 76, 69, 67, 52, 51, 609, 74, 57, 72, 71, 55, 54, 246, 54]","[1697105092052, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324, 1697105093393, 1697105093460, 1697105093512, 1697105093563, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094801, 1697105094855]"
3197,586,38,[],200,EleutherAI/gpt-neox-20b,64,1,3081.0,1.0,1,H100,1697105095103,1697105098184,120,85.0,20.0,"[4, 1087, 74, 56, 74, 56, 71, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 441, 74]","[1697105095107, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096454, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184]"
3198,354,39,[],200,EleutherAI/gpt-neox-20b,64,1,2572.0,1.0,1,H100,1697105098185,1697105100757,120,91.0,20.0,"[4, 409, 55, 56, 69, 61, 394, 72, 126, 66, 55, 285, 63, 51, 62, 57, 54, 460, 116, 56]","[1697105098189, 1697105098598, 1697105098653, 1697105098709, 1697105098778, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756]"
3199,189,37,[],200,EleutherAI/gpt-neox-20b,64,1,2590.0,1.0,1,H100,1697105086595,1697105089185,120,88.0,20.0,"[13, 580, 72, 54, 71, 69, 67, 63, 546, 63, 61, 48, 55, 265, 60, 61, 58, 56, 56, 272]","[1697105086608, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089185]"
3200,637,70,[],200,EleutherAI/gpt-neox-20b,64,1,2423.0,1.0,1,H100,1697105125550,1697105127973,120,96.0,20.0,"[16, 523, 80, 63, 74, 59, 70, 53, 279, 79, 77, 75, 67, 65, 476, 61, 78, 79, 77, 72]","[1697105125566, 1697105126089, 1697105126169, 1697105126232, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973]"
3201,432,58,[],200,EleutherAI/gpt-neox-20b,64,1,930.0,1.0,1,H100,1697105125079,1697105126009,120,13.0,1.0,"[8, 922]","[1697105125087, 1697105126009]"
3202,86,59,[],200,EleutherAI/gpt-neox-20b,64,1,2463.0,1.0,1,H100,1697105126010,1697105128473,120,335.0,17.0,"[5, 752, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71]","[1697105126015, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473]"
3203,797,57,[],200,EleutherAI/gpt-neox-20b,64,1,1002.0,1.0,1,H100,1697105105572,1697105106574,120,26.0,1.0,"[20, 982]","[1697105105592, 1697105106574]"
3204,206,49,[],200,EleutherAI/gpt-neox-20b,64,1,689.0,1.0,1,H100,1697105102066,1697105102755,120,16.0,1.0,"[45, 644]","[1697105102111, 1697105102755]"
3205,883,50,[],200,EleutherAI/gpt-neox-20b,64,1,512.0,1.0,1,H100,1697105102756,1697105103268,120,563.0,1.0,"[4, 508]","[1697105102760, 1697105103268]"
3206,357,39,[],200,EleutherAI/gpt-neox-20b,64,1,3997.0,1.0,1,H100,1697105095555,1697105099552,120,52.0,33.0,"[5, 634, 74, 57, 73, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 441, 74, 68, 52, 52, 242, 54, 57, 68, 62, 394, 72, 126, 66, 55]","[1697105095560, 1697105096194, 1697105096268, 1697105096325, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552]"
3207,536,51,[],200,EleutherAI/gpt-neox-20b,64,1,3437.0,1.0,1,H100,1697105103269,1697105106706,120,83.0,20.0,"[12, 873, 72, 71, 69, 64, 62, 62, 612, 80, 76, 59, 71, 66, 53, 572, 111, 58, 44, 350]","[1697105103281, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104554, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706]"
3208,518,46,[],200,EleutherAI/gpt-neox-20b,64,1,563.0,1.0,1,H100,1697105107811,1697105108374,120,23.0,1.0,"[4, 559]","[1697105107815, 1697105108374]"
3209,176,47,[],200,EleutherAI/gpt-neox-20b,64,1,940.0,1.0,1,H100,1697105108375,1697105109315,120,216.0,2.0,"[5, 935]","[1697105108380, 1697105109315]"
3210,71,47,[],200,EleutherAI/gpt-neox-20b,64,1,2301.0,1.0,1,H100,1697105102065,1697105104366,120,364.0,11.0,"[26, 717, 707, 66, 51, 50, 64, 407, 73, 71, 69]","[1697105102091, 1697105102808, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366]"
3211,770,48,[],200,EleutherAI/gpt-neox-20b,64,1,571.0,1.0,1,H100,1697105104367,1697105104938,120,13.0,1.0,"[4, 567]","[1697105104371, 1697105104938]"
3212,431,49,[],200,EleutherAI/gpt-neox-20b,64,1,3081.0,1.0,1,H100,1697105104939,1697105108020,120,732.0,22.0,"[5, 1199, 111, 58, 44, 350, 58, 57, 55, 44, 287, 60, 58, 55, 55, 53, 259, 62, 61, 49, 56, 45]","[1697105104944, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107267, 1697105107325, 1697105107380, 1697105107435, 1697105107488, 1697105107747, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020]"
3213,127,40,[],200,EleutherAI/gpt-neox-20b,64,1,1265.0,1.0,1,H100,1697105099553,1697105100818,120,100.0,5.0,"[4, 1027, 116, 56, 62]","[1697105099557, 1697105100584, 1697105100700, 1697105100756, 1697105100818]"
3214,715,41,[],200,EleutherAI/gpt-neox-20b,64,1,446.0,1.0,1,H100,1697105100819,1697105101265,120,20.0,1.0,"[4, 442]","[1697105100823, 1697105101265]"
3215,485,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101266,1697105102062,120,,,[5],[1697105101271]
3216,145,43,[],200,EleutherAI/gpt-neox-20b,64,1,2161.0,1.0,1,H100,1697105102066,1697105104227,120,161.0,9.0,"[49, 693, 707, 66, 51, 50, 64, 408, 72]","[1697105102115, 1697105102808, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104226]"
3217,845,35,[],200,EleutherAI/gpt-neox-20b,64,1,7058.0,1.0,1,H100,1697105077707,1697105084765,120,244.0,50.0,"[5, 485, 56, 55, 62, 60, 59, 57, 917, 71, 69, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 142, 74, 70, 68, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 71, 68, 52, 67, 64, 592, 77, 56, 74, 73, 72]","[1697105077712, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079534, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081275, 1697105081349, 1697105081419, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083757, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765]"
3218,320,40,[],200,EleutherAI/gpt-neox-20b,64,1,5845.0,1.0,1,H100,1697105113687,1697105119532,120,109.0,36.0,"[13, 1112, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 71, 328, 74, 72, 64, 330, 73, 71, 72, 65, 348]","[1697105113700, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118363, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119, 1697105119184, 1697105119532]"
3219,846,44,[],200,EleutherAI/gpt-neox-20b,64,1,1224.0,1.0,1,H100,1697105104228,1697105105452,120,140.0,6.0,"[4, 934, 81, 76, 58, 71]","[1697105104232, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452]"
3220,594,49,[],200,EleutherAI/gpt-neox-20b,64,1,15061.0,1.0,1,H100,1697105112763,1697105127824,120,216.0,119.0,"[20, 491, 75, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 71, 328, 74, 72, 64, 330, 73, 71, 72, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 78, 58, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 58, 59, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 278, 80, 76, 76, 67, 65, 476, 60, 79, 79]","[1697105112783, 1697105113274, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118363, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125212, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126922, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824]"
3221,503,45,[],200,EleutherAI/gpt-neox-20b,64,1,2466.0,1.0,1,H100,1697105105453,1697105107919,120,109.0,20.0,"[5, 685, 111, 58, 44, 350, 58, 57, 55, 44, 287, 61, 58, 55, 54, 53, 260, 61, 61, 49]","[1697105105458, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919]"
3222,910,41,[],200,EleutherAI/gpt-neox-20b,64,1,620.0,1.0,1,H100,1697105119533,1697105120153,120,8.0,1.0,"[5, 615]","[1697105119538, 1697105120153]"
3223,715,50,[],200,EleutherAI/gpt-neox-20b,64,1,689.0,1.0,1,H100,1697105107685,1697105108374,120,20.0,1.0,"[21, 668]","[1697105107706, 1697105108374]"
3224,453,58,[],200,EleutherAI/gpt-neox-20b,64,1,565.0,1.0,1,H100,1697105106575,1697105107140,120,26.0,1.0,"[13, 552]","[1697105106588, 1697105107140]"
3225,313,52,[],200,EleutherAI/gpt-neox-20b,64,1,431.0,1.0,1,H100,1697105106709,1697105107140,120,20.0,1.0,"[24, 407]","[1697105106733, 1697105107140]"
3226,353,46,[],200,EleutherAI/gpt-neox-20b,64,1,617.0,1.0,1,H100,1697105106709,1697105107326,120,52.0,4.0,"[20, 478, 61, 58]","[1697105106729, 1697105107207, 1697105107268, 1697105107326]"
3227,3,39,[],200,EleutherAI/gpt-neox-20b,64,1,2439.0,1.0,1,H100,1697105086183,1697105088622,120,89.0,20.0,"[4, 407, 69, 55, 52, 65, 63, 290, 71, 55, 71, 69, 67, 63, 547, 62, 61, 48, 55, 265]","[1697105086187, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835, 1697105086898, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088131, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088622]"
3228,222,59,[],200,EleutherAI/gpt-neox-20b,64,1,3245.0,1.0,1,H100,1697105107141,1697105110386,120,96.0,20.0,"[8, 599, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 66, 63, 54, 573, 71, 121, 70, 53]","[1697105107149, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386]"
3229,13,47,[],200,EleutherAI/gpt-neox-20b,64,1,3059.0,1.0,1,H100,1697105107327,1697105110386,120,90.0,20.0,"[8, 413, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 66, 63, 54, 573, 71, 121, 70, 53]","[1697105107335, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386]"
3230,666,54,[],200,EleutherAI/gpt-neox-20b,64,1,3239.0,1.0,1,H100,1697105110447,1697105113686,120,84.0,20.0,"[4, 1343, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62]","[1697105110451, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686]"
3231,252,54,[],200,EleutherAI/gpt-neox-20b,64,1,2206.0,1.0,1,H100,1697105131358,1697105133564,120,182.0,22.0,"[8, 667, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71, 65, 63, 50, 49, 49, 88, 65, 57, 43]","[1697105131366, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564]"
3232,906,43,[],200,EleutherAI/gpt-neox-20b,64,1,3455.0,1.0,1,H100,1697105107920,1697105111375,120,86.0,20.0,"[9, 514, 57, 50, 49, 716, 65, 64, 54, 573, 72, 120, 74, 49, 60, 458, 215, 128, 73, 55]","[1697105107929, 1697105108443, 1697105108500, 1697105108550, 1697105108599, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375]"
3233,686,44,[],200,EleutherAI/gpt-neox-20b,64,1,345.0,1.0,1,H100,1697105111376,1697105111721,120,31.0,1.0,"[16, 329]","[1697105111392, 1697105111721]"
3234,340,45,[],200,EleutherAI/gpt-neox-20b,64,1,2570.0,1.0,1,H100,1697105111722,1697105114292,120,85.0,20.0,"[9, 758, 73, 56, 72, 71, 62, 451, 75, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 60]","[1697105111731, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113274, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292]"
3235,416,52,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099148,1697105102062,120,,,"[12, 677, 63, 51, 62, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 48, 58]","[1697105099160, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101580, 1697105101638]"
3236,688,59,[],200,EleutherAI/gpt-neox-20b,64,1,565.0,1.0,1,H100,1697105119120,1697105119685,120,345.0,4.0,"[12, 400, 77, 76]","[1697105119132, 1697105119532, 1697105119609, 1697105119685]"
3237,609,45,[],200,EleutherAI/gpt-neox-20b,64,1,3240.0,1.0,1,H100,1697105110446,1697105113686,120,88.0,20.0,"[5, 1343, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62]","[1697105110451, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686]"
3238,458,60,[],200,EleutherAI/gpt-neox-20b,64,1,467.0,1.0,1,H100,1697105119686,1697105120153,120,11.0,1.0,"[5, 462]","[1697105119691, 1697105120153]"
3239,179,60,[],200,EleutherAI/gpt-neox-20b,64,1,824.0,1.0,1,H100,1697105111794,1697105112618,120,161.0,4.0,"[5, 690, 73, 56]","[1697105111799, 1697105112489, 1697105112562, 1697105112618]"
3240,885,61,[],200,EleutherAI/gpt-neox-20b,64,1,5954.0,1.0,1,H100,1697105112619,1697105118573,120,84.0,43.0,"[5, 649, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 47, 61, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 328, 75, 72, 64]","[1697105112624, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114231, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118362, 1697105118437, 1697105118509, 1697105118573]"
3241,202,61,[],200,EleutherAI/gpt-neox-20b,64,1,9226.0,1.0,1,H100,1697105120154,1697105129380,120,874.0,72.0,"[8, 947, 74, 55, 55, 55, 67, 513, 82, 152, 173, 153, 80, 77, 406, 79, 77, 77, 75, 56, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 278, 80, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 76, 72, 70, 66, 52, 395, 72, 126, 57, 69]","[1697105120162, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122335, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128401, 1697105128473, 1697105128543, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380]"
3242,780,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 10.48 GiB. GPU 0 has a total capacty of 79.11 GiB of which 9.04 GiB is free. Process 1645736 has 70.05 GiB memory in use. Of the allocated memory 51.39 GiB is allocated by PyTorch, and 17.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105068302,1697105070950,120,,,"[5, 391, 51, 51, 499, 61, 60, 59, 55, 676, 70, 69, 67, 56]","[1697105068307, 1697105068698, 1697105068749, 1697105068800, 1697105069299, 1697105069360, 1697105069420, 1697105069479, 1697105069534, 1697105070210, 1697105070280, 1697105070349, 1697105070416, 1697105070472]"
3243,893,53,[],200,EleutherAI/gpt-neox-20b,64,1,1409.0,1.0,1,H100,1697105107141,1697105108550,120,335.0,10.0,"[4, 603, 61, 61, 49, 56, 45, 423, 57, 50]","[1697105107145, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550]"
3244,671,54,[],200,EleutherAI/gpt-neox-20b,64,1,540.0,1.0,1,H100,1697105108551,1697105109091,120,12.0,1.0,"[5, 535]","[1697105108556, 1697105109091]"
3245,893,38,[],200,EleutherAI/gpt-neox-20b,64,1,1502.0,1.0,1,H100,1697105089186,1697105090688,120,335.0,10.0,"[4, 577, 64, 57, 56, 45, 56, 529, 65, 49]","[1697105089190, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574, 1697105090639, 1697105090688]"
3246,320,55,[],200,EleutherAI/gpt-neox-20b,64,1,4965.0,1.0,1,H100,1697105109092,1697105114057,120,109.0,36.0,"[12, 967, 71, 121, 70, 53, 60, 459, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 67]","[1697105109104, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057]"
3247,642,60,[],200,EleutherAI/gpt-neox-20b,64,1,2568.0,1.0,1,H100,1697105113562,1697105116130,120,89.0,20.0,"[8, 420, 67, 65, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55]","[1697105113570, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130]"
3248,337,39,[],200,EleutherAI/gpt-neox-20b,64,1,489.0,1.0,1,H100,1697105096824,1697105097313,120,12.0,1.0,"[13, 476]","[1697105096837, 1697105097313]"
3249,918,40,[],200,EleutherAI/gpt-neox-20b,64,1,710.0,1.0,1,H100,1697105097315,1697105098025,120,23.0,1.0,"[20, 690]","[1697105097335, 1697105098025]"
3250,695,41,[],200,EleutherAI/gpt-neox-20b,64,1,2730.0,1.0,1,H100,1697105098026,1697105100756,120,92.0,20.0,"[5, 567, 55, 56, 68, 62, 394, 72, 126, 66, 55, 285, 63, 51, 62, 57, 54, 460, 116, 56]","[1697105098031, 1697105098598, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756]"
3251,625,46,[],200,EleutherAI/gpt-neox-20b,64,1,538.0,1.0,1,H100,1697105130664,1697105131202,120,364.0,2.0,"[5, 533]","[1697105130669, 1697105131202]"
3252,403,47,[],200,EleutherAI/gpt-neox-20b,64,1,830.0,1.0,1,H100,1697105131203,1697105132033,120,874.0,2.0,"[5, 825]","[1697105131208, 1697105132033]"
3253,593,56,[],200,EleutherAI/gpt-neox-20b,64,1,1530.0,1.0,1,H100,1697105113422,1697105114952,120,335.0,9.0,"[4, 564, 67, 65, 62, 48, 60, 520, 140]","[1697105113426, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952]"
3254,347,51,[],200,EleutherAI/gpt-neox-20b,64,1,8933.0,1.0,1,H100,1697105124167,1697105133100,120,100.0,72.0,"[5, 982, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 70, 53, 279, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 66, 52, 395, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71, 65]","[1697105124172, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100]"
3255,85,46,[],200,EleutherAI/gpt-neox-20b,64,1,3742.0,1.0,1,H100,1697105114293,1697105118035,120,88.0,20.0,"[9, 1460, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56, 65, 626, 81, 59, 78, 71]","[1697105114302, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035]"
3256,242,57,[],200,EleutherAI/gpt-neox-20b,64,1,1246.0,1.0,1,H100,1697105114953,1697105116199,120,345.0,9.0,"[4, 805, 75, 56, 56, 72, 54, 55, 69]","[1697105114957, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116199]"
3257,47,64,[],200,EleutherAI/gpt-neox-20b,64,1,2624.0,1.0,1,H100,1697105130412,1697105133036,120,90.0,20.0,"[21, 769, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70]","[1697105130433, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035]"
3258,593,40,[],200,EleutherAI/gpt-neox-20b,64,1,1208.0,1.0,1,H100,1697105088623,1697105089831,120,335.0,9.0,"[4, 558, 57, 47, 57, 46, 55, 320, 64]","[1697105088627, 1697105089185, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831]"
3259,479,47,[],200,EleutherAI/gpt-neox-20b,64,1,4848.0,1.0,1,H100,1697105095103,1697105099951,120,140.0,36.0,"[13, 1078, 74, 56, 74, 56, 71, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 441, 74, 68, 52, 52, 241, 56, 56, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50]","[1697105095116, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096454, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098597, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950]"
3260,362,41,[],200,EleutherAI/gpt-neox-20b,64,1,595.0,1.0,1,H100,1697105089832,1697105090427,120,14.0,1.0,"[13, 582]","[1697105089845, 1697105090427]"
3261,668,47,[],200,EleutherAI/gpt-neox-20b,64,1,1148.0,1.0,1,H100,1697105118036,1697105119184,120,109.0,6.0,"[4, 863, 73, 72, 71, 65]","[1697105118040, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184]"
3262,681,42,[],200,EleutherAI/gpt-neox-20b,64,1,861.0,1.0,1,H100,1697105120154,1697105121015,120,23.0,1.0,"[12, 849]","[1697105120166, 1697105121015]"
3263,920,58,[],200,EleutherAI/gpt-neox-20b,64,1,1687.0,1.0,1,H100,1697105116199,1697105117886,120,96.0,4.0,"[5, 1542, 81, 59]","[1697105116204, 1697105117746, 1697105117827, 1697105117886]"
3264,445,48,[],200,EleutherAI/gpt-neox-20b,64,1,1048.0,1.0,1,H100,1697105119185,1697105120233,120,457.0,2.0,"[4, 1043]","[1697105119189, 1697105120232]"
3265,334,43,[],200,EleutherAI/gpt-neox-20b,64,1,654.0,1.0,1,H100,1697105121016,1697105121670,120,15.0,1.0,"[13, 641]","[1697105121029, 1697105121670]"
3266,687,59,[],200,EleutherAI/gpt-neox-20b,64,1,2422.0,1.0,1,H100,1697105117887,1697105120309,120,96.0,20.0,"[4, 474, 72, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77]","[1697105117891, 1697105118365, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309]"
3267,82,44,[],200,EleutherAI/gpt-neox-20b,64,1,3206.0,1.0,1,H100,1697105121671,1697105124877,120,67.0,20.0,"[5, 1375, 79, 77, 77, 75, 56, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 69]","[1697105121676, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877]"
3268,100,49,[],200,EleutherAI/gpt-neox-20b,64,1,2412.0,1.0,1,H100,1697105120233,1697105122645,120,732.0,14.0,"[5, 871, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77]","[1697105120238, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645]"
3269,348,60,[],200,EleutherAI/gpt-neox-20b,64,1,3105.0,1.0,1,H100,1697105120310,1697105123415,120,91.0,20.0,"[5, 794, 74, 55, 55, 55, 67, 513, 82, 152, 172, 154, 80, 77, 406, 79, 77, 77, 75, 56]","[1697105120315, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122334, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415]"
3270,875,48,[],200,EleutherAI/gpt-neox-20b,64,1,4175.0,1.0,1,H100,1697105109316,1697105113491,120,31.0,31.0,"[4, 597, 154, 71, 121, 74, 49, 60, 459, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70]","[1697105109320, 1697105109917, 1697105110071, 1697105110142, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491]"
3271,401,38,[],200,EleutherAI/gpt-neox-20b,64,1,3086.0,1.0,1,H100,1697105097670,1697105100756,120,84.0,20.0,"[4, 924, 55, 56, 68, 62, 394, 73, 125, 66, 55, 285, 63, 51, 62, 57, 54, 460, 116, 56]","[1697105097674, 1697105098598, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099306, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756]"
3272,364,50,[],200,EleutherAI/gpt-neox-20b,64,1,17610.0,1.0,1,H100,1697105124521,1697105142131,120,244.0,381.0,"[5, 628, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 70, 53, 279, 79, 77, 75, 67, 65, 476, 61, 78, 79, 77, 72, 68, 284, 77, 71, 70, 66, 52, 395, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71, 65, 63, 49, 50, 49, 88, 64, 57, 44, 56, 53, 41, 52, 44, 34, 35, 42, 33, 34, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 31, 32, 32, 31, 38, 31, 32, 31, 31, 31, 38, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 32, 31, 31, 31, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 38, 31, 31, 30, 31, 31, 36, 30, 30, 32, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 30, 30, 30, 30, 31, 30, 30, 30, 31, 30, 30, 31, 30, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 31, 30, 31, 31, 31, 30, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 37, 30, 29, 30, 30, 30, 30, 30, 30, 34, 28, 29, 28, 28, 29, 28, 28, 29, 28, 28, 29, 28, 29, 28, 29, 28, 29, 28, 29, 29, 26, 25, 26, 25, 26, 25, 26, 25, 26, 26, 26, 26, 25, 26, 26, 26, 25, 26, 26, 25, 26, 26, 26, 25, 26, 26, 26, 26, 25, 26, 26, 26, 26, 26, 26, 25, 26, 26, 26, 26, 26, 26, 25, 26, 27, 23, 24, 23, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 25, 23, 23, 22, 23, 23, 23, 22, 23, 23, 23, 22, 23, 23, 23, 21, 22, 21, 21, 22, 21, 21, 21, 22, 21, 21, 22, 21, 21, 22, 21, 22, 21, 21, 22, 21, 22, 21, 21, 22, 21, 22, 21, 22, 21, 21]","[1697105124526, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133520, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133954, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134492, 1697105134524, 1697105134556, 1697105134587, 1697105134625, 1697105134656, 1697105134688, 1697105134719, 1697105134750, 1697105134781, 1697105134819, 1697105134850, 1697105134881, 1697105134912, 1697105134943, 1697105134974, 1697105135005, 1697105135036, 1697105135067, 1697105135098, 1697105135128, 1697105135159, 1697105135190, 1697105135221, 1697105135252, 1697105135283, 1697105135315, 1697105135346, 1697105135377, 1697105135408, 1697105135439, 1697105135470, 1697105135502, 1697105135533, 1697105135564, 1697105135596, 1697105135627, 1697105135658, 1697105135690, 1697105135721, 1697105135752, 1697105135784, 1697105135815, 1697105135853, 1697105135884, 1697105135915, 1697105135945, 1697105135976, 1697105136007, 1697105136043, 1697105136073, 1697105136103, 1697105136135, 1697105136164, 1697105136194, 1697105136224, 1697105136254, 1697105136284, 1697105136314, 1697105136344, 1697105136374, 1697105136404, 1697105136434, 1697105136464, 1697105136494, 1697105136524, 1697105136554, 1697105136584, 1697105136614, 1697105136644, 1697105136674, 1697105136705, 1697105136735, 1697105136765, 1697105136795, 1697105136825, 1697105136856, 1697105136886, 1697105136916, 1697105136946, 1697105136977, 1697105137007, 1697105137037, 1697105137068, 1697105137098, 1697105137128, 1697105137159, 1697105137189, 1697105137220, 1697105137250, 1697105137281, 1697105137311, 1697105137342, 1697105137372, 1697105137403, 1697105137433, 1697105137464, 1697105137494, 1697105137525, 1697105137556, 1697105137586, 1697105137617, 1697105137648, 1697105137679, 1697105137709, 1697105137740, 1697105137771, 1697105137802, 1697105137833, 1697105137863, 1697105137894, 1697105137925, 1697105137956, 1697105137987, 1697105138018, 1697105138048, 1697105138079, 1697105138110, 1697105138141, 1697105138172, 1697105138203, 1697105138234, 1697105138265, 1697105138296, 1697105138327, 1697105138358, 1697105138389, 1697105138420, 1697105138451, 1697105138488, 1697105138518, 1697105138547, 1697105138577, 1697105138607, 1697105138637, 1697105138667, 1697105138697, 1697105138727, 1697105138761, 1697105138789, 1697105138818, 1697105138846, 1697105138874, 1697105138903, 1697105138931, 1697105138959, 1697105138988, 1697105139016, 1697105139044, 1697105139073, 1697105139101, 1697105139130, 1697105139158, 1697105139187, 1697105139215, 1697105139244, 1697105139272, 1697105139301, 1697105139330, 1697105139356, 1697105139381, 1697105139407, 1697105139432, 1697105139458, 1697105139483, 1697105139509, 1697105139534, 1697105139560, 1697105139586, 1697105139612, 1697105139638, 1697105139663, 1697105139689, 1697105139715, 1697105139741, 1697105139766, 1697105139792, 1697105139818, 1697105139843, 1697105139869, 1697105139895, 1697105139921, 1697105139946, 1697105139972, 1697105139998, 1697105140024, 1697105140050, 1697105140075, 1697105140101, 1697105140127, 1697105140153, 1697105140179, 1697105140205, 1697105140231, 1697105140256, 1697105140282, 1697105140308, 1697105140334, 1697105140360, 1697105140386, 1697105140412, 1697105140437, 1697105140463, 1697105140490, 1697105140513, 1697105140537, 1697105140560, 1697105140583, 1697105140607, 1697105140630, 1697105140654, 1697105140677, 1697105140701, 1697105140724, 1697105140748, 1697105140771, 1697105140795, 1697105140818, 1697105140842, 1697105140865, 1697105140889, 1697105140912, 1697105140936, 1697105140959, 1697105140983, 1697105141006, 1697105141030, 1697105141053, 1697105141077, 1697105141100, 1697105141124, 1697105141149, 1697105141172, 1697105141195, 1697105141217, 1697105141240, 1697105141263, 1697105141286, 1697105141308, 1697105141331, 1697105141354, 1697105141377, 1697105141399, 1697105141422, 1697105141445, 1697105141468, 1697105141489, 1697105141511, 1697105141532, 1697105141553, 1697105141575, 1697105141596, 1697105141617, 1697105141638, 1697105141660, 1697105141681, 1697105141702, 1697105141724, 1697105141745, 1697105141766, 1697105141788, 1697105141809, 1697105141831, 1697105141852, 1697105141873, 1697105141895, 1697105141916, 1697105141938, 1697105141959, 1697105141980, 1697105142002, 1697105142023, 1697105142045, 1697105142066, 1697105142088, 1697105142109, 1697105142130]"
3273,22,44,[],200,EleutherAI/gpt-neox-20b,64,1,548.0,1.0,1,H100,1697105096195,1697105096743,120,16.0,1.0,"[20, 528]","[1697105096215, 1697105096743]"
3274,885,43,[],200,EleutherAI/gpt-neox-20b,64,1,5214.0,1.0,1,H100,1697105094856,1697105100070,120,84.0,43.0,"[4, 579, 65, 50, 64, 64, 511, 75, 57, 73, 55, 72, 54, 244, 70, 68, 67, 64, 298, 74, 70, 68, 67, 440, 75, 68, 52, 51, 242, 55, 57, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63, 57]","[1697105094860, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096193, 1697105096268, 1697105096325, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097092, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098109, 1697105098184, 1697105098252, 1697105098304, 1697105098355, 1697105098597, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070]"
3275,348,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100757,1697105102061,120,,,"[5, 578, 67, 66, 59, 47, 59]","[1697105100762, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3276,120,43,[],200,EleutherAI/gpt-neox-20b,64,1,690.0,1.0,1,H100,1697105102065,1697105102755,120,17.0,1.0,"[58, 632]","[1697105102123, 1697105102755]"
3277,706,44,[],200,EleutherAI/gpt-neox-20b,64,1,2815.0,1.0,1,H100,1697105102756,1697105105571,120,86.0,20.0,"[8, 751, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 62, 612, 81, 76, 58, 71, 66, 53]","[1697105102764, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104554, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571]"
3278,476,45,[],200,EleutherAI/gpt-neox-20b,64,1,7118.0,1.0,1,H100,1697105105572,1697105112690,120,6.0,50.0,"[16, 1118, 58, 57, 55, 44, 287, 61, 58, 55, 54, 53, 260, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 65, 64, 54, 572, 72, 121, 70, 53, 59, 459, 215, 128, 73, 54, 65, 62, 49, 243, 65, 50, 50, 50, 58, 423, 72, 57, 72]","[1697105105588, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110070, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111374, 1697105111439, 1697105111501, 1697105111550, 1697105111793, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112561, 1697105112618, 1697105112690]"
3279,589,72,[],200,EleutherAI/gpt-neox-20b,64,1,3105.0,1.0,1,H100,1697105120310,1697105123415,120,92.0,20.0,"[9, 790, 74, 55, 55, 56, 66, 514, 81, 152, 173, 153, 80, 77, 406, 79, 77, 77, 75, 56]","[1697105120319, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121349, 1697105121415, 1697105121929, 1697105122010, 1697105122162, 1697105122335, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415]"
3280,284,54,[],200,EleutherAI/gpt-neox-20b,64,1,5084.0,1.0,1,H100,1697105115225,1697105120309,120,90.0,31.0,"[13, 1531, 80, 76, 74, 56, 65, 626, 81, 59, 78, 70, 332, 71, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77]","[1697105115238, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118366, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309]"
3281,647,62,[],200,EleutherAI/gpt-neox-20b,64,1,2701.0,1.0,1,H100,1697105125272,1697105127973,120,83.0,20.0,"[4, 813, 80, 60, 77, 59, 69, 54, 278, 80, 77, 75, 67, 65, 476, 61, 78, 79, 77, 72]","[1697105125276, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973]"
3282,872,55,[],200,EleutherAI/gpt-neox-20b,64,1,3105.0,1.0,1,H100,1697105120310,1697105123415,120,91.0,20.0,"[5, 794, 74, 55, 55, 55, 67, 513, 82, 152, 173, 153, 80, 77, 406, 79, 77, 77, 75, 56]","[1697105120315, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122335, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415]"
3283,245,73,[],200,EleutherAI/gpt-neox-20b,64,1,2949.0,1.0,1,H100,1697105123416,1697105126365,120,100.0,20.0,"[9, 1094, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 73, 64, 476, 80, 60, 77, 59]","[1697105123425, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125549, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365]"
3284,547,39,[],200,EleutherAI/gpt-neox-20b,64,1,546.0,1.0,1,H100,1697105090689,1697105091235,120,12.0,1.0,"[4, 542]","[1697105090693, 1697105091235]"
3285,323,40,[],200,EleutherAI/gpt-neox-20b,64,1,2936.0,1.0,1,H100,1697105091236,1697105094172,120,84.0,20.0,"[8, 729, 74, 67, 52, 52, 65, 299, 56, 68, 51, 66, 64, 361, 77, 68, 67, 51, 52, 609]","[1697105091244, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092706, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093325, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172]"
3286,394,63,[],200,EleutherAI/gpt-neox-20b,64,1,273.0,1.0,1,H100,1697105127974,1697105128247,120,11.0,1.0,"[13, 260]","[1697105127987, 1697105128247]"
3287,360,63,[],200,EleutherAI/gpt-neox-20b,64,1,965.0,1.0,1,H100,1697105129444,1697105130409,120,16.0,1.0,"[4, 961]","[1697105129448, 1697105130409]"
3288,163,64,[],200,EleutherAI/gpt-neox-20b,64,1,1133.0,1.0,1,H100,1697105128247,1697105129380,120,67.0,6.0,"[5, 804, 72, 126, 57, 69]","[1697105128252, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380]"
3289,753,65,[],200,EleutherAI/gpt-neox-20b,64,1,2653.0,1.0,1,H100,1697105129381,1697105132034,120,83.0,20.0,"[4, 382, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424]","[1697105129385, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033]"
3290,906,41,[],200,EleutherAI/gpt-neox-20b,64,1,2650.0,1.0,1,H100,1697105094173,1697105096823,120,86.0,20.0,"[4, 625, 53, 67, 65, 50, 65, 337, 65, 50, 65, 63, 512, 74, 56, 74, 55, 72, 54, 244]","[1697105094177, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095619, 1697105095682, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823]"
3291,57,39,[],200,EleutherAI/gpt-neox-20b,64,1,507.0,1.0,1,H100,1697105100758,1697105101265,120,13.0,1.0,"[20, 487]","[1697105100778, 1697105101265]"
3292,524,66,[],200,EleutherAI/gpt-neox-20b,64,1,2093.0,1.0,1,H100,1697105132035,1697105134128,120,100.0,30.0,"[20, 637, 73, 57, 71, 72, 70, 65, 63, 50, 49, 49, 88, 65, 57, 43, 56, 53, 42, 51, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33]","[1697105132055, 1697105132692, 1697105132765, 1697105132822, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133715, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128]"
3293,140,64,[],200,EleutherAI/gpt-neox-20b,64,1,2624.0,1.0,1,H100,1697105130412,1697105133036,120,96.0,20.0,"[13, 777, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70]","[1697105130425, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035]"
3294,728,45,[],200,EleutherAI/gpt-neox-20b,64,1,569.0,1.0,1,H100,1697105096744,1697105097313,120,20.0,1.0,"[13, 556]","[1697105096757, 1697105097313]"
3295,380,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105097314,1697105102061,120,,,"[17, 779, 74, 68, 52, 52, 241, 56, 56, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105097331, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098597, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3296,627,53,[],200,EleutherAI/gpt-neox-20b,64,1,3245.0,1.0,1,H100,1697105107141,1697105110386,120,93.0,20.0,"[4, 603, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 65, 64, 54, 572, 72, 121, 70, 53]","[1697105107145, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110070, 1697105110142, 1697105110263, 1697105110333, 1697105110386]"
3297,863,36,[],200,EleutherAI/gpt-neox-20b,64,1,866.0,1.0,1,H100,1697105086246,1697105087112,120,10.0,1.0,"[4, 862]","[1697105086250, 1697105087112]"
3298,515,37,[],200,EleutherAI/gpt-neox-20b,64,1,934.0,1.0,1,H100,1697105087113,1697105088047,120,11.0,1.0,"[12, 922]","[1697105087125, 1697105088047]"
3299,291,38,[],200,EleutherAI/gpt-neox-20b,64,1,2527.0,1.0,1,H100,1697105088048,1697105090575,120,79.0,20.0,"[8, 566, 60, 61, 58, 56, 56, 271, 59, 46, 57, 46, 55, 320, 64, 57, 56, 45, 56, 530]","[1697105088056, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089184, 1697105089243, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090575]"
3300,18,42,[],200,EleutherAI/gpt-neox-20b,64,1,807.0,1.0,1,H100,1697105090428,1697105091235,120,15.0,1.0,"[8, 799]","[1697105090436, 1697105091235]"
3301,720,43,[],200,EleutherAI/gpt-neox-20b,64,1,982.0,1.0,1,H100,1697105091236,1697105092218,120,286.0,6.0,"[4, 733, 74, 67, 52, 52]","[1697105091240, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218]"
3302,877,39,[],200,EleutherAI/gpt-neox-20b,64,1,2748.0,1.0,1,H100,1697105090576,1697105093324,120,85.0,20.0,"[12, 806, 71, 67, 65, 64, 311, 74, 68, 52, 52, 65, 299, 57, 66, 52, 66, 64, 361, 76]","[1697105090588, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092046, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324]"
3303,250,46,[],200,EleutherAI/gpt-neox-20b,64,1,453.0,1.0,1,H100,1697105107921,1697105108374,120,31.0,1.0,"[16, 437]","[1697105107937, 1697105108374]"
3304,833,47,[],200,EleutherAI/gpt-neox-20b,64,1,1888.0,1.0,1,H100,1697105108375,1697105110263,120,563.0,8.0,"[9, 931, 65, 64, 54, 573, 72, 120]","[1697105108384, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263]"
3305,608,48,[],200,EleutherAI/gpt-neox-20b,64,1,2497.0,1.0,1,H100,1697105110264,1697105112761,120,96.0,20.0,"[5, 636, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71]","[1697105110269, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761]"
3306,649,40,[],200,EleutherAI/gpt-neox-20b,64,1,2869.0,1.0,1,H100,1697105093325,1697105096194,120,244.0,20.0,"[5, 842, 74, 57, 72, 71, 55, 54, 247, 53, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512]","[1697105093330, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194]"
3307,386,46,[],200,EleutherAI/gpt-neox-20b,64,1,1471.0,1.0,1,H100,1697105113687,1697105115158,120,140.0,6.0,"[4, 1121, 140, 70, 68, 68]","[1697105113691, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158]"
3308,37,47,[],200,EleutherAI/gpt-neox-20b,64,1,434.0,1.0,1,H100,1697105115160,1697105115594,120,20.0,1.0,"[4, 429]","[1697105115164, 1697105115593]"
3309,710,48,[],200,EleutherAI/gpt-neox-20b,64,1,963.0,1.0,1,H100,1697105115595,1697105116558,120,14.0,1.0,"[8, 955]","[1697105115603, 1697105116558]"
3310,448,47,[],200,EleutherAI/gpt-neox-20b,64,1,1644.0,1.0,1,H100,1697105117965,1697105119609,120,335.0,12.0,"[8, 393, 71, 72, 64, 330, 73, 72, 71, 65, 348, 77]","[1697105117973, 1697105118366, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609]"
3311,291,71,[],200,EleutherAI/gpt-neox-20b,64,1,2521.0,1.0,1,H100,1697105127974,1697105130495,120,79.0,20.0,"[9, 342, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444]","[1697105127983, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495]"
3312,436,26,[],200,EleutherAI/gpt-neox-20b,64,1,2827.0,1.0,1,H100,1697105070952,1697105073779,120,86.0,20.0,"[58, 1023, 57, 47, 46, 56, 55, 604, 73, 68, 63, 60, 49, 59, 47, 234, 52, 64, 61, 51]","[1697105071010, 1697105072033, 1697105072090, 1697105072137, 1697105072183, 1697105072239, 1697105072294, 1697105072898, 1697105072971, 1697105073039, 1697105073102, 1697105073162, 1697105073211, 1697105073270, 1697105073317, 1697105073551, 1697105073603, 1697105073667, 1697105073728, 1697105073779]"
3313,480,49,[],200,EleutherAI/gpt-neox-20b,64,1,932.0,1.0,1,H100,1697105116559,1697105117491,120,26.0,1.0,"[13, 919]","[1697105116572, 1697105117491]"
3314,135,50,[],200,EleutherAI/gpt-neox-20b,64,1,874.0,1.0,1,H100,1697105117492,1697105118366,120,52.0,2.0,"[4, 867]","[1697105117496, 1697105118363]"
3315,783,60,[],200,EleutherAI/gpt-neox-20b,64,1,303.0,1.0,1,H100,1697105110388,1697105110691,120,286.0,1.0,"[20, 283]","[1697105110408, 1697105110691]"
3316,553,61,[],200,EleutherAI/gpt-neox-20b,64,1,2994.0,1.0,1,H100,1697105110692,1697105113686,120,88.0,20.0,"[9, 1093, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62]","[1697105110701, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686]"
3317,421,54,[],200,EleutherAI/gpt-neox-20b,64,1,2344.0,1.0,1,H100,1697105117965,1697105120309,120,85.0,20.0,"[4, 396, 72, 72, 64, 330, 73, 71, 72, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77]","[1697105117969, 1697105118365, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309]"
3318,879,48,[],200,EleutherAI/gpt-neox-20b,64,1,6729.0,1.0,1,H100,1697105126236,1697105132965,120,39.0,55.0,"[9, 522, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 63, 324, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72]","[1697105126245, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129443, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965]"
3319,56,48,[],200,EleutherAI/gpt-neox-20b,64,1,1733.0,1.0,1,H100,1697105132034,1697105133767,120,86.0,20.0,"[9, 649, 73, 56, 72, 72, 70, 65, 63, 50, 49, 49, 88, 65, 57, 43, 56, 53, 42, 52]","[1697105132043, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133715, 1697105133767]"
3320,775,40,[],200,EleutherAI/gpt-neox-20b,64,1,791.0,1.0,1,H100,1697105118036,1697105118827,120,17.0,1.0,"[12, 779]","[1697105118048, 1697105118827]"
3321,394,45,[],200,EleutherAI/gpt-neox-20b,64,1,389.0,1.0,1,H100,1697105118438,1697105118827,120,11.0,1.0,"[5, 384]","[1697105118443, 1697105118827]"
3322,552,41,[],200,EleutherAI/gpt-neox-20b,64,1,2521.0,1.0,1,H100,1697105118828,1697105121349,120,87.0,20.0,"[8, 696, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 56]","[1697105118836, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121349]"
3323,215,62,[],200,EleutherAI/gpt-neox-20b,64,1,909.0,1.0,1,H100,1697105113687,1697105114596,120,12.0,1.0,"[12, 897]","[1697105113699, 1697105114596]"
3324,914,63,[],200,EleutherAI/gpt-neox-20b,64,1,3438.0,1.0,1,H100,1697105114597,1697105118035,120,84.0,20.0,"[4, 1161, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 71]","[1697105114601, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035]"
3325,170,46,[],200,EleutherAI/gpt-neox-20b,64,1,1762.0,1.0,1,H100,1697105118828,1697105120590,120,335.0,15.0,"[4, 700, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68]","[1697105118832, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590]"
3326,118,52,[],200,EleutherAI/gpt-neox-20b,64,1,2435.0,1.0,1,H100,1697105106065,1697105108500,120,85.0,20.0,"[5, 636, 58, 57, 55, 44, 287, 60, 59, 55, 54, 53, 260, 61, 61, 50, 55, 45, 423, 57]","[1697105106070, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107267, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107920, 1697105107975, 1697105108020, 1697105108443, 1697105108500]"
3327,156,47,[],200,EleutherAI/gpt-neox-20b,64,1,3453.0,1.0,1,H100,1697105102065,1697105105518,120,86.0,20.0,"[102, 641, 707, 66, 51, 50, 64, 408, 73, 70, 69, 64, 62, 61, 613, 81, 76, 58, 71, 66]","[1697105102167, 1697105102808, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104227, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518]"
3328,347,51,[],200,EleutherAI/gpt-neox-20b,64,1,10134.0,1.0,1,H100,1697105108375,1697105118509,120,100.0,72.0,"[17, 923, 65, 64, 54, 573, 72, 120, 74, 49, 59, 459, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 47, 61, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 71, 55, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 328, 75, 72]","[1697105108392, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114231, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116020, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118362, 1697105118437, 1697105118509]"
3329,755,47,[],200,EleutherAI/gpt-neox-20b,64,1,4144.0,1.0,1,H100,1697105120591,1697105124735,120,286.0,25.0,"[9, 1328, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59]","[1697105120600, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735]"
3330,466,50,[],200,EleutherAI/gpt-neox-20b,64,1,3506.0,1.0,1,H100,1697105120591,1697105124097,120,457.0,20.0,"[5, 1332, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58]","[1697105120596, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097]"
3331,739,48,[],200,EleutherAI/gpt-neox-20b,64,1,545.0,1.0,1,H100,1697105105519,1697105106064,120,216.0,1.0,"[16, 529]","[1697105105535, 1697105106064]"
3332,507,49,[],200,EleutherAI/gpt-neox-20b,64,1,2436.0,1.0,1,H100,1697105106065,1697105108501,120,83.0,20.0,"[9, 632, 58, 57, 55, 44, 286, 61, 58, 56, 54, 53, 260, 61, 61, 50, 55, 45, 423, 57]","[1697105106074, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107920, 1697105107975, 1697105108020, 1697105108443, 1697105108500]"
3333,842,51,[],200,EleutherAI/gpt-neox-20b,64,1,1942.0,1.0,1,H100,1697105118367,1697105120309,120,161.0,16.0,"[7, 529, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 235, 76]","[1697105118374, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120233, 1697105120309]"
3334,118,52,[],200,EleutherAI/gpt-neox-20b,64,1,2081.0,1.0,1,H100,1697105118510,1697105120591,120,85.0,20.0,"[4, 389, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68]","[1697105118514, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590]"
3335,659,45,[],200,EleutherAI/gpt-neox-20b,64,1,17409.0,1.0,1,H100,1697105124879,1697105142288,120,286.0,381.0,"[11, 1199, 80, 60, 77, 59, 69, 54, 279, 79, 77, 75, 67, 65, 476, 61, 78, 79, 77, 72, 68, 284, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71, 65, 63, 49, 50, 49, 88, 64, 57, 44, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 31, 32, 32, 31, 38, 31, 32, 31, 31, 31, 38, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 32, 31, 31, 31, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 38, 31, 31, 30, 31, 31, 36, 30, 30, 32, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 30, 30, 30, 30, 31, 30, 30, 30, 31, 30, 30, 31, 30, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31, 31, 31, 30, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 37, 30, 29, 30, 30, 30, 30, 30, 30, 34, 28, 29, 28, 28, 29, 28, 28, 29, 28, 28, 29, 28, 29, 28, 29, 28, 29, 28, 29, 29, 26, 25, 26, 25, 26, 25, 26, 25, 26, 26, 26, 26, 25, 26, 26, 26, 25, 26, 26, 25, 26, 26, 26, 25, 26, 26, 26, 26, 25, 26, 26, 26, 26, 26, 26, 25, 26, 26, 26, 26, 26, 26, 25, 26, 27, 23, 24, 23, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 25, 23, 23, 22, 23, 23, 23, 22, 23, 23, 23, 22, 23, 23, 23, 21, 22, 21, 21, 22, 21, 21, 21, 22, 21, 21, 22, 21, 21, 22, 21, 22, 21, 21, 22, 21, 22, 21, 21, 22, 21, 22, 21, 22, 21, 21, 21, 20, 19, 20, 19, 20, 19, 19]","[1697105124890, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133520, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134492, 1697105134524, 1697105134556, 1697105134587, 1697105134625, 1697105134656, 1697105134688, 1697105134719, 1697105134750, 1697105134781, 1697105134819, 1697105134850, 1697105134881, 1697105134912, 1697105134943, 1697105134974, 1697105135005, 1697105135036, 1697105135067, 1697105135098, 1697105135128, 1697105135159, 1697105135190, 1697105135221, 1697105135252, 1697105135283, 1697105135315, 1697105135346, 1697105135377, 1697105135408, 1697105135439, 1697105135470, 1697105135502, 1697105135533, 1697105135564, 1697105135596, 1697105135627, 1697105135658, 1697105135690, 1697105135721, 1697105135752, 1697105135784, 1697105135815, 1697105135853, 1697105135884, 1697105135915, 1697105135945, 1697105135976, 1697105136007, 1697105136043, 1697105136073, 1697105136103, 1697105136135, 1697105136164, 1697105136194, 1697105136224, 1697105136254, 1697105136284, 1697105136314, 1697105136344, 1697105136374, 1697105136404, 1697105136434, 1697105136464, 1697105136494, 1697105136524, 1697105136554, 1697105136584, 1697105136614, 1697105136644, 1697105136674, 1697105136705, 1697105136735, 1697105136765, 1697105136795, 1697105136825, 1697105136856, 1697105136886, 1697105136916, 1697105136946, 1697105136977, 1697105137007, 1697105137037, 1697105137068, 1697105137098, 1697105137128, 1697105137159, 1697105137189, 1697105137220, 1697105137250, 1697105137281, 1697105137311, 1697105137342, 1697105137372, 1697105137403, 1697105137433, 1697105137464, 1697105137495, 1697105137525, 1697105137556, 1697105137586, 1697105137617, 1697105137648, 1697105137679, 1697105137709, 1697105137740, 1697105137771, 1697105137802, 1697105137833, 1697105137863, 1697105137894, 1697105137925, 1697105137956, 1697105137987, 1697105138018, 1697105138048, 1697105138079, 1697105138110, 1697105138141, 1697105138172, 1697105138203, 1697105138234, 1697105138265, 1697105138296, 1697105138327, 1697105138358, 1697105138389, 1697105138420, 1697105138451, 1697105138488, 1697105138518, 1697105138547, 1697105138577, 1697105138607, 1697105138637, 1697105138667, 1697105138697, 1697105138727, 1697105138761, 1697105138789, 1697105138818, 1697105138846, 1697105138874, 1697105138903, 1697105138931, 1697105138959, 1697105138988, 1697105139016, 1697105139044, 1697105139073, 1697105139101, 1697105139130, 1697105139158, 1697105139187, 1697105139215, 1697105139244, 1697105139272, 1697105139301, 1697105139330, 1697105139356, 1697105139381, 1697105139407, 1697105139432, 1697105139458, 1697105139483, 1697105139509, 1697105139534, 1697105139560, 1697105139586, 1697105139612, 1697105139638, 1697105139663, 1697105139689, 1697105139715, 1697105139741, 1697105139766, 1697105139792, 1697105139818, 1697105139843, 1697105139869, 1697105139895, 1697105139921, 1697105139946, 1697105139972, 1697105139998, 1697105140024, 1697105140050, 1697105140075, 1697105140101, 1697105140127, 1697105140153, 1697105140179, 1697105140205, 1697105140231, 1697105140256, 1697105140282, 1697105140308, 1697105140334, 1697105140360, 1697105140386, 1697105140412, 1697105140437, 1697105140463, 1697105140490, 1697105140513, 1697105140537, 1697105140560, 1697105140583, 1697105140607, 1697105140630, 1697105140654, 1697105140677, 1697105140701, 1697105140724, 1697105140748, 1697105140771, 1697105140795, 1697105140818, 1697105140842, 1697105140865, 1697105140889, 1697105140912, 1697105140936, 1697105140959, 1697105140983, 1697105141006, 1697105141030, 1697105141053, 1697105141077, 1697105141100, 1697105141124, 1697105141149, 1697105141172, 1697105141195, 1697105141217, 1697105141240, 1697105141263, 1697105141286, 1697105141308, 1697105141331, 1697105141354, 1697105141377, 1697105141399, 1697105141422, 1697105141445, 1697105141468, 1697105141489, 1697105141511, 1697105141532, 1697105141553, 1697105141575, 1697105141596, 1697105141617, 1697105141638, 1697105141660, 1697105141681, 1697105141702, 1697105141724, 1697105141745, 1697105141766, 1697105141788, 1697105141809, 1697105141831, 1697105141852, 1697105141873, 1697105141895, 1697105141916, 1697105141938, 1697105141959, 1697105141980, 1697105142002, 1697105142023, 1697105142045, 1697105142066, 1697105142088, 1697105142109, 1697105142130, 1697105142151, 1697105142171, 1697105142190, 1697105142210, 1697105142229, 1697105142249, 1697105142268, 1697105142287]"
3336,309,41,[],200,EleutherAI/gpt-neox-20b,64,1,2583.0,1.0,1,H100,1697105096195,1697105098778,120,52.0,20.0,"[8, 620, 70, 68, 67, 65, 297, 74, 70, 68, 67, 441, 74, 68, 52, 52, 242, 54, 57, 69]","[1697105096203, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097093, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098652, 1697105098709, 1697105098778]"
3337,622,62,[],200,EleutherAI/gpt-neox-20b,64,1,320.0,1.0,1,H100,1697105117965,1697105118285,120,20.0,1.0,"[12, 308]","[1697105117977, 1697105118285]"
3338,220,48,[],200,EleutherAI/gpt-neox-20b,64,1,623.0,1.0,1,H100,1697105119610,1697105120233,120,67.0,2.0,"[5, 618]","[1697105119615, 1697105120233]"
3339,809,49,[],200,EleutherAI/gpt-neox-20b,64,1,781.0,1.0,1,H100,1697105120234,1697105121015,120,16.0,1.0,"[4, 777]","[1697105120238, 1697105121015]"
3340,579,50,[],200,EleutherAI/gpt-neox-20b,64,1,654.0,1.0,1,H100,1697105121016,1697105121670,120,19.0,1.0,"[17, 637]","[1697105121033, 1697105121670]"
3341,255,50,[],200,EleutherAI/gpt-neox-20b,64,1,16017.0,1.0,1,H100,1697105108502,1697105124519,120,216.0,119.0,"[20, 793, 66, 63, 54, 572, 72, 121, 74, 49, 59, 459, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 328, 75, 72, 64, 330, 73, 71, 72, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 71, 70, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 78, 58, 58, 58, 69, 353]","[1697105108522, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110070, 1697105110142, 1697105110263, 1697105110337, 1697105110386, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118362, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120452, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519]"
3342,325,51,[],200,EleutherAI/gpt-neox-20b,64,1,3206.0,1.0,1,H100,1697105121671,1697105124877,120,85.0,20.0,"[5, 1375, 79, 77, 77, 75, 56, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 69]","[1697105121676, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877]"
3343,539,62,[],200,EleutherAI/gpt-neox-20b,64,1,2775.0,1.0,1,H100,1697105118574,1697105121349,120,83.0,20.0,"[4, 954, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55]","[1697105118578, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348]"
3344,392,63,[],200,EleutherAI/gpt-neox-20b,64,1,541.0,1.0,1,H100,1697105118286,1697105118827,120,20.0,1.0,"[8, 533]","[1697105118294, 1697105118827]"
3345,51,64,[],200,EleutherAI/gpt-neox-20b,64,1,5017.0,1.0,1,H100,1697105118828,1697105123845,120,364.0,36.0,"[8, 696, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 75, 56, 71, 359]","[1697105118836, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415, 1697105123486, 1697105123845]"
3346,104,53,[],200,EleutherAI/gpt-neox-20b,64,1,2228.0,1.0,1,H100,1697105119120,1697105121348,120,93.0,20.0,"[5, 407, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55]","[1697105119125, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348]"
3347,717,48,[],200,EleutherAI/gpt-neox-20b,64,1,2374.0,1.0,1,H100,1697105110387,1697105112761,120,89.0,20.0,"[5, 512, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71]","[1697105110392, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761]"
3348,371,49,[],200,EleutherAI/gpt-neox-20b,64,1,298.0,1.0,1,H100,1697105112763,1697105113061,120,13.0,1.0,"[24, 274]","[1697105112787, 1697105113061]"
3349,755,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101266,1697105102062,120,,,[9],[1697105101275]
3350,626,49,[],200,EleutherAI/gpt-neox-20b,64,1,429.0,1.0,1,H100,1697105113492,1697105113921,120,10.0,1.0,"[5, 424]","[1697105113497, 1697105113921]"
3351,118,50,[],200,EleutherAI/gpt-neox-20b,64,1,3068.0,1.0,1,H100,1697105113062,1697105116130,120,85.0,20.0,"[4, 924, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55]","[1697105113066, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130]"
3352,416,41,[],200,EleutherAI/gpt-neox-20b,64,1,7429.0,1.0,1,H100,1697105102069,1697105109498,120,286.0,50.0,"[50, 689, 707, 67, 50, 50, 64, 408, 72, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66, 52, 573, 111, 58, 44, 350, 58, 57, 55, 44, 286, 61, 58, 55, 55, 53, 259, 62, 61, 49, 56, 44, 424, 57, 50, 48, 717, 65, 64, 54]","[1697105102119, 1697105102808, 1697105103515, 1697105103582, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105570, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325, 1697105107380, 1697105107435, 1697105107488, 1697105107747, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108019, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109498]"
3353,681,42,[],200,EleutherAI/gpt-neox-20b,64,1,488.0,1.0,1,H100,1697105096825,1697105097313,120,23.0,1.0,"[20, 468]","[1697105096845, 1697105097313]"
3354,763,60,[],200,EleutherAI/gpt-neox-20b,64,1,496.0,1.0,1,H100,1697105128474,1697105128970,120,20.0,1.0,"[5, 491]","[1697105128479, 1697105128970]"
3355,415,61,[],200,EleutherAI/gpt-neox-20b,64,1,3922.0,1.0,1,H100,1697105128971,1697105132893,120,109.0,29.0,"[8, 788, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72]","[1697105128979, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893]"
3356,262,49,[],200,EleutherAI/gpt-neox-20b,64,1,298.0,1.0,1,H100,1697105112763,1697105113061,120,39.0,1.0,"[20, 278]","[1697105112783, 1697105113061]"
3357,505,36,[],200,EleutherAI/gpt-neox-20b,64,1,3916.0,1.0,1,H100,1697105084766,1697105088682,120,100.0,27.0,"[5, 1133, 73, 70, 68, 67, 63, 349, 69, 55, 52, 65, 62, 291, 71, 55, 71, 69, 67, 63, 546, 63, 61, 48, 55, 264, 61]","[1697105084771, 1697105085904, 1697105085977, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086718, 1697105086770, 1697105086835, 1697105086897, 1697105087188, 1697105087259, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584, 1697105088130, 1697105088193, 1697105088254, 1697105088302, 1697105088357, 1697105088621, 1697105088682]"
3358,132,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099951,1697105102062,120,,,"[5, 628, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105099956, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3359,836,49,[],200,EleutherAI/gpt-neox-20b,64,1,1199.0,1.0,1,H100,1697105102069,1697105103268,120,11.0,1.0,"[131, 1068]","[1697105102200, 1697105103268]"
3360,275,37,[],200,EleutherAI/gpt-neox-20b,64,1,606.0,1.0,1,H100,1697105088683,1697105089289,120,161.0,4.0,"[5, 497, 57, 47]","[1697105088688, 1697105089185, 1697105089242, 1697105089289]"
3361,493,50,[],200,EleutherAI/gpt-neox-20b,64,1,3437.0,1.0,1,H100,1697105103269,1697105106706,120,83.0,20.0,"[8, 877, 72, 71, 69, 64, 62, 62, 612, 80, 76, 59, 71, 66, 53, 572, 111, 58, 44, 350]","[1697105103277, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104554, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706]"
3362,859,38,[],200,EleutherAI/gpt-neox-20b,64,1,408.0,1.0,1,H100,1697105089290,1697105089698,120,23.0,1.0,"[4, 404]","[1697105089294, 1697105089698]"
3363,607,39,[],200,EleutherAI/gpt-neox-20b,64,1,1833.0,1.0,1,H100,1697105089699,1697105091532,120,6.0,10.0,"[9, 866, 65, 49, 60, 59, 58, 529, 71, 67]","[1697105089708, 1697105090574, 1697105090639, 1697105090688, 1697105090748, 1697105090807, 1697105090865, 1697105091394, 1697105091465, 1697105091532]"
3364,193,53,[],200,EleutherAI/gpt-neox-20b,64,1,3453.0,1.0,1,H100,1697105102065,1697105105518,120,79.0,20.0,"[17, 725, 708, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66]","[1697105102082, 1697105102807, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518]"
3365,66,72,[],200,EleutherAI/gpt-neox-20b,64,1,2540.0,1.0,1,H100,1697105130496,1697105133036,120,84.0,20.0,"[13, 693, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 71]","[1697105130509, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133036]"
3366,260,40,[],200,EleutherAI/gpt-neox-20b,64,1,2639.0,1.0,1,H100,1697105091533,1697105094172,120,86.0,20.0,"[5, 435, 74, 67, 52, 52, 65, 299, 56, 67, 52, 66, 64, 360, 78, 68, 67, 51, 52, 609]","[1697105091538, 1697105091973, 1697105092047, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092638, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093247, 1697105093325, 1697105093393, 1697105093460, 1697105093511, 1697105093563, 1697105094172]"
3367,184,42,[],200,EleutherAI/gpt-neox-20b,64,1,3262.0,1.0,1,H100,1697105109499,1697105112761,120,87.0,20.0,"[4, 1402, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71]","[1697105109503, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761]"
3368,434,55,[],200,EleutherAI/gpt-neox-20b,64,1,3368.0,1.0,1,H100,1697105113687,1697105117055,120,85.0,20.0,"[8, 1117, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55]","[1697105113695, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054]"
3369,36,41,[],200,EleutherAI/gpt-neox-20b,64,1,2651.0,1.0,1,H100,1697105094173,1697105096824,120,457.0,20.0,"[9, 620, 53, 67, 65, 50, 65, 337, 65, 50, 66, 62, 512, 74, 56, 74, 55, 72, 54, 244]","[1697105094182, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095620, 1697105095682, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823]"
3370,769,43,[],200,EleutherAI/gpt-neox-20b,64,1,2462.0,1.0,1,H100,1697105112762,1697105115224,120,47.0,20.0,"[5, 506, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 47, 61, 520, 140, 70, 68, 68, 66]","[1697105112767, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114231, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224]"
3371,521,48,[],200,EleutherAI/gpt-neox-20b,64,1,342.0,1.0,1,H100,1697105124736,1697105125078,120,18.0,1.0,"[8, 334]","[1697105124744, 1697105125078]"
3372,181,49,[],200,EleutherAI/gpt-neox-20b,64,1,4972.0,1.0,1,H100,1697105125079,1697105130051,120,91.0,39.0,"[4, 1006, 80, 60, 77, 59, 69, 54, 278, 80, 77, 75, 67, 65, 476, 61, 78, 79, 77, 72, 68, 284, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48]","[1697105125083, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051]"
3373,95,56,[],200,EleutherAI/gpt-neox-20b,64,1,435.0,1.0,1,H100,1697105117056,1697105117491,120,12.0,1.0,"[20, 415]","[1697105117076, 1697105117491]"
3374,803,54,[],200,EleutherAI/gpt-neox-20b,64,1,320.0,1.0,1,H100,1697105121350,1697105121670,120,20.0,1.0,"[20, 300]","[1697105121370, 1697105121670]"
3375,467,55,[],200,EleutherAI/gpt-neox-20b,64,1,3206.0,1.0,1,H100,1697105121671,1697105124877,120,93.0,20.0,"[13, 1367, 79, 77, 77, 75, 56, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 69]","[1697105121684, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877]"
3376,793,57,[],200,EleutherAI/gpt-neox-20b,64,1,4436.0,1.0,1,H100,1697105117492,1697105121928,120,92.0,31.0,"[8, 863, 74, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513]","[1697105117500, 1697105118363, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928]"
3377,907,52,[],200,EleutherAI/gpt-neox-20b,64,1,1131.0,1.0,1,H100,1697105124878,1697105126009,120,10.0,1.0,"[4, 1127]","[1697105124882, 1697105126009]"
3378,685,53,[],200,EleutherAI/gpt-neox-20b,64,1,757.0,1.0,1,H100,1697105126010,1697105126767,120,364.0,2.0,"[5, 752]","[1697105126015, 1697105126767]"
3379,336,54,[],200,EleutherAI/gpt-neox-20b,64,1,1206.0,1.0,1,H100,1697105126767,1697105127973,120,58.0,7.0,"[5, 834, 61, 78, 79, 77, 72]","[1697105126772, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973]"
3380,105,55,[],200,EleutherAI/gpt-neox-20b,64,1,4301.0,1.0,1,H100,1697105127974,1697105132275,120,364.0,36.0,"[5, 346, 77, 71, 70, 67, 52, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52]","[1697105127979, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128610, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275]"
3381,15,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100757,1697105102062,120,,,"[17, 566, 67, 66, 59, 48, 58]","[1697105100774, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101580, 1697105101638]"
3382,713,41,[],200,EleutherAI/gpt-neox-20b,64,1,2088.0,1.0,1,H100,1697105102066,1697105104154,120,874.0,8.0,"[41, 701, 707, 67, 50, 50, 64, 408]","[1697105102107, 1697105102808, 1697105103515, 1697105103582, 1697105103632, 1697105103682, 1697105103746, 1697105104154]"
3383,799,50,[],200,EleutherAI/gpt-neox-20b,64,1,2902.0,1.0,1,H100,1697105122646,1697105125548,120,84.0,20.0,"[4, 1195, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 72]","[1697105122650, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548]"
3384,369,42,[],200,EleutherAI/gpt-neox-20b,64,1,2666.0,1.0,1,H100,1697105104155,1697105106821,120,216.0,15.0,"[4, 1007, 81, 76, 58, 71, 66, 53, 572, 111, 58, 44, 350, 58, 57]","[1697105104159, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821]"
3385,903,59,[],200,EleutherAI/gpt-neox-20b,64,1,1028.0,1.0,1,H100,1697105111795,1697105112823,120,244.0,7.0,"[4, 690, 73, 56, 72, 71, 62]","[1697105111799, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823]"
3386,674,60,[],200,EleutherAI/gpt-neox-20b,64,1,15718.0,1.0,1,H100,1697105112825,1697105128543,120,161.0,119.0,"[4, 1161, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 71, 327, 75, 72, 64, 330, 73, 71, 72, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 78, 58, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 58, 59, 74, 57, 74, 72, 65, 476, 80, 60, 77, 58, 70, 54, 278, 80, 76, 76, 67, 65, 476, 60, 79, 79, 76, 73, 68, 284, 76, 72, 69]","[1697105112829, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118362, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125212, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126364, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126922, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127900, 1697105127973, 1697105128041, 1697105128325, 1697105128401, 1697105128473, 1697105128542]"
3387,455,51,[],200,EleutherAI/gpt-neox-20b,64,1,2424.0,1.0,1,H100,1697105125549,1697105127973,120,91.0,20.0,"[5, 535, 80, 60, 77, 59, 69, 54, 278, 80, 77, 75, 67, 65, 476, 61, 78, 79, 77, 72]","[1697105125554, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973]"
3388,118,61,[],200,EleutherAI/gpt-neox-20b,64,1,2133.0,1.0,1,H100,1697105123416,1697105125549,120,85.0,20.0,"[5, 424, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 73]","[1697105123421, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125549]"
3389,224,52,[],200,EleutherAI/gpt-neox-20b,64,1,2521.0,1.0,1,H100,1697105127974,1697105130495,120,85.0,20.0,"[5, 346, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444]","[1697105127979, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495]"
3390,815,51,[],200,EleutherAI/gpt-neox-20b,64,1,794.0,1.0,1,H100,1697105116131,1697105116925,120,52.0,4.0,"[9, 629, 80, 76]","[1697105116140, 1697105116769, 1697105116849, 1697105116925]"
3391,476,52,[],200,EleutherAI/gpt-neox-20b,64,1,6919.0,1.0,1,H100,1697105116926,1697105123845,120,6.0,50.0,"[9, 811, 81, 59, 78, 71, 331, 71, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359]","[1697105116935, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118366, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845]"
3392,146,43,[],200,EleutherAI/gpt-neox-20b,64,1,2622.0,1.0,1,H100,1697105106822,1697105109444,120,96.0,20.0,"[4, 381, 61, 58, 55, 54, 53, 260, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 65, 64]","[1697105106826, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
3393,701,62,[],200,EleutherAI/gpt-neox-20b,64,1,5113.0,1.0,1,H100,1697105125550,1697105130663,120,58.0,43.0,"[20, 519, 80, 63, 74, 59, 70, 53, 279, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51]","[1697105125570, 1697105126089, 1697105126169, 1697105126232, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663]"
3394,813,53,[],200,EleutherAI/gpt-neox-20b,64,1,2540.0,1.0,1,H100,1697105130496,1697105133036,120,85.0,20.0,"[8, 698, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 71]","[1697105130504, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133036]"
3395,107,46,[],200,EleutherAI/gpt-neox-20b,64,1,584.0,1.0,1,H100,1697105112690,1697105113274,120,216.0,2.0,"[5, 578]","[1697105112695, 1697105113273]"
3396,191,55,[],200,EleutherAI/gpt-neox-20b,64,1,3105.0,1.0,1,H100,1697105120310,1697105123415,120,85.0,20.0,"[9, 790, 74, 55, 55, 56, 66, 514, 81, 152, 173, 153, 80, 77, 406, 79, 77, 77, 74, 57]","[1697105120319, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121349, 1697105121415, 1697105121929, 1697105122010, 1697105122162, 1697105122335, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415]"
3397,804,47,[],200,EleutherAI/gpt-neox-20b,64,1,646.0,1.0,1,H100,1697105113275,1697105113921,120,20.0,1.0,"[4, 642]","[1697105113279, 1697105113921]"
3398,464,48,[],200,EleutherAI/gpt-neox-20b,64,1,673.0,1.0,1,H100,1697105113923,1697105114596,120,12.0,1.0,"[12, 661]","[1697105113935, 1697105114596]"
3399,235,49,[],200,EleutherAI/gpt-neox-20b,64,1,2328.0,1.0,1,H100,1697105114597,1697105116925,120,161.0,12.0,"[8, 1157, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76]","[1697105114605, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925]"
3400,837,31,[],200,EleutherAI/gpt-neox-20b,64,1,3453.0,1.0,1,H100,1697105102065,1697105105518,120,85.0,20.0,"[54, 689, 707, 66, 51, 50, 64, 408, 72, 71, 69, 64, 62, 61, 613, 81, 75, 59, 71, 66]","[1697105102119, 1697105102808, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105247, 1697105105322, 1697105105381, 1697105105452, 1697105105518]"
3401,452,58,[],200,EleutherAI/gpt-neox-20b,64,1,1277.0,1.0,1,H100,1697105121930,1697105123207,120,216.0,4.0,"[4, 1020, 97, 79, 77]","[1697105121934, 1697105122954, 1697105123051, 1697105123130, 1697105123207]"
3402,222,59,[],200,EleutherAI/gpt-neox-20b,64,1,2341.0,1.0,1,H100,1697105123208,1697105125549,120,96.0,20.0,"[5, 632, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 73]","[1697105123213, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125549]"
3403,774,56,[],200,EleutherAI/gpt-neox-20b,64,1,255.0,1.0,1,H100,1697105123416,1697105123671,120,8.0,1.0,"[13, 242]","[1697105123429, 1697105123671]"
3404,373,44,[],200,EleutherAI/gpt-neox-20b,64,1,286.0,1.0,1,H100,1697105092220,1697105092506,120,15.0,1.0,"[8, 278]","[1697105092228, 1697105092506]"
3405,202,50,[],200,EleutherAI/gpt-neox-20b,64,1,10488.0,1.0,1,H100,1697105108021,1697105118509,120,874.0,72.0,"[4, 1290, 65, 64, 54, 573, 72, 120, 74, 49, 60, 458, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 47, 61, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 328, 75, 72]","[1697105108025, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114231, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118362, 1697105118437, 1697105118509]"
3406,523,57,[],200,EleutherAI/gpt-neox-20b,64,1,1804.0,1.0,1,H100,1697105123672,1697105125476,120,345.0,13.0,"[12, 835, 79, 78, 59, 73, 69, 277, 58, 59, 74, 57, 74]","[1697105123684, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125212, 1697105125271, 1697105125345, 1697105125402, 1697105125476]"
3407,38,50,[],200,EleutherAI/gpt-neox-20b,64,1,3069.0,1.0,1,H100,1697105113061,1697105116130,120,88.0,20.0,"[5, 924, 67, 65, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55]","[1697105113066, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130]"
3408,574,64,[],200,EleutherAI/gpt-neox-20b,64,1,867.0,1.0,1,H100,1697105118036,1697105118903,120,364.0,2.0,"[4, 863]","[1697105118040, 1697105118903]"
3409,201,27,[],200,EleutherAI/gpt-neox-20b,64,1,2295.0,1.0,1,H100,1697105073780,1697105076075,120,67.0,20.0,"[40, 484, 63, 50, 61, 60, 53, 476, 67, 64, 52, 64, 63, 59, 357, 61, 48, 60, 57, 56]","[1697105073820, 1697105074304, 1697105074367, 1697105074417, 1697105074478, 1697105074538, 1697105074591, 1697105075067, 1697105075134, 1697105075198, 1697105075250, 1697105075314, 1697105075377, 1697105075436, 1697105075793, 1697105075854, 1697105075902, 1697105075962, 1697105076019, 1697105076075]"
3410,346,65,[],200,EleutherAI/gpt-neox-20b,64,1,2444.0,1.0,1,H100,1697105118904,1697105121348,120,85.0,20.0,"[5, 623, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55]","[1697105118909, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348]"
3411,495,52,[],200,EleutherAI/gpt-neox-20b,64,1,705.0,1.0,1,H100,1697105120310,1697105121015,120,13.0,1.0,"[13, 692]","[1697105120323, 1697105121015]"
3412,270,53,[],200,EleutherAI/gpt-neox-20b,64,1,2268.0,1.0,1,H100,1697105121016,1697105123284,120,364.0,12.0,"[5, 907, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77]","[1697105121021, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284]"
3413,816,53,[],200,EleutherAI/gpt-neox-20b,64,1,943.0,1.0,1,H100,1697105108501,1697105109444,120,182.0,4.0,"[5, 809, 65, 64]","[1697105108506, 1697105109315, 1697105109380, 1697105109444]"
3414,749,65,[],200,EleutherAI/gpt-neox-20b,64,1,2519.0,1.0,1,H100,1697105123846,1697105126365,120,47.0,20.0,"[9, 664, 79, 78, 59, 73, 69, 277, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59]","[1697105123855, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365]"
3415,880,50,[],200,EleutherAI/gpt-neox-20b,64,1,1150.0,1.0,1,H100,1697105130052,1697105131202,120,84.0,2.0,"[5, 1145]","[1697105130057, 1697105131202]"
3416,475,54,[],200,EleutherAI/gpt-neox-20b,64,1,2563.0,1.0,1,H100,1697105109445,1697105112008,120,89.0,20.0,"[17, 609, 72, 120, 74, 49, 60, 459, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50]","[1697105109462, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008]"
3417,510,51,[],200,EleutherAI/gpt-neox-20b,64,1,830.0,1.0,1,H100,1697105131203,1697105132033,120,79.0,2.0,"[5, 825]","[1697105131208, 1697105132033]"
3418,282,50,[],200,EleutherAI/gpt-neox-20b,64,1,3133.0,1.0,1,H100,1697105113922,1697105117055,120,87.0,20.0,"[9, 881, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56]","[1697105113931, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055]"
3419,14,74,[],200,EleutherAI/gpt-neox-20b,64,1,2296.0,1.0,1,H100,1697105126366,1697105128662,120,90.0,20.0,"[4, 397, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 67, 52]","[1697105126370, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128610, 1697105128662]"
3420,335,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105097314,1697105102061,120,,,"[17, 779, 74, 68, 52, 52, 241, 55, 57, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105097331, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098597, 1697105098652, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3421,604,75,[],200,EleutherAI/gpt-neox-20b,64,1,1217.0,1.0,1,H100,1697105128663,1697105129880,120,161.0,4.0,"[8, 1096, 64, 49]","[1697105128671, 1697105129767, 1697105129831, 1697105129880]"
3422,373,76,[],200,EleutherAI/gpt-neox-20b,64,1,528.0,1.0,1,H100,1697105129881,1697105130409,120,15.0,1.0,"[5, 523]","[1697105129886, 1697105130409]"
3423,120,77,[],200,EleutherAI/gpt-neox-20b,64,1,634.0,1.0,1,H100,1697105130412,1697105131046,120,17.0,1.0,"[20, 614]","[1697105130432, 1697105131046]"
3424,51,51,[],200,EleutherAI/gpt-neox-20b,64,1,5512.0,1.0,1,H100,1697105117056,1697105122568,120,364.0,36.0,"[12, 1295, 74, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80]","[1697105117068, 1697105118363, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568]"
3425,642,56,[],200,EleutherAI/gpt-neox-20b,64,1,2133.0,1.0,1,H100,1697105123416,1697105125549,120,89.0,20.0,"[5, 424, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 73]","[1697105123421, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125549]"
3426,698,78,[],200,EleutherAI/gpt-neox-20b,64,1,1228.0,1.0,1,H100,1697105131047,1697105132275,120,182.0,6.0,"[9, 977, 70, 68, 52, 52]","[1697105131056, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275]"
3427,302,57,[],200,EleutherAI/gpt-neox-20b,64,1,2424.0,1.0,1,H100,1697105125549,1697105127973,120,85.0,20.0,"[13, 527, 80, 61, 76, 59, 70, 53, 278, 80, 77, 75, 67, 65, 476, 61, 78, 79, 77, 72]","[1697105125562, 1697105126089, 1697105126169, 1697105126230, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126766, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973]"
3428,476,79,[],200,EleutherAI/gpt-neox-20b,64,1,2505.0,1.0,1,H100,1697105132276,1697105134781,120,6.0,50.0,"[13, 403, 73, 57, 71, 72, 70, 65, 63, 50, 49, 49, 88, 65, 57, 43, 56, 53, 41, 53, 43, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 32, 31, 32, 31, 38, 31, 32, 31, 31, 31]","[1697105132289, 1697105132692, 1697105132765, 1697105132822, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133767, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134493, 1697105134524, 1697105134556, 1697105134587, 1697105134625, 1697105134656, 1697105134688, 1697105134719, 1697105134750, 1697105134781]"
3429,227,52,[],200,EleutherAI/gpt-neox-20b,64,1,3283.0,1.0,1,H100,1697105128326,1697105131609,120,364.0,25.0,"[5, 725, 72, 126, 57, 69, 63, 324, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54]","[1697105128331, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129443, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609]"
3430,391,61,[],200,EleutherAI/gpt-neox-20b,64,1,2988.0,1.0,1,H100,1697105116131,1697105119119,120,79.0,20.0,"[9, 629, 80, 76, 74, 56, 66, 625, 81, 59, 78, 71, 330, 72, 72, 64, 330, 73, 72, 71]","[1697105116140, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117121, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118365, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119]"
3431,852,54,[],200,EleutherAI/gpt-neox-20b,64,1,2264.0,1.0,1,H100,1697105123285,1697105125549,120,100.0,20.0,"[4, 556, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 73]","[1697105123289, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125549]"
3432,630,55,[],200,EleutherAI/gpt-neox-20b,64,1,459.0,1.0,1,H100,1697105125550,1697105126009,120,6.0,1.0,"[20, 439]","[1697105125570, 1697105126009]"
3433,288,56,[],200,EleutherAI/gpt-neox-20b,64,1,2652.0,1.0,1,H100,1697105126010,1697105128662,120,93.0,20.0,"[9, 748, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 66, 53]","[1697105126019, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662]"
3434,121,51,[],200,EleutherAI/gpt-neox-20b,64,1,335.0,1.0,1,H100,1697105124098,1697105124433,120,13.0,1.0,"[13, 322]","[1697105124111, 1697105124433]"
3435,92,56,[],200,EleutherAI/gpt-neox-20b,64,1,2997.0,1.0,1,H100,1697105114058,1697105117055,120,85.0,20.0,"[4, 750, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56]","[1697105114062, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055]"
3436,925,53,[],200,EleutherAI/gpt-neox-20b,64,1,2157.0,1.0,1,H100,1697105131610,1697105133767,120,87.0,20.0,"[5, 1077, 73, 56, 72, 71, 71, 65, 63, 50, 49, 49, 88, 65, 57, 43, 56, 53, 41, 53]","[1697105131615, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133767]"
3437,827,52,[],200,EleutherAI/gpt-neox-20b,64,1,2564.0,1.0,1,H100,1697105124434,1697105126998,120,96.0,20.0,"[8, 712, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 70, 53, 279, 79, 77, 75]","[1697105124442, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998]"
3438,480,53,[],200,EleutherAI/gpt-neox-20b,64,1,527.0,1.0,1,H100,1697105126999,1697105127526,120,26.0,1.0,"[21, 506]","[1697105127020, 1697105127526]"
3439,256,54,[],200,EleutherAI/gpt-neox-20b,64,1,5937.0,1.0,1,H100,1697105127527,1697105133464,120,47.0,50.0,"[9, 789, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70, 65, 63, 49, 50, 49, 88, 65]","[1697105127536, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133464]"
3440,614,42,[],200,EleutherAI/gpt-neox-20b,64,1,489.0,1.0,1,H100,1697105096824,1697105097313,120,15.0,1.0,"[13, 476]","[1697105096837, 1697105097313]"
3441,391,43,[],200,EleutherAI/gpt-neox-20b,64,1,2699.0,1.0,1,H100,1697105097314,1697105100013,120,79.0,20.0,"[5, 791, 74, 68, 52, 52, 241, 56, 56, 68, 62, 394, 72, 126, 66, 55, 285, 63, 50, 63]","[1697105097319, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098597, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013]"
3442,235,56,[],200,EleutherAI/gpt-neox-20b,64,1,2120.0,1.0,1,H100,1697105124878,1697105126998,120,161.0,12.0,"[8, 1203, 80, 60, 77, 59, 69, 54, 279, 79, 76, 76]","[1697105124886, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767, 1697105126846, 1697105126922, 1697105126998]"
3443,819,50,[],200,EleutherAI/gpt-neox-20b,64,1,565.0,1.0,1,H100,1697105116926,1697105117491,120,13.0,1.0,"[5, 560]","[1697105116931, 1697105117491]"
3444,599,51,[],200,EleutherAI/gpt-neox-20b,64,1,7316.0,1.0,1,H100,1697105117492,1697105124808,120,58.0,55.0,"[16, 858, 71, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73]","[1697105117508, 1697105118366, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808]"
3445,479,63,[],200,EleutherAI/gpt-neox-20b,64,1,3215.0,1.0,1,H100,1697105130664,1697105133879,120,140.0,36.0,"[9, 529, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 57, 71, 72, 71, 64, 63, 50, 49, 49, 88, 64, 58, 43, 56, 53, 41, 52, 44, 34, 35]","[1697105130673, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132822, 1697105132893, 1697105132965, 1697105133036, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879]"
3446,701,53,[],200,EleutherAI/gpt-neox-20b,64,1,6175.0,1.0,1,H100,1697105120592,1697105126767,120,58.0,43.0,"[16, 1320, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 279]","[1697105120608, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767]"
3447,148,45,[],200,EleutherAI/gpt-neox-20b,64,1,660.0,1.0,1,H100,1697105092507,1697105093167,120,16.0,1.0,"[13, 647]","[1697105092520, 1697105093167]"
3448,731,46,[],200,EleutherAI/gpt-neox-20b,64,1,3026.0,1.0,1,H100,1697105093168,1697105096194,120,89.0,20.0,"[13, 991, 74, 57, 72, 71, 55, 54, 247, 53, 67, 65, 50, 65, 337, 65, 50, 64, 64, 512]","[1697105093181, 1697105094172, 1697105094246, 1697105094303, 1697105094375, 1697105094446, 1697105094501, 1697105094555, 1697105094802, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095618, 1697105095682, 1697105096194]"
3449,205,42,[],200,EleutherAI/gpt-neox-20b,64,1,3527.0,1.0,1,H100,1697105121350,1697105124877,120,87.0,20.0,"[12, 1689, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 69]","[1697105121362, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877]"
3450,537,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100071,1697105102062,120,,,"[4, 509, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105100075, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3451,162,62,[],200,EleutherAI/gpt-neox-20b,64,1,2229.0,1.0,1,H100,1697105119120,1697105121349,120,90.0,20.0,"[8, 404, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55]","[1697105119128, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348]"
3452,539,44,[],200,EleutherAI/gpt-neox-20b,64,1,3894.0,1.0,1,H100,1697105115225,1697105119119,120,83.0,20.0,"[9, 1535, 80, 76, 74, 56, 65, 626, 81, 59, 78, 70, 332, 71, 72, 64, 330, 73, 72, 71]","[1697105115234, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118366, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119]"
3453,682,57,[],200,EleutherAI/gpt-neox-20b,64,1,6789.0,1.0,1,H100,1697105117056,1697105123845,120,244.0,50.0,"[12, 678, 81, 59, 78, 71, 327, 75, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359]","[1697105117068, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118362, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845]"
3454,824,57,[],200,EleutherAI/gpt-neox-20b,64,1,746.0,1.0,1,H100,1697105126999,1697105127745,120,58.0,4.0,"[9, 598, 60, 79]","[1697105127008, 1697105127606, 1697105127666, 1697105127745]"
3455,194,45,[],200,EleutherAI/gpt-neox-20b,64,1,1989.0,1.0,1,H100,1697105119120,1697105121109,120,335.0,16.0,"[9, 403, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519]","[1697105119129, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109]"
3456,594,58,[],200,EleutherAI/gpt-neox-20b,64,1,8068.0,1.0,1,H100,1697105127747,1697105135815,120,216.0,119.0,"[4, 574, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70, 65, 63, 49, 50, 49, 88, 64, 58, 43, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 31, 32, 32, 31, 38, 31, 32, 31, 31, 31, 38, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 32, 31, 31, 31, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31]","[1697105127751, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134492, 1697105134524, 1697105134556, 1697105134587, 1697105134625, 1697105134656, 1697105134688, 1697105134719, 1697105134750, 1697105134781, 1697105134819, 1697105134850, 1697105134881, 1697105134912, 1697105134943, 1697105134974, 1697105135005, 1697105135036, 1697105135067, 1697105135098, 1697105135128, 1697105135159, 1697105135190, 1697105135221, 1697105135252, 1697105135283, 1697105135315, 1697105135346, 1697105135377, 1697105135408, 1697105135439, 1697105135470, 1697105135502, 1697105135533, 1697105135564, 1697105135596, 1697105135627, 1697105135658, 1697105135690, 1697105135721, 1697105135752, 1697105135784, 1697105135815]"
3457,751,63,[],200,EleutherAI/gpt-neox-20b,64,1,7052.0,1.0,1,H100,1697105121350,1697105128402,120,216.0,50.0,"[16, 1685, 79, 77, 77, 75, 56, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 69, 277, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 279, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77]","[1697105121366, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402]"
3458,775,54,[],200,EleutherAI/gpt-neox-20b,64,1,545.0,1.0,1,H100,1697105105519,1697105106064,120,17.0,1.0,"[24, 521]","[1697105105543, 1697105106064]"
3459,786,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099432,1697105102062,120,,,"[4, 401, 63, 51, 62, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105099436, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3460,521,55,[],200,EleutherAI/gpt-neox-20b,64,1,508.0,1.0,1,H100,1697105106066,1697105106574,120,18.0,1.0,"[24, 484]","[1697105106090, 1697105106574]"
3461,563,45,[],200,EleutherAI/gpt-neox-20b,64,1,3316.0,1.0,1,H100,1697105102065,1697105105381,120,874.0,18.0,"[14, 728, 708, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59]","[1697105102079, 1697105102807, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381]"
3462,57,57,[],200,EleutherAI/gpt-neox-20b,64,1,1030.0,1.0,1,H100,1697105128663,1697105129693,120,13.0,1.0,"[12, 1018]","[1697105128675, 1697105129693]"
3463,618,58,[],200,EleutherAI/gpt-neox-20b,64,1,715.0,1.0,1,H100,1697105129694,1697105130409,120,9.0,1.0,"[5, 710]","[1697105129699, 1697105130409]"
3464,280,52,[],200,EleutherAI/gpt-neox-20b,64,1,1733.0,1.0,1,H100,1697105132034,1697105133767,120,91.0,20.0,"[17, 641, 73, 56, 72, 72, 70, 65, 63, 50, 49, 49, 88, 64, 57, 44, 56, 53, 42, 52]","[1697105132051, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133520, 1697105133564, 1697105133620, 1697105133673, 1697105133715, 1697105133767]"
3465,313,45,[],200,EleutherAI/gpt-neox-20b,64,1,1200.0,1.0,1,H100,1697105102068,1697105103268,120,20.0,1.0,"[107, 1093]","[1697105102175, 1697105103268]"
3466,896,46,[],200,EleutherAI/gpt-neox-20b,64,1,674.0,1.0,1,H100,1697105103269,1697105103943,120,15.0,1.0,"[20, 654]","[1697105103289, 1697105103943]"
3467,386,59,[],200,EleutherAI/gpt-neox-20b,64,1,1072.0,1.0,1,H100,1697105130412,1697105131484,120,140.0,6.0,"[10, 780, 80, 75, 72, 55]","[1697105130422, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484]"
3468,410,66,[],200,EleutherAI/gpt-neox-20b,64,1,1535.0,1.0,1,H100,1697105126366,1697105127901,120,364.0,12.0,"[16, 385, 79, 77, 75, 67, 66, 475, 60, 79, 79, 77]","[1697105126382, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127131, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901]"
3469,216,46,[],200,EleutherAI/gpt-neox-20b,64,1,2537.0,1.0,1,H100,1697105105382,1697105107919,120,91.0,20.0,"[4, 757, 111, 58, 44, 350, 58, 57, 55, 44, 287, 61, 57, 56, 54, 53, 260, 61, 61, 49]","[1697105105386, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919]"
3470,185,67,[],200,EleutherAI/gpt-neox-20b,64,1,2593.0,1.0,1,H100,1697105127902,1697105130495,120,93.0,20.0,"[4, 419, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444]","[1697105127906, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495]"
3471,642,47,[],200,EleutherAI/gpt-neox-20b,64,1,3382.0,1.0,1,H100,1697105103944,1697105107326,120,89.0,20.0,"[8, 1214, 81, 76, 58, 71, 66, 53, 572, 111, 58, 44, 350, 58, 57, 55, 44, 286, 61, 58]","[1697105103952, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325]"
3472,47,60,[],200,EleutherAI/gpt-neox-20b,64,1,1979.0,1.0,1,H100,1697105131485,1697105133464,120,90.0,20.0,"[5, 543, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71, 65, 63, 50, 49, 49, 88, 65]","[1697105131490, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464]"
3473,606,32,[],200,EleutherAI/gpt-neox-20b,64,1,545.0,1.0,1,H100,1697105105519,1697105106064,120,9.0,1.0,"[28, 517]","[1697105105547, 1697105106064]"
3474,871,46,[],200,EleutherAI/gpt-neox-20b,64,1,1378.0,1.0,1,H100,1697105121110,1697105122488,120,123.0,6.0,"[5, 813, 82, 152, 171, 155]","[1697105121115, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488]"
3475,640,47,[],200,EleutherAI/gpt-neox-20b,64,1,465.0,1.0,1,H100,1697105122489,1697105122954,120,15.0,1.0,"[5, 460]","[1697105122494, 1697105122954]"
3476,300,48,[],200,EleutherAI/gpt-neox-20b,64,1,716.0,1.0,1,H100,1697105122955,1697105123671,120,9.0,1.0,"[8, 708]","[1697105122963, 1697105123671]"
3477,69,49,[],200,EleutherAI/gpt-neox-20b,64,1,2693.0,1.0,1,H100,1697105123672,1697105126365,120,85.0,20.0,"[4, 843, 79, 78, 59, 73, 69, 277, 59, 58, 74, 57, 74, 73, 64, 476, 80, 60, 77, 59]","[1697105123676, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125549, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365]"
3478,910,43,[],200,EleutherAI/gpt-neox-20b,64,1,1131.0,1.0,1,H100,1697105124878,1697105126009,120,8.0,1.0,"[4, 1127]","[1697105124882, 1697105126009]"
3479,566,44,[],200,EleutherAI/gpt-neox-20b,64,1,4653.0,1.0,1,H100,1697105126010,1697105130663,120,109.0,36.0,"[13, 744, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 67, 52, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51]","[1697105126023, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128610, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663]"
3480,82,44,[],200,EleutherAI/gpt-neox-20b,64,1,3453.0,1.0,1,H100,1697105102065,1697105105518,120,67.0,20.0,"[34, 708, 708, 66, 51, 50, 64, 407, 73, 71, 69, 64, 62, 61, 613, 80, 76, 59, 71, 66]","[1697105102099, 1697105102807, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518]"
3481,451,58,[],200,EleutherAI/gpt-neox-20b,64,1,587.0,1.0,1,H100,1697105123846,1697105124433,120,286.0,1.0,"[5, 582]","[1697105123851, 1697105124433]"
3482,197,59,[],200,EleutherAI/gpt-neox-20b,64,1,1114.0,1.0,1,H100,1697105124434,1697105125548,120,6.0,8.0,"[4, 716, 59, 58, 74, 57, 74, 72]","[1697105124438, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548]"
3483,74,58,[],200,EleutherAI/gpt-neox-20b,64,1,2521.0,1.0,1,H100,1697105127974,1697105130495,120,88.0,20.0,"[9, 342, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444]","[1697105127983, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495]"
3484,779,60,[],200,EleutherAI/gpt-neox-20b,64,1,1296.0,1.0,1,H100,1697105125550,1697105126846,120,563.0,10.0,"[16, 523, 80, 60, 77, 59, 70, 53, 279, 79]","[1697105125566, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846]"
3485,557,61,[],200,EleutherAI/gpt-neox-20b,64,1,679.0,1.0,1,H100,1697105126847,1697105127526,120,31.0,1.0,"[4, 675]","[1697105126851, 1697105127526]"
3486,215,62,[],200,EleutherAI/gpt-neox-20b,64,1,720.0,1.0,1,H100,1697105127527,1697105128247,120,12.0,1.0,"[12, 708]","[1697105127539, 1697105128247]"
3487,914,63,[],200,EleutherAI/gpt-neox-20b,64,1,3034.0,1.0,1,H100,1697105128248,1697105131282,120,84.0,20.0,"[8, 800, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80]","[1697105128256, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282]"
3488,481,54,[],200,EleutherAI/gpt-neox-20b,64,1,758.0,1.0,1,H100,1697105126768,1697105127526,120,10.0,1.0,"[4, 754]","[1697105126772, 1697105127526]"
3489,133,55,[],200,EleutherAI/gpt-neox-20b,64,1,720.0,1.0,1,H100,1697105127527,1697105128247,120,15.0,1.0,"[8, 712]","[1697105127535, 1697105128247]"
3490,786,62,[],200,EleutherAI/gpt-neox-20b,64,1,2652.0,1.0,1,H100,1697105129381,1697105132033,120,87.0,20.0,"[4, 382, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424]","[1697105129385, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033]"
3491,574,64,[],200,EleutherAI/gpt-neox-20b,64,1,750.0,1.0,1,H100,1697105131283,1697105132033,120,364.0,2.0,"[4, 746]","[1697105131287, 1697105132033]"
3492,317,63,[],200,EleutherAI/gpt-neox-20b,64,1,7052.0,1.0,1,H100,1697105121350,1697105128402,120,244.0,50.0,"[16, 1685, 79, 77, 77, 75, 56, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 69, 277, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 279, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77]","[1697105121366, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402]"
3493,342,65,[],200,EleutherAI/gpt-neox-20b,64,1,1430.0,1.0,1,H100,1697105132034,1697105133464,120,364.0,14.0,"[13, 645, 73, 56, 72, 72, 70, 65, 63, 50, 49, 49, 88, 65]","[1697105132047, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464]"
3494,838,56,[],200,EleutherAI/gpt-neox-20b,64,1,3034.0,1.0,1,H100,1697105128248,1697105131282,120,90.0,20.0,"[8, 800, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80]","[1697105128256, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282]"
3495,656,59,[],200,EleutherAI/gpt-neox-20b,64,1,550.0,1.0,1,H100,1697105130496,1697105131046,120,26.0,1.0,"[12, 538]","[1697105130508, 1697105131046]"
3496,921,47,[],200,EleutherAI/gpt-neox-20b,64,1,453.0,1.0,1,H100,1697105107921,1697105108374,120,31.0,1.0,"[16, 437]","[1697105107937, 1697105108374]"
3497,570,48,[],200,EleutherAI/gpt-neox-20b,64,1,716.0,1.0,1,H100,1697105108375,1697105109091,120,18.0,1.0,"[13, 703]","[1697105108388, 1697105109091]"
3498,318,49,[],200,EleutherAI/gpt-neox-20b,64,1,1294.0,1.0,1,H100,1697105109092,1697105110386,120,6.0,6.0,"[4, 975, 71, 121, 70, 53]","[1697105109096, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386]"
3499,903,50,[],200,EleutherAI/gpt-neox-20b,64,1,1052.0,1.0,1,H100,1697105110387,1697105111439,120,244.0,7.0,"[9, 508, 215, 128, 73, 55, 64]","[1697105110396, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439]"
3500,854,63,[],200,EleutherAI/gpt-neox-20b,64,1,3232.0,1.0,1,H100,1697105124809,1697105128041,120,67.0,29.0,"[4, 341, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 279, 79, 77, 75, 67, 65, 476, 61, 78, 79, 77, 72, 68]","[1697105124813, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041]"
3501,556,63,[],200,EleutherAI/gpt-neox-20b,64,1,580.0,1.0,1,H100,1697105132035,1697105132615,120,9.0,1.0,"[20, 560]","[1697105132055, 1697105132615]"
3502,673,51,[],200,EleutherAI/gpt-neox-20b,64,1,2246.0,1.0,1,H100,1697105111440,1697105113686,120,93.0,20.0,"[5, 349, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62]","[1697105111445, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686]"
3503,437,60,[],200,EleutherAI/gpt-neox-20b,64,1,2832.0,1.0,1,H100,1697105131047,1697105133879,120,91.0,29.0,"[9, 977, 70, 68, 52, 52, 59, 358, 73, 57, 71, 72, 71, 64, 63, 50, 49, 49, 88, 64, 58, 43, 56, 53, 41, 52, 44, 34, 35]","[1697105131056, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132822, 1697105132893, 1697105132965, 1697105133036, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879]"
3504,900,64,[],200,EleutherAI/gpt-neox-20b,64,1,978.0,1.0,1,H100,1697105128402,1697105129380,120,67.0,6.0,"[5, 649, 72, 126, 57, 69]","[1697105128407, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380]"
3505,676,65,[],200,EleutherAI/gpt-neox-20b,64,1,312.0,1.0,1,H100,1697105129381,1697105129693,120,19.0,1.0,"[8, 304]","[1697105129389, 1697105129693]"
3506,332,66,[],200,EleutherAI/gpt-neox-20b,64,1,715.0,1.0,1,H100,1697105129694,1697105130409,120,39.0,1.0,"[9, 706]","[1697105129703, 1697105130409]"
3507,246,53,[],200,EleutherAI/gpt-neox-20b,64,1,5596.0,1.0,1,H100,1697105123846,1697105129442,120,58.0,47.0,"[13, 660, 79, 78, 59, 73, 69, 277, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 279, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 66, 52, 395, 72, 126, 57, 69, 62]","[1697105123859, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442]"
3508,768,68,[],200,EleutherAI/gpt-neox-20b,64,1,988.0,1.0,1,H100,1697105130496,1697105131484,120,47.0,6.0,"[4, 702, 80, 75, 72, 55]","[1697105130500, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484]"
3509,898,60,[],200,EleutherAI/gpt-neox-20b,64,1,540.0,1.0,1,H100,1697105125549,1697105126089,120,79.0,2.0,"[5, 535]","[1697105125554, 1697105126089]"
3510,554,61,[],200,EleutherAI/gpt-neox-20b,64,1,595.0,1.0,1,H100,1697105126090,1697105126685,120,26.0,1.0,"[5, 589]","[1697105126095, 1697105126684]"
3511,544,69,[],200,EleutherAI/gpt-neox-20b,64,1,378.0,1.0,1,H100,1697105131485,1697105131863,120,26.0,1.0,"[5, 372]","[1697105131490, 1697105131862]"
3512,197,70,[],200,EleutherAI/gpt-neox-20b,64,1,1237.0,1.0,1,H100,1697105131863,1697105133100,120,6.0,8.0,"[5, 824, 73, 56, 72, 71, 71, 65]","[1697105131868, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100]"
3513,327,62,[],200,EleutherAI/gpt-neox-20b,64,1,1717.0,1.0,1,H100,1697105126685,1697105128402,120,563.0,10.0,"[9, 912, 60, 79, 79, 77, 72, 68, 284, 77]","[1697105126694, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402]"
3514,180,58,[],200,EleutherAI/gpt-neox-20b,64,1,1521.0,1.0,1,H100,1697105125477,1697105126998,120,123.0,12.0,"[5, 607, 80, 60, 77, 59, 69, 54, 279, 79, 77, 75]","[1697105125482, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998]"
3515,928,66,[],200,EleutherAI/gpt-neox-20b,64,1,320.0,1.0,1,H100,1697105121350,1697105121670,120,20.0,1.0,"[20, 300]","[1697105121370, 1697105121670]"
3516,101,67,[],200,EleutherAI/gpt-neox-20b,64,1,634.0,1.0,1,H100,1697105130412,1697105131046,120,13.0,1.0,"[12, 622]","[1697105130424, 1697105131046]"
3517,662,68,[],200,EleutherAI/gpt-neox-20b,64,1,2417.0,1.0,1,H100,1697105131047,1697105133464,120,83.0,20.0,"[5, 981, 70, 68, 52, 52, 59, 358, 73, 57, 71, 72, 71, 64, 63, 50, 49, 49, 88, 65]","[1697105131052, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132822, 1697105132893, 1697105132965, 1697105133036, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464]"
3518,705,67,[],200,EleutherAI/gpt-neox-20b,64,1,3877.0,1.0,1,H100,1697105121671,1697105125548,120,79.0,27.0,"[9, 1371, 79, 77, 77, 75, 56, 71, 359, 78, 58, 58, 58, 69, 353, 79, 78, 59, 73, 69, 277, 59, 58, 74, 57, 74, 72]","[1697105121680, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548]"
3519,446,41,[],200,EleutherAI/gpt-neox-20b,64,1,473.0,1.0,1,H100,1697105118977,1697105119450,120,26.0,1.0,"[5, 468]","[1697105118982, 1697105119450]"
3520,99,42,[],200,EleutherAI/gpt-neox-20b,64,1,701.0,1.0,1,H100,1697105119452,1697105120153,120,10.0,1.0,"[16, 685]","[1697105119468, 1697105120153]"
3521,776,43,[],200,EleutherAI/gpt-neox-20b,64,1,956.0,1.0,1,H100,1697105120153,1697105121109,120,67.0,2.0,"[5, 951]","[1697105120158, 1697105121109]"
3522,271,51,[],200,EleutherAI/gpt-neox-20b,64,1,2735.0,1.0,1,H100,1697105106709,1697105109444,120,87.0,20.0,"[9, 489, 61, 57, 56, 54, 53, 260, 62, 60, 49, 56, 45, 423, 58, 49, 48, 717, 65, 64]","[1697105106718, 1697105107207, 1697105107268, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107810, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108501, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
3523,428,44,[],200,EleutherAI/gpt-neox-20b,64,1,1941.0,1.0,1,H100,1697105121110,1697105123051,120,31.0,9.0,"[5, 555, 258, 82, 152, 171, 155, 80, 77, 406]","[1697105121115, 1697105121670, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051]"
3524,248,50,[],200,EleutherAI/gpt-neox-20b,64,1,2117.0,1.0,1,H100,1697105127825,1697105129942,120,182.0,17.0,"[4, 496, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62]","[1697105127829, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942]"
3525,412,48,[],200,EleutherAI/gpt-neox-20b,64,1,1174.0,1.0,1,H100,1697105107326,1697105108500,120,244.0,9.0,"[5, 417, 61, 61, 49, 56, 45, 423, 57]","[1697105107331, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500]"
3526,314,45,[],200,EleutherAI/gpt-neox-20b,64,1,1611.0,1.0,1,H100,1697105130664,1697105132275,120,335.0,13.0,"[5, 533, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52]","[1697105130669, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275]"
3527,333,52,[],200,EleutherAI/gpt-neox-20b,64,1,2262.0,1.0,1,H100,1697105113687,1697105115949,120,563.0,11.0,"[4, 1121, 140, 70, 68, 68, 66, 538, 75, 56, 56]","[1697105113691, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949]"
3528,521,64,[],200,EleutherAI/gpt-neox-20b,64,1,567.0,1.0,1,H100,1697105128403,1697105128970,120,18.0,1.0,"[8, 559]","[1697105128411, 1697105128970]"
3529,291,56,[],200,EleutherAI/gpt-neox-20b,64,1,2869.0,1.0,1,H100,1697105106575,1697105109444,120,79.0,20.0,"[5, 627, 60, 58, 56, 54, 53, 260, 61, 61, 50, 55, 45, 423, 57, 50, 48, 717, 65, 64]","[1697105106580, 1697105107207, 1697105107267, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107920, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
3530,176,65,[],200,EleutherAI/gpt-neox-20b,64,1,797.0,1.0,1,H100,1697105128970,1697105129767,120,216.0,2.0,"[5, 792]","[1697105128975, 1697105129767]"
3531,86,46,[],200,EleutherAI/gpt-neox-20b,64,1,1344.0,1.0,1,H100,1697105132276,1697105133620,120,335.0,17.0,"[17, 399, 73, 57, 71, 72, 70, 65, 63, 50, 49, 49, 88, 65, 57, 43, 56]","[1697105132293, 1697105132692, 1697105132765, 1697105132822, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620]"
3532,917,63,[],200,EleutherAI/gpt-neox-20b,64,1,653.0,1.0,1,H100,1697105128403,1697105129056,120,123.0,2.0,"[8, 645]","[1697105128411, 1697105129056]"
3533,72,49,[],200,EleutherAI/gpt-neox-20b,64,1,3293.0,1.0,1,H100,1697105108501,1697105111794,120,84.0,20.0,"[9, 805, 66, 63, 54, 573, 72, 120, 74, 49, 60, 458, 215, 128, 73, 55, 64, 62, 49, 244]","[1697105108510, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794]"
3534,883,66,[],200,EleutherAI/gpt-neox-20b,64,1,641.0,1.0,1,H100,1697105129768,1697105130409,120,563.0,1.0,"[6, 635]","[1697105129774, 1697105130409]"
3535,685,64,[],200,EleutherAI/gpt-neox-20b,64,1,710.0,1.0,1,H100,1697105129057,1697105129767,120,364.0,2.0,"[5, 705]","[1697105129062, 1697105129767]"
3536,535,67,[],200,EleutherAI/gpt-neox-20b,64,1,2624.0,1.0,1,H100,1697105130412,1697105133036,120,84.0,20.0,"[6, 784, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70]","[1697105130418, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035]"
3537,791,28,[],200,EleutherAI/gpt-neox-20b,64,1,9016.0,1.0,1,H100,1697105076077,1697105085093,120,182.0,64.0,"[17, 737, 63, 56, 55, 55, 534, 63, 49, 61, 59, 57, 45, 269, 56, 55, 62, 60, 59, 57, 917, 72, 68, 67, 51, 59, 390, 70, 71, 69, 66, 51, 636, 141, 75, 69, 69, 67, 424, 72, 71, 68, 65, 434, 78, 72, 71, 54, 53, 69, 414, 71, 68, 52, 66, 65, 592, 77, 56, 74, 73, 72, 272, 56]","[1697105076094, 1697105076831, 1697105076894, 1697105076950, 1697105077005, 1697105077060, 1697105077594, 1697105077657, 1697105077706, 1697105077767, 1697105077826, 1697105077883, 1697105077928, 1697105078197, 1697105078253, 1697105078308, 1697105078370, 1697105078430, 1697105078489, 1697105078546, 1697105079463, 1697105079535, 1697105079603, 1697105079670, 1697105079721, 1697105079780, 1697105080170, 1697105080240, 1697105080311, 1697105080380, 1697105080446, 1697105080497, 1697105081133, 1697105081274, 1697105081349, 1697105081418, 1697105081487, 1697105081554, 1697105081978, 1697105082050, 1697105082121, 1697105082189, 1697105082254, 1697105082688, 1697105082766, 1697105082838, 1697105082909, 1697105082963, 1697105083016, 1697105083085, 1697105083499, 1697105083570, 1697105083638, 1697105083690, 1697105083756, 1697105083821, 1697105084413, 1697105084490, 1697105084546, 1697105084620, 1697105084693, 1697105084765, 1697105085037, 1697105085093]"
3538,79,42,[],200,EleutherAI/gpt-neox-20b,64,1,365.0,1.0,1,H100,1697105098781,1697105099146,120,12.0,1.0,"[8, 357]","[1697105098789, 1697105099146]"
3539,638,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099147,1697105102062,120,,,"[9, 681, 63, 50, 63, 57, 54, 460, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105099156, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3540,395,54,[],200,EleutherAI/gpt-neox-20b,64,1,2374.0,1.0,1,H100,1697105110387,1697105112761,120,88.0,20.0,"[9, 508, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 72, 57, 72, 71]","[1697105110396, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112561, 1697105112618, 1697105112690, 1697105112761]"
3541,102,53,[],200,EleutherAI/gpt-neox-20b,64,1,3169.0,1.0,1,H100,1697105115950,1697105119119,120,84.0,20.0,"[4, 815, 80, 76, 74, 56, 66, 625, 81, 59, 78, 71, 331, 71, 72, 64, 330, 73, 72, 71]","[1697105115954, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117121, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118366, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119]"
3542,854,52,[],200,EleutherAI/gpt-neox-20b,64,1,3904.0,1.0,1,H100,1697105109445,1697105113349,120,67.0,29.0,"[8, 618, 71, 121, 70, 53, 60, 459, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76]","[1697105109453, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349]"
3543,346,65,[],200,EleutherAI/gpt-neox-20b,64,1,2924.0,1.0,1,H100,1697105129768,1697105132692,120,85.0,20.0,"[5, 722, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358]","[1697105129773, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692]"
3544,265,33,[],200,EleutherAI/gpt-neox-20b,64,1,2436.0,1.0,1,H100,1697105106065,1697105108501,120,86.0,20.0,"[13, 628, 58, 57, 55, 44, 286, 61, 58, 56, 54, 53, 260, 61, 61, 50, 55, 45, 423, 57]","[1697105106078, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107920, 1697105107975, 1697105108020, 1697105108443, 1697105108500]"
3545,204,45,[],200,EleutherAI/gpt-neox-20b,64,1,1045.0,1.0,1,H100,1697105123052,1697105124097,120,67.0,6.0,"[5, 788, 79, 57, 58, 58]","[1697105123057, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097]"
3546,787,46,[],200,EleutherAI/gpt-neox-20b,64,1,710.0,1.0,1,H100,1697105124098,1697105124808,120,123.0,6.0,"[4, 417, 79, 78, 59, 73]","[1697105124102, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808]"
3547,562,47,[],200,EleutherAI/gpt-neox-20b,64,1,4502.0,1.0,1,H100,1697105124809,1697105129311,120,67.0,39.0,"[9, 336, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 279, 79, 77, 75, 67, 65, 476, 61, 78, 79, 77, 72, 68, 284, 77, 71, 70, 66, 53, 394, 72, 126, 57]","[1697105124818, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311]"
3548,408,44,[],200,EleutherAI/gpt-neox-20b,64,1,690.0,1.0,1,H100,1697105102065,1697105102755,120,16.0,1.0,"[38, 652]","[1697105102103, 1697105102755]"
3549,24,51,[],200,EleutherAI/gpt-neox-20b,64,1,1414.0,1.0,1,H100,1697105129943,1697105131357,120,79.0,9.0,"[5, 547, 66, 51, 51, 64, 475, 80, 75]","[1697105129948, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357]"
3550,68,45,[],200,EleutherAI/gpt-neox-20b,64,1,512.0,1.0,1,H100,1697105102756,1697105103268,120,12.0,1.0,"[16, 496]","[1697105102772, 1697105103268]"
3551,880,57,[],200,EleutherAI/gpt-neox-20b,64,1,626.0,1.0,1,H100,1697105109445,1697105110071,120,84.0,2.0,"[4, 622]","[1697105109449, 1697105110071]"
3552,769,46,[],200,EleutherAI/gpt-neox-20b,64,1,3437.0,1.0,1,H100,1697105103269,1697105106706,120,47.0,20.0,"[16, 869, 72, 71, 69, 64, 62, 62, 612, 80, 76, 59, 71, 66, 53, 572, 111, 58, 44, 350]","[1697105103285, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104554, 1697105105166, 1697105105246, 1697105105322, 1697105105381, 1697105105452, 1697105105518, 1697105105571, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706]"
3553,47,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105100014,1697105102062,120,,,"[5, 565, 116, 56, 62, 59, 463, 67, 66, 59, 47, 59]","[1697105100019, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101579, 1697105101638]"
3554,745,45,[],200,EleutherAI/gpt-neox-20b,64,1,1189.0,1.0,1,H100,1697105102079,1697105103268,120,17.0,1.0,"[133, 1056]","[1697105102212, 1697105103268]"
3555,406,46,[],200,EleutherAI/gpt-neox-20b,64,1,1028.0,1.0,1,H100,1697105103269,1697105104297,120,244.0,4.0,"[4, 670, 210, 73, 71]","[1697105103273, 1697105103943, 1697105104153, 1697105104226, 1697105104297]"
3556,175,47,[],200,EleutherAI/gpt-neox-20b,64,1,1273.0,1.0,1,H100,1697105104298,1697105105571,120,140.0,8.0,"[5, 863, 81, 76, 58, 71, 66, 53]","[1697105104303, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571]"
3557,479,47,[],200,EleutherAI/gpt-neox-20b,64,1,4623.0,1.0,1,H100,1697105096195,1697105100818,120,140.0,36.0,"[20, 608, 70, 68, 67, 65, 297, 74, 70, 68, 67, 441, 74, 68, 52, 52, 242, 55, 56, 69, 61, 394, 72, 126, 66, 55, 285, 63, 50, 63, 57, 54, 460, 116, 56, 62]","[1697105096215, 1697105096823, 1697105096893, 1697105096961, 1697105097028, 1697105097093, 1697105097390, 1697105097464, 1697105097534, 1697105097602, 1697105097669, 1697105098110, 1697105098184, 1697105098252, 1697105098304, 1697105098356, 1697105098598, 1697105098653, 1697105098709, 1697105098778, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099950, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756, 1697105100818]"
3558,641,52,[],200,EleutherAI/gpt-neox-20b,64,1,5833.0,1.0,1,H100,1697105122569,1697105128402,120,16.0,50.0,"[5, 477, 79, 77, 77, 75, 56, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 69, 277, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 279, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77]","[1697105122574, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402]"
3559,625,53,[],200,EleutherAI/gpt-neox-20b,64,1,641.0,1.0,1,H100,1697105113349,1697105113990,120,364.0,2.0,"[5, 636]","[1697105113354, 1697105113990]"
3560,285,54,[],200,EleutherAI/gpt-neox-20b,64,1,4373.0,1.0,1,H100,1697105113993,1697105118366,120,100.0,27.0,"[5, 814, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56, 65, 626, 81, 59, 78, 71, 328]","[1697105113998, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118363]"
3561,883,59,[],200,EleutherAI/gpt-neox-20b,64,1,527.0,1.0,1,H100,1697105126999,1697105127526,120,563.0,1.0,"[4, 523]","[1697105127003, 1697105127526]"
3562,537,60,[],200,EleutherAI/gpt-neox-20b,64,1,2968.0,1.0,1,H100,1697105127527,1697105130495,120,83.0,20.0,"[4, 794, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444]","[1697105127531, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495]"
3563,652,58,[],200,EleutherAI/gpt-neox-20b,64,1,619.0,1.0,1,H100,1697105110072,1697105110691,120,14.0,1.0,"[4, 615]","[1697105110076, 1697105110691]"
3564,302,59,[],200,EleutherAI/gpt-neox-20b,64,1,2994.0,1.0,1,H100,1697105110692,1697105113686,120,85.0,20.0,"[5, 1097, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62]","[1697105110697, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686]"
3565,56,55,[],200,EleutherAI/gpt-neox-20b,64,1,2462.0,1.0,1,H100,1697105112762,1697105115224,120,86.0,20.0,"[17, 495, 75, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 60, 520, 140, 70, 68, 68, 66]","[1697105112779, 1697105113274, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224]"
3566,252,55,[],200,EleutherAI/gpt-neox-20b,64,1,2943.0,1.0,1,H100,1697105112009,1697105114952,120,182.0,22.0,"[9, 471, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 47, 61, 520, 140]","[1697105112018, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114231, 1697105114292, 1697105114812, 1697105114952]"
3567,314,61,[],200,EleutherAI/gpt-neox-20b,64,1,1779.0,1.0,1,H100,1697105130496,1697105132275,120,335.0,13.0,"[4, 702, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52]","[1697105130500, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275]"
3568,492,57,[],200,EleutherAI/gpt-neox-20b,64,1,2181.0,1.0,1,H100,1697105131283,1697105133464,120,47.0,20.0,"[4, 746, 70, 68, 52, 52, 59, 358, 73, 57, 71, 72, 71, 64, 63, 50, 49, 49, 88, 65]","[1697105131287, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132822, 1697105132893, 1697105132965, 1697105133036, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464]"
3569,851,48,[],200,EleutherAI/gpt-neox-20b,64,1,1002.0,1.0,1,H100,1697105105572,1697105106574,120,23.0,1.0,"[20, 982]","[1697105105592, 1697105106574]"
3570,507,49,[],200,EleutherAI/gpt-neox-20b,64,1,2869.0,1.0,1,H100,1697105106575,1697105109444,120,83.0,20.0,"[9, 623, 61, 57, 56, 54, 53, 260, 61, 61, 50, 55, 45, 423, 58, 49, 48, 717, 65, 64]","[1697105106584, 1697105107207, 1697105107268, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107920, 1697105107975, 1697105108020, 1697105108443, 1697105108501, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
3571,899,62,[],200,EleutherAI/gpt-neox-20b,64,1,1491.0,1.0,1,H100,1697105132276,1697105133767,120,100.0,20.0,"[9, 407, 73, 57, 71, 72, 70, 65, 63, 50, 49, 49, 88, 65, 57, 43, 56, 53, 41, 53]","[1697105132285, 1697105132692, 1697105132765, 1697105132822, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133767]"
3572,779,45,[],200,EleutherAI/gpt-neox-20b,64,1,1401.0,1.0,1,H100,1697105105519,1697105106920,120,563.0,10.0,"[20, 604, 111, 58, 44, 350, 58, 57, 55, 44]","[1697105105539, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920]"
3573,433,46,[],200,EleutherAI/gpt-neox-20b,64,1,827.0,1.0,1,H100,1697105106921,1697105107748,120,109.0,2.0,"[5, 822]","[1697105106926, 1697105107748]"
3574,79,60,[],200,EleutherAI/gpt-neox-20b,64,1,908.0,1.0,1,H100,1697105113688,1697105114596,120,12.0,1.0,"[20, 888]","[1697105113708, 1697105114596]"
3575,659,61,[],200,EleutherAI/gpt-neox-20b,64,1,25865.0,1.0,1,H100,1697105114598,1697105140463,120,286.0,381.0,"[11, 1153, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 71, 328, 74, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 78, 58, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 58, 59, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 278, 80, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 76, 72, 69, 67, 52, 395, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 443, 67, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71, 65, 63, 49, 50, 49, 88, 64, 57, 44, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 31, 32, 31, 32, 38, 31, 32, 31, 31, 31, 38, 31, 31, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 32, 31, 31, 31, 31, 31, 32, 31, 31, 31, 32, 31, 32, 31, 31, 32, 31, 38, 31, 31, 30, 31, 31, 36, 30, 30, 32, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 30, 30, 30, 30, 31, 30, 30, 30, 31, 30, 30, 31, 30, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 31, 30, 31, 31, 31, 30, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 37, 29, 30, 30, 30, 30, 30, 30, 30, 34, 28, 29, 28, 28, 29, 28, 28, 29, 28, 28, 29, 28, 29, 28, 29, 28, 29, 28, 29, 29, 26, 25, 26, 25, 26, 25, 26, 25, 26, 26, 26, 26, 25, 26, 26, 26, 25, 26, 26, 25, 26, 26, 26, 25, 26, 26, 26, 26, 26, 25, 26, 26, 26, 26, 26, 25, 26, 26, 26, 26, 26, 26, 25, 26]","[1697105114609, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118363, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125212, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128401, 1697105128473, 1697105128542, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130494, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133520, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134492, 1697105134524, 1697105134555, 1697105134587, 1697105134625, 1697105134656, 1697105134688, 1697105134719, 1697105134750, 1697105134781, 1697105134819, 1697105134850, 1697105134881, 1697105134912, 1697105134943, 1697105134974, 1697105135005, 1697105135036, 1697105135067, 1697105135097, 1697105135128, 1697105135159, 1697105135190, 1697105135221, 1697105135252, 1697105135283, 1697105135315, 1697105135346, 1697105135377, 1697105135408, 1697105135439, 1697105135470, 1697105135502, 1697105135533, 1697105135564, 1697105135595, 1697105135627, 1697105135658, 1697105135690, 1697105135721, 1697105135752, 1697105135784, 1697105135815, 1697105135853, 1697105135884, 1697105135915, 1697105135945, 1697105135976, 1697105136007, 1697105136043, 1697105136073, 1697105136103, 1697105136135, 1697105136164, 1697105136194, 1697105136224, 1697105136254, 1697105136284, 1697105136314, 1697105136344, 1697105136374, 1697105136404, 1697105136434, 1697105136464, 1697105136494, 1697105136524, 1697105136554, 1697105136584, 1697105136614, 1697105136644, 1697105136674, 1697105136705, 1697105136735, 1697105136765, 1697105136795, 1697105136825, 1697105136856, 1697105136886, 1697105136916, 1697105136946, 1697105136977, 1697105137007, 1697105137037, 1697105137068, 1697105137098, 1697105137128, 1697105137159, 1697105137189, 1697105137220, 1697105137250, 1697105137281, 1697105137311, 1697105137342, 1697105137372, 1697105137403, 1697105137433, 1697105137464, 1697105137494, 1697105137525, 1697105137556, 1697105137586, 1697105137617, 1697105137648, 1697105137679, 1697105137709, 1697105137740, 1697105137771, 1697105137802, 1697105137832, 1697105137863, 1697105137894, 1697105137925, 1697105137956, 1697105137987, 1697105138018, 1697105138048, 1697105138079, 1697105138110, 1697105138141, 1697105138172, 1697105138203, 1697105138234, 1697105138265, 1697105138296, 1697105138327, 1697105138358, 1697105138389, 1697105138420, 1697105138451, 1697105138488, 1697105138517, 1697105138547, 1697105138577, 1697105138607, 1697105138637, 1697105138667, 1697105138697, 1697105138727, 1697105138761, 1697105138789, 1697105138818, 1697105138846, 1697105138874, 1697105138903, 1697105138931, 1697105138959, 1697105138988, 1697105139016, 1697105139044, 1697105139073, 1697105139101, 1697105139130, 1697105139158, 1697105139187, 1697105139215, 1697105139244, 1697105139272, 1697105139301, 1697105139330, 1697105139356, 1697105139381, 1697105139407, 1697105139432, 1697105139458, 1697105139483, 1697105139509, 1697105139534, 1697105139560, 1697105139586, 1697105139612, 1697105139638, 1697105139663, 1697105139689, 1697105139715, 1697105139741, 1697105139766, 1697105139792, 1697105139818, 1697105139843, 1697105139869, 1697105139895, 1697105139921, 1697105139946, 1697105139972, 1697105139998, 1697105140024, 1697105140050, 1697105140076, 1697105140101, 1697105140127, 1697105140153, 1697105140179, 1697105140205, 1697105140231, 1697105140256, 1697105140282, 1697105140308, 1697105140334, 1697105140360, 1697105140386, 1697105140412, 1697105140437, 1697105140463]"
3576,253,52,[],200,EleutherAI/gpt-neox-20b,64,1,2189.0,1.0,1,H100,1697105124809,1697105126998,120,67.0,20.0,"[4, 341, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 70, 53, 279, 79, 77, 75]","[1697105124813, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998]"
3577,830,54,[],200,EleutherAI/gpt-neox-20b,64,1,1914.0,1.0,1,H100,1697105129443,1697105131357,120,140.0,9.0,"[5, 1047, 66, 51, 51, 64, 475, 80, 75]","[1697105129448, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357]"
3578,285,50,[],200,EleutherAI/gpt-neox-20b,64,1,3378.0,1.0,1,H100,1697105109445,1697105112823,120,100.0,27.0,"[5, 621, 71, 121, 74, 49, 60, 459, 214, 128, 73, 54, 65, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62]","[1697105109450, 1697105110071, 1697105110142, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111374, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823]"
3579,754,56,[],200,EleutherAI/gpt-neox-20b,64,1,1896.0,1.0,1,H100,1697105115225,1697105117121,120,88.0,7.0,"[13, 1531, 80, 76, 74, 56, 66]","[1697105115238, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117121]"
3580,834,56,[],200,EleutherAI/gpt-neox-20b,64,1,3082.0,1.0,1,H100,1697105114953,1697105118035,120,85.0,20.0,"[4, 805, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 71]","[1697105114957, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035]"
3581,406,57,[],200,EleutherAI/gpt-neox-20b,64,1,1388.0,1.0,1,H100,1697105117121,1697105118509,120,244.0,4.0,"[5, 1159, 78, 74, 72]","[1697105117126, 1697105118285, 1697105118363, 1697105118437, 1697105118509]"
3582,695,56,[],200,EleutherAI/gpt-neox-20b,64,1,1491.0,1.0,1,H100,1697105132276,1697105133767,120,92.0,20.0,"[5, 411, 73, 57, 71, 72, 70, 65, 63, 50, 49, 49, 88, 65, 57, 43, 56, 53, 42, 52]","[1697105132281, 1697105132692, 1697105132765, 1697105132822, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133715, 1697105133767]"
3583,34,34,[],200,EleutherAI/gpt-neox-20b,64,1,589.0,1.0,1,H100,1697105108502,1697105109091,120,12.0,1.0,"[20, 569]","[1697105108522, 1697105109091]"
3584,209,47,[],200,EleutherAI/gpt-neox-20b,64,1,625.0,1.0,1,H100,1697105107749,1697105108374,120,20.0,1.0,"[4, 621]","[1697105107753, 1697105108374]"
3585,790,48,[],200,EleutherAI/gpt-neox-20b,64,1,3419.0,1.0,1,H100,1697105108375,1697105111794,120,84.0,20.0,"[9, 931, 65, 64, 54, 573, 72, 120, 74, 50, 58, 459, 215, 128, 73, 55, 64, 62, 49, 244]","[1697105108384, 1697105109315, 1697105109380, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110387, 1697105110445, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794]"
3586,419,47,[],200,EleutherAI/gpt-neox-20b,64,1,2735.0,1.0,1,H100,1697105106709,1697105109444,120,88.0,20.0,"[12, 486, 61, 57, 56, 54, 53, 260, 61, 61, 49, 56, 45, 423, 58, 49, 48, 717, 65, 64]","[1697105106721, 1697105107207, 1697105107268, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108501, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444]"
3587,731,44,[],200,EleutherAI/gpt-neox-20b,64,1,2563.0,1.0,1,H100,1697105109445,1697105112008,120,89.0,20.0,"[21, 605, 72, 120, 74, 49, 60, 459, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50]","[1697105109466, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008]"
3588,357,68,[],200,EleutherAI/gpt-neox-20b,64,1,3894.0,1.0,1,H100,1697105125549,1697105129443,120,52.0,33.0,"[9, 531, 80, 60, 77, 59, 70, 53, 279, 79, 77, 75, 67, 65, 476, 61, 78, 79, 77, 72, 68, 284, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62]","[1697105125558, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442]"
3589,250,48,[],200,EleutherAI/gpt-neox-20b,64,1,446.0,1.0,1,H100,1697105100819,1697105101265,120,31.0,1.0,"[4, 442]","[1697105100823, 1697105101265]"
3590,833,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101266,1697105102062,120,,,[13],[1697105101279]
3591,613,50,[],200,EleutherAI/gpt-neox-20b,64,1,3501.0,1.0,1,H100,1697105102070,1697105105571,120,90.0,20.0,"[117, 1328, 66, 51, 50, 64, 408, 73, 70, 69, 64, 62, 61, 613, 81, 76, 58, 71, 66, 53]","[1697105102187, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104227, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518, 1697105105571]"
3592,478,45,[],200,EleutherAI/gpt-neox-20b,64,1,7988.0,1.0,1,H100,1697105112010,1697105119998,120,161.0,62.0,"[12, 467, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70, 328, 75, 72, 64, 330, 73, 71, 72, 65, 348, 77, 76, 74, 56, 73, 55, 55]","[1697105112022, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034, 1697105118362, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119047, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998]"
3593,100,69,[],200,EleutherAI/gpt-neox-20b,64,1,2591.0,1.0,1,H100,1697105129443,1697105132034,120,732.0,14.0,"[9, 1043, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424]","[1697105129452, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033]"
3594,409,53,[],200,EleutherAI/gpt-neox-20b,64,1,3872.0,1.0,1,H100,1697105128403,1697105132275,120,109.0,30.0,"[4, 649, 72, 126, 57, 69, 63, 324, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52]","[1697105128407, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129443, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275]"
3595,800,70,[],200,EleutherAI/gpt-neox-20b,64,1,1733.0,1.0,1,H100,1697105132034,1697105133767,120,140.0,20.0,"[13, 645, 73, 56, 72, 72, 70, 65, 63, 50, 49, 49, 88, 65, 57, 43, 56, 53, 42, 52]","[1697105132047, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133715, 1697105133767]"
3596,53,55,[],200,EleutherAI/gpt-neox-20b,64,1,6904.0,1.0,1,H100,1697105118367,1697105125271,120,216.0,55.0,"[10, 526, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 235, 76, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58]","[1697105118377, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120233, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271]"
3597,623,51,[],200,EleutherAI/gpt-neox-20b,64,1,718.0,1.0,1,H100,1697105116131,1697105116849,120,140.0,3.0,"[5, 633, 80]","[1697105116136, 1697105116769, 1697105116849]"
3598,773,50,[],200,EleutherAI/gpt-neox-20b,64,1,2498.0,1.0,1,H100,1697105111795,1697105114293,120,90.0,20.0,"[8, 686, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 60]","[1697105111803, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292]"
3599,223,48,[],200,EleutherAI/gpt-neox-20b,64,1,381.0,1.0,1,H100,1697105129312,1697105129693,120,16.0,1.0,"[5, 376]","[1697105129317, 1697105129693]"
3600,921,49,[],200,EleutherAI/gpt-neox-20b,64,1,715.0,1.0,1,H100,1697105129694,1697105130409,120,31.0,1.0,"[5, 710]","[1697105129699, 1697105130409]"
3601,580,50,[],200,EleutherAI/gpt-neox-20b,64,1,2624.0,1.0,1,H100,1697105130412,1697105133036,120,88.0,20.0,"[5, 785, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70]","[1697105130417, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035]"
3602,730,56,[],200,EleutherAI/gpt-neox-20b,64,1,1726.0,1.0,1,H100,1697105125272,1697105126998,120,364.0,12.0,"[4, 813, 80, 60, 77, 59, 69, 54, 278, 80, 77, 75]","[1697105125276, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126923, 1697105126998]"
3603,691,54,[],200,EleutherAI/gpt-neox-20b,64,1,330.0,1.0,1,H100,1697105119120,1697105119450,120,47.0,1.0,"[13, 317]","[1697105119133, 1697105119450]"
3604,463,55,[],200,EleutherAI/gpt-neox-20b,64,1,702.0,1.0,1,H100,1697105119451,1697105120153,120,39.0,1.0,"[13, 689]","[1697105119464, 1697105120153]"
3605,125,56,[],200,EleutherAI/gpt-neox-20b,64,1,861.0,1.0,1,H100,1697105120154,1697105121015,120,13.0,1.0,"[8, 853]","[1697105120162, 1697105121015]"
3606,379,57,[],200,EleutherAI/gpt-neox-20b,64,1,2444.0,1.0,1,H100,1697105126999,1697105129443,120,182.0,20.0,"[17, 590, 61, 78, 79, 77, 72, 68, 284, 77, 71, 70, 67, 52, 394, 72, 126, 57, 69, 63]","[1697105127016, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128610, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129443]"
3607,825,57,[],200,EleutherAI/gpt-neox-20b,64,1,3081.0,1.0,1,H100,1697105121016,1697105124097,120,96.0,20.0,"[5, 907, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58]","[1697105121021, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097]"
3608,267,51,[],200,EleutherAI/gpt-neox-20b,64,1,2928.0,1.0,1,H100,1697105105572,1697105108500,120,83.0,20.0,"[4, 1130, 58, 57, 55, 44, 287, 61, 58, 55, 54, 53, 260, 61, 61, 49, 56, 45, 423, 57]","[1697105105576, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268, 1697105107326, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500]"
3609,595,35,[],200,EleutherAI/gpt-neox-20b,64,1,825.0,1.0,1,H100,1697105109092,1697105109917,120,8.0,1.0,"[16, 809]","[1697105109108, 1697105109917]"
3610,151,58,[],200,EleutherAI/gpt-neox-20b,64,1,965.0,1.0,1,H100,1697105129444,1697105130409,120,39.0,1.0,"[20, 945]","[1697105129464, 1697105130409]"
3611,367,36,[],200,EleutherAI/gpt-neox-20b,64,1,1457.0,1.0,1,H100,1697105109918,1697105111375,120,92.0,6.0,"[4, 983, 214, 128, 73, 55]","[1697105109922, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375]"
3612,389,52,[],200,EleutherAI/gpt-neox-20b,64,1,640.0,1.0,1,H100,1697105116851,1697105117491,120,8.0,1.0,"[4, 636]","[1697105116855, 1697105117491]"
3613,49,53,[],200,EleutherAI/gpt-neox-20b,64,1,945.0,1.0,1,H100,1697105117492,1697105118437,120,109.0,3.0,"[4, 867, 74]","[1697105117496, 1697105118363, 1697105118437]"
3614,746,54,[],200,EleutherAI/gpt-neox-20b,64,1,2015.0,1.0,1,H100,1697105118438,1697105120453,120,345.0,18.0,"[5, 460, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72]","[1697105118443, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453]"
3615,567,49,[],200,EleutherAI/gpt-neox-20b,64,1,2498.0,1.0,1,H100,1697105111795,1697105114293,120,90.0,20.0,"[12, 682, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 67, 65, 62, 48, 60]","[1697105111807, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114057, 1697105114122, 1697105114184, 1697105114232, 1697105114292]"
3616,559,29,[],200,EleutherAI/gpt-neox-20b,64,1,2490.0,1.0,1,H100,1697105085094,1697105087584,120,86.0,20.0,"[4, 807, 73, 69, 68, 67, 63, 349, 69, 53, 53, 66, 62, 291, 72, 54, 71, 69, 67, 63]","[1697105085098, 1697105085905, 1697105085978, 1697105086047, 1697105086115, 1697105086182, 1697105086245, 1697105086594, 1697105086663, 1697105086716, 1697105086769, 1697105086835, 1697105086897, 1697105087188, 1697105087260, 1697105087314, 1697105087385, 1697105087454, 1697105087521, 1697105087584]"
3617,607,52,[],200,EleutherAI/gpt-neox-20b,64,1,1463.0,1.0,1,H100,1697105131358,1697105132821,120,6.0,10.0,"[4, 671, 70, 68, 52, 52, 59, 358, 73, 56]","[1697105131362, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821]"
3618,785,51,[],200,EleutherAI/gpt-neox-20b,64,1,317.0,1.0,1,H100,1697105118510,1697105118827,120,10.0,1.0,"[8, 309]","[1697105118518, 1697105118827]"
3619,558,52,[],200,EleutherAI/gpt-neox-20b,64,1,2521.0,1.0,1,H100,1697105118828,1697105121349,120,58.0,20.0,"[4, 700, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 56]","[1697105118832, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121349]"
3620,154,58,[],200,EleutherAI/gpt-neox-20b,64,1,317.0,1.0,1,H100,1697105118510,1697105118827,120,13.0,1.0,"[8, 309]","[1697105118518, 1697105118827]"
3621,852,59,[],200,EleutherAI/gpt-neox-20b,64,1,2521.0,1.0,1,H100,1697105118828,1697105121349,120,100.0,20.0,"[12, 692, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 56]","[1697105118840, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121349]"
3622,210,53,[],200,EleutherAI/gpt-neox-20b,64,1,1701.0,1.0,1,H100,1697105121350,1697105123051,120,140.0,2.0,"[12, 1689]","[1697105121362, 1697105123051]"
3623,195,48,[],200,EleutherAI/gpt-neox-20b,64,1,8519.0,1.0,1,H100,1697105109445,1697105117964,120,286.0,64.0,"[25, 601, 72, 120, 74, 49, 60, 459, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78]","[1697105109470, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964]"
3624,886,54,[],200,EleutherAI/gpt-neox-20b,64,1,619.0,1.0,1,H100,1697105123052,1697105123671,120,17.0,1.0,"[5, 614]","[1697105123057, 1697105123671]"
3625,540,55,[],200,EleutherAI/gpt-neox-20b,64,1,1063.0,1.0,1,H100,1697105123672,1697105124735,120,140.0,5.0,"[8, 839, 79, 78, 59]","[1697105123680, 1697105124519, 1697105124598, 1697105124676, 1697105124735]"
3626,311,56,[],200,EleutherAI/gpt-neox-20b,64,1,2262.0,1.0,1,H100,1697105124736,1697105126998,120,93.0,20.0,"[4, 414, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 70, 53, 279, 79, 77, 75]","[1697105124740, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998]"
3627,662,50,[],200,EleutherAI/gpt-neox-20b,64,1,2295.0,1.0,1,H100,1697105126366,1697105128661,120,83.0,20.0,"[8, 393, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 67, 51]","[1697105126374, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128610, 1697105128661]"
3628,191,30,[],200,EleutherAI/gpt-neox-20b,64,1,2989.0,1.0,1,H100,1697105087586,1697105090575,120,85.0,20.0,"[16, 1020, 60, 61, 58, 56, 56, 271, 58, 47, 57, 46, 55, 320, 64, 57, 56, 45, 56, 529]","[1697105087602, 1697105088622, 1697105088682, 1697105088743, 1697105088801, 1697105088857, 1697105088913, 1697105089184, 1697105089242, 1697105089289, 1697105089346, 1697105089392, 1697105089447, 1697105089767, 1697105089831, 1697105089888, 1697105089944, 1697105089989, 1697105090045, 1697105090574]"
3629,69,54,[],200,EleutherAI/gpt-neox-20b,64,1,1491.0,1.0,1,H100,1697105132276,1697105133767,120,85.0,20.0,"[9, 407, 73, 57, 71, 72, 70, 65, 63, 50, 49, 49, 88, 65, 57, 43, 56, 53, 42, 52]","[1697105132285, 1697105132692, 1697105132765, 1697105132822, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133715, 1697105133767]"
3630,27,53,[],200,EleutherAI/gpt-neox-20b,64,1,6036.0,1.0,1,H100,1697105126999,1697105133035,120,15.0,50.0,"[21, 586, 61, 78, 79, 77, 72, 68, 284, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 63, 324, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71]","[1697105127020, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129443, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035]"
3631,529,34,[],200,EleutherAI/gpt-neox-20b,64,1,422.0,1.0,1,H100,1697105097603,1697105098025,120,10.0,1.0,"[5, 417]","[1697105097608, 1697105098025]"
3632,220,50,[],200,EleutherAI/gpt-neox-20b,64,1,1469.0,1.0,1,H100,1697105114293,1697105115762,120,67.0,2.0,"[9, 1460]","[1697105114302, 1697105115762]"
3633,740,59,[],200,EleutherAI/gpt-neox-20b,64,1,1922.0,1.0,1,H100,1697105130412,1697105132334,120,563.0,14.0,"[10, 780, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59]","[1697105130422, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334]"
3634,921,51,[],200,EleutherAI/gpt-neox-20b,64,1,795.0,1.0,1,H100,1697105115763,1697105116558,120,31.0,1.0,"[5, 790]","[1697105115768, 1697105116558]"
3635,430,51,[],200,EleutherAI/gpt-neox-20b,64,1,1300.0,1.0,1,H100,1697105114294,1697105115594,120,15.0,1.0,"[16, 1283]","[1697105114310, 1697105115593]"
3636,207,52,[],200,EleutherAI/gpt-neox-20b,64,1,963.0,1.0,1,H100,1697105115595,1697105116558,120,10.0,1.0,"[8, 955]","[1697105115603, 1697105116558]"
3637,789,53,[],200,EleutherAI/gpt-neox-20b,64,1,7286.0,1.0,1,H100,1697105116559,1697105123845,120,6.0,50.0,"[9, 1178, 81, 59, 78, 71, 331, 71, 71, 65, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359]","[1697105116568, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118366, 1697105118437, 1697105118508, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845]"
3638,585,52,[],200,EleutherAI/gpt-neox-20b,64,1,7286.0,1.0,1,H100,1697105116559,1697105123845,120,244.0,50.0,"[5, 1182, 81, 59, 78, 71, 331, 71, 71, 65, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359]","[1697105116564, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118366, 1697105118437, 1697105118508, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845]"
3639,434,51,[],200,EleutherAI/gpt-neox-20b,64,1,3371.0,1.0,1,H100,1697105128662,1697105132033,120,85.0,20.0,"[5, 1100, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424]","[1697105128667, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033]"
3640,602,64,[],200,EleutherAI/gpt-neox-20b,64,1,928.0,1.0,1,H100,1697105128042,1697105128970,120,15.0,1.0,"[5, 923]","[1697105128047, 1697105128970]"
3641,259,65,[],200,EleutherAI/gpt-neox-20b,64,1,3062.0,1.0,1,H100,1697105128971,1697105132033,120,87.0,20.0,"[8, 788, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424]","[1697105128979, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033]"
3642,30,66,[],200,EleutherAI/gpt-neox-20b,64,1,1733.0,1.0,1,H100,1697105132034,1697105133767,120,93.0,20.0,"[9, 649, 73, 56, 72, 72, 70, 65, 63, 49, 50, 49, 88, 65, 57, 43, 56, 53, 42, 52]","[1697105132043, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133715, 1697105133767]"
3643,889,31,[],200,EleutherAI/gpt-neox-20b,64,1,2749.0,1.0,1,H100,1697105090575,1697105093324,120,86.0,20.0,"[5, 814, 71, 67, 65, 64, 311, 74, 68, 52, 52, 65, 299, 57, 66, 52, 66, 64, 361, 76]","[1697105090580, 1697105091394, 1697105091465, 1697105091532, 1697105091597, 1697105091661, 1697105091972, 1697105092046, 1697105092114, 1697105092166, 1697105092218, 1697105092283, 1697105092582, 1697105092639, 1697105092705, 1697105092757, 1697105092823, 1697105092887, 1697105093248, 1697105093324]"
3644,607,55,[],200,EleutherAI/gpt-neox-20b,64,1,1463.0,1.0,1,H100,1697105131358,1697105132821,120,6.0,10.0,"[4, 671, 70, 68, 52, 52, 59, 358, 73, 56]","[1697105131362, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821]"
3645,544,32,[],200,EleutherAI/gpt-neox-20b,64,1,747.0,1.0,1,H100,1697105093326,1697105094073,120,26.0,1.0,"[12, 735]","[1697105093338, 1697105094073]"
3646,322,33,[],200,EleutherAI/gpt-neox-20b,64,1,2749.0,1.0,1,H100,1697105094074,1697105096823,120,93.0,20.0,"[5, 722, 54, 67, 65, 50, 65, 337, 65, 50, 66, 62, 512, 74, 56, 74, 55, 72, 54, 244]","[1697105094079, 1697105094801, 1697105094855, 1697105094922, 1697105094987, 1697105095037, 1697105095102, 1697105095439, 1697105095504, 1697105095554, 1697105095620, 1697105095682, 1697105096194, 1697105096268, 1697105096324, 1697105096398, 1697105096453, 1697105096525, 1697105096579, 1697105096823]"
3647,188,35,[],200,EleutherAI/gpt-neox-20b,64,1,2730.0,1.0,1,H100,1697105098026,1697105100756,120,85.0,20.0,"[9, 563, 55, 56, 68, 62, 394, 72, 126, 66, 55, 285, 63, 51, 62, 57, 54, 460, 116, 56]","[1697105098035, 1697105098598, 1697105098653, 1697105098709, 1697105098777, 1697105098839, 1697105099233, 1697105099305, 1697105099431, 1697105099497, 1697105099552, 1697105099837, 1697105099900, 1697105099951, 1697105100013, 1697105100070, 1697105100124, 1697105100584, 1697105100700, 1697105100756]"
3648,907,34,[],200,EleutherAI/gpt-neox-20b,64,1,488.0,1.0,1,H100,1697105096825,1697105097313,120,10.0,1.0,"[16, 472]","[1697105096841, 1697105097313]"
3649,355,53,[],200,EleutherAI/gpt-neox-20b,64,1,2519.0,1.0,1,H100,1697105123846,1697105126365,120,90.0,20.0,"[5, 668, 79, 78, 59, 73, 69, 277, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59]","[1697105123851, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365]"
3650,685,35,[],200,EleutherAI/gpt-neox-20b,64,1,796.0,1.0,1,H100,1697105097314,1697105098110,120,364.0,2.0,"[5, 791]","[1697105097319, 1697105098110]"
3651,913,54,[],200,EleutherAI/gpt-neox-20b,64,1,2296.0,1.0,1,H100,1697105126366,1697105128662,120,88.0,20.0,"[12, 389, 79, 77, 75, 67, 66, 475, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 67, 51]","[1697105126378, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127131, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128610, 1697105128661]"
3652,336,36,[],200,EleutherAI/gpt-neox-20b,64,1,1123.0,1.0,1,H100,1697105098110,1697105099233,120,58.0,7.0,"[5, 483, 55, 56, 69, 61, 394]","[1697105098115, 1697105098598, 1697105098653, 1697105098709, 1697105098778, 1697105098839, 1697105099233]"
3653,683,55,[],200,EleutherAI/gpt-neox-20b,64,1,1105.0,1.0,1,H100,1697105128662,1697105129767,120,874.0,2.0,"[5, 1100]","[1697105128667, 1697105129767]"
3654,113,37,[],200,EleutherAI/gpt-neox-20b,64,1,536.0,1.0,1,H100,1697105099234,1697105099770,120,13.0,1.0,"[5, 531]","[1697105099239, 1697105099770]"
3655,695,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105099771,1697105102062,120,,,"[4, 809, 116, 56, 62, 59, 463, 67, 66, 59, 48, 58]","[1697105099775, 1697105100584, 1697105100700, 1697105100756, 1697105100818, 1697105100877, 1697105101340, 1697105101407, 1697105101473, 1697105101532, 1697105101580, 1697105101638]"
3656,466,39,[],200,EleutherAI/gpt-neox-20b,64,1,3453.0,1.0,1,H100,1697105102065,1697105105518,120,457.0,20.0,"[62, 681, 707, 67, 50, 50, 64, 408, 72, 71, 69, 64, 62, 61, 613, 81, 76, 58, 71, 66]","[1697105102127, 1697105102808, 1697105103515, 1697105103582, 1697105103632, 1697105103682, 1697105103746, 1697105104154, 1697105104226, 1697105104297, 1697105104366, 1697105104430, 1697105104492, 1697105104553, 1697105105166, 1697105105247, 1697105105323, 1697105105381, 1697105105452, 1697105105518]"
3657,344,56,[],200,EleutherAI/gpt-neox-20b,64,1,641.0,1.0,1,H100,1697105129768,1697105130409,120,13.0,1.0,"[11, 630]","[1697105129779, 1697105130409]"
3658,886,36,[],200,EleutherAI/gpt-neox-20b,64,1,507.0,1.0,1,H100,1697105100758,1697105101265,120,17.0,1.0,"[12, 495]","[1697105100770, 1697105101265]"
3659,494,55,[],200,EleutherAI/gpt-neox-20b,64,1,1708.0,1.0,1,H100,1697105120454,1697105122162,120,6.0,10.0,"[4, 651, 74, 55, 55, 56, 66, 513, 82, 152]","[1697105120458, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121349, 1697105121415, 1697105121928, 1697105122010, 1697105122162]"
3660,41,52,[],200,EleutherAI/gpt-neox-20b,64,1,5731.0,1.0,1,H100,1697105108501,1697105114232,120,39.0,43.0,"[13, 801, 66, 63, 54, 573, 72, 120, 74, 49, 60, 458, 215, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62, 304, 66, 66, 62, 48]","[1697105108514, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110071, 1697105110143, 1697105110263, 1697105110337, 1697105110386, 1697105110446, 1697105110904, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232]"
3661,21,37,[],200,EleutherAI/gpt-neox-20b,64,1,345.0,1.0,1,H100,1697105111376,1697105111721,120,15.0,1.0,"[12, 333]","[1697105111388, 1697105111721]"
3662,730,38,[],200,EleutherAI/gpt-neox-20b,64,1,1839.0,1.0,1,H100,1697105111722,1697105113561,120,364.0,12.0,"[5, 762, 73, 56, 72, 71, 62, 451, 75, 72, 70, 70]","[1697105111727, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113274, 1697105113349, 1697105113421, 1697105113491, 1697105113561]"
3663,776,49,[],200,EleutherAI/gpt-neox-20b,64,1,401.0,1.0,1,H100,1697105117965,1697105118366,120,67.0,2.0,"[4, 396]","[1697105117969, 1697105118365]"
3664,513,60,[],200,EleutherAI/gpt-neox-20b,64,1,2747.0,1.0,1,H100,1697105121350,1697105124097,120,83.0,20.0,"[8, 570, 82, 152, 174, 152, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58]","[1697105121358, 1697105121928, 1697105122010, 1697105122162, 1697105122336, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097]"
3665,553,50,[],200,EleutherAI/gpt-neox-20b,64,1,2221.0,1.0,1,H100,1697105118369,1697105120590,120,88.0,20.0,"[10, 524, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 235, 76, 72, 72, 69, 68]","[1697105118379, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120233, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590]"
3666,479,58,[],200,EleutherAI/gpt-neox-20b,64,1,4227.0,1.0,1,H100,1697105124098,1697105128325,120,140.0,36.0,"[13, 408, 79, 78, 59, 73, 69, 277, 59, 58, 74, 57, 74, 72, 65, 476, 80, 63, 74, 59, 70, 53, 279, 79, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284]","[1697105124111, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126232, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325]"
3667,510,60,[],200,EleutherAI/gpt-neox-20b,64,1,1064.0,1.0,1,H100,1697105132335,1697105133399,120,79.0,2.0,"[4, 1060]","[1697105132339, 1697105133399]"
3668,869,51,[],200,EleutherAI/gpt-neox-20b,64,1,2334.0,1.0,1,H100,1697105112824,1697105115158,120,244.0,12.0,"[5, 1161, 66, 66, 62, 48, 60, 520, 140, 70, 68, 68]","[1697105112829, 1697105113990, 1697105114056, 1697105114122, 1697105114184, 1697105114232, 1697105114292, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158]"
3669,215,40,[],200,EleutherAI/gpt-neox-20b,64,1,545.0,1.0,1,H100,1697105105519,1697105106064,120,12.0,1.0,"[24, 521]","[1697105105543, 1697105106064]"
3670,611,57,[],200,EleutherAI/gpt-neox-20b,64,1,791.0,1.0,1,H100,1697105118036,1697105118827,120,14.0,1.0,"[16, 775]","[1697105118052, 1697105118827]"
3671,638,52,[],200,EleutherAI/gpt-neox-20b,64,1,2876.0,1.0,1,H100,1697105115159,1697105118035,120,88.0,20.0,"[5, 598, 75, 56, 56, 72, 54, 55, 69, 570, 80, 76, 74, 55, 66, 626, 81, 59, 78, 70]","[1697105115164, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116199, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118034]"
3672,263,58,[],200,EleutherAI/gpt-neox-20b,64,1,622.0,1.0,1,H100,1697105118828,1697105119450,120,15.0,1.0,"[12, 610]","[1697105118840, 1697105119450]"
3673,793,41,[],200,EleutherAI/gpt-neox-20b,64,1,4321.0,1.0,1,H100,1697105106065,1697105110386,120,92.0,31.0,"[13, 628, 58, 57, 55, 44, 286, 61, 58, 56, 54, 53, 260, 61, 61, 50, 55, 45, 423, 57, 50, 48, 717, 65, 64, 53, 573, 72, 121, 70, 53]","[1697105106078, 1697105106706, 1697105106764, 1697105106821, 1697105106876, 1697105106920, 1697105107206, 1697105107267, 1697105107325, 1697105107381, 1697105107435, 1697105107488, 1697105107748, 1697105107809, 1697105107870, 1697105107920, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109380, 1697105109444, 1697105109497, 1697105110070, 1697105110142, 1697105110263, 1697105110333, 1697105110386]"
3674,281,61,[],200,EleutherAI/gpt-neox-20b,64,1,335.0,1.0,1,H100,1697105124098,1697105124433,120,23.0,1.0,"[17, 318]","[1697105124115, 1697105124433]"
3675,148,56,[],200,EleutherAI/gpt-neox-20b,64,1,791.0,1.0,1,H100,1697105122163,1697105122954,120,16.0,1.0,"[5, 786]","[1697105122168, 1697105122954]"
3676,871,62,[],200,EleutherAI/gpt-neox-20b,64,1,968.0,1.0,1,H100,1697105124434,1697105125402,120,123.0,6.0,"[4, 716, 59, 58, 74, 57]","[1697105124438, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402]"
3677,640,63,[],200,EleutherAI/gpt-neox-20b,64,1,606.0,1.0,1,H100,1697105125403,1697105126009,120,15.0,1.0,"[5, 601]","[1697105125408, 1697105126009]"
3678,848,57,[],200,EleutherAI/gpt-neox-20b,64,1,716.0,1.0,1,H100,1697105122955,1697105123671,120,47.0,1.0,"[8, 708]","[1697105122963, 1697105123671]"
3679,296,64,[],200,EleutherAI/gpt-neox-20b,64,1,674.0,1.0,1,H100,1697105126010,1697105126684,120,6.0,1.0,"[13, 661]","[1697105126023, 1697105126684]"
3680,509,58,[],200,EleutherAI/gpt-neox-20b,64,1,926.0,1.0,1,H100,1697105123672,1697105124598,120,286.0,3.0,"[8, 839, 79]","[1697105123680, 1697105124519, 1697105124598]"
3681,282,59,[],200,EleutherAI/gpt-neox-20b,64,1,2399.0,1.0,1,H100,1697105124599,1697105126998,120,87.0,20.0,"[5, 550, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 70, 53, 279, 79, 77, 75]","[1697105124604, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998]"
3682,76,65,[],200,EleutherAI/gpt-neox-20b,64,1,1858.0,1.0,1,H100,1697105126685,1697105128543,120,364.0,12.0,"[9, 912, 60, 79, 79, 77, 72, 68, 285, 76, 71, 70]","[1697105126694, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128326, 1697105128402, 1697105128473, 1697105128543]"
3683,40,59,[],200,EleutherAI/gpt-neox-20b,64,1,3194.0,1.0,1,H100,1697105119451,1697105122645,120,86.0,20.0,"[5, 776, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77]","[1697105119456, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645]"
3684,870,60,[],200,EleutherAI/gpt-neox-20b,64,1,2444.0,1.0,1,H100,1697105126999,1697105129443,120,88.0,20.0,"[16, 591, 61, 78, 79, 77, 72, 68, 284, 77, 71, 70, 67, 52, 394, 72, 126, 57, 69, 63]","[1697105127015, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128610, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129443]"
3685,658,66,[],200,EleutherAI/gpt-neox-20b,64,1,426.0,1.0,1,H100,1697105128544,1697105128970,120,11.0,1.0,"[5, 421]","[1697105128549, 1697105128970]"
3686,405,67,[],200,EleutherAI/gpt-neox-20b,64,1,3062.0,1.0,1,H100,1697105128971,1697105132033,120,87.0,20.0,"[12, 784, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424]","[1697105128983, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033]"
3687,639,61,[],200,EleutherAI/gpt-neox-20b,64,1,1283.0,1.0,1,H100,1697105129444,1697105130727,120,100.0,6.0,"[8, 1043, 66, 51, 51, 64]","[1697105129452, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727]"
3688,300,62,[],200,EleutherAI/gpt-neox-20b,64,1,1135.0,1.0,1,H100,1697105130728,1697105131863,120,9.0,1.0,"[5, 1129]","[1697105130733, 1697105131862]"
3689,299,53,[],200,EleutherAI/gpt-neox-20b,64,1,791.0,1.0,1,H100,1697105118036,1697105118827,120,14.0,1.0,"[8, 783]","[1697105118044, 1697105118827]"
3690,901,57,[],200,EleutherAI/gpt-neox-20b,64,1,526.0,1.0,1,H100,1697105127000,1697105127526,120,17.0,1.0,"[24, 502]","[1697105127024, 1697105127526]"
3691,67,54,[],200,EleutherAI/gpt-neox-20b,64,1,22295.0,1.0,1,H100,1697105118829,1697105141124,120,140.0,381.0,"[23, 680, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 278, 80, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 66, 52, 395, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 443, 67, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71, 65, 63, 49, 50, 49, 88, 64, 57, 44, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 31, 32, 31, 32, 38, 31, 32, 31, 31, 31, 38, 31, 31, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 32, 31, 31, 31, 31, 31, 32, 31, 31, 31, 32, 31, 32, 31, 31, 32, 31, 38, 31, 31, 30, 31, 31, 36, 30, 30, 32, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 30, 30, 30, 30, 31, 30, 30, 30, 31, 30, 30, 31, 30, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 31, 30, 31, 31, 31, 30, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 37, 29, 30, 30, 30, 30, 30, 30, 30, 34, 28, 29, 28, 28, 29, 28, 28, 29, 28, 28, 29, 28, 29, 28, 29, 28, 29, 28, 29, 29, 26, 25, 26, 25, 26, 25, 26, 25, 26, 26, 26, 26, 25, 26, 26, 26, 25, 26, 26, 25, 26, 26, 26, 25, 26, 26, 26, 26, 25, 26, 26, 26, 26, 26, 26, 25, 26, 26, 26, 26, 26, 26, 25, 26, 27, 23, 24, 23, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24]","[1697105118852, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130494, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133520, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134492, 1697105134524, 1697105134555, 1697105134587, 1697105134625, 1697105134656, 1697105134688, 1697105134719, 1697105134750, 1697105134781, 1697105134819, 1697105134850, 1697105134881, 1697105134912, 1697105134943, 1697105134974, 1697105135005, 1697105135036, 1697105135067, 1697105135097, 1697105135128, 1697105135159, 1697105135190, 1697105135221, 1697105135252, 1697105135283, 1697105135315, 1697105135346, 1697105135377, 1697105135408, 1697105135439, 1697105135470, 1697105135502, 1697105135533, 1697105135564, 1697105135595, 1697105135627, 1697105135658, 1697105135690, 1697105135721, 1697105135752, 1697105135784, 1697105135815, 1697105135853, 1697105135884, 1697105135915, 1697105135945, 1697105135976, 1697105136007, 1697105136043, 1697105136073, 1697105136103, 1697105136135, 1697105136164, 1697105136194, 1697105136224, 1697105136254, 1697105136284, 1697105136314, 1697105136344, 1697105136374, 1697105136404, 1697105136434, 1697105136464, 1697105136494, 1697105136524, 1697105136554, 1697105136584, 1697105136614, 1697105136644, 1697105136674, 1697105136705, 1697105136735, 1697105136765, 1697105136795, 1697105136825, 1697105136856, 1697105136886, 1697105136916, 1697105136946, 1697105136977, 1697105137007, 1697105137037, 1697105137068, 1697105137098, 1697105137128, 1697105137159, 1697105137189, 1697105137220, 1697105137250, 1697105137281, 1697105137311, 1697105137342, 1697105137372, 1697105137403, 1697105137433, 1697105137464, 1697105137494, 1697105137525, 1697105137556, 1697105137586, 1697105137617, 1697105137648, 1697105137679, 1697105137709, 1697105137740, 1697105137771, 1697105137802, 1697105137833, 1697105137863, 1697105137894, 1697105137925, 1697105137956, 1697105137987, 1697105138018, 1697105138048, 1697105138079, 1697105138110, 1697105138141, 1697105138172, 1697105138203, 1697105138234, 1697105138265, 1697105138296, 1697105138327, 1697105138358, 1697105138389, 1697105138420, 1697105138451, 1697105138488, 1697105138517, 1697105138547, 1697105138577, 1697105138607, 1697105138637, 1697105138667, 1697105138697, 1697105138727, 1697105138761, 1697105138789, 1697105138818, 1697105138846, 1697105138874, 1697105138903, 1697105138931, 1697105138959, 1697105138988, 1697105139016, 1697105139044, 1697105139073, 1697105139101, 1697105139130, 1697105139158, 1697105139187, 1697105139215, 1697105139244, 1697105139272, 1697105139301, 1697105139330, 1697105139356, 1697105139381, 1697105139407, 1697105139432, 1697105139458, 1697105139483, 1697105139509, 1697105139534, 1697105139560, 1697105139586, 1697105139612, 1697105139638, 1697105139663, 1697105139689, 1697105139715, 1697105139741, 1697105139766, 1697105139792, 1697105139818, 1697105139843, 1697105139869, 1697105139895, 1697105139921, 1697105139946, 1697105139972, 1697105139998, 1697105140024, 1697105140050, 1697105140075, 1697105140101, 1697105140127, 1697105140153, 1697105140179, 1697105140205, 1697105140231, 1697105140256, 1697105140282, 1697105140308, 1697105140334, 1697105140360, 1697105140386, 1697105140412, 1697105140437, 1697105140463, 1697105140490, 1697105140513, 1697105140537, 1697105140560, 1697105140583, 1697105140607, 1697105140630, 1697105140654, 1697105140677, 1697105140701, 1697105140724, 1697105140748, 1697105140771, 1697105140795, 1697105140818, 1697105140842, 1697105140865, 1697105140889, 1697105140912, 1697105140936, 1697105140959, 1697105140983, 1697105141006, 1697105141030, 1697105141053, 1697105141077, 1697105141100, 1697105141124]"
3692,72,63,[],200,EleutherAI/gpt-neox-20b,64,1,1904.0,1.0,1,H100,1697105131863,1697105133767,120,84.0,20.0,"[5, 824, 73, 56, 72, 71, 71, 65, 63, 50, 49, 49, 88, 65, 57, 43, 56, 53, 41, 53]","[1697105131868, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133767]"
3693,671,58,[],200,EleutherAI/gpt-neox-20b,64,1,720.0,1.0,1,H100,1697105127527,1697105128247,120,12.0,1.0,"[4, 716]","[1697105127531, 1697105128247]"
3694,335,59,[],200,EleutherAI/gpt-neox-20b,64,1,5955.0,1.0,1,H100,1697105128248,1697105134203,120,58.0,62.0,"[12, 796, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70, 65, 63, 50, 49, 49, 88, 64, 57, 44, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34]","[1697105128260, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133520, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203]"
3695,207,51,[],200,EleutherAI/gpt-neox-20b,64,1,1078.0,1.0,1,H100,1697105120592,1697105121670,120,10.0,1.0,"[16, 1062]","[1697105120608, 1697105121670]"
3696,907,52,[],200,EleutherAI/gpt-neox-20b,64,1,1283.0,1.0,1,H100,1697105121671,1697105122954,120,10.0,1.0,"[13, 1270]","[1697105121684, 1697105122954]"
3697,655,53,[],200,EleutherAI/gpt-neox-20b,64,1,1780.0,1.0,1,H100,1697105122955,1697105124735,120,335.0,11.0,"[4, 886, 79, 57, 58, 58, 69, 353, 79, 78, 59]","[1697105122959, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735]"
3698,311,54,[],200,EleutherAI/gpt-neox-20b,64,1,2262.0,1.0,1,H100,1697105124736,1697105126998,120,93.0,20.0,"[4, 414, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 70, 53, 279, 79, 77, 75]","[1697105124740, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998]"
3699,116,57,[],200,EleutherAI/gpt-neox-20b,64,1,634.0,1.0,1,H100,1697105130412,1697105131046,120,23.0,1.0,"[16, 618]","[1697105130428, 1697105131046]"
3700,88,55,[],200,EleutherAI/gpt-neox-20b,64,1,2444.0,1.0,1,H100,1697105126999,1697105129443,120,58.0,20.0,"[13, 594, 61, 78, 79, 77, 72, 68, 284, 77, 71, 70, 67, 52, 394, 72, 126, 57, 69, 63]","[1697105127012, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128610, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129443]"
3701,698,58,[],200,EleutherAI/gpt-neox-20b,64,1,1228.0,1.0,1,H100,1697105131047,1697105132275,120,182.0,6.0,"[4, 982, 70, 68, 52, 52]","[1697105131051, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275]"
3702,548,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacty of 79.11 GiB of which 10.12 GiB is free. Process 1645736 has 68.98 GiB memory in use. Of the allocated memory 53.69 GiB is allocated by PyTorch, and 14.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,EleutherAI/gpt-neox-20b,64,1,,,1,H100,1697105101266,1697105102062,120,,,[9],[1697105101275]
3703,319,38,[],200,EleutherAI/gpt-neox-20b,64,1,690.0,1.0,1,H100,1697105102065,1697105102755,120,31.0,1.0,"[6, 684]","[1697105102071, 1697105102755]"
3704,902,39,[],200,EleutherAI/gpt-neox-20b,64,1,1610.0,1.0,1,H100,1697105102756,1697105104366,120,335.0,10.0,"[8, 751, 66, 51, 50, 64, 407, 73, 71, 69]","[1697105102764, 1697105103515, 1697105103581, 1697105103632, 1697105103682, 1697105103746, 1697105104153, 1697105104226, 1697105104297, 1697105104366]"
3705,675,40,[],200,EleutherAI/gpt-neox-20b,64,1,1014.0,1.0,1,H100,1697105104367,1697105105381,120,563.0,5.0,"[4, 795, 81, 76, 58]","[1697105104371, 1697105105166, 1697105105247, 1697105105323, 1697105105381]"
3706,383,39,[],200,EleutherAI/gpt-neox-20b,64,1,359.0,1.0,1,H100,1697105113562,1697105113921,120,15.0,1.0,"[12, 347]","[1697105113574, 1697105113921]"
3707,87,52,[],200,EleutherAI/gpt-neox-20b,64,1,1681.0,1.0,1,H100,1697105132034,1697105133715,120,335.0,19.0,"[5, 653, 73, 56, 72, 71, 71, 65, 63, 49, 50, 49, 88, 65, 57, 43, 56, 53, 42]","[1697105132039, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133715]"
3708,334,61,[],200,EleutherAI/gpt-neox-20b,64,1,426.0,1.0,1,H100,1697105128544,1697105128970,120,15.0,1.0,"[4, 422]","[1697105128548, 1697105128970]"
3709,104,62,[],200,EleutherAI/gpt-neox-20b,64,1,3062.0,1.0,1,H100,1697105128971,1697105132033,120,93.0,20.0,"[12, 784, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424]","[1697105128983, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033]"
3710,624,53,[],200,EleutherAI/gpt-neox-20b,64,1,15078.0,1.0,1,H100,1697105114233,1697105129311,120,563.0,119.0,"[5, 574, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56, 65, 626, 81, 59, 78, 71, 328, 74, 72, 64, 330, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 78, 58, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 58, 59, 74, 57, 74, 72, 65, 476, 80, 60, 77, 58, 70, 54, 278, 80, 76, 76, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 76, 72, 69, 67, 52, 395, 72, 126, 57]","[1697105114238, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118363, 1697105118437, 1697105118509, 1697105118573, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123923, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125212, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126364, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126922, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128401, 1697105128473, 1697105128542, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311]"
3711,158,40,[],200,EleutherAI/gpt-neox-20b,64,1,3133.0,1.0,1,H100,1697105113922,1697105117055,120,85.0,20.0,"[9, 881, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56]","[1697105113931, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055]"
3712,247,46,[],200,EleutherAI/gpt-neox-20b,64,1,21444.0,1.0,1,H100,1697105120001,1697105141445,120,216.0,381.0,"[5, 1103, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 75, 56, 71, 359, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 69, 54, 278, 80, 77, 75, 67, 65, 476, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 66, 52, 395, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 71, 71, 65, 63, 49, 50, 49, 88, 64, 57, 44, 56, 53, 41, 52, 44, 34, 35, 42, 33, 34, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 31, 32, 32, 31, 38, 31, 32, 31, 31, 31, 38, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 32, 31, 31, 31, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 31, 32, 31, 38, 31, 31, 30, 31, 31, 36, 30, 30, 32, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 30, 30, 30, 30, 31, 30, 30, 30, 31, 30, 30, 31, 30, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31, 31, 31, 30, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 37, 29, 30, 30, 30, 30, 30, 30, 30, 34, 28, 29, 28, 28, 29, 28, 28, 29, 28, 28, 29, 28, 29, 28, 29, 28, 29, 28, 29, 29, 26, 25, 26, 25, 26, 25, 26, 25, 26, 26, 26, 26, 25, 26, 26, 26, 25, 26, 26, 25, 26, 26, 26, 25, 26, 26, 26, 26, 25, 26, 26, 26, 26, 26, 26, 25, 26, 26, 26, 26, 26, 26, 25, 26, 27, 23, 24, 23, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 23, 24, 25, 23, 23, 22, 23, 23, 23, 22, 23, 23, 23, 22, 23, 23]","[1697105120006, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123359, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132964, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133520, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133954, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134492, 1697105134524, 1697105134556, 1697105134587, 1697105134625, 1697105134656, 1697105134688, 1697105134719, 1697105134750, 1697105134781, 1697105134819, 1697105134850, 1697105134881, 1697105134912, 1697105134943, 1697105134974, 1697105135005, 1697105135036, 1697105135067, 1697105135098, 1697105135128, 1697105135159, 1697105135190, 1697105135221, 1697105135252, 1697105135283, 1697105135315, 1697105135346, 1697105135377, 1697105135408, 1697105135439, 1697105135470, 1697105135502, 1697105135533, 1697105135564, 1697105135596, 1697105135627, 1697105135658, 1697105135690, 1697105135721, 1697105135752, 1697105135784, 1697105135815, 1697105135853, 1697105135884, 1697105135915, 1697105135945, 1697105135976, 1697105136007, 1697105136043, 1697105136073, 1697105136103, 1697105136135, 1697105136164, 1697105136194, 1697105136224, 1697105136254, 1697105136284, 1697105136314, 1697105136344, 1697105136374, 1697105136404, 1697105136434, 1697105136464, 1697105136494, 1697105136524, 1697105136554, 1697105136584, 1697105136614, 1697105136644, 1697105136674, 1697105136705, 1697105136735, 1697105136765, 1697105136795, 1697105136825, 1697105136856, 1697105136886, 1697105136916, 1697105136946, 1697105136977, 1697105137007, 1697105137037, 1697105137068, 1697105137098, 1697105137128, 1697105137159, 1697105137189, 1697105137220, 1697105137250, 1697105137281, 1697105137311, 1697105137342, 1697105137372, 1697105137403, 1697105137433, 1697105137464, 1697105137495, 1697105137525, 1697105137556, 1697105137586, 1697105137617, 1697105137648, 1697105137679, 1697105137709, 1697105137740, 1697105137771, 1697105137802, 1697105137833, 1697105137863, 1697105137894, 1697105137925, 1697105137956, 1697105137987, 1697105138018, 1697105138048, 1697105138079, 1697105138110, 1697105138141, 1697105138172, 1697105138203, 1697105138234, 1697105138265, 1697105138296, 1697105138327, 1697105138358, 1697105138389, 1697105138420, 1697105138451, 1697105138488, 1697105138517, 1697105138547, 1697105138577, 1697105138607, 1697105138637, 1697105138667, 1697105138697, 1697105138727, 1697105138761, 1697105138789, 1697105138818, 1697105138846, 1697105138874, 1697105138903, 1697105138931, 1697105138959, 1697105138988, 1697105139016, 1697105139044, 1697105139073, 1697105139101, 1697105139130, 1697105139158, 1697105139187, 1697105139215, 1697105139244, 1697105139272, 1697105139301, 1697105139330, 1697105139356, 1697105139381, 1697105139407, 1697105139432, 1697105139458, 1697105139483, 1697105139509, 1697105139534, 1697105139560, 1697105139586, 1697105139612, 1697105139638, 1697105139663, 1697105139689, 1697105139715, 1697105139741, 1697105139766, 1697105139792, 1697105139818, 1697105139843, 1697105139869, 1697105139895, 1697105139921, 1697105139946, 1697105139972, 1697105139998, 1697105140024, 1697105140050, 1697105140075, 1697105140101, 1697105140127, 1697105140153, 1697105140179, 1697105140205, 1697105140231, 1697105140256, 1697105140282, 1697105140308, 1697105140334, 1697105140360, 1697105140386, 1697105140412, 1697105140437, 1697105140463, 1697105140490, 1697105140513, 1697105140537, 1697105140560, 1697105140583, 1697105140607, 1697105140630, 1697105140654, 1697105140677, 1697105140701, 1697105140724, 1697105140748, 1697105140771, 1697105140795, 1697105140818, 1697105140842, 1697105140865, 1697105140889, 1697105140912, 1697105140936, 1697105140959, 1697105140983, 1697105141006, 1697105141030, 1697105141053, 1697105141077, 1697105141100, 1697105141124, 1697105141149, 1697105141172, 1697105141195, 1697105141217, 1697105141240, 1697105141263, 1697105141286, 1697105141308, 1697105141331, 1697105141354, 1697105141377, 1697105141399, 1697105141422, 1697105141445]"
3713,568,42,[],200,EleutherAI/gpt-neox-20b,64,1,304.0,1.0,1,H100,1697105110387,1697105110691,120,11.0,1.0,"[13, 291]","[1697105110400, 1697105110691]"
3714,326,41,[],200,EleutherAI/gpt-neox-20b,64,1,1886.0,1.0,1,H100,1697105105382,1697105107268,120,345.0,12.0,"[5, 756, 111, 58, 44, 350, 59, 56, 55, 44, 287, 61]","[1697105105387, 1697105106143, 1697105106254, 1697105106312, 1697105106356, 1697105106706, 1697105106765, 1697105106821, 1697105106876, 1697105106920, 1697105107207, 1697105107268]"
3715,74,42,[],200,EleutherAI/gpt-neox-20b,64,1,3117.0,1.0,1,H100,1697105107269,1697105110386,120,88.0,20.0,"[4, 475, 61, 61, 49, 56, 45, 423, 57, 50, 48, 717, 66, 63, 54, 573, 71, 121, 70, 53]","[1697105107273, 1697105107748, 1697105107809, 1697105107870, 1697105107919, 1697105107975, 1697105108020, 1697105108443, 1697105108500, 1697105108550, 1697105108598, 1697105109315, 1697105109381, 1697105109444, 1697105109498, 1697105110071, 1697105110142, 1697105110263, 1697105110333, 1697105110386]"
3716,58,68,[],200,EleutherAI/gpt-neox-20b,64,1,580.0,1.0,1,H100,1697105132035,1697105132615,120,15.0,1.0,"[25, 555]","[1697105132060, 1697105132615]"
3717,741,41,[],200,EleutherAI/gpt-neox-20b,64,1,1453.0,1.0,1,H100,1697105117056,1697105118509,120,364.0,9.0,"[4, 686, 81, 59, 78, 71, 327, 75, 72]","[1697105117060, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035, 1697105118362, 1697105118437, 1697105118509]"
3718,518,42,[],200,EleutherAI/gpt-neox-20b,64,1,317.0,1.0,1,H100,1697105118510,1697105118827,120,23.0,1.0,"[12, 305]","[1697105118522, 1697105118827]"
3719,670,56,[],200,EleutherAI/gpt-neox-20b,64,1,2831.0,1.0,1,H100,1697105129444,1697105132275,120,67.0,18.0,"[12, 1039, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52]","[1697105129456, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275]"
3720,172,43,[],200,EleutherAI/gpt-neox-20b,64,1,623.0,1.0,1,H100,1697105118828,1697105119451,120,19.0,1.0,"[20, 602]","[1697105118848, 1697105119450]"
3721,872,44,[],200,EleutherAI/gpt-neox-20b,64,1,3194.0,1.0,1,H100,1697105119451,1697105122645,120,91.0,20.0,"[9, 772, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55, 67, 513, 82, 152, 171, 155, 80, 77]","[1697105119460, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348, 1697105121415, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645]"
3722,226,59,[],200,EleutherAI/gpt-neox-20b,64,1,5294.0,1.0,1,H100,1697105128326,1697105133620,120,216.0,47.0,"[5, 638, 87, 72, 126, 57, 69, 63, 324, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70, 65, 63, 50, 49, 49, 88, 64, 58, 43, 56]","[1697105128331, 1697105128969, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129443, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133521, 1697105133564, 1697105133620]"
3723,622,60,[],200,EleutherAI/gpt-neox-20b,64,1,1025.0,1.0,1,H100,1697105122646,1697105123671,120,20.0,1.0,"[12, 1013]","[1697105122658, 1697105123671]"
3724,390,61,[],200,EleutherAI/gpt-neox-20b,64,1,2694.0,1.0,1,H100,1697105123671,1697105126365,120,84.0,20.0,"[5, 843, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 73, 64, 476, 80, 60, 77, 59]","[1697105123676, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125549, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365]"
3725,475,59,[],200,EleutherAI/gpt-neox-20b,64,1,1491.0,1.0,1,H100,1697105132276,1697105133767,120,89.0,20.0,"[5, 411, 73, 57, 71, 72, 70, 65, 63, 50, 49, 49, 88, 65, 57, 43, 56, 53, 42, 52]","[1697105132281, 1697105132692, 1697105132765, 1697105132822, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133715, 1697105133767]"
3726,693,63,[],200,EleutherAI/gpt-neox-20b,64,1,657.0,1.0,1,H100,1697105132035,1697105132692,120,67.0,2.0,"[24, 633]","[1697105132059, 1697105132692]"
3727,222,43,[],200,EleutherAI/gpt-neox-20b,64,1,2994.0,1.0,1,H100,1697105110692,1697105113686,120,96.0,20.0,"[5, 1097, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71, 62, 450, 76, 72, 70, 70, 63, 62]","[1697105110697, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761, 1697105112823, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686]"
3728,772,43,[],200,EleutherAI/gpt-neox-20b,64,1,2375.0,1.0,1,H100,1697105110387,1697105112762,120,83.0,20.0,"[17, 501, 214, 128, 73, 55, 64, 62, 49, 244, 64, 50, 50, 50, 58, 423, 73, 56, 72, 71]","[1697105110404, 1697105110905, 1697105111119, 1697105111247, 1697105111320, 1697105111375, 1697105111439, 1697105111501, 1697105111550, 1697105111794, 1697105111858, 1697105111908, 1697105111958, 1697105112008, 1697105112066, 1697105112489, 1697105112562, 1697105112618, 1697105112690, 1697105112761]"
3729,928,44,[],200,EleutherAI/gpt-neox-20b,64,1,909.0,1.0,1,H100,1697105113687,1697105114596,120,20.0,1.0,"[17, 892]","[1697105113704, 1697105114596]"
3730,583,45,[],200,EleutherAI/gpt-neox-20b,64,1,3438.0,1.0,1,H100,1697105114597,1697105118035,120,96.0,20.0,"[4, 1161, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 55, 66, 626, 81, 59, 78, 71]","[1697105114601, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117054, 1697105117120, 1697105117746, 1697105117827, 1697105117886, 1697105117964, 1697105118035]"
3731,22,62,[],200,EleutherAI/gpt-neox-20b,64,1,319.0,1.0,1,H100,1697105126366,1697105126685,120,16.0,1.0,"[12, 306]","[1697105126378, 1697105126684]"
3732,837,51,[],200,EleutherAI/gpt-neox-20b,64,1,2478.0,1.0,1,H100,1697105124520,1697105126998,120,85.0,20.0,"[4, 630, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59, 70, 53, 279, 79, 77, 75]","[1697105124524, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126767, 1697105126846, 1697105126923, 1697105126998]"
3733,615,45,[],200,EleutherAI/gpt-neox-20b,64,1,2902.0,1.0,1,H100,1697105122646,1697105125548,120,93.0,20.0,"[4, 1195, 79, 57, 58, 58, 69, 353, 79, 78, 59, 73, 68, 278, 59, 58, 74, 57, 74, 72]","[1697105122650, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097, 1697105124166, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124876, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548]"
3734,612,52,[],200,EleutherAI/gpt-neox-20b,64,1,2444.0,1.0,1,H100,1697105126999,1697105129443,120,93.0,20.0,"[5, 602, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 67, 51, 395, 72, 126, 57, 69, 63]","[1697105127004, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128610, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129443]"
3735,428,44,[],200,EleutherAI/gpt-neox-20b,64,1,1228.0,1.0,1,H100,1697105112762,1697105113990,120,31.0,9.0,"[13, 286, 212, 76, 72, 70, 70, 63, 62, 304]","[1697105112775, 1697105113061, 1697105113273, 1697105113349, 1697105113421, 1697105113491, 1697105113561, 1697105113624, 1697105113686, 1697105113990]"
3736,446,57,[],200,EleutherAI/gpt-neox-20b,64,1,339.0,1.0,1,H100,1697105132276,1697105132615,120,26.0,1.0,"[13, 326]","[1697105132289, 1697105132615]"
3737,267,46,[],200,EleutherAI/gpt-neox-20b,64,1,2424.0,1.0,1,H100,1697105125549,1697105127973,120,83.0,20.0,"[13, 527, 80, 60, 77, 59, 70, 53, 278, 80, 77, 75, 67, 65, 476, 61, 78, 79, 77, 72]","[1697105125562, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126435, 1697105126488, 1697105126766, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127130, 1697105127606, 1697105127667, 1697105127745, 1697105127824, 1697105127901, 1697105127973]"
3738,720,63,[],200,EleutherAI/gpt-neox-20b,64,1,1216.0,1.0,1,H100,1697105126685,1697105127901,120,286.0,6.0,"[5, 916, 60, 79, 79, 77]","[1697105126690, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901]"
3739,205,45,[],200,EleutherAI/gpt-neox-20b,64,1,3063.0,1.0,1,H100,1697105113992,1697105117055,120,87.0,20.0,"[7, 813, 140, 70, 68, 68, 66, 538, 75, 56, 56, 72, 54, 55, 68, 571, 80, 76, 74, 56]","[1697105113999, 1697105114812, 1697105114952, 1697105115022, 1697105115090, 1697105115158, 1697105115224, 1697105115762, 1697105115837, 1697105115893, 1697105115949, 1697105116021, 1697105116075, 1697105116130, 1697105116198, 1697105116769, 1697105116849, 1697105116925, 1697105116999, 1697105117055]"
3740,788,46,[],200,EleutherAI/gpt-neox-20b,64,1,435.0,1.0,1,H100,1697105117056,1697105117491,120,31.0,1.0,"[16, 419]","[1697105117072, 1697105117491]"
3741,568,47,[],200,EleutherAI/gpt-neox-20b,64,1,793.0,1.0,1,H100,1697105117492,1697105118285,120,11.0,1.0,"[12, 781]","[1697105117504, 1697105118285]"
3742,219,48,[],200,EleutherAI/gpt-neox-20b,64,1,2304.0,1.0,1,H100,1697105118286,1697105120590,120,90.0,20.0,"[8, 609, 73, 72, 71, 65, 348, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68]","[1697105118294, 1697105118903, 1697105118976, 1697105119048, 1697105119119, 1697105119184, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590]"
3743,379,64,[],200,EleutherAI/gpt-neox-20b,64,1,2593.0,1.0,1,H100,1697105127902,1697105130495,120,182.0,20.0,"[8, 415, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444]","[1697105127910, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495]"
3744,149,65,[],200,EleutherAI/gpt-neox-20b,64,1,1608.0,1.0,1,H100,1697105130496,1697105132104,120,563.0,10.0,"[17, 689, 80, 75, 72, 55, 71, 54, 424, 71]","[1697105130513, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132104]"
3745,353,46,[],200,EleutherAI/gpt-neox-20b,64,1,1012.0,1.0,1,H100,1697105118036,1697105119048,120,52.0,4.0,"[12, 855, 73, 72]","[1697105118048, 1697105118903, 1697105118976, 1697105119048]"
3746,266,53,[],200,EleutherAI/gpt-neox-20b,64,1,965.0,1.0,1,H100,1697105129444,1697105130409,120,9.0,1.0,"[16, 949]","[1697105129460, 1697105130409]"
3747,42,54,[],200,EleutherAI/gpt-neox-20b,64,1,634.0,1.0,1,H100,1697105130412,1697105131046,120,10.0,1.0,"[17, 617]","[1697105130429, 1697105131046]"
3748,627,55,[],200,EleutherAI/gpt-neox-20b,64,1,2417.0,1.0,1,H100,1697105131047,1697105133464,120,93.0,20.0,"[13, 973, 70, 68, 52, 52, 59, 358, 73, 57, 71, 72, 71, 64, 63, 50, 49, 49, 88, 65]","[1697105131060, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132822, 1697105132893, 1697105132965, 1697105133036, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464]"
3749,44,47,[],200,EleutherAI/gpt-neox-20b,64,1,273.0,1.0,1,H100,1697105127974,1697105128247,120,12.0,1.0,"[13, 260]","[1697105127987, 1697105128247]"
3750,626,48,[],200,EleutherAI/gpt-neox-20b,64,1,722.0,1.0,1,H100,1697105128248,1697105128970,120,10.0,1.0,"[12, 710]","[1697105128260, 1697105128970]"
3751,739,66,[],200,EleutherAI/gpt-neox-20b,64,1,511.0,1.0,1,H100,1697105132104,1697105132615,120,216.0,1.0,"[5, 506]","[1697105132109, 1697105132615]"
3752,564,54,[],200,EleutherAI/gpt-neox-20b,64,1,2519.0,1.0,1,H100,1697105123846,1697105126365,120,84.0,20.0,"[9, 664, 79, 78, 59, 73, 69, 277, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59]","[1697105123855, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365]"
3753,218,55,[],200,EleutherAI/gpt-neox-20b,64,1,765.0,1.0,1,H100,1697105126366,1697105127131,120,109.0,7.0,"[8, 393, 79, 77, 75, 67, 66]","[1697105126374, 1697105126767, 1697105126846, 1697105126923, 1697105126998, 1697105127065, 1697105127131]"
3754,925,49,[],200,EleutherAI/gpt-neox-20b,64,1,3506.0,1.0,1,H100,1697105120591,1697105124097,120,87.0,20.0,"[9, 1328, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58]","[1697105120600, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097]"
3755,398,49,[],200,EleutherAI/gpt-neox-20b,64,1,3063.0,1.0,1,H100,1697105128970,1697105132033,120,87.0,20.0,"[5, 792, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424]","[1697105128975, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033]"
3756,894,56,[],200,EleutherAI/gpt-neox-20b,64,1,1115.0,1.0,1,H100,1697105127132,1697105128247,120,14.0,1.0,"[4, 1111]","[1697105127136, 1697105128247]"
3757,550,57,[],200,EleutherAI/gpt-neox-20b,64,1,3034.0,1.0,1,H100,1697105128248,1697105131282,120,91.0,20.0,"[4, 804, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80]","[1697105128252, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282]"
3758,13,47,[],200,EleutherAI/gpt-neox-20b,64,1,2299.0,1.0,1,H100,1697105119049,1697105121348,120,90.0,20.0,"[4, 479, 77, 76, 74, 56, 73, 55, 55, 234, 77, 72, 72, 69, 68, 519, 74, 55, 55, 55]","[1697105119053, 1697105119532, 1697105119609, 1697105119685, 1697105119759, 1697105119815, 1697105119888, 1697105119943, 1697105119998, 1697105120232, 1697105120309, 1697105120381, 1697105120453, 1697105120522, 1697105120590, 1697105121109, 1697105121183, 1697105121238, 1697105121293, 1697105121348]"
3759,578,50,[],200,EleutherAI/gpt-neox-20b,64,1,335.0,1.0,1,H100,1697105124098,1697105124433,120,31.0,1.0,"[21, 314]","[1697105124119, 1697105124433]"
3760,350,51,[],200,EleutherAI/gpt-neox-20b,64,1,644.0,1.0,1,H100,1697105124434,1697105125078,120,216.0,1.0,"[8, 636]","[1697105124442, 1697105125078]"
3761,911,52,[],200,EleutherAI/gpt-neox-20b,64,1,1844.0,1.0,1,H100,1697105125079,1697105126923,120,335.0,11.0,"[4, 1006, 80, 60, 77, 59, 69, 54, 278, 80, 77]","[1697105125083, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365, 1697105126434, 1697105126488, 1697105126766, 1697105126846, 1697105126923]"
3762,681,53,[],200,EleutherAI/gpt-neox-20b,64,1,602.0,1.0,1,H100,1697105126924,1697105127526,120,23.0,1.0,"[4, 598]","[1697105126928, 1697105127526]"
3763,316,58,[],200,EleutherAI/gpt-neox-20b,64,1,2181.0,1.0,1,H100,1697105131283,1697105133464,120,86.0,20.0,"[8, 742, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 71, 64, 63, 50, 49, 49, 88, 65]","[1697105131291, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133036, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464]"
3764,335,54,[],200,EleutherAI/gpt-neox-20b,64,1,6461.0,1.0,1,H100,1697105127527,1697105133988,120,58.0,62.0,"[13, 785, 77, 71, 70, 66, 53, 394, 72, 126, 57, 69, 62, 325, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70, 65, 63, 49, 50, 49, 88, 64, 57, 44, 56, 53, 41, 52, 44, 34, 35, 42, 34, 33]","[1697105127540, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128609, 1697105128662, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129442, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133463, 1697105133520, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133766, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988]"
3765,59,50,[],200,EleutherAI/gpt-neox-20b,64,1,1733.0,1.0,1,H100,1697105132034,1697105133767,120,91.0,20.0,"[5, 653, 73, 56, 72, 72, 70, 65, 63, 49, 50, 49, 88, 65, 57, 43, 56, 53, 42, 52]","[1697105132039, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133212, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133715, 1697105133767]"
3766,714,48,[],200,EleutherAI/gpt-neox-20b,64,1,2748.0,1.0,1,H100,1697105121349,1697105124097,120,83.0,20.0,"[5, 574, 82, 152, 171, 155, 80, 77, 406, 79, 77, 77, 74, 57, 71, 359, 79, 57, 58, 58]","[1697105121354, 1697105121928, 1697105122010, 1697105122162, 1697105122333, 1697105122488, 1697105122568, 1697105122645, 1697105123051, 1697105123130, 1697105123207, 1697105123284, 1697105123358, 1697105123415, 1697105123486, 1697105123845, 1697105123924, 1697105123981, 1697105124039, 1697105124097]"
3767,346,49,[],200,EleutherAI/gpt-neox-20b,64,1,2267.0,1.0,1,H100,1697105124098,1697105126365,120,85.0,20.0,"[5, 416, 79, 78, 59, 73, 69, 277, 59, 58, 74, 57, 74, 72, 65, 476, 80, 60, 77, 59]","[1697105124103, 1697105124519, 1697105124598, 1697105124676, 1697105124735, 1697105124808, 1697105124877, 1697105125154, 1697105125213, 1697105125271, 1697105125345, 1697105125402, 1697105125476, 1697105125548, 1697105125613, 1697105126089, 1697105126169, 1697105126229, 1697105126306, 1697105126365]"
3768,115,50,[],200,EleutherAI/gpt-neox-20b,64,1,319.0,1.0,1,H100,1697105126366,1697105126685,120,13.0,1.0,"[20, 298]","[1697105126386, 1697105126684]"
3769,700,51,[],200,EleutherAI/gpt-neox-20b,64,1,4597.0,1.0,1,H100,1697105126685,1697105131282,120,140.0,33.0,"[13, 908, 60, 79, 79, 77, 72, 68, 284, 77, 71, 70, 67, 51, 395, 72, 126, 57, 69, 63, 324, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80]","[1697105126698, 1697105127606, 1697105127666, 1697105127745, 1697105127824, 1697105127901, 1697105127973, 1697105128041, 1697105128325, 1697105128402, 1697105128473, 1697105128543, 1697105128610, 1697105128661, 1697105129056, 1697105129128, 1697105129254, 1697105129311, 1697105129380, 1697105129443, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282]"
3770,476,52,[],200,EleutherAI/gpt-neox-20b,64,1,3304.0,1.0,1,H100,1697105131283,1697105134587,120,6.0,50.0,"[9, 741, 70, 68, 52, 52, 59, 358, 73, 56, 72, 72, 70, 65, 63, 50, 49, 49, 88, 65, 57, 43, 56, 53, 41, 53, 43, 34, 35, 42, 34, 33, 41, 33, 33, 33, 41, 34, 38, 31, 32, 31, 32, 31, 31, 32, 31, 32, 32, 31]","[1697105131292, 1697105132033, 1697105132103, 1697105132171, 1697105132223, 1697105132275, 1697105132334, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133521, 1697105133564, 1697105133620, 1697105133673, 1697105133714, 1697105133767, 1697105133810, 1697105133844, 1697105133879, 1697105133921, 1697105133955, 1697105133988, 1697105134029, 1697105134062, 1697105134095, 1697105134128, 1697105134169, 1697105134203, 1697105134241, 1697105134272, 1697105134304, 1697105134335, 1697105134367, 1697105134398, 1697105134429, 1697105134461, 1697105134492, 1697105134524, 1697105134556, 1697105134587]"
3771,401,54,[],200,EleutherAI/gpt-neox-20b,64,1,2721.0,1.0,1,H100,1697105129312,1697105132033,120,84.0,20.0,"[4, 451, 64, 49, 62, 61, 48, 444, 66, 51, 51, 64, 475, 80, 75, 72, 55, 71, 54, 424]","[1697105129316, 1697105129767, 1697105129831, 1697105129880, 1697105129942, 1697105130003, 1697105130051, 1697105130495, 1697105130561, 1697105130612, 1697105130663, 1697105130727, 1697105131202, 1697105131282, 1697105131357, 1697105131429, 1697105131484, 1697105131555, 1697105131609, 1697105132033]"
3772,56,55,[],200,EleutherAI/gpt-neox-20b,64,1,1733.0,1.0,1,H100,1697105132034,1697105133767,120,86.0,20.0,"[17, 641, 73, 56, 72, 72, 70, 65, 63, 50, 49, 49, 88, 65, 56, 44, 56, 53, 42, 52]","[1697105132051, 1697105132692, 1697105132765, 1697105132821, 1697105132893, 1697105132965, 1697105133035, 1697105133100, 1697105133163, 1697105133213, 1697105133262, 1697105133311, 1697105133399, 1697105133464, 1697105133520, 1697105133564, 1697105133620, 1697105133673, 1697105133715, 1697105133767]"
