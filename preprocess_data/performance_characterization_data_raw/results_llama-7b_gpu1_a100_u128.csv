,Unnamed: 0,smpnum,reqnum,errors,status,model,num_users,requests,latency_ms,records,n_gpus,gpu_type,start_timestamp,end_timestamp,experiment_duration_s,n_input_tokens,n_output_tokens,latency_ms_per_token,timestamps_per_token
0,0,308,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581084,120,,,"[122, 1189, 59, 1824, 93, 92, 70, 85, 85, 503, 96, 94, 91, 94, 86, 82]","[1697548576053, 1697548577242, 1697548577301, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579465, 1697548579550, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
1,1,802,0,[],200,llama-7b,128,1,2502.0,1.0,1,A100,1697548575938,1697548578440,120,9.0,1.0,"[431, 2071]","[1697548576369, 1697548578440]"
2,2,427,0,[],200,llama-7b,128,1,3443.0,1.0,1,A100,1697548575938,1697548579381,120,58.0,5.0,"[441, 2058, 688, 93, 93, 69]","[1697548576379, 1697548578437, 1697548579125, 1697548579218, 1697548579311, 1697548579380]"
3,3,723,0,[],200,llama-7b,128,1,1312.0,1.0,1,A100,1697548575931,1697548577243,120,14.0,1.0,"[248, 1063]","[1697548576179, 1697548577242]"
4,4,859,0,[],200,llama-7b,128,1,2461.0,1.0,1,A100,1697548575976,1697548578437,120,23.0,1.0,"[470, 1991]","[1697548576446, 1697548578437]"
5,5,673,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575936,1697548581082,120,,,"[413, 2090, 686, 93, 92, 70, 86, 85, 503, 97, 94, 98, 85, 86, 83]","[1697548576349, 1697548578439, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579466, 1697548579551, 1697548580054, 1697548580151, 1697548580245, 1697548580343, 1697548580428, 1697548580514, 1697548580597]"
6,6,231,0,[],200,llama-7b,128,1,1310.0,1.0,1,A100,1697548575932,1697548577242,120,13.0,1.0,"[124, 1186]","[1697548576056, 1697548577242]"
7,7,69,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575938,1697548581082,120,,,"[499, 2000, 688, 93, 93, 70, 85, 85, 501, 97, 94, 91, 94, 86, 83]","[1697548576437, 1697548578437, 1697548579125, 1697548579218, 1697548579311, 1697548579381, 1697548579466, 1697548579551, 1697548580052, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580597]"
8,8,284,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575927,1697548581083,120,,,"[48, 1326, 1823, 93, 93, 69, 86, 85, 502, 97, 94, 90, 94, 87, 82]","[1697548575975, 1697548577301, 1697548579124, 1697548579217, 1697548579310, 1697548579379, 1697548579465, 1697548579550, 1697548580052, 1697548580149, 1697548580243, 1697548580333, 1697548580427, 1697548580514, 1697548580596]"
9,9,205,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575927,1697548581083,120,,,"[48, 1266, 60, 1823, 93, 93, 69, 86, 85, 502, 97, 94, 91, 94, 86, 82]","[1697548575975, 1697548577241, 1697548577301, 1697548579124, 1697548579217, 1697548579310, 1697548579379, 1697548579465, 1697548579550, 1697548580052, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
10,10,852,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575926,1697548581083,120,,,"[30, 200, 25, 1120, 1823, 93, 93, 69, 86, 85, 502, 97, 94, 90, 95, 86, 82]","[1697548575956, 1697548576156, 1697548576181, 1697548577301, 1697548579124, 1697548579217, 1697548579310, 1697548579379, 1697548579465, 1697548579550, 1697548580052, 1697548580149, 1697548580243, 1697548580333, 1697548580428, 1697548580514, 1697548580596]"
11,11,202,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575976,1697548581082,120,,,"[486, 1976, 688, 93, 92, 70, 85, 85, 502, 96, 94, 91, 94, 86, 83]","[1697548576462, 1697548578438, 1697548579126, 1697548579219, 1697548579311, 1697548579381, 1697548579466, 1697548579551, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580597]"
12,12,554,0,[],200,llama-7b,128,1,1311.0,1.0,1,A100,1697548575931,1697548577242,120,26.0,1.0,"[233, 1078]","[1697548576164, 1697548577242]"
13,13,389,0,[],200,llama-7b,128,1,2463.0,1.0,1,A100,1697548575975,1697548578438,120,8.0,1.0,"[477, 1985]","[1697548576452, 1697548578437]"
14,14,13,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581084,120,,,"[146, 1164, 60, 1824, 93, 93, 69, 85, 86, 502, 96, 95, 90, 94, 86, 82]","[1697548576077, 1697548577241, 1697548577301, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579465, 1697548579551, 1697548580053, 1697548580149, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
15,15,885,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575933,1697548581085,120,,,"[307, 2197, 689, 91, 93, 70, 86, 84, 504, 96, 94, 100, 84, 87, 81]","[1697548576240, 1697548578437, 1697548579126, 1697548579217, 1697548579310, 1697548579380, 1697548579466, 1697548579550, 1697548580054, 1697548580150, 1697548580244, 1697548580344, 1697548580428, 1697548580515, 1697548580596]"
16,16,494,0,[],200,llama-7b,128,1,4218.0,1.0,1,A100,1697548575931,1697548580149,120,6.0,10.0,"[120, 1191, 59, 1824, 93, 92, 70, 85, 85, 503, 96]","[1697548576051, 1697548577242, 1697548577301, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579465, 1697548579550, 1697548580053, 1697548580149]"
17,17,448,0,[],200,llama-7b,128,1,4493.0,1.0,1,A100,1697548575935,1697548580428,120,335.0,12.0,"[409, 2095, 686, 93, 92, 70, 86, 85, 503, 97, 94, 100, 83]","[1697548576344, 1697548578439, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579466, 1697548579551, 1697548580054, 1697548580151, 1697548580245, 1697548580345, 1697548580428]"
18,18,641,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575932,1697548581084,120,,,"[144, 1166, 60, 1823, 93, 93, 69, 85, 86, 502, 96, 94, 91, 94, 86, 82]","[1697548576076, 1697548577242, 1697548577302, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579465, 1697548579551, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
19,19,554,0,[],200,llama-7b,128,1,1311.0,1.0,1,A100,1697548575931,1697548577242,120,26.0,1.0,"[208, 1102]","[1697548576139, 1697548577241]"
20,20,553,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575936,1697548581082,120,,,"[433, 2071, 685, 93, 93, 69, 86, 85, 503, 97, 92, 102, 83, 86, 83]","[1697548576369, 1697548578440, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579466, 1697548579551, 1697548580054, 1697548580151, 1697548580243, 1697548580345, 1697548580428, 1697548580514, 1697548580597]"
21,21,743,0,[],200,llama-7b,128,1,3530.0,1.0,1,A100,1697548575936,1697548579466,120,123.0,6.0,"[418, 2085, 686, 93, 92, 70, 86]","[1697548576354, 1697548578439, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579466]"
22,22,99,0,[],200,llama-7b,128,1,1310.0,1.0,1,A100,1697548575932,1697548577242,120,10.0,1.0,"[114, 1196]","[1697548576046, 1697548577242]"
23,23,444,0,[],200,llama-7b,128,1,3448.0,1.0,1,A100,1697548575932,1697548579380,120,457.0,6.0,"[104, 1205, 60, 1824, 93, 92, 69]","[1697548576036, 1697548577241, 1697548577301, 1697548579125, 1697548579218, 1697548579310, 1697548579379]"
24,24,474,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575976,1697548581082,120,,,"[491, 2659, 93, 92, 70, 85, 85, 502, 96, 94, 91, 94, 86, 84]","[1697548576467, 1697548579126, 1697548579219, 1697548579311, 1697548579381, 1697548579466, 1697548579551, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580598]"
25,25,208,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575926,1697548581083,120,,,"[35, 196, 24, 1120, 1823, 93, 93, 69, 85, 86, 502, 97, 94, 90, 95, 86, 82]","[1697548575961, 1697548576157, 1697548576181, 1697548577301, 1697548579124, 1697548579217, 1697548579310, 1697548579379, 1697548579464, 1697548579550, 1697548580052, 1697548580149, 1697548580243, 1697548580333, 1697548580428, 1697548580514, 1697548580596]"
26,26,330,0,[],200,llama-7b,128,1,4583.0,1.0,1,A100,1697548575932,1697548580515,120,345.0,14.0,"[139, 1171, 60, 1823, 93, 93, 69, 85, 86, 502, 96, 94, 91, 94, 86]","[1697548576071, 1697548577242, 1697548577302, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579465, 1697548579551, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514]"
27,27,613,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581083,120,,,"[52, 1258, 60, 1823, 94, 92, 69, 86, 85, 502, 97, 94, 91, 94, 86, 82]","[1697548575983, 1697548577241, 1697548577301, 1697548579124, 1697548579218, 1697548579310, 1697548579379, 1697548579465, 1697548579550, 1697548580052, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
28,28,263,0,[],200,llama-7b,128,1,230.0,1.0,1,A100,1697548575927,1697548576157,120,15.0,1.0,"[26, 204]","[1697548575953, 1697548576157]"
29,29,618,1,[],200,llama-7b,128,1,2278.0,1.0,1,A100,1697548576161,1697548578439,120,9.0,1.0,"[387, 1891]","[1697548576548, 1697548578439]"
30,30,477,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581085,120,,,"[223, 1088, 59, 1824, 94, 92, 68, 87, 85, 502, 97, 94, 100, 84, 87, 81]","[1697548576154, 1697548577242, 1697548577301, 1697548579125, 1697548579219, 1697548579311, 1697548579379, 1697548579466, 1697548579551, 1697548580053, 1697548580150, 1697548580244, 1697548580344, 1697548580428, 1697548580515, 1697548580596]"
31,31,47,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578443,1697548581084,120,,,"[60, 1451, 100, 97, 94, 99, 84, 87, 81]","[1697548578503, 1697548579954, 1697548580054, 1697548580151, 1697548580245, 1697548580344, 1697548580428, 1697548580515, 1697548580596]"
32,32,368,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575932,1697548581085,120,,,"[222, 1088, 60, 1824, 93, 92, 68, 87, 85, 502, 97, 94, 99, 85, 87, 81]","[1697548576154, 1697548577242, 1697548577302, 1697548579126, 1697548579219, 1697548579311, 1697548579379, 1697548579466, 1697548579551, 1697548580053, 1697548580150, 1697548580244, 1697548580343, 1697548580428, 1697548580515, 1697548580596]"
33,33,905,0,[],200,llama-7b,128,1,2505.0,1.0,1,A100,1697548575933,1697548578438,120,11.0,1.0,"[347, 2158]","[1697548576280, 1697548578438]"
34,34,394,0,[],200,llama-7b,128,1,2503.0,1.0,1,A100,1697548575936,1697548578439,120,11.0,1.0,"[418, 2085]","[1697548576354, 1697548578439]"
35,35,337,1,[],200,llama-7b,128,1,1512.0,1.0,1,A100,1697548578443,1697548579955,120,12.0,1.0,"[69, 1443]","[1697548578512, 1697548579955]"
36,36,119,0,[],200,llama-7b,128,1,2506.0,1.0,1,A100,1697548575931,1697548578437,120,31.0,1.0,"[263, 2243]","[1697548576194, 1697548578437]"
37,37,476,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578444,1697548581084,120,,,"[39, 1472, 99, 96, 94, 101, 83, 87, 81]","[1697548578483, 1697548579955, 1697548580054, 1697548580150, 1697548580244, 1697548580345, 1697548580428, 1697548580515, 1697548580596]"
38,38,831,1,[],200,llama-7b,128,1,1720.0,1.0,1,A100,1697548581089,1697548582809,120,11.0,1.0,"[258, 1462]","[1697548581347, 1697548582809]"
39,39,696,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579958,1697548583919,120,,,"[25, 1486, 1448, 102, 89, 88, 86, 84]","[1697548579983, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583282, 1697548583366]"
40,40,233,2,[],200,llama-7b,128,1,1090.0,1.0,1,A100,1697548582812,1697548583902,120,6.0,1.0,"[18, 1072]","[1697548582830, 1697548583902]"
41,41,591,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583905,1697548586200,120,,,"[11, 509]","[1697548583916, 1697548584425]"
42,42,698,1,[],200,llama-7b,128,1,2193.0,1.0,1,A100,1697548581090,1697548583283,120,182.0,6.0,"[182, 1539, 108, 100, 91, 88, 85]","[1697548581272, 1697548582811, 1697548582919, 1697548583019, 1697548583110, 1697548583198, 1697548583283]"
43,43,836,2,[],200,llama-7b,128,1,1719.0,1.0,1,A100,1697548581090,1697548582809,120,11.0,1.0,"[68, 1651]","[1697548581158, 1697548582809]"
44,44,264,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582813,1697548583922,120,,,"[33, 1056]","[1697548582846, 1697548583902]"
45,45,262,0,[],200,llama-7b,128,1,1310.0,1.0,1,A100,1697548575931,1697548577241,120,39.0,1.0,"[47, 1263]","[1697548575978, 1697548577241]"
46,46,231,0,[],200,llama-7b,128,1,2499.0,1.0,1,A100,1697548575938,1697548578437,120,13.0,1.0,"[436, 2063]","[1697548576374, 1697548578437]"
47,47,21,4,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548586205,1697548588171,120,15.0,1.0,"[141, 1825]","[1697548586346, 1697548588171]"
48,48,130,2,[],200,llama-7b,128,1,1140.0,1.0,1,A100,1697548583286,1697548584426,120,14.0,1.0,"[6, 1134]","[1697548583292, 1697548584426]"
49,49,400,3,[],200,llama-7b,128,1,2277.0,1.0,1,A100,1697548581090,1697548583367,120,123.0,7.0,"[58, 1661, 109, 102, 89, 88, 86, 84]","[1697548581148, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
50,50,592,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578444,1697548581084,120,,,"[44, 1466, 100, 96, 94, 100, 84, 87, 81]","[1697548578488, 1697548579954, 1697548580054, 1697548580150, 1697548580244, 1697548580344, 1697548580428, 1697548580515, 1697548580596]"
51,51,212,3,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548583932,1697548585957,120,31.0,1.0,"[248, 1777]","[1697548584180, 1697548585957]"
52,52,486,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548584431,1697548589557,120,,,"[7, 2776, 1175, 79, 78, 75]","[1697548584438, 1697548587214, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
53,53,379,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588174,1697548589558,120,,,[30],[1697548588204]
54,54,618,1,[],200,llama-7b,128,1,1194.0,1.0,1,A100,1697548577246,1697548578440,120,9.0,1.0,"[40, 1153]","[1697548577286, 1697548578439]"
55,55,736,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592062,120,,,"[91, 1924]","[1697548589665, 1697548591589]"
56,56,915,1,[],200,llama-7b,128,1,2709.0,1.0,1,A100,1697548577245,1697548579954,120,182.0,1.0,"[26, 2683]","[1697548577271, 1697548579954]"
57,57,25,2,[],200,llama-7b,128,1,1720.0,1.0,1,A100,1697548581089,1697548582809,120,12.0,1.0,"[61, 1659]","[1697548581150, 1697548582809]"
58,58,149,0,[],200,llama-7b,128,1,4268.0,1.0,1,A100,1697548575976,1697548580244,120,563.0,10.0,"[475, 1986, 688, 93, 93, 70, 85, 85, 502, 96, 94]","[1697548576451, 1697548578437, 1697548579125, 1697548579218, 1697548579311, 1697548579381, 1697548579466, 1697548579551, 1697548580053, 1697548580149, 1697548580243]"
59,59,175,0,[],200,llama-7b,128,1,3623.0,1.0,1,A100,1697548575927,1697548579550,120,140.0,8.0,"[48, 1326, 1824, 92, 93, 69, 86, 85]","[1697548575975, 1697548577301, 1697548579125, 1697548579217, 1697548579310, 1697548579379, 1697548579465, 1697548579550]"
60,60,383,3,[],200,llama-7b,128,1,1089.0,1.0,1,A100,1697548582813,1697548583902,120,15.0,1.0,"[63, 1026]","[1697548582876, 1697548583902]"
61,61,844,4,[],200,llama-7b,128,1,2016.0,1.0,1,A100,1697548589573,1697548591589,120,10.0,1.0,"[67, 1949]","[1697548589640, 1697548591589]"
62,62,477,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581084,120,,,"[141, 1170, 60, 1823, 93, 93, 69, 85, 86, 502, 96, 94, 91, 94, 86, 82]","[1697548576072, 1697548577242, 1697548577302, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579465, 1697548579551, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
63,63,427,0,[],200,llama-7b,128,1,3448.0,1.0,1,A100,1697548575932,1697548579380,120,58.0,5.0,"[252, 2252, 690, 93, 92, 69]","[1697548576184, 1697548578436, 1697548579126, 1697548579219, 1697548579311, 1697548579380]"
64,64,343,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575936,1697548581082,120,,,"[350, 2152, 687, 93, 92, 70, 85, 85, 504, 97, 93, 102, 82, 87, 82]","[1697548576286, 1697548578438, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579465, 1697548579550, 1697548580054, 1697548580151, 1697548580244, 1697548580346, 1697548580428, 1697548580515, 1697548580597]"
65,65,225,0,[],200,llama-7b,128,1,231.0,1.0,1,A100,1697548575926,1697548576157,120,23.0,1.0,"[40, 191]","[1697548575966, 1697548576157]"
66,66,268,5,[],200,llama-7b,128,1,1257.0,1.0,1,A100,1697548591594,1697548592851,120,19.0,1.0,"[52, 1205]","[1697548591646, 1697548592851]"
67,67,43,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578443,1697548581081,120,,,"[65, 2555]","[1697548578508, 1697548581063]"
68,68,836,0,[],200,llama-7b,128,1,231.0,1.0,1,A100,1697548575926,1697548576157,120,11.0,1.0,"[13, 218]","[1697548575939, 1697548576157]"
69,69,708,4,[],200,llama-7b,128,1,2051.0,1.0,1,A100,1697548583906,1697548585957,120,140.0,1.0,"[37, 2014]","[1697548583943, 1697548585957]"
70,70,602,6,[],200,llama-7b,128,1,2072.0,1.0,1,A100,1697548592854,1697548594926,120,15.0,1.0,"[12, 2060]","[1697548592866, 1697548594926]"
71,71,400,3,[],200,llama-7b,128,1,2279.0,1.0,1,A100,1697548581088,1697548583367,120,123.0,7.0,"[169, 1553, 109, 100, 90, 89, 86, 83]","[1697548581257, 1697548582810, 1697548582919, 1697548583019, 1697548583109, 1697548583198, 1697548583284, 1697548583367]"
72,72,139,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585964,1697548589557,120,,,"[72, 1177, 1176, 78, 79, 75]","[1697548586036, 1697548587213, 1697548588389, 1697548588467, 1697548588546, 1697548588621]"
73,73,267,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548576158,1697548581082,120,,,"[312, 1968, 688, 93, 92, 70, 85, 85, 502, 96, 94, 91, 94, 86, 83]","[1697548576470, 1697548578438, 1697548579126, 1697548579219, 1697548579311, 1697548579381, 1697548579466, 1697548579551, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580597]"
74,74,285,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575933,1697548581082,120,,,"[352, 2153, 687, 93, 92, 70, 85, 85, 504, 96, 94, 101, 83, 87, 82]","[1697548576285, 1697548578438, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579465, 1697548579550, 1697548580054, 1697548580150, 1697548580244, 1697548580345, 1697548580428, 1697548580515, 1697548580597]"
75,75,258,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575932,1697548581085,120,,,"[227, 1083, 60, 1824, 93, 92, 68, 87, 85, 502, 97, 94, 100, 84, 87, 81]","[1697548576159, 1697548577242, 1697548577302, 1697548579126, 1697548579219, 1697548579311, 1697548579379, 1697548579466, 1697548579551, 1697548580053, 1697548580150, 1697548580244, 1697548580344, 1697548580428, 1697548580515, 1697548580596]"
76,76,69,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575936,1697548581083,120,,,"[438, 2066, 685, 93, 92, 70, 86, 85, 503, 97, 92, 101, 84, 86, 83]","[1697548576374, 1697548578440, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579466, 1697548579551, 1697548580054, 1697548580151, 1697548580243, 1697548580344, 1697548580428, 1697548580514, 1697548580597]"
77,77,672,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575925,1697548581082,120,,,"[9, 34, 213, 1120, 1823, 93, 93, 69, 85, 86, 502, 97, 94, 90, 94, 87, 82]","[1697548575934, 1697548575968, 1697548576181, 1697548577301, 1697548579124, 1697548579217, 1697548579310, 1697548579379, 1697548579464, 1697548579550, 1697548580052, 1697548580149, 1697548580243, 1697548580333, 1697548580427, 1697548580514, 1697548580596]"
78,78,553,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575936,1697548581082,120,,,"[399, 2103, 687, 93, 92, 70, 85, 85, 504, 97, 93, 100, 84, 86, 83]","[1697548576335, 1697548578438, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579465, 1697548579550, 1697548580054, 1697548580151, 1697548580244, 1697548580344, 1697548580428, 1697548580514, 1697548580597]"
79,79,802,0,[],200,llama-7b,128,1,231.0,1.0,1,A100,1697548575926,1697548576157,120,9.0,1.0,"[22, 209]","[1697548575948, 1697548576157]"
80,80,693,0,[],200,llama-7b,128,1,1370.0,1.0,1,A100,1697548575932,1697548577302,120,67.0,2.0,"[129, 1181, 60]","[1697548576061, 1697548577242, 1697548577302]"
81,81,609,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575926,1697548581083,120,,,"[25, 205, 25, 1120, 1823, 93, 93, 69, 86, 85, 502, 97, 94, 90, 95, 86, 82]","[1697548575951, 1697548576156, 1697548576181, 1697548577301, 1697548579124, 1697548579217, 1697548579310, 1697548579379, 1697548579465, 1697548579550, 1697548580052, 1697548580149, 1697548580243, 1697548580333, 1697548580428, 1697548580514, 1697548580596]"
82,82,42,0,[],200,llama-7b,128,1,1310.0,1.0,1,A100,1697548575932,1697548577242,120,10.0,1.0,"[237, 1073]","[1697548576169, 1697548577242]"
83,83,392,0,[],200,llama-7b,128,1,2505.0,1.0,1,A100,1697548575932,1697548578437,120,20.0,1.0,"[262, 2242]","[1697548576194, 1697548578436]"
84,84,118,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548577304,1697548581084,120,,,"[10, 2640, 99, 97, 94, 90, 94, 86, 82]","[1697548577314, 1697548579954, 1697548580053, 1697548580150, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
85,85,751,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578443,1697548581084,120,,,"[59, 1552, 97, 94, 101, 82, 87, 81]","[1697548578502, 1697548580054, 1697548580151, 1697548580245, 1697548580346, 1697548580428, 1697548580515, 1697548580596]"
86,86,63,0,[],200,llama-7b,128,1,2502.0,1.0,1,A100,1697548575938,1697548578440,120,39.0,1.0,"[426, 2076]","[1697548576364, 1697548578440]"
87,87,89,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575938,1697548581082,120,,,"[498, 2001, 688, 93, 93, 70, 85, 85, 501, 99, 92, 103, 82, 86, 83]","[1697548576436, 1697548578437, 1697548579125, 1697548579218, 1697548579311, 1697548579381, 1697548579466, 1697548579551, 1697548580052, 1697548580151, 1697548580243, 1697548580346, 1697548580428, 1697548580514, 1697548580597]"
88,88,396,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548577246,1697548581083,120,,,"[34, 2674, 99, 96, 95, 90, 94, 86, 82]","[1697548577280, 1697548579954, 1697548580053, 1697548580149, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
89,89,509,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578444,1697548581081,120,,,"[69, 2550]","[1697548578513, 1697548581063]"
90,90,664,0,[],200,llama-7b,128,1,4219.0,1.0,1,A100,1697548575931,1697548580150,120,364.0,9.0,"[309, 2197, 689, 91, 93, 70, 86, 84, 504, 96]","[1697548576240, 1697548578437, 1697548579126, 1697548579217, 1697548579310, 1697548579380, 1697548579466, 1697548579550, 1697548580054, 1697548580150]"
91,91,508,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575932,1697548581085,120,,,"[160, 1149, 60, 1824, 93, 93, 69, 86, 85, 502, 96, 95, 90, 94, 86, 82]","[1697548576092, 1697548577241, 1697548577301, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579466, 1697548579551, 1697548580053, 1697548580149, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
92,92,95,0,[],200,llama-7b,128,1,2506.0,1.0,1,A100,1697548575932,1697548578438,120,12.0,1.0,"[343, 2163]","[1697548576275, 1697548578438]"
93,93,837,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575932,1697548581085,120,,,"[257, 2247, 690, 93, 91, 69, 87, 84, 504, 96, 94, 101, 83, 87, 81]","[1697548576189, 1697548578436, 1697548579126, 1697548579219, 1697548579310, 1697548579379, 1697548579466, 1697548579550, 1697548580054, 1697548580150, 1697548580244, 1697548580345, 1697548580428, 1697548580515, 1697548580596]"
94,94,882,0,[],200,llama-7b,128,1,4405.0,1.0,1,A100,1697548575936,1697548580341,120,345.0,11.0,"[428, 2075, 686, 93, 92, 70, 86, 85, 503, 97, 94, 95]","[1697548576364, 1697548578439, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579466, 1697548579551, 1697548580054, 1697548580151, 1697548580245, 1697548580340]"
95,95,886,0,[],200,llama-7b,128,1,2462.0,1.0,1,A100,1697548575976,1697548578438,120,17.0,1.0,"[471, 1990]","[1697548576447, 1697548578437]"
96,96,229,0,[],200,llama-7b,128,1,2506.0,1.0,1,A100,1697548575932,1697548578438,120,15.0,1.0,"[323, 2182]","[1697548576255, 1697548578437]"
97,97,590,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581085,120,,,"[206, 1104, 60, 1824, 93, 93, 69, 86, 85, 502, 96, 95, 91, 93, 87, 81]","[1697548576137, 1697548577241, 1697548577301, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579466, 1697548579551, 1697548580053, 1697548580149, 1697548580244, 1697548580335, 1697548580428, 1697548580515, 1697548580596]"
98,98,718,0,[],200,llama-7b,128,1,2505.0,1.0,1,A100,1697548575932,1697548578437,120,13.0,1.0,"[304, 2201]","[1697548576236, 1697548578437]"
99,99,233,1,[],200,llama-7b,128,1,1512.0,1.0,1,A100,1697548578443,1697548579955,120,6.0,1.0,"[25, 1487]","[1697548578468, 1697548579955]"
100,100,645,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581091,1697548583920,120,,,"[283, 1435, 109, 102, 89, 88, 86, 84]","[1697548581374, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
101,101,586,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578442,1697548581084,120,,,"[11, 1501, 99, 97, 94, 90, 94, 86, 82]","[1697548578453, 1697548579954, 1697548580053, 1697548580150, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
102,102,921,0,[],200,llama-7b,128,1,2462.0,1.0,1,A100,1697548575976,1697548578438,120,31.0,1.0,"[485, 1977]","[1697548576461, 1697548578438]"
103,103,316,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578443,1697548581084,120,,,"[49, 1462, 100, 96, 94, 101, 83, 87, 81]","[1697548578492, 1697548579954, 1697548580054, 1697548580150, 1697548580244, 1697548580345, 1697548580428, 1697548580515, 1697548580596]"
104,104,593,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579958,1697548583919,120,,,"[24, 1487, 1448, 102, 89, 88, 86, 84]","[1697548579982, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583282, 1697548583366]"
105,105,267,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583921,120,,,"[357, 1359, 109, 102, 88, 89, 86, 84]","[1697548581451, 1697548582810, 1697548582919, 1697548583021, 1697548583109, 1697548583198, 1697548583284, 1697548583368]"
106,106,879,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581084,120,,,"[132, 1179, 60, 1823, 93, 92, 70, 85, 85, 503, 96, 94, 91, 94, 86, 82]","[1697548576063, 1697548577242, 1697548577302, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579465, 1697548579550, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
107,107,39,0,[],200,llama-7b,128,1,2504.0,1.0,1,A100,1697548575933,1697548578437,120,8.0,1.0,"[312, 2192]","[1697548576245, 1697548578437]"
108,108,778,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575976,1697548581083,120,,,"[481, 1980, 688, 94, 92, 70, 85, 85, 502, 96, 94, 91, 94, 86, 83]","[1697548576457, 1697548578437, 1697548579125, 1697548579219, 1697548579311, 1697548579381, 1697548579466, 1697548579551, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580597]"
109,109,507,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575936,1697548581081,120,,,"[345, 2157, 687, 93, 92, 70, 85, 85, 504, 96, 95, 99, 84, 87, 82]","[1697548576281, 1697548578438, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579465, 1697548579550, 1697548580054, 1697548580150, 1697548580245, 1697548580344, 1697548580428, 1697548580515, 1697548580597]"
110,110,116,0,[],200,llama-7b,128,1,1310.0,1.0,1,A100,1697548575932,1697548577242,120,23.0,1.0,"[212, 1098]","[1697548576144, 1697548577242]"
111,111,450,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575927,1697548581083,120,,,"[49, 1265, 60, 1823, 94, 92, 69, 86, 85, 502, 97, 94, 91, 94, 86, 82]","[1697548575976, 1697548577241, 1697548577301, 1697548579124, 1697548579218, 1697548579310, 1697548579379, 1697548579465, 1697548579550, 1697548580052, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
112,112,371,0,[],200,llama-7b,128,1,2505.0,1.0,1,A100,1697548575932,1697548578437,120,13.0,1.0,"[252, 2253]","[1697548576184, 1697548578437]"
113,113,478,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548577247,1697548581083,120,,,"[48, 2659, 99, 97, 94, 90, 94, 86, 82]","[1697548577295, 1697548579954, 1697548580053, 1697548580150, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
114,114,396,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578442,1697548581083,120,,,"[21, 1491, 100, 96, 94, 101, 83, 87, 81]","[1697548578463, 1697548579954, 1697548580054, 1697548580150, 1697548580244, 1697548580345, 1697548580428, 1697548580515, 1697548580596]"
115,115,858,0,[],200,llama-7b,128,1,4402.0,1.0,1,A100,1697548575932,1697548580334,120,182.0,12.0,"[61, 1308, 1823, 94, 92, 69, 86, 85, 503, 96, 94, 91]","[1697548575993, 1697548577301, 1697548579124, 1697548579218, 1697548579310, 1697548579379, 1697548579465, 1697548579550, 1697548580053, 1697548580149, 1697548580243, 1697548580334]"
116,116,840,2,[],200,llama-7b,128,1,2808.0,1.0,1,A100,1697548581094,1697548583902,120,17.0,1.0,"[389, 2418]","[1697548581483, 1697548583901]"
117,117,673,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575932,1697548581085,120,,,"[237, 1073, 60, 1824, 93, 92, 68, 87, 85, 502, 97, 94, 100, 84, 87, 81]","[1697548576169, 1697548577242, 1697548577302, 1697548579126, 1697548579219, 1697548579311, 1697548579379, 1697548579466, 1697548579551, 1697548580053, 1697548580150, 1697548580244, 1697548580344, 1697548580428, 1697548580515, 1697548580596]"
118,118,778,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575932,1697548581085,120,,,"[242, 1068, 60, 1824, 93, 92, 68, 87, 86, 502, 96, 94, 101, 83, 87, 81]","[1697548576174, 1697548577242, 1697548577302, 1697548579126, 1697548579219, 1697548579311, 1697548579379, 1697548579466, 1697548579552, 1697548580054, 1697548580150, 1697548580244, 1697548580345, 1697548580428, 1697548580515, 1697548580596]"
119,119,239,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583905,1697548586200,120,,,"[6, 514]","[1697548583911, 1697548584425]"
120,120,312,0,[],200,llama-7b,128,1,2505.0,1.0,1,A100,1697548575932,1697548578437,120,23.0,1.0,"[257, 2247]","[1697548576189, 1697548578436]"
121,121,668,1,[],200,llama-7b,128,1,1985.0,1.0,1,A100,1697548578443,1697548580428,120,109.0,6.0,"[49, 1462, 100, 96, 95, 99, 84]","[1697548578492, 1697548579954, 1697548580054, 1697548580150, 1697548580245, 1697548580344, 1697548580428]"
122,122,722,2,[],200,llama-7b,128,1,1721.0,1.0,1,A100,1697548581090,1697548582811,120,39.0,1.0,"[174, 1546]","[1697548581264, 1697548582810]"
123,123,286,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548580349,1697548583920,120,,,"[7, 2561, 102, 89, 88, 87, 83]","[1697548580356, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583366]"
124,124,93,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548580429,1697548583921,120,,,"[6, 1034, 1448, 102, 89, 89, 86, 83]","[1697548580435, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583197, 1697548583283, 1697548583366]"
125,125,155,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582814,1697548583922,120,,,"[36, 1052]","[1697548582850, 1697548583902]"
126,126,640,2,[],200,llama-7b,128,1,2032.0,1.0,1,A100,1697548583926,1697548585958,120,15.0,1.0,"[60, 1972]","[1697548583986, 1697548585958]"
127,127,71,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585963,1697548589556,120,,,"[13, 1239, 1175, 78, 78, 75]","[1697548585976, 1697548587215, 1697548588390, 1697548588468, 1697548588546, 1697548588621]"
128,128,422,3,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548583932,1697548585957,120,26.0,1.0,"[248, 1777]","[1697548584180, 1697548585957]"
129,129,281,0,[],200,llama-7b,128,1,2504.0,1.0,1,A100,1697548575933,1697548578437,120,23.0,1.0,"[312, 2192]","[1697548576245, 1697548578437]"
130,130,642,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578442,1697548581084,120,,,"[13, 1499, 100, 96, 94, 91, 93, 86, 82]","[1697548578455, 1697548579954, 1697548580054, 1697548580150, 1697548580244, 1697548580335, 1697548580428, 1697548580514, 1697548580596]"
131,131,595,4,[],200,llama-7b,128,1,1962.0,1.0,1,A100,1697548586209,1697548588171,120,8.0,1.0,"[162, 1800]","[1697548586371, 1697548588171]"
132,132,854,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575926,1697548581083,120,,,"[32, 199, 24, 1120, 1824, 92, 93, 69, 86, 85, 502, 97, 94, 91, 94, 86, 82]","[1697548575958, 1697548576157, 1697548576181, 1697548577301, 1697548579125, 1697548579217, 1697548579310, 1697548579379, 1697548579465, 1697548579550, 1697548580052, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
133,133,807,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579383,1697548581082,120,,,"[6, 1675]","[1697548579389, 1697548581064]"
134,134,570,4,[],200,llama-7b,128,1,1249.0,1.0,1,A100,1697548585964,1697548587213,120,18.0,1.0,"[95, 1154]","[1697548586059, 1697548587213]"
135,135,536,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579554,1697548581082,120,,,"[9, 1501]","[1697548579563, 1697548581064]"
136,136,866,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583922,120,,,"[147, 1573, 108, 103, 88, 89, 86, 83]","[1697548581237, 1697548582810, 1697548582918, 1697548583021, 1697548583109, 1697548583198, 1697548583284, 1697548583367]"
137,137,30,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594931,1697548597357,120,,,"[18, 2384]","[1697548594949, 1697548597333]"
138,138,392,8,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597367,1697548599717,120,20.0,1.0,"[211, 2139]","[1697548597578, 1697548599717]"
139,139,96,0,[],200,llama-7b,128,1,1314.0,1.0,1,A100,1697548575927,1697548577241,120,31.0,1.0,"[54, 1260]","[1697548575981, 1697548577241]"
140,140,827,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583920,120,,,"[349, 1367, 109, 101, 89, 88, 87, 84]","[1697548581443, 1697548582810, 1697548582919, 1697548583020, 1697548583109, 1697548583197, 1697548583284, 1697548583368]"
141,141,158,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575976,1697548581083,120,,,"[490, 1971, 689, 93, 92, 70, 85, 85, 502, 96, 94, 91, 94, 86, 83]","[1697548576466, 1697548578437, 1697548579126, 1697548579219, 1697548579311, 1697548579381, 1697548579466, 1697548579551, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580597]"
142,142,452,1,[],200,llama-7b,128,1,2998.0,1.0,1,A100,1697548577246,1697548580244,120,216.0,4.0,"[44, 2664, 99, 97, 94]","[1697548577290, 1697548579954, 1697548580053, 1697548580150, 1697548580244]"
143,143,257,2,[],200,llama-7b,128,1,2032.0,1.0,1,A100,1697548583925,1697548585957,120,14.0,1.0,"[34, 1998]","[1697548583959, 1697548585957]"
144,144,810,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548580247,1697548583920,120,,,"[6, 1216, 1448, 102, 89, 88, 87, 83]","[1697548580253, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583366]"
145,145,702,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585965,1697548589559,120,,,"[109, 1139, 1176, 79, 78, 76]","[1697548586074, 1697548587213, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
146,146,697,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583923,120,,,"[203, 1516, 109, 102, 89, 88, 87, 84]","[1697548581292, 1697548582808, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583367]"
147,147,855,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581083,120,,,"[57, 1253, 60, 1824, 93, 92, 69, 86, 85, 503, 96, 94, 91, 94, 86, 82]","[1697548575988, 1697548577241, 1697548577301, 1697548579125, 1697548579218, 1697548579310, 1697548579379, 1697548579465, 1697548579550, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
148,148,134,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589571,1697548592061,120,,,"[32, 1985]","[1697548589603, 1697548591588]"
149,149,498,5,[],200,llama-7b,128,1,2087.0,1.0,1,A100,1697548592072,1697548594159,120,9.0,1.0,"[32, 2055]","[1697548592104, 1697548594159]"
150,150,855,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594164,1697548597360,120,,,"[15, 747, 1420, 72, 58]","[1697548594179, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
151,151,99,2,[],200,llama-7b,128,1,2028.0,1.0,1,A100,1697548583932,1697548585960,120,10.0,1.0,"[359, 1668]","[1697548584291, 1697548585959]"
152,152,280,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599941,120,,,"[398, 1952]","[1697548597765, 1697548599717]"
153,153,610,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602108,120,,,"[107, 1826]","[1697548600058, 1697548601884]"
154,154,460,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585965,1697548589559,120,,,"[116, 1132, 1176, 79, 78, 76]","[1697548586081, 1697548587213, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
155,155,430,1,[],200,llama-7b,128,1,2807.0,1.0,1,A100,1697548581094,1697548583901,120,15.0,1.0,"[377, 2430]","[1697548581471, 1697548583901]"
156,156,861,0,[],200,llama-7b,128,1,1307.0,1.0,1,A100,1697548575935,1697548577242,120,10.0,1.0,"[239, 1068]","[1697548576174, 1697548577242]"
157,157,35,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605266,120,,,"[224, 1500, 263, 78, 76]","[1697548602351, 1697548603851, 1697548604114, 1697548604192, 1697548604268]"
158,158,396,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608965,120,,,"[186, 1766, 507, 79, 77, 80]","[1697548605466, 1697548607232, 1697548607739, 1697548607818, 1697548607895, 1697548607975]"
159,159,262,1,[],200,llama-7b,128,1,2707.0,1.0,1,A100,1697548577247,1697548579954,120,39.0,1.0,"[49, 2658]","[1697548577296, 1697548579954]"
160,160,234,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583925,1697548586198,120,,,"[49, 1984]","[1697548583974, 1697548585958]"
161,161,620,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579957,1697548581083,120,,,"[14, 1093]","[1697548579971, 1697548581064]"
162,162,52,3,[],200,llama-7b,128,1,2194.0,1.0,1,A100,1697548581090,1697548583284,120,58.0,6.0,"[152, 1568, 108, 103, 88, 89, 86]","[1697548581242, 1697548582810, 1697548582918, 1697548583021, 1697548583109, 1697548583198, 1697548583284]"
163,163,609,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581085,120,,,"[213, 1097, 60, 1824, 93, 93, 69, 86, 85, 502, 97, 94, 90, 94, 86, 82]","[1697548576144, 1697548577241, 1697548577301, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579466, 1697548579551, 1697548580053, 1697548580150, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
164,164,119,0,[],200,llama-7b,128,1,231.0,1.0,1,A100,1697548575926,1697548576157,120,31.0,1.0,"[18, 213]","[1697548575944, 1697548576157]"
165,165,642,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581088,1697548583922,120,,,"[161, 1561, 109, 102, 88, 89, 86, 83]","[1697548581249, 1697548582810, 1697548582919, 1697548583021, 1697548583109, 1697548583198, 1697548583284, 1697548583367]"
166,166,789,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583903,1697548586200,120,,,"[13, 509]","[1697548583916, 1697548584425]"
167,167,214,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586210,1697548589561,120,,,"[171, 1790, 218, 79, 79, 75]","[1697548586381, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
168,168,148,1,[],200,llama-7b,128,1,1194.0,1.0,1,A100,1697548577246,1697548578440,120,16.0,1.0,"[39, 1154]","[1697548577285, 1697548578439]"
169,169,821,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592063,120,,,"[169, 1847]","[1697548589743, 1697548591590]"
170,170,507,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578446,1697548581081,120,,,"[102, 2515]","[1697548578548, 1697548581063]"
171,171,568,4,[],200,llama-7b,128,1,2020.0,1.0,1,A100,1697548589568,1697548591588,120,11.0,1.0,"[30, 1990]","[1697548589598, 1697548591588]"
172,172,251,5,[],200,llama-7b,128,1,2087.0,1.0,1,A100,1697548592073,1697548594160,120,31.0,1.0,"[76, 2011]","[1697548592149, 1697548594160]"
173,173,899,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,"[33, 1224]","[1697548591627, 1697548592851]"
174,174,576,6,[],200,llama-7b,128,1,762.0,1.0,1,A100,1697548594164,1697548594926,120,14.0,1.0,"[32, 730]","[1697548594196, 1697548594926]"
175,175,865,3,[],200,llama-7b,128,1,1722.0,1.0,1,A100,1697548581088,1697548582810,120,9.0,1.0,"[159, 1563]","[1697548581247, 1697548582810]"
176,176,1,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594930,1697548597362,120,,,"[14, 2389]","[1697548594944, 1697548597333]"
177,177,360,8,[],200,llama-7b,128,1,2346.0,1.0,1,A100,1697548597372,1697548599718,120,16.0,1.0,"[478, 1868]","[1697548597850, 1697548599718]"
178,178,331,6,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548594350,1697548596097,120,26.0,1.0,"[104, 1643]","[1697548594454, 1697548596097]"
179,179,690,7,[],200,llama-7b,128,1,1232.0,1.0,1,A100,1697548596102,1697548597334,120,39.0,1.0,"[70, 1162]","[1697548596172, 1697548597334]"
180,180,122,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597337,1697548599939,120,,,"[47, 560, 36]","[1697548597384, 1697548597944, 1697548597980]"
181,181,41,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581091,1697548583920,120,,,"[263, 1455, 108, 103, 89, 88, 86, 84]","[1697548581354, 1697548582809, 1697548582917, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
182,182,718,9,[],200,llama-7b,128,1,820.0,1.0,1,A100,1697548599719,1697548600539,120,13.0,1.0,"[15, 805]","[1697548599734, 1697548600539]"
183,183,475,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602108,120,,,"[40, 1893]","[1697548599991, 1697548601884]"
184,184,563,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581081,120,,,"[334, 2172, 690, 91, 92, 70, 85, 85, 504, 96, 94, 100, 84, 87, 82]","[1697548576265, 1697548578437, 1697548579127, 1697548579218, 1697548579310, 1697548579380, 1697548579465, 1697548579550, 1697548580054, 1697548580150, 1697548580244, 1697548580344, 1697548580428, 1697548580515, 1697548580597]"
185,185,404,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583925,1697548586198,120,,,"[41, 1991]","[1697548583966, 1697548585957]"
186,186,806,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602123,1697548605265,120,,,"[62, 1668, 262, 78, 76]","[1697548602185, 1697548603853, 1697548604115, 1697548604193, 1697548604269]"
187,187,849,3,[],200,llama-7b,128,1,1962.0,1.0,1,A100,1697548586210,1697548588172,120,10.0,1.0,"[225, 1737]","[1697548586435, 1697548588172]"
188,188,231,11,[],200,llama-7b,128,1,1950.0,1.0,1,A100,1697548605283,1697548607233,120,13.0,1.0,"[198, 1752]","[1697548605481, 1697548607233]"
189,189,154,10,[],200,llama-7b,128,1,2131.0,1.0,1,A100,1697548600544,1697548602675,120,13.0,1.0,"[23, 2108]","[1697548600567, 1697548602675]"
190,190,592,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607238,1697548608968,120,,,"[80, 1452]","[1697548607318, 1697548608770]"
191,191,264,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582814,1697548583922,120,,,"[61, 1027]","[1697548582875, 1697548583902]"
192,192,23,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610832,120,,,[307],[1697548609282]
193,193,467,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610853,1697548612999,120,,,"[334, 1630]","[1697548611187, 1697548612817]"
194,194,484,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602678,1697548605263,120,,,[20],[1697548602698]
195,195,623,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583927,1697548586197,120,,,[228],[1697548584155]
196,196,52,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586205,1697548589560,120,,,"[65, 1901, 218, 79, 79, 75]","[1697548586270, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
197,197,753,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581081,120,,,"[319, 2187, 689, 91, 93, 70, 86, 84, 504, 96, 94, 101, 83, 87, 82]","[1697548576250, 1697548578437, 1697548579126, 1697548579217, 1697548579310, 1697548579380, 1697548579466, 1697548579550, 1697548580054, 1697548580150, 1697548580244, 1697548580345, 1697548580428, 1697548580515, 1697548580597]"
198,198,698,0,[],200,llama-7b,128,1,3533.0,1.0,1,A100,1697548575932,1697548579465,120,182.0,6.0,"[328, 2177, 689, 91, 93, 70, 85]","[1697548576260, 1697548578437, 1697548579126, 1697548579217, 1697548579310, 1697548579380, 1697548579465]"
199,199,596,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548577246,1697548581083,120,,,"[45, 2663, 99, 97, 94, 90, 94, 87, 81]","[1697548577291, 1697548579954, 1697548580053, 1697548580150, 1697548580244, 1697548580334, 1697548580428, 1697548580515, 1697548580596]"
200,200,825,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613020,1697548615233,120,,,"[349, 1663]","[1697548613369, 1697548615032]"
201,201,24,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583920,120,,,"[268, 1561, 102, 89, 88, 86, 84]","[1697548581357, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
202,202,98,1,[],200,llama-7b,128,1,1595.0,1.0,1,A100,1697548579469,1697548581064,120,14.0,1.0,"[11, 1584]","[1697548579480, 1697548581064]"
203,203,384,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583925,1697548586197,120,,,"[36, 1996]","[1697548583961, 1697548585957]"
204,204,733,4,[],200,llama-7b,128,1,1968.0,1.0,1,A100,1697548586204,1697548588172,120,31.0,1.0,"[24, 1943]","[1697548586228, 1697548588171]"
205,205,459,2,[],200,llama-7b,128,1,2132.0,1.0,1,A100,1697548581065,1697548583197,120,58.0,5.0,"[15, 389, 1449, 102, 89, 88]","[1697548581080, 1697548581469, 1697548582918, 1697548583020, 1697548583109, 1697548583197]"
206,206,133,5,[],200,llama-7b,128,1,1205.0,1.0,1,A100,1697548588175,1697548589380,120,15.0,1.0,"[59, 1146]","[1697548588234, 1697548589380]"
207,207,492,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589384,1697548592066,120,,,"[37, 738]","[1697548589421, 1697548590159]"
208,208,659,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575976,1697548581082,120,,,"[501, 1961, 688, 93, 92, 70, 85, 85, 502, 96, 94, 91, 94, 86, 84]","[1697548576477, 1697548578438, 1697548579126, 1697548579219, 1697548579311, 1697548579381, 1697548579466, 1697548579551, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580598]"
209,209,723,2,[],200,llama-7b,128,1,1719.0,1.0,1,A100,1697548581090,1697548582809,120,14.0,1.0,"[65, 1654]","[1697548581155, 1697548582809]"
210,210,818,3,[],200,llama-7b,128,1,1225.0,1.0,1,A100,1697548583201,1697548584426,120,13.0,1.0,"[18, 1206]","[1697548583219, 1697548584425]"
211,211,151,3,[],200,llama-7b,128,1,1088.0,1.0,1,A100,1697548582814,1697548583902,120,39.0,1.0,"[31, 1057]","[1697548582845, 1697548583902]"
212,212,513,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583905,1697548586198,120,,,"[29, 2022]","[1697548583934, 1697548585956]"
213,213,872,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586204,1697548589559,120,,,"[26, 1942, 217, 79, 78, 76]","[1697548586230, 1697548588172, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
214,214,242,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548584427,1697548589557,120,,,"[6, 2781, 1175, 79, 78, 75]","[1697548584433, 1697548587214, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
215,215,275,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589578,1697548592063,120,,,"[393, 1620]","[1697548589971, 1697548591591]"
216,216,635,7,[],200,llama-7b,128,1,2087.0,1.0,1,A100,1697548592073,1697548594160,120,23.0,1.0,"[96, 1991]","[1697548592169, 1697548594160]"
217,217,59,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594164,1697548597360,120,,,"[31, 731, 1420, 72, 58]","[1697548594195, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
218,218,88,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583920,120,,,"[299, 1420, 109, 102, 89, 88, 86, 85]","[1697548581389, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583368]"
219,219,853,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,"[278, 1808]","[1697548592351, 1697548594159]"
220,220,65,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548580153,1697548583920,120,,,"[16, 1300, 1448, 102, 89, 88, 87, 83]","[1697548580169, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583366]"
221,221,413,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597371,1697548599942,120,,,"[418, 1928]","[1697548597789, 1697548599717]"
222,222,287,8,[],200,llama-7b,128,1,1744.0,1.0,1,A100,1697548594356,1697548596100,120,10.0,1.0,"[387, 1356]","[1697548594743, 1697548596099]"
223,223,772,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[313, 1621]","[1697548600265, 1697548601886]"
224,224,752,0,[],200,llama-7b,128,1,3243.0,1.0,1,A100,1697548575976,1697548579219,120,39.0,3.0,"[480, 1981, 688, 94]","[1697548576456, 1697548578437, 1697548579125, 1697548579219]"
225,225,61,0,[],200,llama-7b,128,1,2499.0,1.0,1,A100,1697548575938,1697548578437,120,9.0,1.0,"[446, 2053]","[1697548576384, 1697548578437]"
226,226,225,0,[],200,llama-7b,128,1,2505.0,1.0,1,A100,1697548575933,1697548578438,120,23.0,1.0,"[337, 2168]","[1697548576270, 1697548578438]"
227,227,365,0,[],200,llama-7b,128,1,231.0,1.0,1,A100,1697548575926,1697548576157,120,23.0,1.0,"[37, 194]","[1697548575963, 1697548576157]"
228,228,597,5,[],200,llama-7b,128,1,2016.0,1.0,1,A100,1697548589573,1697548591589,120,39.0,1.0,"[72, 1944]","[1697548589645, 1697548591589]"
229,229,927,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,"[30, 1227]","[1697548591624, 1697548592851]"
230,230,422,2,[],200,llama-7b,128,1,2029.0,1.0,1,A100,1697548583927,1697548585956,120,26.0,1.0,"[218, 1811]","[1697548584145, 1697548585956]"
231,231,292,11,[],200,llama-7b,128,1,1725.0,1.0,1,A100,1697548602127,1697548603852,120,286.0,1.0,"[43, 1682]","[1697548602170, 1697548603852]"
232,232,359,7,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548594350,1697548596097,120,10.0,1.0,"[90, 1656]","[1697548594440, 1697548596096]"
233,233,779,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585963,1697548589557,120,,,"[37, 1212, 1177, 79, 78, 75]","[1697548586000, 1697548587212, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
234,234,723,1,[],200,llama-7b,128,1,2279.0,1.0,1,A100,1697548576160,1697548578439,120,14.0,1.0,"[315, 1963]","[1697548576475, 1697548578438]"
235,235,210,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592062,120,,,"[376, 1646]","[1697548589944, 1697548591590]"
236,236,721,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596100,1697548597357,120,,,"[27, 1207]","[1697548596127, 1697548597334]"
237,237,151,9,[],200,llama-7b,128,1,2351.0,1.0,1,A100,1697548597366,1697548599717,120,39.0,1.0,"[120, 2230]","[1697548597486, 1697548599716]"
238,238,567,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594346,120,,,"[87, 2001]","[1697548592159, 1697548594160]"
239,239,153,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578445,1697548581081,120,,,"[97, 2521]","[1697548578542, 1697548581063]"
240,240,649,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603856,1697548605264,120,,,"[17, 1196]","[1697548603873, 1697548605069]"
241,241,83,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597358,120,,,"[37, 1709, 251, 72, 58]","[1697548594387, 1697548596096, 1697548596347, 1697548596419, 1697548596477]"
242,242,510,3,[],200,llama-7b,128,1,1826.0,1.0,1,A100,1697548581093,1697548582919,120,79.0,2.0,"[373, 1453]","[1697548581466, 1697548582919]"
243,243,595,10,[],200,llama-7b,128,1,819.0,1.0,1,A100,1697548599721,1697548600540,120,8.0,1.0,"[63, 756]","[1697548599784, 1697548600540]"
244,244,280,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581084,120,,,"[60, 1250, 60, 1824, 93, 92, 69, 86, 85, 503, 96, 94, 91, 94, 86, 82]","[1697548575991, 1697548577241, 1697548577301, 1697548579125, 1697548579218, 1697548579310, 1697548579379, 1697548579465, 1697548579550, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
245,245,870,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582921,1697548586200,120,,,"[19, 1485]","[1697548582940, 1697548584425]"
246,246,442,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599941,120,,,"[110, 2240]","[1697548597476, 1697548599716]"
247,247,269,5,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548586208,1697548588171,120,11.0,1.0,"[148, 1815]","[1697548586356, 1697548588171]"
248,248,626,6,[],200,llama-7b,128,1,1208.0,1.0,1,A100,1697548588174,1697548589382,120,10.0,1.0,"[29, 1179]","[1697548588203, 1697548589382]"
249,249,796,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602106,120,,,"[140, 1794]","[1697548600091, 1697548601885]"
250,250,22,11,[],200,llama-7b,128,1,2131.0,1.0,1,A100,1697548600544,1697548602675,120,16.0,1.0,"[42, 2088]","[1697548600586, 1697548602674]"
251,251,376,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602678,1697548605263,120,,,"[15, 2376]","[1697548602693, 1697548605069]"
252,252,230,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602109,1697548605264,120,,,"[21, 1721, 263, 78, 76]","[1697548602130, 1697548603851, 1697548604114, 1697548604192, 1697548604268]"
253,253,145,0,[],200,llama-7b,128,1,4215.0,1.0,1,A100,1697548575936,1697548580151,120,161.0,9.0,"[408, 2094, 687, 93, 92, 70, 85, 86, 503, 97]","[1697548576344, 1697548578438, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579465, 1697548579551, 1697548580054, 1697548580151]"
254,254,11,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581085,120,,,"[228, 1083, 60, 1824, 93, 92, 68, 87, 85, 502, 97, 94, 98, 86, 87, 81]","[1697548576159, 1697548577242, 1697548577302, 1697548579126, 1697548579219, 1697548579311, 1697548579379, 1697548579466, 1697548579551, 1697548580053, 1697548580150, 1697548580244, 1697548580342, 1697548580428, 1697548580515, 1697548580596]"
255,255,57,7,[],200,llama-7b,128,1,773.0,1.0,1,A100,1697548589387,1697548590160,120,13.0,1.0,"[59, 714]","[1697548589446, 1697548590160]"
256,256,93,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575976,1697548581082,120,,,"[465, 1996, 688, 93, 93, 70, 85, 85, 501, 97, 94, 91, 94, 86, 83]","[1697548576441, 1697548578437, 1697548579125, 1697548579218, 1697548579311, 1697548579381, 1697548579466, 1697548579551, 1697548580052, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580597]"
257,257,733,13,[],200,llama-7b,128,1,1950.0,1.0,1,A100,1697548605279,1697548607229,120,31.0,1.0,"[18, 1932]","[1697548605297, 1697548607229]"
258,258,167,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607236,1697548608966,120,,,"[32, 1502]","[1697548607268, 1697548608770]"
259,259,670,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583921,120,,,"[63, 1656, 109, 102, 89, 88, 86, 84]","[1697548581153, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
260,260,499,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610832,120,,,[283],[1697548609257]
261,261,414,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548590163,1697548594346,120,,,"[19, 2669]","[1697548590182, 1697548592851]"
262,262,369,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581091,1697548583920,120,,,"[278, 1440, 109, 102, 89, 88, 86, 84]","[1697548581369, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
263,263,101,3,[],200,llama-7b,128,1,2027.0,1.0,1,A100,1697548583932,1697548585959,120,13.0,1.0,"[338, 1689]","[1697548584270, 1697548585959]"
264,264,426,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585964,1697548589558,120,,,"[107, 2318, 79, 78, 76]","[1697548586071, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
265,265,704,2,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548583925,1697548585956,120,14.0,1.0,"[16, 2015]","[1697548583941, 1697548585956]"
266,266,135,3,[],200,llama-7b,128,1,2427.0,1.0,1,A100,1697548585962,1697548588389,120,52.0,2.0,"[107, 1144, 1176]","[1697548586069, 1697548587213, 1697548588389]"
267,267,787,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589575,1697548592065,120,,,"[213, 1803]","[1697548589788, 1697548591591]"
268,268,218,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592074,1697548594348,120,,,[295],[1697548592369]
269,269,207,1,[],200,llama-7b,128,1,2808.0,1.0,1,A100,1697548581094,1697548583902,120,10.0,1.0,"[382, 2426]","[1697548581476, 1697548583902]"
270,270,578,7,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548594356,1697548596098,120,31.0,1.0,"[200, 1542]","[1697548594556, 1697548596098]"
271,271,701,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578443,1697548581084,120,,,"[35, 1477, 99, 96, 94, 102, 82, 87, 81]","[1697548578478, 1697548579955, 1697548580054, 1697548580150, 1697548580244, 1697548580346, 1697548580428, 1697548580515, 1697548580596]"
272,272,565,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583905,1697548586197,120,,,"[24, 2026]","[1697548583929, 1697548585955]"
273,273,857,1,[],200,llama-7b,128,1,1316.0,1.0,1,A100,1697548580153,1697548581469,120,18.0,1.0,"[6, 1310]","[1697548580159, 1697548581469]"
274,274,376,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581472,1697548583921,120,,,"[16, 2413]","[1697548581488, 1697548583901]"
275,275,3,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596101,1697548597357,120,,,"[27, 1206]","[1697548596128, 1697548597334]"
276,276,895,3,[],200,llama-7b,128,1,1962.0,1.0,1,A100,1697548586209,1697548588171,120,15.0,1.0,"[163, 1799]","[1697548586372, 1697548588171]"
277,277,735,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586199,120,,,"[315, 1711]","[1697548584247, 1697548585958]"
278,278,164,4,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548586205,1697548588171,120,15.0,1.0,"[60, 1906]","[1697548586265, 1697548588171]"
279,279,515,5,[],200,llama-7b,128,1,1203.0,1.0,1,A100,1697548588178,1697548589381,120,11.0,1.0,"[86, 1117]","[1697548588264, 1697548589381]"
280,280,874,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589384,1697548592065,120,,,"[27, 748]","[1697548589411, 1697548590159]"
281,281,324,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588176,1697548589559,120,,,"[57, 1146]","[1697548588233, 1697548589379]"
282,282,273,7,[],200,llama-7b,128,1,2086.0,1.0,1,A100,1697548592076,1697548594162,120,19.0,1.0,"[362, 1723]","[1697548592438, 1697548594161]"
283,283,679,5,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548589567,1697548591590,120,15.0,1.0,"[326, 1697]","[1697548589893, 1697548591590]"
284,284,111,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,[23],[1697548591617]
285,285,439,7,[],200,llama-7b,128,1,2120.0,1.0,1,A100,1697548594356,1697548596476,120,13.0,4.0,"[215, 1525, 250, 72, 58]","[1697548594571, 1697548596096, 1697548596346, 1697548596418, 1697548596476]"
286,286,634,8,[],200,llama-7b,128,1,754.0,1.0,1,A100,1697548594173,1697548594927,120,13.0,1.0,"[68, 686]","[1697548594241, 1697548594927]"
287,287,68,9,[],200,llama-7b,128,1,2401.0,1.0,1,A100,1697548594933,1697548597334,120,12.0,1.0,"[35, 2366]","[1697548594968, 1697548597334]"
288,288,449,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599943,120,,,"[180, 2171]","[1697548597546, 1697548599717]"
289,289,639,0,[],200,llama-7b,128,1,3448.0,1.0,1,A100,1697548575932,1697548579380,120,100.0,6.0,"[154, 1155, 60, 1824, 93, 93, 69]","[1697548576086, 1697548577241, 1697548577301, 1697548579125, 1697548579218, 1697548579311, 1697548579380]"
290,290,130,2,[],200,llama-7b,128,1,1717.0,1.0,1,A100,1697548581094,1697548582811,120,14.0,1.0,"[367, 1349]","[1697548581461, 1697548582810]"
291,291,799,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596479,1697548599938,120,,,"[6, 977, 518]","[1697548596485, 1697548597462, 1697548597980]"
292,292,807,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,"[410, 1523]","[1697548600363, 1697548601886]"
293,293,837,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581088,1697548583923,120,,,"[186, 1537, 108, 100, 91, 88, 84, 85]","[1697548581274, 1697548582811, 1697548582919, 1697548583019, 1697548583110, 1697548583198, 1697548583282, 1697548583367]"
294,294,719,0,[],200,llama-7b,128,1,3449.0,1.0,1,A100,1697548575931,1697548579380,120,182.0,6.0,"[107, 1263, 1824, 93, 92, 70]","[1697548576038, 1697548577301, 1697548579125, 1697548579218, 1697548579310, 1697548579380]"
295,295,480,3,[],200,llama-7b,128,1,1082.0,1.0,1,A100,1697548582820,1697548583902,120,26.0,1.0,"[60, 1022]","[1697548582880, 1697548583902]"
296,296,73,2,[],200,llama-7b,128,1,2808.0,1.0,1,A100,1697548581094,1697548583902,120,9.0,1.0,"[384, 2423]","[1697548581478, 1697548583901]"
297,297,405,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583905,1697548586197,120,,,"[19, 2032]","[1697548583924, 1697548585956]"
298,298,426,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597337,1697548599939,120,,,"[49, 594]","[1697548597386, 1697548597980]"
299,299,68,1,[],200,llama-7b,128,1,1679.0,1.0,1,A100,1697548579386,1697548581065,120,12.0,1.0,"[18, 1660]","[1697548579404, 1697548581064]"
300,300,396,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581068,1697548583921,120,,,"[31, 1709, 110, 102, 89, 88, 86, 83]","[1697548581099, 1697548582808, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583366]"
301,301,780,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602108,120,,,"[90, 1843]","[1697548600041, 1697548601884]"
302,302,840,4,[],200,llama-7b,128,1,2050.0,1.0,1,A100,1697548583906,1697548585956,120,17.0,1.0,"[25, 2025]","[1697548583931, 1697548585956]"
303,303,758,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586210,1697548589562,120,,,"[228, 1733, 218, 79, 79, 75]","[1697548586438, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
304,304,269,5,[],200,llama-7b,128,1,1251.0,1.0,1,A100,1697548585962,1697548587213,120,11.0,1.0,"[88, 1163]","[1697548586050, 1697548587213]"
305,305,262,2,[],200,llama-7b,128,1,3279.0,1.0,1,A100,1697548583935,1697548587214,120,39.0,1.0,"[420, 2859]","[1697548584355, 1697548587214]"
306,306,177,12,[],200,llama-7b,128,1,1725.0,1.0,1,A100,1697548602127,1697548603852,120,14.0,1.0,"[38, 1687]","[1697548602165, 1697548603852]"
307,307,603,6,[],200,llama-7b,128,1,2160.0,1.0,1,A100,1697548587219,1697548589379,120,9.0,1.0,"[8, 2152]","[1697548587227, 1697548589379]"
308,308,535,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603857,1697548605265,120,,,"[44, 1169]","[1697548603901, 1697548605070]"
309,309,625,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548587219,1697548589558,120,,,"[17, 2143]","[1697548587236, 1697548589379]"
310,310,189,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592063,120,,,"[184, 1833]","[1697548589758, 1697548591591]"
311,311,894,14,[],200,llama-7b,128,1,1951.0,1.0,1,A100,1697548605280,1697548607231,120,14.0,1.0,"[358, 1593]","[1697548605638, 1697548607231]"
312,312,542,6,[],200,llama-7b,128,1,2086.0,1.0,1,A100,1697548592073,1697548594159,120,11.0,1.0,"[273, 1813]","[1697548592346, 1697548594159]"
313,313,900,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594164,1697548597361,120,,,"[37, 725, 1420, 72, 58]","[1697548594201, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
314,314,327,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608967,120,,,"[51, 1483]","[1697548607288, 1697548608771]"
315,315,305,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597372,1697548599943,120,,,"[514, 1833]","[1697548597886, 1697548599719]"
316,316,54,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589575,1697548592062,120,,,"[309, 1705]","[1697548589884, 1697548591589]"
317,317,755,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583927,1697548586198,120,,,"[248, 1782]","[1697548584175, 1697548585957]"
318,318,666,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,"[445, 1489]","[1697548600398, 1697548601887]"
319,319,415,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,[162],[1697548592235]
320,320,97,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605265,120,,,"[28, 1696, 264, 77, 77]","[1697548602155, 1697548603851, 1697548604115, 1697548604192, 1697548604269]"
321,321,659,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610834,120,,,[302],[1697548609277]
322,322,694,0,[],200,llama-7b,128,1,4497.0,1.0,1,A100,1697548575931,1697548580428,120,161.0,13.0,"[150, 1220, 1824, 93, 93, 69, 85, 86, 502, 96, 95, 90, 94]","[1697548576081, 1697548577301, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579465, 1697548579551, 1697548580053, 1697548580149, 1697548580244, 1697548580334, 1697548580428]"
323,323,184,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586204,1697548589558,120,,,"[16, 1951, 218, 79, 78, 76]","[1697548586220, 1697548588171, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
324,324,34,7,[],200,llama-7b,128,1,775.0,1.0,1,A100,1697548589384,1697548590159,120,12.0,1.0,"[12, 763]","[1697548589396, 1697548590159]"
325,325,20,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588175,1697548589559,120,,,"[49, 1155]","[1697548588224, 1697548589379]"
326,326,88,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610851,1697548612997,120,,,"[239, 1725]","[1697548611090, 1697548612815]"
327,327,454,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608964,120,,,"[179, 1770, 507, 79, 79, 78]","[1697548605462, 1697548607232, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
328,328,378,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592062,120,,,[86],[1697548589660]
329,329,442,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615232,120,,,"[45, 1980]","[1697548613051, 1697548615031]"
330,330,547,5,[],200,llama-7b,128,1,2013.0,1.0,1,A100,1697548589578,1697548591591,120,12.0,1.0,"[388, 1625]","[1697548589966, 1697548591591]"
331,331,797,19,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548615243,1697548617266,120,26.0,1.0,"[196, 1827]","[1697548615439, 1697548617266]"
332,332,804,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610824,120,,,[32],[1697548609005]
333,333,745,6,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548594350,1697548596097,120,17.0,1.0,"[95, 1652]","[1697548594445, 1697548596097]"
334,334,913,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548577244,1697548581083,120,,,"[21, 1174, 687, 93, 92, 70, 86, 84, 502, 96, 95, 90, 94, 86, 84]","[1697548577265, 1697548578439, 1697548579126, 1697548579219, 1697548579311, 1697548579381, 1697548579467, 1697548579551, 1697548580053, 1697548580149, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580598]"
335,335,321,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610847,1697548612996,120,,,"[204, 1766]","[1697548611051, 1697548612817]"
336,336,226,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617269,1697548620354,120,,,"[16, 671, 1440, 84, 82, 80, 81]","[1697548617285, 1697548617956, 1697548619396, 1697548619480, 1697548619562, 1697548619642, 1697548619723]"
337,337,392,8,[],200,llama-7b,128,1,2688.0,1.0,1,A100,1697548590163,1697548592851,120,20.0,1.0,"[14, 2674]","[1697548590177, 1697548592851]"
338,338,432,4,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548589568,1697548591591,120,13.0,1.0,"[393, 1630]","[1697548589961, 1697548591591]"
339,339,880,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591595,1697548594347,120,,,"[66, 1191]","[1697548591661, 1697548592852]"
340,340,311,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594351,1697548597359,120,,,"[118, 1628, 251, 72, 56]","[1697548594469, 1697548596097, 1697548596348, 1697548596420, 1697548596476]"
341,341,170,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596101,1697548597359,120,,,"[63, 1171]","[1697548596164, 1697548597335]"
342,342,342,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583920,120,,,"[344, 1372, 109, 102, 88, 88, 87, 84]","[1697548581438, 1697548582810, 1697548582919, 1697548583021, 1697548583109, 1697548583197, 1697548583284, 1697548583368]"
343,343,669,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599941,120,,,"[393, 1957]","[1697548597760, 1697548599717]"
344,344,699,3,[],200,llama-7b,128,1,2032.0,1.0,1,A100,1697548583925,1697548585957,120,39.0,1.0,"[31, 2001]","[1697548583956, 1697548585957]"
345,345,751,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592854,1697548597360,120,,,"[23, 3469, 72, 57]","[1697548592877, 1697548596346, 1697548596418, 1697548596475]"
346,346,94,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[291, 1643]","[1697548600243, 1697548601886]"
347,347,101,4,[],200,llama-7b,128,1,1250.0,1.0,1,A100,1697548585963,1697548587213,120,13.0,1.0,"[47, 1203]","[1697548586010, 1697548587213]"
348,348,524,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599944,120,,,"[195, 2155]","[1697548597562, 1697548599717]"
349,349,172,10,[],200,llama-7b,128,1,2346.0,1.0,1,A100,1697548597372,1697548599718,120,19.0,1.0,"[492, 1854]","[1697548597864, 1697548599718]"
350,350,448,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602120,1697548605265,120,,,"[30, 1701, 264, 77, 77]","[1697548602150, 1697548603851, 1697548604115, 1697548604192, 1697548604269]"
351,351,458,5,[],200,llama-7b,128,1,2161.0,1.0,1,A100,1697548587220,1697548589381,120,11.0,1.0,"[43, 2118]","[1697548587263, 1697548589381]"
352,352,617,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599722,1697548602107,120,,,"[74, 744]","[1697548599796, 1697548600540]"
353,353,812,6,[],200,llama-7b,128,1,771.0,1.0,1,A100,1697548589388,1697548590159,120,16.0,1.0,"[63, 708]","[1697548589451, 1697548590159]"
354,354,236,2,[],200,llama-7b,128,1,2807.0,1.0,1,A100,1697548581095,1697548583902,120,8.0,1.0,"[386, 2420]","[1697548581481, 1697548583901]"
355,355,47,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602123,1697548605265,120,,,"[37, 1692, 263, 77, 77]","[1697548602160, 1697548603852, 1697548604115, 1697548604192, 1697548604269]"
356,356,566,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583905,1697548586200,120,,,"[16, 2035]","[1697548583921, 1697548585956]"
357,357,695,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548580516,1697548583921,120,,,"[10, 943, 1448, 102, 90, 88, 86, 83]","[1697548580526, 1697548581469, 1697548582917, 1697548583019, 1697548583109, 1697548583197, 1697548583283, 1697548583366]"
358,358,38,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583922,120,,,"[164, 1556, 109, 100, 90, 89, 86, 83]","[1697548581254, 1697548582810, 1697548582919, 1697548583019, 1697548583109, 1697548583198, 1697548583284, 1697548583367]"
359,359,883,9,[],200,llama-7b,128,1,1934.0,1.0,1,A100,1697548599953,1697548601887,120,563.0,1.0,"[425, 1509]","[1697548600378, 1697548601887]"
360,360,246,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548590161,1697548594346,120,,,"[6, 2683]","[1697548590167, 1697548592850]"
361,361,315,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601891,1697548605271,120,,,"[68, 716, 1439, 78, 76]","[1697548601959, 1697548602675, 1697548604114, 1697548604192, 1697548604268]"
362,362,401,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583933,1697548589557,120,,,"[363, 2917, 1176, 79, 78, 75]","[1697548584296, 1697548587213, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
363,363,409,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605284,1697548608962,120,,,"[351, 1596, 508, 79, 79, 78]","[1697548605635, 1697548607231, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
364,364,604,8,[],200,llama-7b,128,1,2127.0,1.0,1,A100,1697548594350,1697548596477,120,161.0,4.0,"[10, 1736, 250, 73, 58]","[1697548594360, 1697548596096, 1697548596346, 1697548596419, 1697548596477]"
365,365,729,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592066,120,,,[301],[1697548589869]
366,366,160,4,[],200,llama-7b,128,1,2086.0,1.0,1,A100,1697548592076,1697548594162,120,13.0,1.0,"[372, 1714]","[1697548592448, 1697548594162]"
367,367,769,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610835,120,,,[193],[1697548609166]
368,368,517,5,[],200,llama-7b,128,1,759.0,1.0,1,A100,1697548594168,1697548594927,120,15.0,1.0,"[58, 701]","[1697548594226, 1697548594927]"
369,369,875,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594932,1697548597357,120,,,"[30, 2372]","[1697548594962, 1697548597334]"
370,370,213,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583927,1697548586199,120,,,"[223, 1806]","[1697548584150, 1697548585956]"
371,371,170,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610849,1697548612999,120,,,"[104, 1863]","[1697548610953, 1697548612816]"
372,372,276,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599940,120,,,[369],[1697548597735]
373,373,571,3,[],200,llama-7b,128,1,2180.0,1.0,1,A100,1697548586210,1697548588390,120,67.0,2.0,"[255, 1707, 217]","[1697548586465, 1697548588172, 1697548588389]"
374,374,524,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613008,1697548615237,120,,,"[236, 1786]","[1697548613244, 1697548615030]"
375,375,924,4,[],200,llama-7b,128,1,989.0,1.0,1,A100,1697548588392,1697548589381,120,9.0,1.0,"[11, 978]","[1697548588403, 1697548589381]"
376,376,351,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589384,1697548592066,120,,,"[33, 741]","[1697548589417, 1697548590158]"
377,377,708,6,[],200,llama-7b,128,1,2084.0,1.0,1,A100,1697548592076,1697548594160,120,140.0,1.0,"[303, 1781]","[1697548592379, 1697548594160]"
378,378,647,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608966,120,,,"[267, 1680, 510, 77, 79, 78]","[1697548605550, 1697548607230, 1697548607740, 1697548607817, 1697548607896, 1697548607974]"
379,379,111,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594164,1697548597361,120,,,"[26, 2156, 72, 58]","[1697548594190, 1697548596346, 1697548596418, 1697548596476]"
380,380,880,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615241,1697548617367,120,,,"[13, 2011]","[1697548615254, 1697548617265]"
381,381,709,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583934,1697548589557,120,,,"[413, 2867, 1175, 79, 78, 75]","[1697548584347, 1697548587214, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
382,382,123,9,[],200,llama-7b,128,1,982.0,1.0,1,A100,1697548596480,1697548597462,120,14.0,1.0,"[6, 976]","[1697548596486, 1697548597462]"
383,383,309,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617379,1697548620359,120,,,"[270, 1637, 110, 84, 83, 80, 81]","[1697548617649, 1697548619286, 1697548619396, 1697548619480, 1697548619563, 1697548619643, 1697548619724]"
384,384,478,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597464,1697548599943,120,,,"[427, 1828]","[1697548597891, 1697548599719]"
385,385,664,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583921,120,,,"[374, 1342, 109, 102, 89, 88, 86, 84]","[1697548581468, 1697548582810, 1697548582919, 1697548583021, 1697548583110, 1697548583198, 1697548583284, 1697548583368]"
386,386,139,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592064,120,,,"[216, 1807]","[1697548589784, 1697548591591]"
387,387,78,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610841,120,,,[72],[1697548609045]
388,388,836,11,[],200,llama-7b,128,1,1934.0,1.0,1,A100,1697548599953,1697548601887,120,11.0,1.0,"[420, 1514]","[1697548600373, 1697548601887]"
389,389,93,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583925,1697548586197,120,,,"[26, 2005]","[1697548583951, 1697548585956]"
390,390,436,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610850,1697548612999,120,,,"[123, 1843]","[1697548610973, 1697548612816]"
391,391,541,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586210,1697548589561,120,,,"[234, 1728, 217, 79, 79, 76]","[1697548586444, 1697548588172, 1697548588389, 1697548588468, 1697548588547, 1697548588623]"
392,392,174,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579469,1697548581082,120,,,"[11, 1584]","[1697548579480, 1697548581064]"
393,393,266,12,[],200,llama-7b,128,1,784.0,1.0,1,A100,1697548601891,1697548602675,120,9.0,1.0,"[68, 716]","[1697548601959, 1697548602675]"
394,394,496,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,"[263, 1823]","[1697548592336, 1697548594159]"
395,395,620,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602679,1697548605264,120,,,"[38, 2352]","[1697548602717, 1697548605069]"
396,396,670,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622597,120,,,"[319, 1640]","[1697548620689, 1697548622329]"
397,397,24,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608963,120,,,"[88, 2367, 79, 80, 77]","[1697548605371, 1697548607738, 1697548607817, 1697548607897, 1697548607974]"
398,398,383,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610834,120,,,[168],[1697548609141]
399,399,851,7,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548594356,1697548596098,120,23.0,1.0,"[324, 1418]","[1697548594680, 1697548596098]"
400,400,744,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610847,1697548612999,120,,,"[121, 1848]","[1697548610968, 1697548612816]"
401,401,285,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596102,1697548597358,120,,,"[77, 1155]","[1697548596179, 1697548597334]"
402,402,169,17,[],200,llama-7b,128,1,2024.0,1.0,1,A100,1697548613008,1697548615032,120,10.0,1.0,"[78, 1946]","[1697548613086, 1697548615032]"
403,403,791,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613008,1697548615233,120,,,[73],[1697548613081]
404,404,900,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592065,120,,,"[219, 1798]","[1697548589793, 1697548591591]"
405,405,216,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617364,120,,,"[136, 1888]","[1697548615378, 1697548617266]"
406,406,613,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599942,120,,,"[123, 2227]","[1697548597489, 1697548599716]"
407,407,332,5,[],200,llama-7b,128,1,2084.0,1.0,1,A100,1697548592076,1697548594160,120,39.0,1.0,"[310, 1774]","[1697548592386, 1697548594160]"
408,408,499,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615037,1697548617367,120,,,"[102, 734]","[1697548615139, 1697548615873]"
409,409,853,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620357,120,,,"[94, 1813, 113, 83, 83, 80, 80]","[1697548617472, 1697548619285, 1697548619398, 1697548619481, 1697548619564, 1697548619644, 1697548619724]"
410,410,686,6,[],200,llama-7b,128,1,762.0,1.0,1,A100,1697548594164,1697548594926,120,31.0,1.0,"[22, 740]","[1697548594186, 1697548594926]"
411,411,116,7,[],200,llama-7b,128,1,2405.0,1.0,1,A100,1697548594928,1697548597333,120,23.0,1.0,"[7, 2398]","[1697548594935, 1697548597333]"
412,412,345,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579958,1697548583919,120,,,"[19, 1492, 1448, 102, 89, 88, 86, 84]","[1697548579977, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583282, 1697548583366]"
413,413,283,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622600,120,,,"[181, 1782]","[1697548620549, 1697548622331]"
414,414,441,8,[],200,llama-7b,128,1,126.0,1.0,1,A100,1697548597336,1697548597462,120,6.0,1.0,"[15, 111]","[1697548597351, 1697548597462]"
415,415,645,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622608,1697548624958,120,,,"[285, 1786]","[1697548622893, 1697548624679]"
416,416,781,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579383,1697548581082,120,,,"[12, 1669]","[1697548579395, 1697548581064]"
417,417,802,9,[],200,llama-7b,128,1,2254.0,1.0,1,A100,1697548597465,1697548599719,120,9.0,1.0,"[419, 1835]","[1697548597884, 1697548599719]"
418,418,77,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627101,120,,,"[212, 1834]","[1697548625186, 1697548627020]"
419,419,407,23,[],200,llama-7b,128,1,2232.0,1.0,1,A100,1697548627106,1697548629338,120,16.0,1.0,"[78, 2153]","[1697548627184, 1697548629337]"
420,420,765,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629348,1697548631886,120,,,[20],[1697548629368]
421,421,233,10,[],200,llama-7b,128,1,818.0,1.0,1,A100,1697548599723,1697548600541,120,6.0,1.0,"[81, 736]","[1697548599804, 1697548600540]"
422,422,590,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548600544,1697548605272,120,,,"[15, 2116, 1438, 78, 76]","[1697548600559, 1697548602675, 1697548604113, 1697548604191, 1697548604267]"
423,423,185,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633848,120,,,[287],[1697548632188]
424,424,544,26,[],200,llama-7b,128,1,1745.0,1.0,1,A100,1697548633853,1697548635598,120,26.0,1.0,"[111, 1633]","[1697548633964, 1697548635597]"
425,425,499,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548580247,1697548583920,120,,,"[11, 1211, 1448, 102, 89, 88, 87, 83]","[1697548580258, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583366]"
426,426,902,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635603,1697548639636,120,,,"[34, 872, 1222, 417]","[1697548635637, 1697548636509, 1697548637731, 1697548638148]"
427,427,211,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581096,1697548583921,120,,,"[395, 2410]","[1697548581491, 1697548583901]"
428,428,856,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583925,1697548586198,120,,,"[21, 2010]","[1697548583946, 1697548585956]"
429,429,565,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583925,1697548586198,120,,,"[54, 1978]","[1697548583979, 1697548585957]"
430,430,425,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641374,120,,,[31],[1697548639677]
431,431,257,3,[],200,llama-7b,128,1,1962.0,1.0,1,A100,1697548586210,1697548588172,120,14.0,1.0,"[239, 1723]","[1697548586449, 1697548588172]"
432,432,298,3,[],200,llama-7b,128,1,3280.0,1.0,1,A100,1697548583933,1697548587213,120,17.0,1.0,"[403, 2877]","[1697548584336, 1697548587213]"
433,433,536,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575926,1697548581083,120,,,"[20, 211, 24, 1120, 1823, 93, 93, 69, 85, 86, 502, 97, 94, 90, 94, 87, 82]","[1697548575946, 1697548576157, 1697548576181, 1697548577301, 1697548579124, 1697548579217, 1697548579310, 1697548579379, 1697548579464, 1697548579550, 1697548580052, 1697548580149, 1697548580243, 1697548580333, 1697548580427, 1697548580514, 1697548580596]"
434,434,652,4,[],200,llama-7b,128,1,2160.0,1.0,1,A100,1697548587220,1697548589380,120,14.0,1.0,"[34, 2126]","[1697548587254, 1697548589380]"
435,435,785,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642999,120,,,[289],[1697548641674]
436,436,515,1,[],200,llama-7b,128,1,1718.0,1.0,1,A100,1697548581091,1697548582809,120,11.0,1.0,"[261, 1457]","[1697548581352, 1697548582809]"
437,437,865,2,[],200,llama-7b,128,1,1089.0,1.0,1,A100,1697548582814,1697548583903,120,9.0,1.0,"[47, 1042]","[1697548582861, 1697548583903]"
438,438,894,4,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548586205,1697548588171,120,14.0,1.0,"[70, 1896]","[1697548586275, 1697548588171]"
439,439,266,3,[],200,llama-7b,128,1,2051.0,1.0,1,A100,1697548583906,1697548585957,120,9.0,1.0,"[43, 2008]","[1697548583949, 1697548585957]"
440,440,328,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588174,1697548589558,120,,,"[18, 1189]","[1697548588192, 1697548589381]"
441,441,82,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589384,1697548592064,120,,,"[18, 757]","[1697548589402, 1697548590159]"
442,442,624,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585963,1697548589558,120,,,"[98, 1152, 1176, 79, 78, 76]","[1697548586061, 1697548587213, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
443,443,56,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589575,1697548592064,120,,,"[208, 1808]","[1697548589783, 1697548591591]"
444,444,689,6,[],200,llama-7b,128,1,2013.0,1.0,1,A100,1697548589576,1697548591589,120,15.0,1.0,"[283, 1730]","[1697548589859, 1697548591589]"
445,445,213,30,[],200,llama-7b,128,1,3649.0,1.0,1,A100,1697548643008,1697548646657,120,123.0,6.0,"[331, 2398, 578, 94, 91, 89, 68]","[1697548643339, 1697548645737, 1697548646315, 1697548646409, 1697548646500, 1697548646589, 1697548646657]"
446,446,419,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[81, 2006]","[1697548592154, 1697548594160]"
447,447,117,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591593,1697548594347,120,,,"[19, 1239]","[1697548591612, 1697548592851]"
448,448,748,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597361,120,,,"[305, 1685, 73, 58]","[1697548594661, 1697548596346, 1697548596419, 1697548596477]"
449,449,612,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583920,120,,,"[302, 1414, 108, 102, 89, 88, 87, 84]","[1697548581396, 1697548582810, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583284, 1697548583368]"
450,450,691,0,[],200,llama-7b,128,1,2503.0,1.0,1,A100,1697548575936,1697548578439,120,47.0,1.0,"[423, 2080]","[1697548576359, 1697548578439]"
451,451,173,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599939,120,,,"[307, 2043]","[1697548597674, 1697548599717]"
452,452,573,31,[],200,llama-7b,128,1,3375.0,1.0,1,A100,1697548646660,1697548650035,120,874.0,2.0,"[19, 3356]","[1697548646679, 1697548650035]"
453,453,923,32,[],200,llama-7b,128,1,4210.0,1.0,1,A100,1697548650037,1697548654247,120,140.0,6.0,"[10, 3180, 100, 96, 89, 84, 651]","[1697548650047, 1697548653227, 1697548653327, 1697548653423, 1697548653512, 1697548653596, 1697548654247]"
454,454,532,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602108,120,,,"[318, 1613]","[1697548600270, 1697548601883]"
455,455,322,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548654250,1697548657501,120,,,"[6, 1923]","[1697548654256, 1697548656179]"
456,456,228,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575932,1697548581081,120,,,"[333, 2172, 690, 91, 92, 70, 85, 85, 504, 96, 94, 102, 82, 87, 82]","[1697548576265, 1697548578437, 1697548579127, 1697548579218, 1697548579310, 1697548579380, 1697548579465, 1697548579550, 1697548580054, 1697548580150, 1697548580244, 1697548580346, 1697548580428, 1697548580515, 1697548580597]"
457,457,121,1,[],200,llama-7b,128,1,1510.0,1.0,1,A100,1697548578445,1697548579955,120,13.0,1.0,"[78, 1432]","[1697548578523, 1697548579955]"
458,458,479,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579959,1697548583920,120,,,"[39, 1471, 1448, 102, 89, 88, 87, 83]","[1697548579998, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583366]"
459,459,865,1,[],200,llama-7b,128,1,1720.0,1.0,1,A100,1697548581090,1697548582810,120,9.0,1.0,"[70, 1649]","[1697548581160, 1697548582809]"
460,460,841,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583925,1697548586198,120,,,[29],[1697548583954]
461,461,289,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582816,1697548583922,120,,,"[55, 1031]","[1697548582871, 1697548583902]"
462,462,681,34,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548657548,1697548659579,120,23.0,1.0,"[332, 1699]","[1697548657880, 1697548659579]"
463,463,117,35,[],200,llama-7b,128,1,2314.0,1.0,1,A100,1697548659582,1697548661896,120,364.0,2.0,"[21, 1097, 1195]","[1697548659603, 1697548660700, 1697548661895]"
464,464,244,4,[],200,llama-7b,128,1,1969.0,1.0,1,A100,1697548586202,1697548588171,120,9.0,1.0,"[10, 1959]","[1697548586212, 1697548588171]"
465,465,645,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586198,120,,,"[253, 1772]","[1697548584185, 1697548585957]"
466,466,475,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661897,1697548664208,120,,,"[348, 1534]","[1697548662245, 1697548663779]"
467,467,886,10,[],200,llama-7b,128,1,1725.0,1.0,1,A100,1697548602127,1697548603852,120,17.0,1.0,"[48, 1677]","[1697548602175, 1697548603852]"
468,468,316,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603857,1697548605264,120,,,"[31, 1181]","[1697548603888, 1697548605069]"
469,469,805,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667371,120,,,"[273, 1546, 430, 84, 65, 84]","[1697548664491, 1697548666037, 1697548666467, 1697548666551, 1697548666616, 1697548666700]"
470,470,648,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608967,120,,,"[296, 1654, 508, 79, 80, 78]","[1697548605576, 1697548607230, 1697548607738, 1697548607817, 1697548607897, 1697548607975]"
471,471,408,4,[],200,llama-7b,128,1,1138.0,1.0,1,A100,1697548583287,1697548584425,120,16.0,1.0,"[7, 1131]","[1697548583294, 1697548584425]"
472,472,601,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588174,1697548589558,120,,,"[14, 1193]","[1697548588188, 1697548589381]"
473,473,32,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592061,120,,,"[81, 1934]","[1697548589655, 1697548591589]"
474,474,80,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610833,120,,,[312],[1697548609287]
475,475,371,0,[],200,llama-7b,128,1,2503.0,1.0,1,A100,1697548575936,1697548578439,120,13.0,1.0,"[413, 2090]","[1697548576349, 1697548578439]"
476,476,382,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594348,120,,,"[7, 2079]","[1697548592079, 1697548594158]"
477,477,230,38,[],200,llama-7b,128,1,3012.0,1.0,1,A100,1697548667378,1697548670390,120,86.0,5.0,"[286, 2044, 471, 84, 63, 64]","[1697548667664, 1697548669708, 1697548670179, 1697548670263, 1697548670326, 1697548670390]"
478,478,724,1,[],200,llama-7b,128,1,2618.0,1.0,1,A100,1697548578445,1697548581063,120,11.0,1.0,"[90, 2528]","[1697548578535, 1697548581063]"
479,479,737,5,[],200,llama-7b,128,1,3962.0,1.0,1,A100,1697548584427,1697548588389,120,216.0,2.0,"[9, 2778, 1175]","[1697548584436, 1697548587214, 1697548588389]"
480,480,440,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610853,1697548612999,120,,,"[339, 1625]","[1697548611192, 1697548612817]"
481,481,126,2,[],200,llama-7b,128,1,1743.0,1.0,1,A100,1697548581065,1697548582808,120,19.0,1.0,"[24, 1719]","[1697548581089, 1697548582808]"
482,482,740,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597361,120,,,"[325, 1417, 248, 74, 57]","[1697548594681, 1697548596098, 1697548596346, 1697548596420, 1697548596477]"
483,483,480,3,[],200,llama-7b,128,1,1088.0,1.0,1,A100,1697548582814,1697548583902,120,26.0,1.0,"[56, 1032]","[1697548582870, 1697548583902]"
484,484,794,15,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548613009,1697548615032,120,11.0,1.0,"[284, 1739]","[1697548613293, 1697548615032]"
485,485,141,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599941,120,,,"[403, 1948]","[1697548597770, 1697548599718]"
486,486,840,4,[],200,llama-7b,128,1,2051.0,1.0,1,A100,1697548583905,1697548585956,120,17.0,1.0,"[33, 2018]","[1697548583938, 1697548585956]"
487,487,502,10,[],200,llama-7b,128,1,1932.0,1.0,1,A100,1697548599952,1697548601884,120,19.0,1.0,"[328, 1604]","[1697548600280, 1697548601884]"
488,488,272,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585962,1697548589558,120,,,"[102, 1149, 1176, 79, 78, 76]","[1697548586064, 1697548587213, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
489,489,162,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588392,1697548589559,120,,,"[10, 979]","[1697548588402, 1697548589381]"
490,490,223,16,[],200,llama-7b,128,1,835.0,1.0,1,A100,1697548615038,1697548615873,120,16.0,1.0,"[111, 724]","[1697548615149, 1697548615873]"
491,491,524,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592065,120,,,"[230, 1790]","[1697548589798, 1697548591588]"
492,492,665,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615876,1697548620353,120,,,"[16, 2064, 1440, 83, 83, 80, 81]","[1697548615892, 1697548617956, 1697548619396, 1697548619479, 1697548619562, 1697548619642, 1697548619723]"
493,493,777,1,[],200,llama-7b,128,1,1680.0,1.0,1,A100,1697548579384,1697548581064,120,9.0,1.0,"[10, 1670]","[1697548579394, 1697548581064]"
494,494,882,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[106, 1981]","[1697548592179, 1697548594160]"
495,495,716,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592062,120,,,[96],[1697548589670]
496,496,314,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597357,120,,,"[29, 1717, 251, 72, 58]","[1697548594379, 1697548596096, 1697548596347, 1697548596419, 1697548596477]"
497,497,150,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,[16],[1697548592089]
498,498,206,2,[],200,llama-7b,128,1,1743.0,1.0,1,A100,1697548581065,1697548582808,120,16.0,1.0,"[18, 1725]","[1697548581083, 1697548582808]"
499,499,175,0,[],200,llama-7b,128,1,3619.0,1.0,1,A100,1697548575932,1697548579551,120,140.0,8.0,"[136, 1234, 1823, 93, 93, 69, 85, 86]","[1697548576068, 1697548577302, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579465, 1697548579551]"
500,500,97,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622596,120,,,"[28, 1938]","[1697548620390, 1697548622328]"
501,501,503,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597362,120,,,"[329, 1413, 249, 73, 57]","[1697548594685, 1697548596098, 1697548596347, 1697548596420, 1697548596477]"
502,502,565,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582813,1697548583922,120,,,"[20, 1069]","[1697548582833, 1697548583902]"
503,503,534,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579554,1697548581082,120,,,"[8, 1502]","[1697548579562, 1697548581064]"
504,504,861,9,[],200,llama-7b,128,1,2346.0,1.0,1,A100,1697548597372,1697548599718,120,10.0,1.0,"[484, 1862]","[1697548597856, 1697548599718]"
505,505,288,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599721,1697548602107,120,,,"[48, 772]","[1697548599769, 1697548600541]"
506,506,897,4,[],200,llama-7b,128,1,2026.0,1.0,1,A100,1697548583932,1697548585958,120,9.0,1.0,"[310, 1715]","[1697548584242, 1697548585957]"
507,507,620,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605266,120,,,"[138, 1588, 260, 80, 75]","[1697548602265, 1697548603853, 1697548604113, 1697548604193, 1697548604268]"
508,508,72,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583919,120,,,"[255, 1464, 109, 102, 89, 89, 86, 84]","[1697548581344, 1697548582808, 1697548582917, 1697548583019, 1697548583108, 1697548583197, 1697548583283, 1697548583367]"
509,509,331,5,[],200,llama-7b,128,1,1250.0,1.0,1,A100,1697548585963,1697548587213,120,26.0,1.0,"[56, 1194]","[1697548586019, 1697548587213]"
510,510,689,6,[],200,llama-7b,128,1,2161.0,1.0,1,A100,1697548587219,1697548589380,120,15.0,1.0,"[10, 2150]","[1697548587229, 1697548589379]"
511,511,456,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624958,120,,,"[43, 2031]","[1697548622646, 1697548624677]"
512,512,51,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608964,120,,,"[162, 1787, 507, 79, 79, 78]","[1697548605445, 1697548607232, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
513,513,431,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583925,1697548586198,120,,,"[59, 1974]","[1697548583984, 1697548585958]"
514,514,408,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610833,120,,,[190],[1697548609163]
515,515,788,3,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548586205,1697548588171,120,31.0,1.0,"[40, 1925]","[1697548586245, 1697548588170]"
516,516,219,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588174,1697548589558,120,,,"[25, 1183]","[1697548588199, 1697548589382]"
517,517,766,14,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610851,1697548612818,120,11.0,1.0,"[216, 1751]","[1697548611067, 1697548612818]"
518,518,196,15,[],200,llama-7b,128,1,716.0,1.0,1,A100,1697548612823,1697548613539,120,13.0,1.0,"[64, 651]","[1697548612887, 1697548613538]"
519,519,477,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583921,120,,,"[83, 1636, 109, 102, 89, 88, 87, 83]","[1697548581173, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583284, 1697548583367]"
520,520,866,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581088,1697548583922,120,,,"[156, 1566, 108, 103, 88, 89, 86, 83]","[1697548581244, 1697548582810, 1697548582918, 1697548583021, 1697548583109, 1697548583198, 1697548583284, 1697548583367]"
521,521,409,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589577,1697548592062,120,,,"[369, 1644]","[1697548589946, 1697548591590]"
522,522,524,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613541,1697548617364,120,,,"[15, 2316]","[1697548613556, 1697548615872]"
523,523,576,5,[],200,llama-7b,128,1,2014.0,1.0,1,A100,1697548589576,1697548591590,120,14.0,1.0,"[313, 1700]","[1697548589889, 1697548591589]"
524,524,882,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620356,120,,,"[62, 1845, 112, 84, 82, 80, 81]","[1697548617440, 1697548619285, 1697548619397, 1697548619481, 1697548619563, 1697548619643, 1697548619724]"
525,525,908,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591595,1697548594347,120,,,"[42, 1214]","[1697548591637, 1697548592851]"
526,526,389,1,[],200,llama-7b,128,1,1512.0,1.0,1,A100,1697548578443,1697548579955,120,8.0,1.0,"[19, 1492]","[1697548578462, 1697548579954]"
527,527,835,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586199,120,,,"[343, 1684]","[1697548584275, 1697548585959]"
528,528,312,18,[],200,llama-7b,128,1,1962.0,1.0,1,A100,1697548620369,1697548622331,120,23.0,1.0,"[407, 1555]","[1697548620776, 1697548622331]"
529,529,754,2,[],200,llama-7b,128,1,3408.0,1.0,1,A100,1697548579958,1697548583366,120,88.0,7.0,"[14, 2945, 102, 89, 88, 86, 84]","[1697548579972, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583282, 1697548583366]"
530,530,670,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624951,120,,,"[24, 1097]","[1697548622359, 1697548623456]"
531,531,335,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597362,120,,,"[339, 1403, 248, 74, 57]","[1697548594695, 1697548596098, 1697548596346, 1697548596420, 1697548596477]"
532,532,453,1,[],200,llama-7b,128,1,1512.0,1.0,1,A100,1697548578443,1697548579955,120,26.0,1.0,"[39, 1473]","[1697548578482, 1697548579955]"
533,533,185,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579222,1697548581082,120,,,"[18, 1824]","[1697548579240, 1697548581064]"
534,534,430,1,[],200,llama-7b,128,1,1720.0,1.0,1,A100,1697548581090,1697548582810,120,15.0,1.0,"[88, 1631]","[1697548581178, 1697548582809]"
535,535,806,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579958,1697548583920,120,,,"[29, 1482, 1448, 102, 89, 88, 86, 84]","[1697548579987, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583282, 1697548583366]"
536,536,540,2,[],200,llama-7b,128,1,2108.0,1.0,1,A100,1697548581089,1697548583197,120,140.0,5.0,"[272, 1448, 109, 102, 89, 88]","[1697548581361, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197]"
537,537,789,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582813,1697548583921,120,,,"[27, 1062]","[1697548582840, 1697548583902]"
538,538,218,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583926,1697548586198,120,,,[131],[1697548584057]
539,539,545,4,[],200,llama-7b,128,1,2414.0,1.0,1,A100,1697548586210,1697548588624,120,216.0,5.0,"[244, 1718, 217, 79, 79, 76]","[1697548586454, 1697548588172, 1697548588389, 1697548588468, 1697548588547, 1697548588623]"
540,540,899,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588626,1697548592064,120,,,"[6, 1527]","[1697548588632, 1697548590159]"
541,541,185,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583370,1697548586200,120,,,"[9, 1046]","[1697548583379, 1697548584425]"
542,542,328,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[169, 1919]","[1697548592242, 1697548594161]"
543,543,543,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586208,1697548589561,120,,,"[154, 1809, 218, 79, 79, 75]","[1697548586362, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
544,544,687,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594351,1697548597359,120,,,"[200, 1547, 248, 72, 58]","[1697548594551, 1697548596098, 1697548596346, 1697548596418, 1697548596476]"
545,545,897,5,[],200,llama-7b,128,1,2022.0,1.0,1,A100,1697548589568,1697548591590,120,9.0,1.0,"[171, 1851]","[1697548589739, 1697548591590]"
546,546,526,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575938,1697548581082,120,,,"[504, 1995, 688, 93, 93, 70, 85, 85, 501, 97, 94, 91, 94, 86, 83]","[1697548576442, 1697548578437, 1697548579125, 1697548579218, 1697548579311, 1697548579381, 1697548579466, 1697548579551, 1697548580052, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580597]"
547,547,83,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605279,1697548608966,120,,,"[222, 1728, 510, 78, 79, 78]","[1697548605501, 1697548607229, 1697548607739, 1697548607817, 1697548607896, 1697548607974]"
548,548,123,8,[],200,llama-7b,128,1,2347.0,1.0,1,A100,1697548597372,1697548599719,120,14.0,1.0,"[509, 1838]","[1697548597881, 1697548599719]"
549,549,451,9,[],200,llama-7b,128,1,820.0,1.0,1,A100,1697548599720,1697548600540,120,286.0,1.0,"[27, 793]","[1697548599747, 1697548600540]"
550,550,294,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,[50],[1697548591644]
551,551,810,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548600545,1697548605272,120,,,"[19, 2111, 1438, 78, 76]","[1697548600564, 1697548602675, 1697548604113, 1697548604191, 1697548604267]"
552,552,652,7,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548594356,1697548596098,120,14.0,1.0,"[315, 1427]","[1697548594671, 1697548596098]"
553,553,81,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596102,1697548597358,120,,,"[32, 1200]","[1697548596134, 1697548597334]"
554,554,869,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583200,1697548586200,120,,,"[13, 1212]","[1697548583213, 1697548584425]"
555,555,39,0,[],200,llama-7b,128,1,2462.0,1.0,1,A100,1697548575976,1697548578438,120,8.0,1.0,"[496, 1966]","[1697548576472, 1697548578438]"
556,556,424,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575933,1697548581081,120,,,"[327, 2177, 689, 91, 93, 70, 85, 85, 504, 96, 94, 101, 83, 87, 82]","[1697548576260, 1697548578437, 1697548579126, 1697548579217, 1697548579310, 1697548579380, 1697548579465, 1697548579550, 1697548580054, 1697548580150, 1697548580244, 1697548580345, 1697548580428, 1697548580515, 1697548580597]"
557,557,857,16,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610850,1697548612817,120,18.0,1.0,"[123, 1844]","[1697548610973, 1697548612817]"
558,558,614,0,[],200,llama-7b,128,1,2505.0,1.0,1,A100,1697548575933,1697548578438,120,15.0,1.0,"[343, 2162]","[1697548576276, 1697548578438]"
559,559,287,17,[],200,llama-7b,128,1,718.0,1.0,1,A100,1697548612821,1697548613539,120,10.0,1.0,"[61, 656]","[1697548612882, 1697548613538]"
560,560,47,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578443,1697548581084,120,,,"[54, 1457, 100, 96, 95, 100, 83, 87, 81]","[1697548578497, 1697548579954, 1697548580054, 1697548580150, 1697548580245, 1697548580345, 1697548580428, 1697548580515, 1697548580596]"
561,561,638,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613541,1697548617363,120,,,"[6, 2325]","[1697548613547, 1697548615872]"
562,562,400,1,[],200,llama-7b,128,1,2072.0,1.0,1,A100,1697548578443,1697548580515,120,123.0,7.0,"[55, 1456, 100, 96, 95, 100, 83, 87]","[1697548578498, 1697548579954, 1697548580054, 1697548580150, 1697548580245, 1697548580345, 1697548580428, 1697548580515]"
563,563,68,19,[],200,llama-7b,128,1,1908.0,1.0,1,A100,1697548617377,1697548619285,120,12.0,1.0,"[11, 1896]","[1697548617388, 1697548619284]"
564,564,781,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581093,1697548583920,120,,,"[301, 1416, 108, 102, 89, 88, 87, 84]","[1697548581394, 1697548582810, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583284, 1697548583368]"
565,565,396,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619288,1697548620357,120,,,"[7, 1027]","[1697548619295, 1697548620322]"
566,566,376,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583921,120,,,"[80, 1639, 109, 102, 89, 88, 87, 83]","[1697548581170, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583284, 1697548583367]"
567,567,212,2,[],200,llama-7b,128,1,2028.0,1.0,1,A100,1697548583931,1697548585959,120,31.0,1.0,"[138, 1890]","[1697548584069, 1697548585959]"
568,568,540,3,[],200,llama-7b,128,1,2658.0,1.0,1,A100,1697548585963,1697548588621,120,140.0,5.0,"[51, 1198, 1177, 79, 78, 75]","[1697548586014, 1697548587212, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
569,569,895,4,[],200,llama-7b,128,1,1534.0,1.0,1,A100,1697548588625,1697548590159,120,15.0,1.0,"[17, 1517]","[1697548588642, 1697548590159]"
570,570,761,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622599,120,,,"[109, 1853]","[1697548620477, 1697548622330]"
571,571,328,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548590162,1697548594346,120,,,"[10, 2678]","[1697548590172, 1697548592850]"
572,572,685,6,[],200,llama-7b,128,1,1997.0,1.0,1,A100,1697548594351,1697548596348,120,364.0,2.0,"[108, 1638, 251]","[1697548594459, 1697548596097, 1697548596348]"
573,573,191,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622608,1697548624959,120,,,"[350, 1722]","[1697548622958, 1697548624680]"
574,574,116,7,[],200,llama-7b,128,1,987.0,1.0,1,A100,1697548596349,1697548597336,120,23.0,1.0,"[15, 971]","[1697548596364, 1697548597335]"
575,575,444,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597338,1697548599940,120,,,"[49, 557, 36]","[1697548597387, 1697548597944, 1697548597980]"
576,576,549,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627100,120,,,"[102, 1944]","[1697548625076, 1697548627020]"
577,577,802,9,[],200,llama-7b,128,1,1933.0,1.0,1,A100,1697548599952,1697548601885,120,9.0,1.0,"[335, 1597]","[1697548600287, 1697548601884]"
578,578,903,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627105,1697548629598,120,,,"[45, 2187]","[1697548627150, 1697548629337]"
579,579,232,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601889,1697548605272,120,,,"[12, 773, 1439, 78, 76]","[1697548601901, 1697548602674, 1697548604113, 1697548604191, 1697548604267]"
580,580,512,4,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548583926,1697548585959,120,11.0,1.0,"[128, 1904]","[1697548584054, 1697548585958]"
581,581,300,25,[],200,llama-7b,128,1,2017.0,1.0,1,A100,1697548629603,1697548631620,120,9.0,1.0,"[134, 1882]","[1697548629737, 1697548631619]"
582,582,658,26,[],200,llama-7b,128,1,1011.0,1.0,1,A100,1697548631627,1697548632638,120,11.0,1.0,"[68, 942]","[1697548631695, 1697548632637]"
583,583,90,27,[],200,llama-7b,128,1,1790.0,1.0,1,A100,1697548632641,1697548634431,120,19.0,1.0,"[28, 1762]","[1697548632669, 1697548634431]"
584,584,872,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585964,1697548589557,120,,,"[85, 1164, 1176, 79, 78, 75]","[1697548586049, 1697548587213, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
585,585,453,28,[],200,llama-7b,128,1,1164.0,1.0,1,A100,1697548634435,1697548635599,120,26.0,1.0,"[24, 1140]","[1697548634459, 1697548635599]"
586,586,897,29,[],200,llama-7b,128,1,906.0,1.0,1,A100,1697548635603,1697548636509,120,9.0,1.0,"[44, 861]","[1697548635647, 1697548636508]"
587,587,586,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605285,1697548608963,120,,,"[383, 1564, 507, 80, 79, 78]","[1697548605668, 1697548607232, 1697548607739, 1697548607819, 1697548607898, 1697548607976]"
588,588,533,0,[],200,llama-7b,128,1,255.0,1.0,1,A100,1697548575926,1697548576181,120,216.0,2.0,"[13, 242]","[1697548575939, 1697548576181]"
589,589,328,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548636511,1697548639638,120,,,"[10, 1573, 54]","[1697548636521, 1697548638094, 1697548638148]"
590,590,737,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594348,120,,,"[12, 2074]","[1697548592084, 1697548594158]"
591,591,300,6,[],200,llama-7b,128,1,2022.0,1.0,1,A100,1697548589568,1697548591590,120,9.0,1.0,"[114, 1908]","[1697548589682, 1697548591590]"
592,592,861,1,[],200,llama-7b,128,1,2255.0,1.0,1,A100,1697548576184,1697548578439,120,10.0,1.0,"[301, 1953]","[1697548576485, 1697548578438]"
593,593,682,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639651,1697548641381,120,,,[228],[1697548639879]
594,594,901,6,[],200,llama-7b,128,1,1256.0,1.0,1,A100,1697548591596,1697548592852,120,17.0,1.0,"[73, 1183]","[1697548591669, 1697548592852]"
595,595,19,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610825,120,,,[62],[1697548609035]
596,596,301,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592855,1697548597360,120,,,"[32, 2039, 1420, 72, 58]","[1697548592887, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
597,597,292,2,[],200,llama-7b,128,1,1513.0,1.0,1,A100,1697548578442,1697548579955,120,286.0,1.0,"[30, 1483]","[1697548578472, 1697548579955]"
598,598,107,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642997,120,,,[209],[1697548641595]
599,599,745,7,[],200,llama-7b,128,1,1256.0,1.0,1,A100,1697548591597,1697548592853,120,17.0,1.0,"[82, 1173]","[1697548591679, 1697548592852]"
600,600,175,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592855,1697548597360,120,,,"[27, 3464, 72, 58]","[1697548592882, 1697548596346, 1697548596418, 1697548596476]"
601,601,144,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594351,1697548597359,120,,,"[186, 1560, 251, 70, 58]","[1697548594537, 1697548596097, 1697548596348, 1697548596418, 1697548596476]"
602,602,532,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599940,120,,,"[378, 1972]","[1697548597745, 1697548599717]"
603,603,591,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575932,1697548581083,120,,,"[109, 1200, 60, 1824, 93, 92, 70, 85, 85, 503, 96, 94, 91, 94, 86, 82]","[1697548576041, 1697548577241, 1697548577301, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579465, 1697548579550, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
604,604,892,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602108,120,,,"[121, 1811]","[1697548600073, 1697548601884]"
605,605,449,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548577245,1697548581083,120,,,"[26, 2683, 99, 96, 95, 90, 94, 86, 82]","[1697548577271, 1697548579954, 1697548580053, 1697548580149, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
606,606,649,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579959,1697548583920,120,,,"[33, 1477, 1448, 102, 89, 88, 87, 83]","[1697548579992, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583366]"
607,607,71,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583922,120,,,"[179, 1542, 108, 100, 90, 89, 86, 82]","[1697548581269, 1697548582811, 1697548582919, 1697548583019, 1697548583109, 1697548583198, 1697548583284, 1697548583366]"
608,608,80,4,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548583926,1697548585959,120,13.0,1.0,"[153, 1880]","[1697548584079, 1697548585959]"
609,609,502,9,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597367,1697548599717,120,19.0,1.0,"[215, 2135]","[1697548597582, 1697548599717]"
610,610,660,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599944,120,,,"[196, 2154]","[1697548597562, 1697548599716]"
611,611,89,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,"[482, 1453]","[1697548600435, 1697548601888]"
612,612,291,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602128,1697548605266,120,,,"[233, 1753, 78, 76]","[1697548602361, 1697548604114, 1697548604192, 1697548604268]"
613,613,860,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599719,1697548602106,120,,,"[18, 802]","[1697548599737, 1697548600539]"
614,614,445,10,[],200,llama-7b,128,1,1988.0,1.0,1,A100,1697548602127,1697548604115,120,457.0,2.0,"[43, 1682, 263]","[1697548602170, 1697548603852, 1697548604115]"
615,615,281,11,[],200,llama-7b,128,1,1726.0,1.0,1,A100,1697548602127,1697548603853,120,23.0,1.0,"[117, 1609]","[1697548602244, 1697548603853]"
616,616,641,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603858,1697548605266,120,,,"[58, 1154]","[1697548603916, 1697548605070]"
617,617,156,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608963,120,,,"[112, 1839, 507, 80, 79, 78]","[1697548605392, 1697548607231, 1697548607738, 1697548607818, 1697548607897, 1697548607975]"
618,618,892,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548604117,1697548605271,120,,,"[10, 943]","[1697548604127, 1697548605070]"
619,619,21,1,[],200,llama-7b,128,1,1720.0,1.0,1,A100,1697548581089,1697548582809,120,15.0,1.0,"[49, 1671]","[1697548581138, 1697548582809]"
620,620,645,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608965,120,,,"[185, 1767, 507, 79, 79, 78]","[1697548605465, 1697548607232, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
621,621,320,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608964,120,,,"[154, 2301, 80, 79, 78]","[1697548605437, 1697548607738, 1697548607818, 1697548607897, 1697548607975]"
622,622,408,5,[],200,llama-7b,128,1,1250.0,1.0,1,A100,1697548585963,1697548587213,120,16.0,1.0,"[93, 1157]","[1697548586056, 1697548587213]"
623,623,375,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582813,1697548583921,120,,,[28],[1697548582841]
624,624,762,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548587220,1697548589558,120,,,"[48, 2113]","[1697548587268, 1697548589381]"
625,625,80,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610836,120,,,[379],[1697548609354]
626,626,705,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583926,1697548586198,120,,,[111],[1697548584037]
627,627,196,7,[],200,llama-7b,128,1,2016.0,1.0,1,A100,1697548589575,1697548591591,120,13.0,1.0,"[193, 1823]","[1697548589768, 1697548591591]"
628,628,553,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,"[55, 1202]","[1697548591649, 1697548592851]"
629,629,680,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610825,120,,,[62],[1697548609035]
630,630,777,10,[],200,llama-7b,128,1,1951.0,1.0,1,A100,1697548605279,1697548607230,120,9.0,1.0,"[36, 1914]","[1697548605315, 1697548607229]"
631,631,109,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548612999,120,,,"[325, 1639]","[1697548611177, 1697548612816]"
632,632,914,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594351,1697548597359,120,,,"[133, 1613, 251, 72, 56]","[1697548594484, 1697548596097, 1697548596348, 1697548596420, 1697548596476]"
633,633,211,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608967,120,,,"[66, 1467]","[1697548607303, 1697548608770]"
634,634,437,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610850,1697548612995,120,,,"[138, 1829]","[1697548610988, 1697548612817]"
635,635,470,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613009,1697548615232,120,,,"[290, 1733]","[1697548613299, 1697548615032]"
636,636,572,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610834,120,,,[394],[1697548609369]
637,637,312,10,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597368,1697548599718,120,23.0,1.0,"[372, 1977]","[1697548597740, 1697548599717]"
638,638,798,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615231,120,,,[14],[1697548613020]
639,639,797,16,[],200,llama-7b,128,1,2024.0,1.0,1,A100,1697548615243,1697548617267,120,26.0,1.0,"[215, 1808]","[1697548615458, 1697548617266]"
640,640,389,0,[],200,llama-7b,128,1,1310.0,1.0,1,A100,1697548575932,1697548577242,120,8.0,1.0,"[116, 1194]","[1697548576048, 1697548577242]"
641,641,557,1,[],200,llama-7b,128,1,2281.0,1.0,1,A100,1697548576158,1697548578439,120,31.0,1.0,"[380, 1901]","[1697548576538, 1697548578439]"
642,642,916,2,[],200,llama-7b,128,1,2618.0,1.0,1,A100,1697548578446,1697548581064,120,8.0,1.0,"[108, 2510]","[1697548578554, 1697548581064]"
643,643,43,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[338, 1594]","[1697548600290, 1697548601884]"
644,644,734,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583370,1697548586200,120,,,"[9, 1046]","[1697548583379, 1697548584425]"
645,645,924,12,[],200,llama-7b,128,1,1949.0,1.0,1,A100,1697548605283,1697548607232,120,9.0,1.0,"[157, 1792]","[1697548605440, 1697548607232]"
646,646,166,5,[],200,llama-7b,128,1,1962.0,1.0,1,A100,1697548586211,1697548588173,120,14.0,1.0,"[331, 1631]","[1697548586542, 1697548588173]"
647,647,400,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605265,120,,,"[132, 1594, 260, 80, 74]","[1697548602259, 1697548603853, 1697548604113, 1697548604193, 1697548604267]"
648,648,42,2,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548583932,1697548585957,120,10.0,1.0,"[233, 1792]","[1697548584165, 1697548585957]"
649,649,365,0,[],200,llama-7b,128,1,1316.0,1.0,1,A100,1697548575927,1697548577243,120,23.0,1.0,"[139, 1176]","[1697548576066, 1697548577242]"
650,650,233,1,[],200,llama-7b,128,1,2618.0,1.0,1,A100,1697548578445,1697548581063,120,6.0,1.0,"[96, 2522]","[1697548578541, 1697548581063]"
651,651,755,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610832,120,,,[198],[1697548609171]
652,652,865,11,[],200,llama-7b,128,1,785.0,1.0,1,A100,1697548601890,1697548602675,120,9.0,1.0,"[39, 746]","[1697548601929, 1697548602675]"
653,653,233,1,[],200,llama-7b,128,1,2277.0,1.0,1,A100,1697548576161,1697548578438,120,6.0,1.0,"[382, 1895]","[1697548576543, 1697548578438]"
654,654,294,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602677,1697548605263,120,,,[11],[1697548602688]
655,655,558,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578442,1697548581083,120,,,"[15, 1497, 99, 97, 94, 91, 93, 86, 82]","[1697548578457, 1697548579954, 1697548580053, 1697548580150, 1697548580244, 1697548580335, 1697548580428, 1697548580514, 1697548580596]"
656,656,481,1,[],200,llama-7b,128,1,2277.0,1.0,1,A100,1697548576161,1697548578438,120,10.0,1.0,"[319, 1958]","[1697548576480, 1697548578438]"
657,657,561,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581066,1697548583921,120,,,"[25, 1717, 110, 102, 89, 88, 86, 83]","[1697548581091, 1697548582808, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583366]"
658,658,839,2,[],200,llama-7b,128,1,1903.0,1.0,1,A100,1697548578443,1697548580346,120,58.0,5.0,"[24, 1487, 100, 96, 94, 101]","[1697548578467, 1697548579954, 1697548580054, 1697548580150, 1697548580244, 1697548580345]"
659,659,866,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583923,120,,,"[195, 1527, 106, 102, 89, 88, 87, 84]","[1697548581284, 1697548582811, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583367]"
660,660,919,3,[],200,llama-7b,128,1,2026.0,1.0,1,A100,1697548583932,1697548585958,120,14.0,1.0,"[316, 1710]","[1697548584248, 1697548585958]"
661,661,349,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585964,1697548589557,120,,,"[51, 1197, 1177, 79, 78, 75]","[1697548586015, 1697548587212, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
662,662,264,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548580350,1697548583920,120,,,"[11, 1108, 1448, 102, 89, 88, 87, 83]","[1697548580361, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583366]"
663,663,703,5,[],200,llama-7b,128,1,2014.0,1.0,1,A100,1697548589575,1697548591589,120,12.0,1.0,"[279, 1735]","[1697548589854, 1697548591589]"
664,664,135,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,"[65, 1192]","[1697548591659, 1697548592851]"
665,665,581,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594351,1697548597359,120,,,"[191, 1556, 250, 70, 58]","[1697548594542, 1697548596098, 1697548596348, 1697548596418, 1697548596476]"
666,666,298,3,[],200,llama-7b,128,1,2027.0,1.0,1,A100,1697548583932,1697548585959,120,17.0,1.0,"[331, 1695]","[1697548584263, 1697548585958]"
667,667,13,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599943,120,,,"[170, 2180]","[1697548597536, 1697548599716]"
668,668,591,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583927,1697548586199,120,,,"[162, 1870]","[1697548584089, 1697548585959]"
669,669,369,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,"[399, 1533]","[1697548600352, 1697548601885]"
670,670,727,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605264,120,,,"[257, 1466, 263, 77, 76]","[1697548602386, 1697548603852, 1697548604115, 1697548604192, 1697548604268]"
671,671,114,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589384,1697548592066,120,,,"[47, 728]","[1697548589431, 1697548590159]"
672,672,657,4,[],200,llama-7b,128,1,1248.0,1.0,1,A100,1697548585965,1697548587213,120,10.0,1.0,"[114, 1134]","[1697548586079, 1697548587213]"
673,673,469,8,[],200,llama-7b,128,1,2084.0,1.0,1,A100,1697548592076,1697548594160,120,17.0,1.0,"[308, 1776]","[1697548592384, 1697548594160]"
674,674,913,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594164,1697548597360,120,,,"[16, 746, 1420, 72, 58]","[1697548594180, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
675,675,86,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548587220,1697548589558,120,,,"[24, 2136]","[1697548587244, 1697548589380]"
676,676,346,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599940,120,,,"[387, 1963]","[1697548597754, 1697548599717]"
677,677,705,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602107,120,,,[85],[1697548600036]
678,678,437,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589575,1697548592066,120,,,"[284, 1730]","[1697548589859, 1697548591589]"
679,679,138,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605263,120,,,"[333, 1391, 262, 78, 76]","[1697548602462, 1697548603853, 1697548604115, 1697548604193, 1697548604269]"
680,680,492,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605279,1697548608962,120,,,"[13, 1937, 508, 80, 79, 78]","[1697548605292, 1697548607229, 1697548607737, 1697548607817, 1697548607896, 1697548607974]"
681,681,152,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608963,120,,,"[369, 1583, 507, 79, 80, 77]","[1697548605649, 1697548607232, 1697548607739, 1697548607818, 1697548607898, 1697548607975]"
682,682,822,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610823,120,,,[21],[1697548608994]
683,683,482,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610823,120,,,[16],[1697548608989]
684,684,273,4,[],200,llama-7b,128,1,1203.0,1.0,1,A100,1697548588177,1697548589380,120,19.0,1.0,"[82, 1121]","[1697548588259, 1697548589380]"
685,685,180,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583920,120,,,"[294, 1425, 109, 102, 89, 88, 86, 85]","[1697548581384, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583368]"
686,686,255,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617364,120,,,"[120, 1903]","[1697548615363, 1697548617266]"
687,687,344,0,[],200,llama-7b,128,1,2504.0,1.0,1,A100,1697548575936,1697548578440,120,13.0,1.0,"[423, 2081]","[1697548576359, 1697548578440]"
688,688,616,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596107,1697548597359,120,,,"[99, 1129]","[1697548596206, 1697548597335]"
689,689,699,1,[],200,llama-7b,128,1,2618.0,1.0,1,A100,1697548578445,1697548581063,120,39.0,1.0,"[104, 2514]","[1697548578549, 1697548581063]"
690,690,251,15,[],200,llama-7b,128,1,1964.0,1.0,1,A100,1697548610853,1697548612817,120,31.0,1.0,"[329, 1635]","[1697548611182, 1697548612817]"
691,691,127,2,[],200,llama-7b,128,1,2132.0,1.0,1,A100,1697548581065,1697548583197,120,100.0,5.0,"[13, 391, 1449, 102, 88, 89]","[1697548581078, 1697548581469, 1697548582918, 1697548583020, 1697548583108, 1697548583197]"
692,692,629,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589384,1697548592066,120,,,"[42, 733]","[1697548589426, 1697548590159]"
693,693,556,1,[],200,llama-7b,128,1,1512.0,1.0,1,A100,1697548578443,1697548579955,120,9.0,1.0,"[30, 1482]","[1697548578473, 1697548579955]"
694,694,612,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617377,1697548620356,120,,,"[58, 1850, 112, 83, 83, 80, 81]","[1697548617435, 1697548619285, 1697548619397, 1697548619480, 1697548619563, 1697548619643, 1697548619724]"
695,695,913,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579958,1697548583920,120,,,"[35, 1476, 1448, 102, 89, 88, 87, 83]","[1697548579993, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583366]"
696,696,483,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583200,1697548586200,120,,,"[19, 1206]","[1697548583219, 1697548584425]"
697,697,605,16,[],200,llama-7b,128,1,715.0,1.0,1,A100,1697548612824,1697548613539,120,8.0,1.0,"[83, 632]","[1697548612907, 1697548613539]"
698,698,348,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583926,1697548586198,120,,,"[126, 1906]","[1697548584052, 1697548585958]"
699,699,707,4,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548586205,1697548588171,120,8.0,1.0,"[53, 1913]","[1697548586258, 1697548588171]"
700,700,533,1,[],200,llama-7b,128,1,1831.0,1.0,1,A100,1697548581088,1697548582919,120,216.0,2.0,"[174, 1657]","[1697548581262, 1697548582919]"
701,701,107,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588174,1697548589559,120,,,[20],[1697548588194]
702,702,844,4,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548586209,1697548588172,120,10.0,1.0,"[168, 1794]","[1697548586377, 1697548588171]"
703,703,457,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592063,120,,,[396],[1697548589964]
704,704,889,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582922,1697548586199,120,,,"[26, 1477]","[1697548582948, 1697548584425]"
705,705,817,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,"[26, 2060]","[1697548592099, 1697548594159]"
706,706,246,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594355,1697548597360,120,,,"[197, 1546, 248, 72, 58]","[1697548594552, 1697548596098, 1697548596346, 1697548596418, 1697548596476]"
707,707,321,3,[],200,llama-7b,128,1,2342.0,1.0,1,A100,1697548586205,1697548588547,120,182.0,4.0,"[43, 1923, 218, 79, 79]","[1697548586248, 1697548588171, 1697548588389, 1697548588468, 1697548588547]"
708,708,605,9,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597367,1697548599717,120,8.0,1.0,"[287, 2063]","[1697548597654, 1697548599717]"
709,709,128,10,[],200,llama-7b,128,1,821.0,1.0,1,A100,1697548599719,1697548600540,120,9.0,1.0,"[20, 801]","[1697548599739, 1697548600540]"
710,710,243,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588175,1697548589559,120,,,"[48, 1156]","[1697548588223, 1697548589379]"
711,711,485,11,[],200,llama-7b,128,1,3648.0,1.0,1,A100,1697548600543,1697548604191,120,67.0,3.0,"[6, 2126, 1438, 78]","[1697548600549, 1697548602675, 1697548604113, 1697548604191]"
712,712,606,6,[],200,llama-7b,128,1,2022.0,1.0,1,A100,1697548589568,1697548591590,120,9.0,1.0,"[326, 1696]","[1697548589894, 1697548591590]"
713,713,844,12,[],200,llama-7b,128,1,875.0,1.0,1,A100,1697548604195,1697548605070,120,10.0,1.0,"[13, 862]","[1697548604208, 1697548605070]"
714,714,31,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591595,1697548594347,120,,,"[59, 1197]","[1697548591654, 1697548592851]"
715,715,390,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597360,120,,,"[205, 1537, 248, 72, 58]","[1697548594561, 1697548596098, 1697548596346, 1697548596418, 1697548596476]"
716,716,38,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613542,1697548617364,120,,,"[19, 2311]","[1697548613561, 1697548615872]"
717,717,748,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597368,1697548599941,120,,,[387],[1697548597755]
718,718,148,10,[],200,llama-7b,128,1,1934.0,1.0,1,A100,1697548599952,1697548601886,120,16.0,1.0,"[298, 1636]","[1697548600250, 1697548601886]"
719,719,507,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601890,1697548605272,120,,,"[74, 711, 1439, 78, 76]","[1697548601964, 1697548602675, 1697548604114, 1697548604192, 1697548604268]"
720,720,679,4,[],200,llama-7b,128,1,830.0,1.0,1,A100,1697548588551,1697548589381,120,15.0,1.0,"[14, 816]","[1697548588565, 1697548589381]"
721,721,366,18,[],200,llama-7b,128,1,2346.0,1.0,1,A100,1697548617378,1697548619724,120,85.0,6.0,"[67, 1840, 112, 84, 82, 80, 81]","[1697548617445, 1697548619285, 1697548619397, 1697548619481, 1697548619563, 1697548619643, 1697548619724]"
722,722,729,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619727,1697548622595,120,,,[13],[1697548619740]
723,723,57,6,[],200,llama-7b,128,1,2085.0,1.0,1,A100,1697548592076,1697548594161,120,13.0,1.0,"[318, 1767]","[1697548592394, 1697548594161]"
724,724,202,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575932,1697548581085,120,,,"[217, 1092, 60, 1824, 93, 93, 69, 86, 85, 502, 97, 94, 90, 94, 87, 81]","[1697548576149, 1697548577241, 1697548577301, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579466, 1697548579551, 1697548580053, 1697548580150, 1697548580244, 1697548580334, 1697548580428, 1697548580515, 1697548580596]"
725,725,154,20,[],200,llama-7b,128,1,2073.0,1.0,1,A100,1697548622603,1697548624676,120,13.0,1.0,"[23, 2050]","[1697548622626, 1697548624676]"
726,726,419,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594164,1697548597361,120,,,"[57, 706, 1419, 72, 58]","[1697548594221, 1697548594927, 1697548596346, 1697548596418, 1697548596476]"
727,727,39,18,[],200,llama-7b,128,1,1962.0,1.0,1,A100,1697548620368,1697548622330,120,8.0,1.0,"[104, 1858]","[1697548620472, 1697548622330]"
728,728,371,19,[],200,llama-7b,128,1,1121.0,1.0,1,A100,1697548622338,1697548623459,120,13.0,1.0,"[106, 1015]","[1697548622444, 1697548623459]"
729,729,729,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548623462,1697548627100,120,,,[19],[1697548623481]
730,730,512,21,[],200,llama-7b,128,1,1093.0,1.0,1,A100,1697548624682,1697548625775,120,11.0,1.0,"[11, 1082]","[1697548624693, 1697548625775]"
731,731,864,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608963,120,,,"[104, 1844, 508, 79, 79, 78]","[1697548605387, 1697548607231, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
732,732,271,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586205,1697548589560,120,,,"[63, 1903, 218, 79, 79, 75]","[1697548586268, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
733,733,871,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548625778,1697548629597,120,,,"[38, 2122]","[1697548625816, 1697548627938]"
734,734,159,21,[],200,llama-7b,128,1,2240.0,1.0,1,A100,1697548627106,1697548629346,120,31.0,1.0,"[152, 2081]","[1697548627258, 1697548629339]"
735,735,295,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583931,1697548586198,120,,,"[239, 1786]","[1697548584170, 1697548585956]"
736,736,716,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592064,120,,,[205],[1697548589779]
737,737,293,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610833,120,,,[110],[1697548609083]
738,738,145,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[164, 1923]","[1697548592237, 1697548594160]"
739,739,652,14,[],200,llama-7b,128,1,1964.0,1.0,1,A100,1697548610853,1697548612817,120,14.0,1.0,"[339, 1625]","[1697548611192, 1697548612817]"
740,740,501,7,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548594351,1697548596098,120,19.0,1.0,"[123, 1623]","[1697548594474, 1697548596097]"
741,741,52,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612822,1697548615231,120,,,"[79, 638]","[1697548612901, 1697548613539]"
742,742,749,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599939,120,,,"[286, 2063]","[1697548597653, 1697548599716]"
743,743,855,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596102,1697548597358,120,,,"[55, 1178]","[1697548596157, 1697548597335]"
744,744,517,22,[],200,llama-7b,128,1,921.0,1.0,1,A100,1697548629352,1697548630273,120,15.0,1.0,"[52, 869]","[1697548629404, 1697548630273]"
745,745,874,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548630276,1697548635893,120,,,"[25, 2334, 1250, 57, 547]","[1697548630301, 1697548632635, 1697548633885, 1697548633942, 1697548634489]"
746,746,413,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617364,120,,,"[195, 1829]","[1697548615437, 1697548617266]"
747,747,284,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599939,120,,,[291],[1697548597658]
748,748,276,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635913,1697548637694,120,,,[54],[1697548635967]
749,749,769,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617377,1697548620357,120,,,"[74, 1834, 112, 84, 82, 80, 81]","[1697548617451, 1697548619285, 1697548619397, 1697548619481, 1697548619563, 1697548619643, 1697548619724]"
750,750,633,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639645,120,,,"[48, 1548]","[1697548637750, 1697548639298]"
751,751,614,10,[],200,llama-7b,128,1,1932.0,1.0,1,A100,1697548599952,1697548601884,120,15.0,1.0,"[325, 1607]","[1697548600277, 1697548601884]"
752,752,63,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641376,120,,,[323],[1697548639978]
753,753,50,11,[],200,llama-7b,128,1,2378.0,1.0,1,A100,1697548601890,1697548604268,120,90.0,4.0,"[49, 736, 1439, 78, 76]","[1697548601939, 1697548602675, 1697548604114, 1697548604192, 1697548604268]"
754,754,421,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642998,120,,,[274],[1697548641659]
755,755,774,28,[],200,llama-7b,128,1,2724.0,1.0,1,A100,1697548643013,1697548645737,120,8.0,1.0,"[339, 2385]","[1697548643352, 1697548645737]"
756,756,180,29,[],200,llama-7b,128,1,5681.0,1.0,1,A100,1697548645740,1697548651421,120,123.0,12.0,"[16, 2682, 600, 92, 88, 86, 730, 87, 85, 75, 74, 74, 992]","[1697548645756, 1697548648438, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421]"
757,757,197,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622597,120,,,"[386, 1574]","[1697548620756, 1697548622330]"
758,758,313,1,[],200,llama-7b,128,1,1122.0,1.0,1,A100,1697548580348,1697548581470,120,20.0,1.0,"[8, 1113]","[1697548580356, 1697548581469]"
759,759,828,0,[],200,llama-7b,128,1,3533.0,1.0,1,A100,1697548575932,1697548579465,120,182.0,6.0,"[338, 2855, 93, 92, 70, 85]","[1697548576270, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579465]"
760,760,643,19,[],200,llama-7b,128,1,2071.0,1.0,1,A100,1697548622608,1697548624679,120,18.0,1.0,"[279, 1792]","[1697548622887, 1697548624679]"
761,761,670,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581472,1697548583921,120,,,"[21, 2408]","[1697548581493, 1697548583901]"
762,762,887,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583920,120,,,"[282, 1438, 109, 102, 89, 88, 86, 84]","[1697548581371, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
763,763,102,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583931,1697548586199,120,,,"[209, 1816]","[1697548584140, 1697548585956]"
764,764,228,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579469,1697548581082,120,,,"[6, 1589]","[1697548579475, 1697548581064]"
765,765,69,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624683,1697548627102,120,,,"[46, 1045]","[1697548624729, 1697548625774]"
766,766,591,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583923,120,,,"[193, 1529, 106, 102, 89, 88, 87, 84]","[1697548581282, 1697548582811, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583367]"
767,767,429,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[83, 2149]","[1697548627189, 1697548629338]"
768,768,16,3,[],200,llama-7b,128,1,3281.0,1.0,1,A100,1697548583933,1697548587214,120,9.0,1.0,"[417, 2864]","[1697548584350, 1697548587214]"
769,769,446,9,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597367,1697548599717,120,26.0,1.0,"[190, 2160]","[1697548597557, 1697548599717]"
770,770,373,4,[],200,llama-7b,128,1,2156.0,1.0,1,A100,1697548587225,1697548589381,120,15.0,1.0,"[39, 2117]","[1697548587264, 1697548589381]"
771,771,316,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583926,1697548586198,120,,,"[136, 1896]","[1697548584062, 1697548585958]"
772,772,733,5,[],200,llama-7b,128,1,775.0,1.0,1,A100,1697548589385,1697548590160,120,31.0,1.0,"[47, 728]","[1697548589432, 1697548590160]"
773,773,789,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631888,120,,,"[239, 1770]","[1697548629847, 1697548631617]"
774,774,248,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548590162,1697548594346,120,,,"[15, 2674]","[1697548590177, 1697548592851]"
775,775,220,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633847,120,,,[272],[1697548632168]
776,776,891,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599720,1697548602106,120,,,"[39, 781]","[1697548599759, 1697548600540]"
777,777,679,3,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548586210,1697548588173,120,15.0,1.0,"[326, 1637]","[1697548586536, 1697548588173]"
778,778,606,7,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548594356,1697548596098,120,9.0,1.0,"[305, 1437]","[1697548594661, 1697548596098]"
779,779,104,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588177,1697548589559,120,,,"[77, 1126]","[1697548588254, 1697548589380]"
780,780,339,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575931,1697548581084,120,,,"[156, 1154, 60, 1824, 93, 93, 69, 86, 85, 502, 96, 95, 90, 94, 86, 82]","[1697548576087, 1697548577241, 1697548577301, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579466, 1697548579551, 1697548580053, 1697548580149, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
781,781,322,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602109,1697548605264,120,,,"[31, 1711, 263, 78, 76]","[1697548602140, 1697548603851, 1697548604114, 1697548604192, 1697548604268]"
782,782,36,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596102,1697548597358,120,,,"[40, 1192]","[1697548596142, 1697548597334]"
783,783,697,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583921,120,,,"[73, 1646, 109, 102, 89, 88, 87, 83]","[1697548581163, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583284, 1697548583367]"
784,784,434,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589578,1697548592063,120,,,"[378, 1634]","[1697548589956, 1697548591590]"
785,785,792,6,[],200,llama-7b,128,1,2088.0,1.0,1,A100,1697548592073,1697548594161,120,11.0,1.0,"[98, 1989]","[1697548592171, 1697548594160]"
786,786,220,7,[],200,llama-7b,128,1,2181.0,1.0,1,A100,1697548594165,1697548596346,120,67.0,2.0,"[40, 721, 1420]","[1697548594205, 1697548594926, 1697548596346]"
787,787,393,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599943,120,,,"[174, 2175]","[1697548597541, 1697548599716]"
788,788,579,8,[],200,llama-7b,128,1,987.0,1.0,1,A100,1697548596348,1697548597335,120,19.0,1.0,"[11, 976]","[1697548596359, 1697548597335]"
789,789,908,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597339,1697548599940,120,,,"[115, 490, 36]","[1697548597454, 1697548597944, 1697548597980]"
790,790,336,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602108,120,,,"[131, 1801]","[1697548600083, 1697548601884]"
791,791,696,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602128,1697548605263,120,,,"[248, 1476, 262, 78, 76]","[1697548602376, 1697548603852, 1697548604114, 1697548604192, 1697548604268]"
792,792,753,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[300, 1634]","[1697548600252, 1697548601886]"
793,793,153,11,[],200,llama-7b,128,1,2154.0,1.0,1,A100,1697548602115,1697548604269,120,335.0,4.0,"[60, 1677, 263, 77, 77]","[1697548602175, 1697548603852, 1697548604115, 1697548604192, 1697548604269]"
794,794,100,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583925,1697548586198,120,,,"[56, 1977]","[1697548583981, 1697548585958]"
795,795,646,1,[],200,llama-7b,128,1,1716.0,1.0,1,A100,1697548581094,1697548582810,120,14.0,1.0,"[341, 1375]","[1697548581435, 1697548582810]"
796,796,675,12,[],200,llama-7b,128,1,2695.0,1.0,1,A100,1697548605279,1697548607974,120,563.0,5.0,"[41, 1910, 508, 79, 79, 78]","[1697548605320, 1697548607230, 1697548607738, 1697548607817, 1697548607896, 1697548607974]"
797,797,74,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582814,1697548583922,120,,,"[41, 1047]","[1697548582855, 1697548583902]"
798,798,514,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548604272,1697548608961,120,,,"[6, 1459, 2000, 80, 78, 78]","[1697548604278, 1697548605737, 1697548607737, 1697548607817, 1697548607895, 1697548607973]"
799,799,404,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583927,1697548586199,120,,,"[208, 1824]","[1697548584135, 1697548585959]"
800,800,759,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586205,1697548589560,120,,,"[132, 1834, 218, 79, 79, 75]","[1697548586337, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
801,801,100,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607977,1697548612996,120,,,"[11, 1637, 1243, 569]","[1697548607988, 1697548609625, 1697548610868, 1697548611437]"
802,802,870,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608972,1697548610823,120,,,[12],[1697548608984]
803,803,463,3,[],200,llama-7b,128,1,1962.0,1.0,1,A100,1697548586210,1697548588172,120,39.0,1.0,"[235, 1727]","[1697548586445, 1697548588172]"
804,804,297,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610847,1697548612998,120,,,"[305, 1664]","[1697548611152, 1697548612816]"
805,805,460,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615231,120,,,"[258, 1766]","[1697548613264, 1697548615030]"
806,806,184,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592061,120,,,"[300, 1715]","[1697548589874, 1697548591589]"
807,807,792,15,[],200,llama-7b,128,1,2022.0,1.0,1,A100,1697548615243,1697548617265,120,11.0,1.0,"[238, 1784]","[1697548615481, 1697548617265]"
808,808,821,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588176,1697548589559,120,,,"[63, 1141]","[1697548588239, 1697548589380]"
809,809,225,16,[],200,llama-7b,128,1,687.0,1.0,1,A100,1697548617269,1697548617956,120,23.0,1.0,"[21, 666]","[1697548617290, 1697548617956]"
810,810,542,6,[],200,llama-7b,128,1,2087.0,1.0,1,A100,1697548592073,1697548594160,120,11.0,1.0,"[276, 1810]","[1697548592349, 1697548594159]"
811,811,583,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617959,1697548620357,120,,,"[25, 2338]","[1697548617984, 1697548620322]"
812,812,903,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594164,1697548597361,120,,,"[42, 720, 1420, 72, 58]","[1697548594206, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
813,813,125,12,[],200,llama-7b,128,1,1950.0,1.0,1,A100,1697548605279,1697548607229,120,13.0,1.0,"[6, 1944]","[1697548605285, 1697548607229]"
814,814,424,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599945,120,,,"[276, 2074]","[1697548597643, 1697548599717]"
815,815,8,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622598,120,,,"[92, 1870]","[1697548620460, 1697548622330]"
816,816,782,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599950,1697548602107,120,,,"[9, 1924]","[1697548599959, 1697548601883]"
817,817,367,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624952,120,,,"[166, 1909]","[1697548622770, 1697548624679]"
818,818,486,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608968,120,,,"[100, 1433]","[1697548607337, 1697548608770]"
819,819,246,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589567,1697548592066,120,,,"[9, 2012]","[1697548589576, 1697548591588]"
820,820,601,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592074,1697548594348,120,,,"[282, 1803]","[1697548592356, 1697548594159]"
821,821,1,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594355,1697548597362,120,,,"[345, 1399, 247, 74, 57]","[1697548594700, 1697548596099, 1697548596346, 1697548596420, 1697548596477]"
822,822,692,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624962,1697548627103,120,,,"[35, 2023]","[1697548624997, 1697548627020]"
823,823,207,10,[],200,llama-7b,128,1,1724.0,1.0,1,A100,1697548602130,1697548603854,120,10.0,1.0,"[326, 1397]","[1697548602456, 1697548603853]"
824,824,125,21,[],200,llama-7b,128,1,2208.0,1.0,1,A100,1697548627139,1697548629347,120,13.0,1.0,"[333, 1867]","[1697548627472, 1697548629339]"
825,825,651,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613013,1697548615233,120,,,"[346, 1673]","[1697548613359, 1697548615032]"
826,826,484,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629358,1697548631889,120,,,[86],[1697548629444]
827,827,841,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631903,1697548633850,120,,,[354],[1697548632257]
828,828,566,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603859,1697548605272,120,,,"[67, 1144]","[1697548603926, 1697548605070]"
829,829,360,8,[],200,llama-7b,128,1,2347.0,1.0,1,A100,1697548597372,1697548599719,120,16.0,1.0,"[502, 1844]","[1697548597874, 1697548599718]"
830,830,270,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635903,120,,,"[300, 1442]","[1697548634155, 1697548635597]"
831,831,56,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617364,120,,,"[201, 1822]","[1697548615444, 1697548617266]"
832,832,721,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599721,1697548602107,120,,,"[58, 760]","[1697548599779, 1697548600539]"
833,833,600,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637693,120,,,[240],[1697548636155]
834,834,370,1,[],200,llama-7b,128,1,1720.0,1.0,1,A100,1697548581089,1697548582809,120,31.0,1.0,"[17, 1702]","[1697548581106, 1697548582808]"
835,835,414,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617377,1697548620357,120,,,"[93, 1815, 112, 84, 83, 80, 80]","[1697548617470, 1697548619285, 1697548619397, 1697548619481, 1697548619564, 1697548619644, 1697548619724]"
836,836,153,10,[],200,llama-7b,128,1,2154.0,1.0,1,A100,1697548602115,1697548604269,120,335.0,4.0,"[30, 1706, 263, 78, 77]","[1697548602145, 1697548603851, 1697548604114, 1697548604192, 1697548604269]"
837,837,727,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582813,1697548583921,120,,,"[23, 1066]","[1697548582836, 1697548583902]"
838,838,28,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637701,1697548639636,120,,,"[135, 1464]","[1697548637836, 1697548639300]"
839,839,768,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597358,120,,,"[42, 1704, 251, 72, 58]","[1697548594392, 1697548596096, 1697548596347, 1697548596419, 1697548596477]"
840,840,173,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599941,120,,,"[98, 2252]","[1697548597464, 1697548599716]"
841,841,531,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602108,120,,,"[35, 1897]","[1697548599986, 1697548601883]"
842,842,892,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602128,1697548605265,120,,,"[111, 1614, 262, 78, 76]","[1697548602239, 1697548603853, 1697548604115, 1697548604193, 1697548604269]"
843,843,156,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583926,1697548586198,120,,,"[121, 1911]","[1697548584047, 1697548585958]"
844,844,318,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608964,120,,,"[386, 1563, 507, 80, 79, 78]","[1697548605669, 1697548607232, 1697548607739, 1697548607819, 1697548607898, 1697548607976]"
845,845,600,11,[],200,llama-7b,128,1,1466.0,1.0,1,A100,1697548604272,1697548605738,120,23.0,1.0,"[21, 1445]","[1697548604293, 1697548605738]"
846,846,515,4,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548586210,1697548588173,120,11.0,1.0,"[327, 1636]","[1697548586537, 1697548588173]"
847,847,25,12,[],200,llama-7b,128,1,3029.0,1.0,1,A100,1697548605741,1697548608770,120,12.0,1.0,"[22, 3007]","[1697548605763, 1697548608770]"
848,848,718,1,[],200,llama-7b,128,1,1511.0,1.0,1,A100,1697548578444,1697548579955,120,13.0,1.0,"[63, 1448]","[1697548578507, 1697548579955]"
849,849,870,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588177,1697548589559,120,,,"[71, 1132]","[1697548588248, 1697548589380]"
850,850,383,13,[],200,llama-7b,128,1,853.0,1.0,1,A100,1697548608773,1697548609626,120,15.0,1.0,"[26, 826]","[1697548608799, 1697548609625]"
851,851,150,2,[],200,llama-7b,128,1,2959.0,1.0,1,A100,1697548579958,1697548582917,120,216.0,2.0,"[19, 2940]","[1697548579977, 1697548582917]"
852,852,737,14,[],200,llama-7b,128,1,1810.0,1.0,1,A100,1697548609628,1697548611438,120,216.0,2.0,"[17, 1743, 50]","[1697548609645, 1697548611388, 1697548611438]"
853,853,625,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586198,120,,,"[259, 1766]","[1697548584191, 1697548585957]"
854,854,274,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592064,120,,,"[196, 1827]","[1697548589764, 1697548591591]"
855,855,513,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582921,1697548586199,120,,,"[14, 1490]","[1697548582935, 1697548584425]"
856,856,55,3,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548586205,1697548588171,120,12.0,1.0,"[33, 1932]","[1697548586238, 1697548588170]"
857,857,631,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592074,1697548594348,120,,,[297],[1697548592371]
858,858,170,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548611441,1697548615234,120,,,"[19, 2078]","[1697548611460, 1697548613538]"
859,859,63,8,[],200,llama-7b,128,1,1740.0,1.0,1,A100,1697548594356,1697548596096,120,39.0,1.0,"[211, 1529]","[1697548594567, 1697548596096]"
860,860,419,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596101,1697548597357,120,,,"[22, 1211]","[1697548596123, 1697548597334]"
861,861,871,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586210,1697548589562,120,,,"[254, 1708, 217, 79, 79, 76]","[1697548586464, 1697548588172, 1697548588389, 1697548588468, 1697548588547, 1697548588623]"
862,862,388,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592063,120,,,"[185, 1832]","[1697548589759, 1697548591591]"
863,863,372,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581091,1697548583920,120,,,"[288, 1430, 109, 102, 89, 88, 86, 84]","[1697548581379, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
864,864,25,3,[],200,llama-7b,128,1,2032.0,1.0,1,A100,1697548583925,1697548585957,120,12.0,1.0,"[46, 1986]","[1697548583971, 1697548585957]"
865,865,500,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617368,120,,,"[323, 1699]","[1697548615567, 1697548617266]"
866,866,748,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599942,120,,,[118],[1697548597484]
867,867,861,17,[],200,llama-7b,128,1,1902.0,1.0,1,A100,1697548617387,1697548619289,120,10.0,1.0,"[378, 1524]","[1697548617765, 1697548619289]"
868,868,286,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619295,1697548622594,120,,,[69],[1697548619364]
869,869,646,19,[],200,llama-7b,128,1,2075.0,1.0,1,A100,1697548622604,1697548624679,120,14.0,1.0,"[162, 1913]","[1697548622766, 1697548624679]"
870,870,173,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[323, 1608]","[1697548600275, 1697548601883]"
871,871,763,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610833,120,,,[102],[1697548609075]
872,872,74,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624684,1697548627102,120,,,"[63, 1027]","[1697548624747, 1697548625774]"
873,873,193,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610853,1697548612999,120,,,[344],[1697548611197]
874,874,403,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[168, 2073]","[1697548627274, 1697548629347]"
875,875,532,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602128,1697548605263,120,,,"[238, 1486, 262, 78, 76]","[1697548602366, 1697548603852, 1697548604114, 1697548604192, 1697548604268]"
876,876,547,16,[],200,llama-7b,128,1,2018.0,1.0,1,A100,1697548613014,1697548615032,120,12.0,1.0,"[325, 1693]","[1697548613339, 1697548615032]"
877,877,405,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588174,1697548589558,120,,,"[9, 1198]","[1697548588183, 1697548589381]"
878,878,743,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[92, 1995]","[1697548592165, 1697548594160]"
879,879,762,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631890,120,,,"[284, 1726]","[1697548629892, 1697548631618]"
880,880,190,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633849,120,,,[165],[1697548632061]
881,881,893,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605279,1697548608962,120,,,"[16, 1934, 508, 80, 79, 78]","[1697548605295, 1697548607229, 1697548607737, 1697548607817, 1697548607896, 1697548607974]"
882,882,548,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635903,120,,,"[296, 1446]","[1697548634151, 1697548635597]"
883,883,386,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585963,1697548589557,120,,,"[22, 1230, 1175, 77, 79, 75]","[1697548585985, 1697548587215, 1697548588390, 1697548588467, 1697548588546, 1697548588621]"
884,884,228,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602107,120,,,"[25, 1907]","[1697548599976, 1697548601883]"
885,885,166,7,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548594350,1697548596097,120,14.0,1.0,"[114, 1633]","[1697548594464, 1697548596097]"
886,886,585,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605264,120,,,"[262, 1462, 262, 77, 77]","[1697548602391, 1697548603853, 1697548604115, 1697548604192, 1697548604269]"
887,887,148,1,[],200,llama-7b,128,1,1679.0,1.0,1,A100,1697548579386,1697548581065,120,16.0,1.0,"[15, 1663]","[1697548579401, 1697548581064]"
888,888,734,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589578,1697548592063,120,,,"[391, 1622]","[1697548589969, 1697548591591]"
889,889,907,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637696,120,,,[335],[1697548636251]
890,890,307,26,[],200,llama-7b,128,1,1598.0,1.0,1,A100,1697548637703,1697548639301,120,26.0,1.0,"[166, 1432]","[1697548637869, 1697548639301]"
891,891,43,0,[],200,llama-7b,128,1,4115.0,1.0,1,A100,1697548575938,1697548580053,120,732.0,8.0,"[441, 2058, 688, 93, 93, 69, 86, 85, 501]","[1697548576379, 1697548578437, 1697548579125, 1697548579218, 1697548579311, 1697548579380, 1697548579466, 1697548579551, 1697548580052]"
892,892,526,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596101,1697548597357,120,,,"[17, 1216]","[1697548596118, 1697548597334]"
893,893,666,27,[],200,llama-7b,128,1,8840.0,1.0,1,A100,1697548639306,1697548648146,120,84.0,20.0,"[56, 821, 1231, 365, 1260, 584, 1588, 85, 64, 953, 94, 91, 89, 68, 969, 96, 95, 94, 72, 93, 72]","[1697548639362, 1697548640183, 1697548641414, 1697548641779, 1697548643039, 1697548643623, 1697548645211, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646587, 1697548646655, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647981, 1697548648074, 1697548648146]"
894,894,639,0,[],200,llama-7b,128,1,3538.0,1.0,1,A100,1697548575927,1697548579465,120,100.0,6.0,"[328, 2182, 689, 91, 93, 70, 85]","[1697548576255, 1697548578437, 1697548579126, 1697548579217, 1697548579310, 1697548579380, 1697548579465]"
895,895,15,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608965,120,,,"[211, 1738, 510, 78, 79, 78]","[1697548605491, 1697548607229, 1697548607739, 1697548607817, 1697548607896, 1697548607974]"
896,896,717,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592063,120,,,"[186, 1837]","[1697548589754, 1697548591591]"
897,897,349,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610848,1697548612998,120,,,"[34, 1933]","[1697548610882, 1697548612815]"
898,898,373,1,[],200,llama-7b,128,1,1006.0,1.0,1,A100,1697548580058,1697548581064,120,15.0,1.0,"[6, 1000]","[1697548580064, 1697548581064]"
899,899,464,12,[],200,llama-7b,128,1,2413.0,1.0,1,A100,1697548608975,1697548611388,120,12.0,1.0,"[404, 2009]","[1697548609379, 1697548611388]"
900,900,509,2,[],200,llama-7b,128,1,1953.0,1.0,1,A100,1697548581067,1697548583020,120,286.0,3.0,"[29, 1712, 110, 102]","[1697548581096, 1697548582808, 1697548582918, 1697548583020]"
901,901,727,2,[],200,llama-7b,128,1,2132.0,1.0,1,A100,1697548581065,1697548583197,120,58.0,5.0,"[21, 1722, 110, 102, 88, 89]","[1697548581086, 1697548582808, 1697548582918, 1697548583020, 1697548583108, 1697548583197]"
902,902,152,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583199,1697548586200,120,,,"[12, 1214]","[1697548583211, 1697548584425]"
903,903,868,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583022,1697548586200,120,,,"[7, 1396]","[1697548583029, 1697548584425]"
904,904,71,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579468,1697548581082,120,,,"[7, 1589]","[1697548579475, 1697548581064]"
905,905,511,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586208,1697548589561,120,,,"[153, 1810, 218, 79, 79, 75]","[1697548586361, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
906,906,710,14,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548613006,1697548615031,120,14.0,1.0,"[273, 1752]","[1697548613279, 1697548615031]"
907,907,396,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583921,120,,,"[75, 1644, 109, 102, 89, 88, 87, 83]","[1697548581165, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583284, 1697548583367]"
908,908,95,28,[],200,llama-7b,128,1,1790.0,1.0,1,A100,1697548648150,1697548649940,120,12.0,1.0,"[30, 1760]","[1697548648180, 1697548649940]"
909,909,869,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592063,120,,,"[174, 1842]","[1697548589748, 1697548591590]"
910,910,755,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583926,1697548586198,120,,,"[116, 1916]","[1697548584042, 1697548585958]"
911,911,670,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599720,1697548602106,120,,,"[32, 788]","[1697548599752, 1697548600540]"
912,912,452,29,[],200,llama-7b,128,1,3568.0,1.0,1,A100,1697548649944,1697548653512,120,216.0,4.0,"[37, 3245, 101, 96, 89]","[1697548649981, 1697548653226, 1697548653327, 1697548653423, 1697548653512]"
913,913,518,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610834,120,,,[188],[1697548609161]
914,914,734,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583370,1697548586200,120,,,"[20, 1035]","[1697548583390, 1697548584425]"
915,915,95,12,[],200,llama-7b,128,1,1723.0,1.0,1,A100,1697548602129,1697548603852,120,12.0,1.0,"[247, 1476]","[1697548602376, 1697548603852]"
916,916,1,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610850,1697548612999,120,,,"[128, 1839]","[1697548610978, 1697548612817]"
917,917,0,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548587220,1697548589558,120,,,"[38, 2122]","[1697548587258, 1697548589380]"
918,918,881,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548613000,120,,,"[335, 1630]","[1697548611187, 1697548612817]"
919,919,454,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603857,1697548605264,120,,,"[21, 1191]","[1697548603878, 1697548605069]"
920,920,358,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592061,120,,,"[311, 1710]","[1697548589879, 1697548591589]"
921,921,815,14,[],200,llama-7b,128,1,2614.0,1.0,1,A100,1697548605283,1697548607897,120,52.0,4.0,"[84, 1864, 507, 79, 80]","[1697548605367, 1697548607231, 1697548607738, 1697548607817, 1697548607897]"
922,922,719,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594348,120,,,[17],[1697548592089]
923,923,116,8,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548594356,1697548596098,120,23.0,1.0,"[330, 1412]","[1697548594686, 1697548596098]"
924,924,531,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583921,120,,,"[20, 1699, 110, 102, 89, 88, 86, 83]","[1697548581109, 1697548582808, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583366]"
925,925,163,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586205,1697548589561,120,,,"[147, 1819, 218, 79, 79, 75]","[1697548586352, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
926,926,837,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548577244,1697548581083,120,,,"[17, 1178, 687, 93, 92, 70, 86, 84, 502, 96, 95, 90, 94, 86, 84]","[1697548577261, 1697548578439, 1697548579126, 1697548579219, 1697548579311, 1697548579381, 1697548579467, 1697548579551, 1697548580053, 1697548580149, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580598]"
927,927,885,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583933,1697548586199,120,,,"[335, 1691]","[1697548584268, 1697548585959]"
928,928,71,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624953,120,,,"[238, 1835]","[1697548622842, 1697548624677]"
929,929,309,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613014,1697548615232,120,,,"[326, 1692]","[1697548613340, 1697548615032]"
930,930,432,21,[],200,llama-7b,128,1,2048.0,1.0,1,A100,1697548624974,1697548627022,120,13.0,1.0,"[314, 1734]","[1697548625288, 1697548627022]"
931,931,333,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607899,1697548612995,120,,,"[14, 1712, 1243, 569]","[1697548607913, 1697548609625, 1697548610868, 1697548611437]"
932,932,786,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627026,1697548629598,120,,,"[66, 847]","[1697548627092, 1697548627939]"
933,933,694,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613007,1697548615236,120,,,[173],[1697548613180]
934,934,118,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617369,120,,,"[350, 1672]","[1697548615594, 1697548617266]"
935,935,216,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629603,1697548631886,120,,,"[70, 1945]","[1697548629673, 1697548631618]"
936,936,478,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617386,1697548620354,120,,,"[278, 1622, 111, 83, 83, 80, 81]","[1697548617664, 1697548619286, 1697548619397, 1697548619480, 1697548619563, 1697548619643, 1697548619724]"
937,937,474,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596101,1697548597357,120,,,[16],[1697548596117]
938,938,492,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592067,120,,,"[18, 2002]","[1697548589586, 1697548591588]"
939,939,570,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633844,120,,,[26],[1697548631921]
940,940,663,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617367,120,,,[22],[1697548615265]
941,941,751,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599720,1697548602106,120,,,[34],[1697548599754]
942,942,406,4,[],200,llama-7b,128,1,2342.0,1.0,1,A100,1697548586205,1697548588547,120,244.0,4.0,"[38, 1927, 219, 79, 78]","[1697548586243, 1697548588170, 1697548588389, 1697548588468, 1697548588546]"
943,943,60,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617381,1697548620353,120,,,"[278, 1738, 83, 83, 80, 81]","[1697548617659, 1697548619397, 1697548619480, 1697548619563, 1697548619643, 1697548619724]"
944,944,833,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599941,120,,,"[95, 483, 36]","[1697548597461, 1697548597944, 1697548597980]"
945,945,261,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602108,120,,,[95],[1697548600046]
946,946,619,12,[],200,llama-7b,128,1,1724.0,1.0,1,A100,1697548602130,1697548603854,120,10.0,1.0,"[342, 1382]","[1697548602472, 1697548603854]"
947,947,21,13,[],200,llama-7b,128,1,1213.0,1.0,1,A100,1697548603857,1697548605070,120,15.0,1.0,"[40, 1173]","[1697548603897, 1697548605070]"
948,948,382,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605073,1697548608961,120,,,"[20, 645, 1999, 80, 78, 78]","[1697548605093, 1697548605738, 1697548607737, 1697548607817, 1697548607895, 1697548607973]"
949,949,147,10,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548602109,1697548603851,120,182.0,1.0,"[19, 1723]","[1697548602128, 1697548603851]"
950,950,736,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608976,1697548612997,120,,,"[413, 1999, 50]","[1697548609389, 1697548611388, 1697548611438]"
951,951,763,5,[],200,llama-7b,128,1,832.0,1.0,1,A100,1697548588549,1697548589381,120,20.0,1.0,"[6, 826]","[1697548588555, 1697548589381]"
952,952,165,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613013,1697548615232,120,,,"[276, 1742]","[1697548613289, 1697548615031]"
953,953,501,11,[],200,llama-7b,128,1,1213.0,1.0,1,A100,1697548603857,1697548605070,120,19.0,1.0,"[19, 1193]","[1697548603876, 1697548605069]"
954,954,519,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617364,120,,,"[129, 1894]","[1697548615372, 1697548617266]"
955,955,861,12,[],200,llama-7b,128,1,665.0,1.0,1,A100,1697548605073,1697548605738,120,10.0,1.0,"[37, 628]","[1697548605110, 1697548605738]"
956,956,853,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620356,120,,,"[58, 1849, 112, 84, 82, 80, 81]","[1697548617436, 1697548619285, 1697548619397, 1697548619481, 1697548619563, 1697548619643, 1697548619724]"
957,957,292,13,[],200,llama-7b,128,1,3029.0,1.0,1,A100,1697548605741,1697548608770,120,286.0,1.0,"[17, 3012]","[1697548605758, 1697548608770]"
958,958,196,6,[],200,llama-7b,128,1,775.0,1.0,1,A100,1697548589384,1697548590159,120,13.0,1.0,"[38, 737]","[1697548589422, 1697548590159]"
959,959,651,14,[],200,llama-7b,128,1,2094.0,1.0,1,A100,1697548608774,1697548610868,120,457.0,2.0,"[30, 821, 1243]","[1697548608804, 1697548609625, 1697548610868]"
960,960,282,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622594,120,,,"[218, 1751]","[1697548620581, 1697548622332]"
961,961,903,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635902,120,,,"[24, 554, 58]","[1697548633878, 1697548634432, 1697548634490]"
962,962,171,15,[],200,llama-7b,128,1,1946.0,1.0,1,A100,1697548610871,1697548612817,120,6.0,1.0,"[373, 1573]","[1697548611244, 1697548612817]"
963,963,551,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548590162,1697548594346,120,,,"[6, 2682]","[1697548590168, 1697548592850]"
964,964,643,20,[],200,llama-7b,128,1,2073.0,1.0,1,A100,1697548622603,1697548624676,120,18.0,1.0,"[34, 2039]","[1697548622637, 1697548624676]"
965,965,13,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575927,1697548581082,120,,,"[412, 2099, 687, 93, 92, 70, 85, 85, 504, 97, 94, 96, 87, 86, 83]","[1697548576339, 1697548578438, 1697548579125, 1697548579218, 1697548579310, 1697548579380, 1697548579465, 1697548579550, 1697548580054, 1697548580151, 1697548580245, 1697548580341, 1697548580428, 1697548580514, 1697548580597]"
966,966,266,2,[],200,llama-7b,128,1,1719.0,1.0,1,A100,1697548581089,1697548582808,120,9.0,1.0,"[248, 1471]","[1697548581337, 1697548582808]"
967,967,629,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582822,1697548586199,120,,,"[64, 1539]","[1697548582886, 1697548584425]"
968,968,528,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612821,1697548615235,120,,,"[41, 676]","[1697548612862, 1697548613538]"
969,969,54,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586210,1697548589562,120,,,"[240, 1721, 218, 79, 79, 76]","[1697548586450, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588623]"
970,970,412,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592065,120,,,"[270, 1744]","[1697548589844, 1697548591588]"
971,971,884,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617369,120,,,"[353, 1670]","[1697548615597, 1697548617267]"
972,972,742,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592077,1697548594349,120,,,"[377, 1708]","[1697548592454, 1697548594162]"
973,973,418,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622596,120,,,"[18, 1948]","[1697548620380, 1697548622328]"
974,974,309,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617381,1697548620359,120,,,"[213, 1692, 110, 84, 82, 81, 81]","[1697548617594, 1697548619286, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619724]"
975,975,171,7,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548594357,1697548596099,120,6.0,1.0,"[381, 1361]","[1697548594738, 1697548596099]"
976,976,74,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624682,1697548627102,120,,,"[42, 1050]","[1697548624724, 1697548625774]"
977,977,529,8,[],200,llama-7b,128,1,1233.0,1.0,1,A100,1697548596102,1697548597335,120,10.0,1.0,"[52, 1181]","[1697548596154, 1697548597335]"
978,978,515,22,[],200,llama-7b,128,1,2200.0,1.0,1,A100,1697548627139,1697548629339,120,11.0,1.0,"[323, 1876]","[1697548627462, 1697548629338]"
979,979,777,20,[],200,llama-7b,128,1,2074.0,1.0,1,A100,1697548622604,1697548624678,120,9.0,1.0,"[156, 1918]","[1697548622760, 1697548624678]"
980,980,210,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624682,1697548627101,120,,,"[31, 1061]","[1697548624713, 1697548625774]"
981,981,349,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608967,120,,,"[41, 1493]","[1697548607278, 1697548608771]"
982,982,542,22,[],200,llama-7b,128,1,2201.0,1.0,1,A100,1697548627138,1697548629339,120,11.0,1.0,"[325, 1875]","[1697548627463, 1697548629338]"
983,983,901,23,[],200,llama-7b,128,1,920.0,1.0,1,A100,1697548629353,1697548630273,120,17.0,1.0,"[87, 833]","[1697548629440, 1697548630273]"
984,984,326,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548630276,1697548635893,120,,,"[28, 2331, 1250, 57, 547]","[1697548630304, 1697548632635, 1697548633885, 1697548633942, 1697548634489]"
985,985,667,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622596,120,,,"[285, 1673]","[1697548620655, 1697548622328]"
986,986,706,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610833,120,,,[317],[1697548609292]
987,987,68,20,[],200,llama-7b,128,1,2071.0,1.0,1,A100,1697548622608,1697548624679,120,12.0,1.0,"[284, 1787]","[1697548622892, 1697548624679]"
988,988,432,21,[],200,llama-7b,128,1,1089.0,1.0,1,A100,1697548624685,1697548625774,120,13.0,1.0,"[64, 1025]","[1697548624749, 1697548625774]"
989,989,333,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637696,120,,,[326],[1697548636242]
990,990,133,15,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610849,1697548612816,120,15.0,1.0,"[99, 1868]","[1697548610948, 1697548612816]"
991,991,792,22,[],200,llama-7b,128,1,2162.0,1.0,1,A100,1697548625776,1697548627938,120,11.0,1.0,"[25, 2137]","[1697548625801, 1697548627938]"
992,992,220,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627941,1697548631886,120,,,"[26, 2304]","[1697548627967, 1697548630271]"
993,993,680,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635910,1697548637693,120,,,[29],[1697548635939]
994,994,110,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637700,1697548639636,120,,,"[141, 1459]","[1697548637841, 1697548639300]"
995,995,691,27,[],200,llama-7b,128,1,1598.0,1.0,1,A100,1697548637703,1697548639301,120,47.0,1.0,"[178, 1420]","[1697548637881, 1697548639301]"
996,996,571,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633843,120,,,[19],[1697548631914]
997,997,758,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605285,1697548608964,120,,,"[398, 1550, 506, 80, 79, 78]","[1697548605683, 1697548607233, 1697548607739, 1697548607819, 1697548607898, 1697548607976]"
998,998,442,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641374,120,,,[41],[1697548639687]
999,999,125,28,[],200,llama-7b,128,1,877.0,1.0,1,A100,1697548639306,1697548640183,120,13.0,1.0,"[65, 812]","[1697548639371, 1697548640183]"
1000,1000,597,4,[],200,llama-7b,128,1,1964.0,1.0,1,A100,1697548586208,1697548588172,120,39.0,1.0,"[159, 1804]","[1697548586367, 1697548588171]"
1001,1001,479,29,[],200,llama-7b,128,1,12736.0,1.0,1,A100,1697548640185,1697548652921,120,140.0,36.0,"[5, 1547, 41, 1261, 584, 1588, 85, 64, 954, 94, 91, 89, 68, 968, 96, 95, 94, 72, 93, 72, 890, 93, 88, 86, 731, 86, 85, 76, 74, 74, 991, 91, 74, 859, 319, 88, 70]","[1697548640190, 1697548641737, 1697548641778, 1697548643039, 1697548643623, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649036, 1697548649129, 1697548649217, 1697548649303, 1697548650034, 1697548650120, 1697548650205, 1697548650281, 1697548650355, 1697548650429, 1697548651420, 1697548651511, 1697548651585, 1697548652444, 1697548652763, 1697548652851, 1697548652921]"
1002,1002,801,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643001,120,,,[308],[1697548641694]
1003,1003,159,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610824,120,,,[267],[1697548609241]
1004,1004,805,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548575932,1697548581086,120,,,"[303, 2202, 689, 93, 91, 70, 86, 84, 504, 96, 94, 101, 83, 87, 81]","[1697548576235, 1697548578437, 1697548579126, 1697548579219, 1697548579310, 1697548579380, 1697548579466, 1697548579550, 1697548580054, 1697548580150, 1697548580244, 1697548580345, 1697548580428, 1697548580515, 1697548580596]"
1005,1005,234,29,[],200,llama-7b,128,1,8500.0,1.0,1,A100,1697548643013,1697548651513,120,457.0,25.0,"[329, 2395, 578, 94, 91, 89, 68, 968, 96, 95, 94, 71, 94, 71, 891, 92, 88, 86, 732, 87, 85, 74, 74, 75, 991, 92]","[1697548643342, 1697548645737, 1697548646315, 1697548646409, 1697548646500, 1697548646589, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648075, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650035, 1697548650122, 1697548650207, 1697548650281, 1697548650355, 1697548650430, 1697548651421, 1697548651513]"
1006,1006,513,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610847,1697548613003,120,,,"[136, 1834]","[1697548610983, 1697548612817]"
1007,1007,901,25,[],200,llama-7b,128,1,1744.0,1.0,1,A100,1697548633854,1697548635598,120,17.0,1.0,"[116, 1628]","[1697548633970, 1697548635598]"
1008,1008,875,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613008,1697548615231,120,,,"[246, 1776]","[1697548613254, 1697548615030]"
1009,1009,330,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635603,1697548639636,120,,,"[31, 875, 1222, 417]","[1697548635634, 1697548636509, 1697548637731, 1697548638148]"
1010,1010,691,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641375,120,,,[99],[1697548639745]
1011,1011,125,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642996,120,,,[32],[1697548641416]
1012,1012,453,29,[],200,llama-7b,128,1,1772.0,1.0,1,A100,1697548643005,1697548644777,120,26.0,1.0,"[59, 1713]","[1697548643064, 1697548644777]"
1013,1013,760,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599943,120,,,"[185, 2165]","[1697548597551, 1697548599716]"
1014,1014,807,30,[],200,llama-7b,128,1,6734.0,1.0,1,A100,1697548644780,1697548651514,120,90.0,20.0,"[22, 2273, 550, 96, 96, 93, 72, 93, 71, 891, 92, 89, 86, 731, 87, 85, 75, 74, 74, 991, 93]","[1697548644802, 1697548647075, 1697548647625, 1697548647721, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648146, 1697548649037, 1697548649129, 1697548649218, 1697548649304, 1697548650035, 1697548650122, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651421, 1697548651514]"
1015,1015,186,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602108,120,,,"[391, 1542]","[1697548600343, 1697548601885]"
1016,1016,305,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617368,120,,,"[42, 1981]","[1697548615284, 1697548617265]"
1017,1017,666,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617381,1697548620354,120,,,"[293, 1613, 110, 84, 82, 81, 80]","[1697548617674, 1697548619287, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619724]"
1018,1018,920,1,[],200,llama-7b,128,1,2020.0,1.0,1,A100,1697548581088,1697548583108,120,96.0,4.0,"[191, 1532, 106, 102, 89]","[1697548581279, 1697548582811, 1697548582917, 1697548583019, 1697548583108]"
1019,1019,545,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602130,1697548605263,120,,,"[331, 1392, 262, 78, 76]","[1697548602461, 1697548603853, 1697548604115, 1697548604193, 1697548604269]"
1020,1020,67,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622595,120,,,"[224, 1734]","[1697548620594, 1697548622328]"
1021,1021,842,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605279,1697548608962,120,,,"[23, 1927, 508, 80, 79, 78]","[1697548605302, 1697548607229, 1697548607737, 1697548607817, 1697548607896, 1697548607974]"
1022,1022,19,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586210,1697548589557,120,,,"[342, 1621, 216, 79, 79, 77]","[1697548586552, 1697548588173, 1697548588389, 1697548588468, 1697548588547, 1697548588624]"
1023,1023,843,13,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610849,1697548612816,120,14.0,1.0,"[104, 1863]","[1697548610953, 1697548612816]"
1024,1024,234,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548651517,1697548655269,120,,,"[29, 2347, 355, 96, 84, 62, 81]","[1697548651546, 1697548653893, 1697548654248, 1697548654344, 1697548654428, 1697548654490, 1697548654571]"
1025,1025,899,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605279,1697548608962,120,,,"[21, 1929, 508, 80, 79, 78]","[1697548605300, 1697548607229, 1697548607737, 1697548607817, 1697548607896, 1697548607974]"
1026,1026,275,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612821,1697548615235,120,,,"[35, 682]","[1697548612856, 1697548613538]"
1027,1027,265,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605073,1697548608961,120,,,"[30, 635, 1999, 80, 79, 77]","[1697548605103, 1697548605738, 1697548607737, 1697548607817, 1697548607896, 1697548607973]"
1028,1028,268,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610823,120,,,[11],[1697548608984]
1029,1029,421,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[59, 2015]","[1697548622662, 1697548624677]"
1030,1030,591,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657536,120,,,"[291, 1854]","[1697548655572, 1697548657426]"
1031,1031,21,33,[],200,llama-7b,128,1,2034.0,1.0,1,A100,1697548657545,1697548659579,120,15.0,1.0,"[121, 1912]","[1697548657666, 1697548659578]"
1032,1032,775,20,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624974,1697548627021,120,17.0,1.0,"[177, 1870]","[1697548625151, 1697548627021]"
1033,1033,634,15,[],200,llama-7b,128,1,2024.0,1.0,1,A100,1697548615243,1697548617267,120,13.0,1.0,"[225, 1799]","[1697548615468, 1697548617267]"
1034,1034,470,34,[],200,llama-7b,128,1,2314.0,1.0,1,A100,1697548659582,1697548661896,120,39.0,2.0,"[27, 1092, 1195]","[1697548659609, 1697548660701, 1697548661896]"
1035,1035,205,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627024,1697548629597,120,,,"[43, 871]","[1697548627067, 1697548627938]"
1036,1036,30,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617270,1697548620355,120,,,"[40, 646, 1440, 84, 82, 81, 80]","[1697548617310, 1697548617956, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
1037,1037,693,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597372,1697548599943,120,,,"[482, 1864]","[1697548597854, 1697548599718]"
1038,1038,384,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620369,1697548622596,120,,,"[311, 1649]","[1697548620680, 1697548622329]"
1039,1039,623,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610833,120,,,[100],[1697548609073]
1040,1040,561,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583921,120,,,"[369, 1347, 109, 102, 89, 88, 86, 84]","[1697548581463, 1697548582810, 1697548582919, 1697548583021, 1697548583110, 1697548583198, 1697548583284, 1697548583368]"
1041,1041,24,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610849,1697548612998,120,,,[94],[1697548610943]
1042,1042,566,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631889,120,,,"[259, 1750]","[1697548629867, 1697548631617]"
1043,1043,386,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615232,120,,,"[40, 1985]","[1697548613046, 1697548615031]"
1044,1044,831,35,[],200,llama-7b,128,1,1882.0,1.0,1,A100,1697548661897,1697548663779,120,11.0,1.0,"[358, 1524]","[1697548662255, 1697548663779]"
1045,1045,118,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,"[435, 1499]","[1697548600388, 1697548601887]"
1046,1046,237,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583931,1697548586198,120,,,"[143, 1885]","[1697548584074, 1697548585959]"
1047,1047,749,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617368,120,,,"[33, 1990]","[1697548615275, 1697548617265]"
1048,1048,177,18,[],200,llama-7b,128,1,1902.0,1.0,1,A100,1697548617387,1697548619289,120,14.0,1.0,"[372, 1530]","[1697548617759, 1697548619289]"
1049,1049,569,4,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548586205,1697548588171,120,16.0,1.0,"[137, 1829]","[1697548586342, 1697548588171]"
1050,1050,477,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605263,120,,,"[322, 1402, 262, 77, 77]","[1697548602451, 1697548603853, 1697548604115, 1697548604192, 1697548604269]"
1051,1051,0,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588174,1697548589559,120,,,"[39, 1169]","[1697548588213, 1697548589382]"
1052,1052,530,19,[],200,llama-7b,128,1,1632.0,1.0,1,A100,1697548619292,1697548620924,120,26.0,1.0,"[71, 1561]","[1697548619363, 1697548620924]"
1053,1053,857,20,[],200,llama-7b,128,1,2527.0,1.0,1,A100,1697548620928,1697548623455,120,18.0,1.0,"[11, 2516]","[1697548620939, 1697548623455]"
1054,1054,286,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548623460,1697548627100,120,,,[16],[1697548623476]
1055,1055,809,11,[],200,llama-7b,128,1,1951.0,1.0,1,A100,1697548605279,1697548607230,120,16.0,1.0,"[283, 1668]","[1697548605562, 1697548607230]"
1056,1056,241,12,[],200,llama-7b,128,1,1534.0,1.0,1,A100,1697548607236,1697548608770,120,19.0,1.0,"[72, 1462]","[1697548607308, 1697548608770]"
1057,1057,690,0,[],200,llama-7b,128,1,1310.0,1.0,1,A100,1697548575932,1697548577242,120,39.0,1.0,"[126, 1184]","[1697548576058, 1697548577242]"
1058,1058,600,13,[],200,llama-7b,128,1,852.0,1.0,1,A100,1697548608774,1697548609626,120,23.0,1.0,"[35, 816]","[1697548608809, 1697548609625]"
1059,1059,647,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[147, 2085]","[1697548627253, 1697548629338]"
1060,1060,121,1,[],200,llama-7b,128,1,1193.0,1.0,1,A100,1697548577246,1697548578439,120,13.0,1.0,"[30, 1163]","[1697548577276, 1697548578439]"
1061,1061,0,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610835,120,,,[374],[1697548609349]
1062,1062,493,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588391,1697548589560,120,,,"[6, 984]","[1697548588397, 1697548589381]"
1063,1063,840,2,[],200,llama-7b,128,1,952.0,1.0,1,A100,1697548580518,1697548581470,120,17.0,1.0,"[13, 939]","[1697548580531, 1697548581470]"
1064,1064,25,14,[],200,llama-7b,128,1,1759.0,1.0,1,A100,1697548609629,1697548611388,120,12.0,1.0,"[21, 1738]","[1697548609650, 1697548611388]"
1065,1065,384,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639650,1697548641379,120,,,[193],[1697548639843]
1066,1066,847,5,[],200,llama-7b,128,1,2014.0,1.0,1,A100,1697548589574,1697548591588,120,10.0,1.0,"[265, 1749]","[1697548589839, 1697548591588]"
1067,1067,357,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548612997,120,,,"[235, 1728]","[1697548611087, 1697548612815]"
1068,1068,77,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631890,120,,,"[274, 1736]","[1697548629882, 1697548631618]"
1069,1069,273,6,[],200,llama-7b,128,1,1255.0,1.0,1,A100,1697548591597,1697548592852,120,19.0,1.0,"[77, 1178]","[1697548591674, 1697548592852]"
1070,1070,716,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613011,1697548615237,120,,,[238],[1697548613249]
1071,1071,410,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633848,120,,,[149],[1697548632045]
1072,1072,271,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581472,1697548583921,120,,,"[24, 2406]","[1697548581496, 1697548583902]"
1073,1073,601,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592856,1697548597360,120,,,"[36, 2034, 1420, 72, 58]","[1697548592892, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
1074,1074,482,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578444,1697548581084,120,,,"[73, 1438, 99, 97, 94, 100, 83, 87, 81]","[1697548578517, 1697548579955, 1697548580054, 1697548580151, 1697548580245, 1697548580345, 1697548580428, 1697548580515, 1697548580596]"
1075,1075,764,25,[],200,llama-7b,128,1,1745.0,1.0,1,A100,1697548633855,1697548635600,120,39.0,1.0,"[208, 1537]","[1697548634063, 1697548635600]"
1076,1076,145,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617369,120,,,"[400, 1623]","[1697548615644, 1697548617267]"
1077,1077,194,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635603,1697548639637,120,,,"[61, 845, 1222, 417]","[1697548635664, 1697548636509, 1697548637731, 1697548638148]"
1078,1078,624,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586199,120,,,"[333, 1694]","[1697548584265, 1697548585959]"
1079,1079,502,18,[],200,llama-7b,128,1,1901.0,1.0,1,A100,1697548617387,1697548619288,120,19.0,1.0,"[357, 1544]","[1697548617744, 1697548619288]"
1080,1080,832,19,[],200,llama-7b,128,1,1634.0,1.0,1,A100,1697548619291,1697548620925,120,15.0,1.0,"[67, 1566]","[1697548619358, 1697548620924]"
1081,1081,842,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581091,1697548583920,120,,,"[268, 1450, 109, 102, 89, 88, 86, 84]","[1697548581359, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
1082,1082,472,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548611390,1697548613003,120,,,"[11, 1417]","[1697548611401, 1697548612818]"
1083,1083,33,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597368,1697548599942,120,,,"[426, 1923]","[1697548597794, 1697548599717]"
1084,1084,265,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620928,1697548624950,120,,,"[10, 2517]","[1697548620938, 1697548623455]"
1085,1085,55,5,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548586205,1697548588171,120,12.0,1.0,"[45, 1920]","[1697548586250, 1697548588170]"
1086,1086,619,21,[],200,llama-7b,128,1,2058.0,1.0,1,A100,1697548624962,1697548627020,120,10.0,1.0,"[12, 2046]","[1697548624974, 1697548627020]"
1087,1087,826,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613019,1697548615233,120,,,"[331, 1682]","[1697548613350, 1697548615032]"
1088,1088,416,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588176,1697548589559,120,,,"[62, 1142]","[1697548588238, 1697548589380]"
1089,1089,748,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592064,120,,,[206],[1697548589774]
1090,1090,49,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627024,1697548629597,120,,,"[29, 885]","[1697548627053, 1697548627938]"
1091,1091,403,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631890,120,,,"[46, 1969]","[1697548629648, 1697548631617]"
1092,1092,179,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[91, 1996]","[1697548592164, 1697548594160]"
1093,1093,244,4,[],200,llama-7b,128,1,2029.0,1.0,1,A100,1697548583927,1697548585956,120,9.0,1.0,"[228, 1801]","[1697548584155, 1697548585956]"
1094,1094,536,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597357,120,,,"[24, 1722, 251, 72, 58]","[1697548594374, 1697548596096, 1697548596347, 1697548596419, 1697548596477]"
1095,1095,736,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631910,1697548633849,120,,,[344],[1697548632254]
1096,1096,886,10,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597367,1697548599717,120,17.0,1.0,"[301, 2049]","[1697548597668, 1697548599717]"
1097,1097,316,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599720,1697548602106,120,,,"[42, 778]","[1697548599762, 1697548600540]"
1098,1098,598,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585962,1697548589557,120,,,"[21, 1232, 1175, 79, 77, 75]","[1697548585983, 1697548587215, 1697548588390, 1697548588469, 1697548588546, 1697548588621]"
1099,1099,645,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605265,120,,,"[38, 1687, 263, 78, 76]","[1697548602165, 1697548603852, 1697548604115, 1697548604193, 1697548604269]"
1100,1100,236,11,[],200,llama-7b,128,1,1723.0,1.0,1,A100,1697548602128,1697548603851,120,8.0,1.0,"[213, 1510]","[1697548602341, 1697548603851]"
1101,1101,595,12,[],200,llama-7b,128,1,1214.0,1.0,1,A100,1697548603856,1697548605070,120,8.0,1.0,"[30, 1184]","[1697548603886, 1697548605070]"
1102,1102,730,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586197,120,,,"[233, 1792]","[1697548584165, 1697548585957]"
1103,1103,924,13,[],200,llama-7b,128,1,665.0,1.0,1,A100,1697548605073,1697548605738,120,9.0,1.0,"[40, 625]","[1697548605113, 1697548605738]"
1104,1104,160,4,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548586204,1697548588171,120,13.0,1.0,"[31, 1936]","[1697548586235, 1697548588171]"
1105,1105,514,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588174,1697548589558,120,,,"[24, 1183]","[1697548588198, 1697548589381]"
1106,1106,354,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605741,1697548608965,120,,,"[9, 3020]","[1697548605750, 1697548608770]"
1107,1107,875,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592060,120,,,"[25, 1995]","[1697548589593, 1697548591588]"
1108,1108,277,7,[],200,llama-7b,128,1,2088.0,1.0,1,A100,1697548592072,1697548594160,120,18.0,1.0,"[83, 2005]","[1697548592155, 1697548594160]"
1109,1109,81,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608966,120,,,"[268, 1681, 511, 77, 79, 78]","[1697548605548, 1697548607229, 1697548607740, 1697548607817, 1697548607896, 1697548607974]"
1110,1110,634,8,[],200,llama-7b,128,1,762.0,1.0,1,A100,1697548594165,1697548594927,120,13.0,1.0,"[45, 717]","[1697548594210, 1697548594927]"
1111,1111,466,33,[],200,llama-7b,128,1,6297.0,1.0,1,A100,1697548643008,1697548649305,120,457.0,20.0,"[233, 1536, 434, 86, 64, 953, 94, 91, 89, 68, 968, 96, 95, 94, 72, 93, 72, 893, 91, 89, 86]","[1697548643241, 1697548644777, 1697548645211, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649039, 1697548649130, 1697548649219, 1697548649305]"
1112,1112,887,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599942,120,,,"[115, 2235]","[1697548597481, 1697548599716]"
1113,1113,68,9,[],200,llama-7b,128,1,2401.0,1.0,1,A100,1697548594933,1697548597334,120,12.0,1.0,"[21, 2380]","[1697548594954, 1697548597334]"
1114,1114,291,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,[305],[1697548600257]
1115,1115,650,11,[],200,llama-7b,128,1,1725.0,1.0,1,A100,1697548602129,1697548603854,120,13.0,1.0,"[317, 1407]","[1697548602446, 1697548603853]"
1116,1116,75,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603859,1697548605267,120,,,"[62, 1149]","[1697548603921, 1697548605070]"
1117,1117,440,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610824,120,,,[263],[1697548609236]
1118,1118,434,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605284,1697548608962,120,,,"[307, 1639, 508, 80, 79, 78]","[1697548605591, 1697548607230, 1697548607738, 1697548607818, 1697548607897, 1697548607975]"
1119,1119,798,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610849,1697548612998,120,,,[48],[1697548610897]
1120,1120,819,13,[],200,llama-7b,128,1,1429.0,1.0,1,A100,1697548611389,1697548612818,120,13.0,1.0,"[7, 1422]","[1697548611396, 1697548612818]"
1121,1121,788,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610832,120,,,[163],[1697548609136]
1122,1122,798,34,[],200,llama-7b,128,1,4016.0,1.0,1,A100,1697548649310,1697548653326,120,79.0,6.0,"[34, 3102, 319, 88, 69, 404]","[1697548649344, 1697548652446, 1697548652765, 1697548652853, 1697548652922, 1697548653326]"
1123,1123,191,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610848,1697548612998,120,,,"[44, 1923]","[1697548610892, 1697548612815]"
1124,1124,247,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612822,1697548615231,120,,,"[73, 644]","[1697548612895, 1697548613539]"
1125,1125,199,16,[],200,llama-7b,128,1,2013.0,1.0,1,A100,1697548613020,1697548615033,120,13.0,1.0,"[350, 1663]","[1697548613370, 1697548615033]"
1126,1126,130,4,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548586205,1697548588171,120,14.0,1.0,"[146, 1820]","[1697548586351, 1697548588171]"
1127,1127,287,1,[],200,llama-7b,128,1,1716.0,1.0,1,A100,1697548581094,1697548582810,120,10.0,1.0,"[354, 1362]","[1697548581448, 1697548582810]"
1128,1128,491,5,[],200,llama-7b,128,1,1208.0,1.0,1,A100,1697548588175,1697548589383,120,14.0,1.0,"[39, 1168]","[1697548588214, 1697548589382]"
1129,1129,927,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586205,1697548589561,120,,,"[136, 1830, 218, 79, 79, 75]","[1697548586341, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
1130,1130,851,6,[],200,llama-7b,128,1,773.0,1.0,1,A100,1697548589387,1697548590160,120,23.0,1.0,"[55, 718]","[1697548589442, 1697548590160]"
1131,1131,646,2,[],200,llama-7b,128,1,1082.0,1.0,1,A100,1697548582820,1697548583902,120,14.0,1.0,"[61, 1021]","[1697548582881, 1697548583902]"
1132,1132,283,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548590164,1697548594346,120,,,"[18, 2669]","[1697548590182, 1697548592851]"
1133,1133,352,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592063,120,,,"[170, 1846]","[1697548589744, 1697548591590]"
1134,1134,550,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613008,1697548615232,120,,,"[261, 1762]","[1697548613269, 1697548615031]"
1135,1135,712,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,"[36, 2050]","[1697548592109, 1697548594159]"
1136,1136,611,8,[],200,llama-7b,128,1,1746.0,1.0,1,A100,1697548594350,1697548596096,120,14.0,1.0,"[22, 1724]","[1697548594372, 1697548596096]"
1137,1137,136,7,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548594356,1697548596098,120,31.0,1.0,"[320, 1422]","[1697548594676, 1697548596098]"
1138,1138,911,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617369,120,,,"[101, 1922]","[1697548615343, 1697548617265]"
1139,1139,199,16,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548615242,1697548617265,120,13.0,1.0,"[46, 1977]","[1697548615288, 1697548617265]"
1140,1140,38,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596102,1697548597358,120,,,"[44, 1188]","[1697548596146, 1697548597334]"
1141,1141,468,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596103,1697548597358,120,,,"[66, 1166]","[1697548596169, 1697548597335]"
1142,1142,336,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617386,1697548620355,120,,,"[303, 1598, 110, 84, 82, 81, 80]","[1697548617689, 1697548619287, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619724]"
1143,1143,75,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583905,1697548586197,120,,,"[21, 2029]","[1697548583926, 1697548585955]"
1144,1144,432,4,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548586205,1697548588171,120,13.0,1.0,"[131, 1834]","[1697548586336, 1697548588170]"
1145,1145,763,5,[],200,llama-7b,128,1,1208.0,1.0,1,A100,1697548588174,1697548589382,120,20.0,1.0,"[34, 1174]","[1697548588208, 1697548589382]"
1146,1146,191,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589385,1697548592065,120,,,"[27, 747]","[1697548589412, 1697548590159]"
1147,1147,828,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599940,120,,,"[90, 524]","[1697548597456, 1697548597980]"
1148,1148,548,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,"[266, 1820]","[1697548592339, 1697548594159]"
1149,1149,908,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597360,120,,,"[206, 1536, 248, 72, 58]","[1697548594562, 1697548596098, 1697548596346, 1697548596418, 1697548596476]"
1150,1150,696,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622597,120,,,"[372, 1590]","[1697548620740, 1697548622330]"
1151,1151,260,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[283, 1651]","[1697548600235, 1697548601886]"
1152,1152,96,20,[],200,llama-7b,128,1,2072.0,1.0,1,A100,1697548622606,1697548624678,120,31.0,1.0,"[247, 1825]","[1697548622853, 1697548624678]"
1153,1153,553,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617269,1697548620355,120,,,"[31, 656, 1440, 84, 82, 81, 80]","[1697548617300, 1697548617956, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
1154,1154,453,21,[],200,llama-7b,128,1,1092.0,1.0,1,A100,1697548624682,1697548625774,120,26.0,1.0,"[41, 1051]","[1697548624723, 1697548625774]"
1155,1155,423,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599944,120,,,"[200, 2149]","[1697548597567, 1697548599716]"
1156,1156,137,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[108, 1980]","[1697548592181, 1697548594161]"
1157,1157,812,22,[],200,llama-7b,128,1,2159.0,1.0,1,A100,1697548625779,1697548627938,120,16.0,1.0,"[37, 2122]","[1697548625816, 1697548627938]"
1158,1158,619,11,[],200,llama-7b,128,1,1724.0,1.0,1,A100,1697548602129,1697548603853,120,10.0,1.0,"[307, 1417]","[1697548602436, 1697548603853]"
1159,1159,494,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594351,1697548597359,120,,,"[185, 1561, 251, 72, 56]","[1697548594536, 1697548596097, 1697548596348, 1697548596420, 1697548596476]"
1160,1160,131,12,[],200,llama-7b,128,1,1212.0,1.0,1,A100,1697548603858,1697548605070,120,8.0,1.0,"[53, 1159]","[1697548603911, 1697548605070]"
1161,1161,660,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617377,1697548620357,120,,,"[88, 1820, 112, 84, 82, 80, 81]","[1697548617465, 1697548619285, 1697548619397, 1697548619481, 1697548619563, 1697548619643, 1697548619724]"
1162,1162,522,6,[],200,llama-7b,128,1,2017.0,1.0,1,A100,1697548589574,1697548591591,120,20.0,1.0,"[175, 1842]","[1697548589749, 1697548591591]"
1163,1163,391,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599943,120,,,[180],[1697548597546]
1164,1164,884,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591596,1697548594347,120,,,"[80, 1176]","[1697548591676, 1697548592852]"
1165,1165,831,19,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548620362,1697548622328,120,11.0,1.0,"[33, 1933]","[1697548620395, 1697548622328]"
1166,1166,279,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597362,120,,,"[319, 1423, 248, 73, 58]","[1697548594675, 1697548596098, 1697548596346, 1697548596419, 1697548596477]"
1167,1167,751,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,[439],[1697548600392]
1168,1168,231,20,[],200,llama-7b,128,1,1122.0,1.0,1,A100,1697548622335,1697548623457,120,13.0,1.0,"[53, 1069]","[1697548622388, 1697548623457]"
1169,1169,880,8,[],200,llama-7b,128,1,1997.0,1.0,1,A100,1697548594350,1697548596347,120,84.0,2.0,"[33, 1713, 251]","[1697548594383, 1697548596096, 1697548596347]"
1170,1170,183,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605266,120,,,"[133, 1593, 260, 80, 74]","[1697548602260, 1697548603853, 1697548604113, 1697548604193, 1697548604267]"
1171,1171,594,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548623462,1697548627100,120,,,"[29, 2283]","[1697548623491, 1697548625774]"
1172,1172,94,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622593,120,,,"[412, 1551]","[1697548620780, 1697548622331]"
1173,1173,310,9,[],200,llama-7b,128,1,985.0,1.0,1,A100,1697548596351,1697548597336,120,26.0,1.0,"[18, 967]","[1697548596369, 1697548597336]"
1174,1174,455,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622606,1697548624966,120,,,"[262, 1810]","[1697548622868, 1697548624678]"
1175,1175,664,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597340,1697548599940,120,,,"[106, 498, 36]","[1697548597446, 1697548597944, 1697548597980]"
1176,1176,639,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597372,1697548599943,120,,,"[507, 1839]","[1697548597879, 1697548599718]"
1177,1177,813,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624976,1697548627102,120,,,"[370, 1677]","[1697548625346, 1697548627023]"
1178,1178,97,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602108,120,,,"[116, 1817]","[1697548600067, 1697548601884]"
1179,1179,215,20,[],200,llama-7b,128,1,2241.0,1.0,1,A100,1697548627106,1697548629347,120,12.0,1.0,"[167, 2073]","[1697548627273, 1697548629346]"
1180,1180,631,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605285,1697548608967,120,,,"[292, 2161, 79, 80, 78]","[1697548605577, 1697548607738, 1697548607817, 1697548607897, 1697548607975]"
1181,1181,565,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629352,1697548631888,120,,,"[57, 864]","[1697548629409, 1697548630273]"
1182,1182,924,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633848,120,,,[138],[1697548632039]
1183,1183,27,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[73, 2158]","[1697548627179, 1697548629337]"
1184,1184,353,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635896,120,,,"[228, 1518]","[1697548634083, 1697548635601]"
1185,1185,456,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605266,120,,,"[222, 1500, 263, 80, 74]","[1697548602351, 1697548603851, 1697548604114, 1697548604194, 1697548604268]"
1186,1186,869,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629358,1697548631888,120,,,"[81, 834]","[1697548629439, 1697548630273]"
1187,1187,64,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,"[405, 1528]","[1697548600358, 1697548601886]"
1188,1188,717,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637694,120,,,[47],[1697548635962]
1189,1189,386,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629601,1697548631890,120,,,"[22, 1993]","[1697548629623, 1697548631616]"
1190,1190,710,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631902,1697548633849,120,,,[345],[1697548632247]
1191,1191,118,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637701,1697548639636,120,,,"[143, 1456]","[1697548637844, 1697548639300]"
1192,1192,476,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639645,1697548641374,120,,,[31],[1697548639676]
1193,1193,421,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602124,1697548605265,120,,,"[66, 1663, 262, 78, 76]","[1697548602190, 1697548603853, 1697548604115, 1697548604193, 1697548604269]"
1194,1194,830,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643001,120,,,[308],[1697548641694]
1195,1195,783,12,[],200,llama-7b,128,1,1951.0,1.0,1,A100,1697548605279,1697548607230,120,286.0,1.0,"[38, 1913]","[1697548605317, 1697548607230]"
1196,1196,135,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635894,120,,,"[125, 1619]","[1697548633979, 1697548635598]"
1197,1197,495,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635909,1697548637692,120,,,[23],[1697548635932]
1198,1198,852,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639646,120,,,"[65, 1531]","[1697548637768, 1697548639299]"
1199,1199,787,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608965,120,,,"[202, 1748, 506, 79, 78, 78]","[1697548605485, 1697548607233, 1697548607739, 1697548607818, 1697548607896, 1697548607974]"
1200,1200,187,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607236,1697548608967,120,,,"[47, 1488]","[1697548607283, 1697548608771]"
1201,1201,288,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641377,120,,,[395],[1697548640050]
1202,1202,256,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548643008,1697548655266,120,,,"[238, 1531, 434, 86, 64, 953, 94, 91, 89, 68, 968, 96, 96, 93, 72, 93, 72, 890, 94, 87, 88, 730, 86, 85, 75, 74, 74, 992, 91, 75, 858, 319, 88, 70, 403, 97, 89, 84, 650, 97, 84, 63, 80]","[1697548643246, 1697548644777, 1697548645211, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647816, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649036, 1697548649130, 1697548649217, 1697548649305, 1697548650035, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421, 1697548651512, 1697548651587, 1697548652445, 1697548652764, 1697548652852, 1697548652922, 1697548653325, 1697548653422, 1697548653511, 1697548653595, 1697548654245, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
1203,1203,623,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588175,1697548589559,120,,,[53],[1697548588228]
1204,1204,546,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610823,120,,,[210],[1697548609183]
1205,1205,618,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548643000,120,,,[170],[1697548641555]
1206,1206,53,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589576,1697548592062,120,,,"[322, 1692]","[1697548589898, 1697548591590]"
1207,1207,444,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597357,120,,,"[19, 1727, 251, 72, 58]","[1697548594369, 1697548596096, 1697548596347, 1697548596419, 1697548596477]"
1208,1208,212,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610834,120,,,[298],[1697548609272]
1209,1209,900,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548612998,120,,,"[240, 1723]","[1697548611092, 1697548612815]"
1210,1210,572,15,[],200,llama-7b,128,1,1964.0,1.0,1,A100,1697548610852,1697548612816,120,16.0,1.0,"[315, 1649]","[1697548611167, 1697548612816]"
1211,1211,926,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612820,1697548615234,120,,,"[22, 696]","[1697548612842, 1697548613538]"
1212,1212,356,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617365,120,,,"[223, 1801]","[1697548615466, 1697548617267]"
1213,1213,798,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599940,120,,,"[85, 529]","[1697548597451, 1697548597980]"
1214,1214,436,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,"[200, 1888]","[1697548592273, 1697548594161]"
1215,1215,325,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613007,1697548615232,120,,,"[262, 1762]","[1697548613269, 1697548615031]"
1216,1216,74,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586210,1697548589563,120,,,"[259, 1703, 217, 79, 79, 77]","[1697548586469, 1697548588172, 1697548588389, 1697548588468, 1697548588547, 1697548588624]"
1217,1217,685,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617367,120,,,"[17, 2006]","[1697548615259, 1697548617265]"
1218,1218,718,1,[],200,llama-7b,128,1,1195.0,1.0,1,A100,1697548577245,1697548578440,120,13.0,1.0,"[36, 1158]","[1697548577281, 1697548578439]"
1219,1219,200,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617386,1697548620354,120,,,"[293, 1608, 110, 84, 82, 81, 80]","[1697548617679, 1697548619287, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619724]"
1220,1220,149,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578445,1697548581081,120,,,"[77, 2541]","[1697548578522, 1697548581063]"
1221,1221,510,3,[],200,llama-7b,128,1,1826.0,1.0,1,A100,1697548581093,1697548582919,120,79.0,2.0,"[343, 1483]","[1697548581436, 1697548582919]"
1222,1222,565,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622597,120,,,"[80, 1887]","[1697548620442, 1697548622329]"
1223,1223,924,20,[],200,llama-7b,128,1,2071.0,1.0,1,A100,1697548622608,1697548624679,120,9.0,1.0,"[280, 1791]","[1697548622888, 1697548624679]"
1224,1224,352,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624686,1697548627102,120,,,"[66, 1022]","[1697548624752, 1697548625774]"
1225,1225,871,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582921,1697548586199,120,,,"[14, 1490]","[1697548582935, 1697548584425]"
1226,1226,273,5,[],200,llama-7b,128,1,1964.0,1.0,1,A100,1697548586208,1697548588172,120,19.0,1.0,"[168, 1795]","[1697548586376, 1697548588171]"
1227,1227,626,6,[],200,llama-7b,128,1,1203.0,1.0,1,A100,1697548588178,1697548589381,120,10.0,1.0,"[86, 1116]","[1697548588264, 1697548589380]"
1228,1228,73,2,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548583933,1697548585958,120,9.0,1.0,"[325, 1700]","[1697548584258, 1697548585958]"
1229,1229,51,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589384,1697548592066,120,,,"[52, 723]","[1697548589436, 1697548590159]"
1230,1230,703,22,[],200,llama-7b,128,1,2208.0,1.0,1,A100,1697548627139,1697548629347,120,12.0,1.0,"[344, 1864]","[1697548627483, 1697548629347]"
1231,1231,104,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629354,1697548631888,120,,,"[61, 858]","[1697548629415, 1697548630273]"
1232,1232,38,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583920,120,,,"[287, 1433, 109, 102, 89, 88, 86, 84]","[1697548581376, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
1233,1233,410,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592074,1697548594348,120,,,"[290, 1796]","[1697548592364, 1697548594160]"
1234,1234,462,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633845,120,,,[45],[1697548631941]
1235,1235,821,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635896,120,,,"[223, 1523]","[1697548634078, 1697548635601]"
1236,1236,768,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597360,120,,,"[210, 1530, 250, 72, 58]","[1697548594566, 1697548596096, 1697548596346, 1697548596418, 1697548596476]"
1237,1237,63,0,[],200,llama-7b,128,1,1309.0,1.0,1,A100,1697548575932,1697548577241,120,39.0,1.0,"[150, 1159]","[1697548576082, 1697548577241]"
1238,1238,257,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637696,120,,,[140],[1697548636056]
1239,1239,531,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583927,1697548586199,120,,,"[157, 1875]","[1697548584084, 1697548585959]"
1240,1240,585,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639646,120,,,"[265, 1331]","[1697548637968, 1697548639299]"
1241,1241,15,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639651,1697548641382,120,,,[233],[1697548639884]
1242,1242,366,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642999,120,,,[278],[1697548641664]
1243,1243,381,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592061,120,,,"[25, 1995]","[1697548589593, 1697548591588]"
1244,1244,723,30,[],200,llama-7b,128,1,568.0,1.0,1,A100,1697548643004,1697548643572,120,14.0,1.0,"[25, 543]","[1697548643029, 1697548643572]"
1245,1245,284,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578443,1697548581083,120,,,"[35, 1576, 96, 94, 101, 83, 87, 81]","[1697548578478, 1697548580054, 1697548580150, 1697548580244, 1697548580345, 1697548580428, 1697548580515, 1697548580596]"
1246,1246,154,31,[],200,llama-7b,128,1,3498.0,1.0,1,A100,1697548643577,1697548647075,120,13.0,1.0,"[19, 3479]","[1697548643596, 1697548647075]"
1247,1247,486,32,[],200,llama-7b,128,1,7264.0,1.0,1,A100,1697548647080,1697548654344,120,14.0,20.0,"[42, 2817, 97, 86, 85, 75, 74, 74, 1000, 83, 74, 859, 319, 89, 69, 404, 97, 88, 84, 651, 97]","[1697548647122, 1697548649939, 1697548650036, 1697548650122, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651430, 1697548651513, 1697548651587, 1697548652446, 1697548652765, 1697548652854, 1697548652923, 1697548653327, 1697548653424, 1697548653512, 1697548653596, 1697548654247, 1697548654344]"
1248,1248,740,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594347,120,,,"[209, 1878]","[1697548592281, 1697548594159]"
1249,1249,259,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597360,120,,,"[285, 1456, 249, 73, 58]","[1697548594641, 1697548596097, 1697548596346, 1697548596419, 1697548596477]"
1250,1250,393,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548577244,1697548581083,120,,,"[15, 1180, 687, 93, 92, 70, 86, 84, 502, 96, 94, 91, 94, 86, 83]","[1697548577259, 1697548578439, 1697548579126, 1697548579219, 1697548579311, 1697548579381, 1697548579467, 1697548579551, 1697548580053, 1697548580149, 1697548580243, 1697548580334, 1697548580428, 1697548580514, 1697548580597]"
1251,1251,644,2,[],200,llama-7b,128,1,1721.0,1.0,1,A100,1697548581090,1697548582811,120,19.0,1.0,"[177, 1544]","[1697548581267, 1697548582811]"
1252,1252,72,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582814,1697548583922,120,,,"[46, 1043]","[1697548582860, 1697548583903]"
1253,1253,890,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586210,1697548589561,120,,,"[230, 1732, 217, 79, 79, 75]","[1697548586440, 1697548588172, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
1254,1254,406,4,[],200,llama-7b,128,1,4614.0,1.0,1,A100,1697548583932,1697548588546,120,244.0,4.0,"[354, 2926, 1177, 79, 78]","[1697548584286, 1697548587212, 1697548588389, 1697548588468, 1697548588546]"
1255,1255,613,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597368,1697548599941,120,,,"[382, 1967]","[1697548597750, 1697548599717]"
1256,1256,767,5,[],200,llama-7b,128,1,831.0,1.0,1,A100,1697548588550,1697548589381,120,11.0,1.0,"[18, 813]","[1697548588568, 1697548589381]"
1257,1257,262,36,[],200,llama-7b,128,1,939.0,1.0,1,A100,1697548663783,1697548664722,120,39.0,1.0,"[44, 895]","[1697548663827, 1697548664722]"
1258,1258,545,4,[],200,llama-7b,128,1,2417.0,1.0,1,A100,1697548586205,1697548588622,120,216.0,5.0,"[68, 1898, 218, 79, 79, 75]","[1697548586273, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
1259,1259,231,11,[],200,llama-7b,128,1,1948.0,1.0,1,A100,1697548605285,1697548607233,120,13.0,1.0,"[389, 1559]","[1697548605674, 1697548607233]"
1260,1260,407,12,[],200,llama-7b,128,1,1465.0,1.0,1,A100,1697548604272,1697548605737,120,16.0,1.0,"[16, 1449]","[1697548604288, 1697548605737]"
1261,1261,591,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607239,1697548612995,120,,,"[118, 2267, 1244, 568]","[1697548607357, 1697548609624, 1697548610868, 1697548611436]"
1262,1262,546,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633853,1697548635894,120,,,"[131, 1614]","[1697548633984, 1697548635598]"
1263,1263,766,13,[],200,llama-7b,128,1,3031.0,1.0,1,A100,1697548605739,1697548608770,120,11.0,1.0,"[6, 3025]","[1697548605745, 1697548608770]"
1264,1264,903,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588625,1697548592064,120,,,"[11, 1522]","[1697548588636, 1697548590158]"
1265,1265,19,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613008,1697548615237,120,,,"[235, 1787]","[1697548613243, 1697548615030]"
1266,1266,537,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548651434,1697548655268,120,,,"[9, 2449, 355, 96, 84, 63, 80]","[1697548651443, 1697548653892, 1697548654247, 1697548654343, 1697548654427, 1697548654490, 1697548654570]"
1267,1267,346,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578445,1697548581081,120,,,"[91, 2527]","[1697548578536, 1697548581063]"
1268,1268,702,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583923,120,,,"[205, 1514, 109, 102, 89, 88, 87, 84]","[1697548581294, 1697548582808, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583367]"
1269,1269,187,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608773,1697548612996,120,,,"[21, 831, 1243, 569]","[1697548608794, 1697548609625, 1697548610868, 1697548611437]"
1270,1270,351,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617369,120,,,"[358, 1664]","[1697548615602, 1697548617266]"
1271,1271,299,4,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548586208,1697548588171,120,14.0,1.0,"[149, 1814]","[1697548586357, 1697548588171]"
1272,1272,328,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[184, 1904]","[1697548592257, 1697548594161]"
1273,1273,656,5,[],200,llama-7b,128,1,1205.0,1.0,1,A100,1697548588175,1697548589380,120,26.0,1.0,"[43, 1161]","[1697548588218, 1697548589379]"
1274,1274,715,15,[],200,llama-7b,128,1,1901.0,1.0,1,A100,1697548617386,1697548619287,120,20.0,1.0,"[308, 1593]","[1697548617694, 1697548619287]"
1275,1275,905,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637695,120,,,[274],[1697548636190]
1276,1276,518,15,[],200,llama-7b,128,1,2026.0,1.0,1,A100,1697548613006,1697548615032,120,23.0,1.0,"[65, 1961]","[1697548613071, 1697548615032]"
1277,1277,85,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589384,1697548592066,120,,,"[32, 743]","[1697548589416, 1697548590159]"
1278,1278,899,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657536,120,,,"[74, 2070]","[1697548655355, 1697548657425]"
1279,1279,330,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639638,120,,,"[181, 1417]","[1697548637884, 1697548639301]"
1280,1280,875,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617365,120,,,"[40, 798]","[1697548615075, 1697548615873]"
1281,1281,444,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592076,1697548594348,120,,,"[367, 1718]","[1697548592443, 1697548594161]"
1282,1282,689,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639651,1697548641380,120,,,[213],[1697548639864]
1283,1283,329,32,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548657546,1697548659576,120,15.0,1.0,"[210, 1820]","[1697548657756, 1697548659576]"
1284,1284,122,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643001,120,,,[184],[1697548641570]
1285,1285,143,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619290,1697548620357,120,,,"[15, 1017]","[1697548619305, 1697548620322]"
1286,1286,769,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664204,120,,,"[17, 1101, 1196, 57, 1026]","[1697548659599, 1697548660700, 1697548661896, 1697548661953, 1697548662979]"
1287,1287,453,29,[],200,llama-7b,128,1,2723.0,1.0,1,A100,1697548643014,1697548645737,120,26.0,1.0,"[330, 2393]","[1697548643344, 1697548645737]"
1288,1288,308,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620358,120,,,"[194, 1713, 111, 84, 82, 81, 80]","[1697548617572, 1697548619285, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
1289,1289,812,30,[],200,llama-7b,128,1,2697.0,1.0,1,A100,1697548645741,1697548648438,120,16.0,1.0,"[27, 2670]","[1697548645768, 1697548648438]"
1290,1290,194,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664216,1697548667374,120,,,"[68, 1752, 431, 82, 67, 85]","[1697548664284, 1697548666036, 1697548666467, 1697548666549, 1697548666616, 1697548666701]"
1291,1291,553,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667381,1697548671225,120,,,"[487, 1841, 470, 84, 64, 64, 81]","[1697548667868, 1697548669709, 1697548670179, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
1292,1292,238,31,[],200,llama-7b,128,1,4323.0,1.0,1,A100,1697548648441,1697548652764,120,563.0,6.0,"[11, 2417, 561, 83, 74, 857, 320]","[1697548648452, 1697548650869, 1697548651430, 1697548651513, 1697548651587, 1697548652444, 1697548652764]"
1293,1293,915,36,[],200,llama-7b,128,1,3080.0,1.0,1,A100,1697548671238,1697548674318,120,182.0,1.0,"[397, 2683]","[1697548671635, 1697548674318]"
1294,1294,592,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548652768,1697548655271,120,,,"[7, 2292]","[1697548652775, 1697548655067]"
1295,1295,345,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674321,1697548677256,120,,,"[21, 1131]","[1697548674342, 1697548675473]"
1296,1296,22,33,[],200,llama-7b,128,1,2144.0,1.0,1,A100,1697548655281,1697548657425,120,16.0,1.0,"[15, 2129]","[1697548655296, 1697548657425]"
1297,1297,671,18,[],200,llama-7b,128,1,1962.0,1.0,1,A100,1697548620368,1697548622330,120,12.0,1.0,"[87, 1875]","[1697548620455, 1697548622330]"
1298,1298,351,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657429,1697548659864,120,,,"[16, 843]","[1697548657445, 1697548658288]"
1299,1299,70,19,[],200,llama-7b,128,1,1121.0,1.0,1,A100,1697548622338,1697548623459,120,39.0,1.0,"[114, 1007]","[1697548622452, 1697548623459]"
1300,1300,424,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548623462,1697548627100,120,,,"[24, 2288]","[1697548623486, 1697548625774]"
1301,1301,678,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679650,120,,,"[31, 2173]","[1697548677300, 1697548679473]"
1302,1302,254,17,[],200,llama-7b,128,1,2024.0,1.0,1,A100,1697548615243,1697548617267,120,58.0,1.0,"[228, 1796]","[1697548615471, 1697548617267]"
1303,1303,810,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548580429,1697548583920,120,,,"[11, 1029, 1448, 102, 89, 89, 86, 83]","[1697548580440, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583197, 1697548583283, 1697548583366]"
1304,1304,396,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602108,120,,,"[386, 1547]","[1697548600338, 1697548601885]"
1305,1305,617,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617270,1697548620355,120,,,"[35, 651, 1440, 84, 82, 81, 80]","[1697548617305, 1697548617956, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
1306,1306,106,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683206,120,,,"[290, 1739, 286, 90, 86, 81, 81]","[1697548679948, 1697548681687, 1697548681973, 1697548682063, 1697548682149, 1697548682230, 1697548682311]"
1307,1307,716,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661854,120,,,[275],[1697548660147]
1308,1308,754,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605264,120,,,"[267, 1719, 77, 77]","[1697548602396, 1697548604115, 1697548604192, 1697548604269]"
1309,1309,380,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586211,1697548589557,120,,,"[263, 1915, 79, 79, 77]","[1697548586474, 1697548588389, 1697548588468, 1697548588547, 1697548588624]"
1310,1310,102,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583921,120,,,"[362, 1354, 109, 102, 89, 88, 86, 84]","[1697548581456, 1697548582810, 1697548582919, 1697548583021, 1697548583110, 1697548583198, 1697548583284, 1697548583368]"
1311,1311,28,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592062,120,,,"[381, 1641]","[1697548589949, 1697548591590]"
1312,1312,711,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610832,120,,,[85],[1697548609058]
1313,1313,422,10,[],200,llama-7b,128,1,606.0,1.0,1,A100,1697548597338,1697548597944,120,26.0,1.0,"[106, 500]","[1697548597444, 1697548597944]"
1314,1314,679,14,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548613008,1697548615033,120,15.0,1.0,"[142, 1883]","[1697548613150, 1697548615033]"
1315,1315,115,15,[],200,llama-7b,128,1,836.0,1.0,1,A100,1697548615037,1697548615873,120,13.0,1.0,"[68, 768]","[1697548615105, 1697548615873]"
1316,1316,155,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605284,1697548608965,120,,,"[196, 1753, 506, 79, 78, 77]","[1697548605480, 1697548607233, 1697548607739, 1697548607818, 1697548607896, 1697548607973]"
1317,1317,474,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615876,1697548620353,120,,,"[6, 3514, 83, 83, 80, 81]","[1697548615882, 1697548619396, 1697548619479, 1697548619562, 1697548619642, 1697548619723]"
1318,1318,238,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583926,1697548586198,120,,,"[151, 1882]","[1697548584077, 1697548585959]"
1319,1319,675,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622598,120,,,"[103, 1865]","[1697548620465, 1697548622330]"
1320,1320,593,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586205,1697548589560,120,,,"[55, 1910, 219, 79, 79, 75]","[1697548586260, 1697548588170, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
1321,1321,779,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597945,1697548599944,120,,,"[6, 1768]","[1697548597951, 1697548599719]"
1322,1322,177,12,[],200,llama-7b,128,1,1932.0,1.0,1,A100,1697548599953,1697548601885,120,14.0,1.0,"[110, 1821]","[1697548600063, 1697548601884]"
1323,1323,538,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601890,1697548605272,120,,,"[40, 745, 1439, 78, 76]","[1697548601930, 1697548602675, 1697548604114, 1697548604192, 1697548604268]"
1324,1324,110,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589578,1697548592063,120,,,"[381, 1631]","[1697548589959, 1697548591590]"
1325,1325,734,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589573,1697548592062,120,,,"[84, 1932]","[1697548589657, 1697548591589]"
1326,1326,464,5,[],200,llama-7b,128,1,2089.0,1.0,1,A100,1697548592073,1697548594162,120,12.0,1.0,"[195, 1893]","[1697548592268, 1697548594161]"
1327,1327,139,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610849,1697548612999,120,,,"[109, 1858]","[1697548610958, 1697548612816]"
1328,1328,276,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,[210],[1697548592283]
1329,1329,105,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624952,120,,,"[167, 1908]","[1697548622771, 1697548624679]"
1330,1330,825,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594165,1697548597357,120,,,"[72, 690, 1419, 72, 58]","[1697548594237, 1697548594927, 1697548596346, 1697548596418, 1697548596476]"
1331,1331,467,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627100,120,,,"[172, 1875]","[1697548625146, 1697548627021]"
1332,1332,896,14,[],200,llama-7b,128,1,1949.0,1.0,1,A100,1697548605283,1697548607232,120,15.0,1.0,"[174, 1775]","[1697548605457, 1697548607232]"
1333,1333,821,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629596,120,,,"[68, 2163]","[1697548627174, 1697548629337]"
1334,1334,803,17,[],200,llama-7b,128,1,1968.0,1.0,1,A100,1697548620362,1697548622330,120,20.0,1.0,"[90, 1877]","[1697548620452, 1697548622329]"
1335,1335,250,25,[],200,llama-7b,128,1,2015.0,1.0,1,A100,1697548629602,1697548631617,120,31.0,1.0,"[31, 1984]","[1697548629633, 1697548631617]"
1336,1336,326,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608967,120,,,"[56, 1478]","[1697548607293, 1697548608771]"
1337,1337,634,7,[],200,llama-7b,128,1,1743.0,1.0,1,A100,1697548594356,1697548596099,120,13.0,1.0,"[340, 1403]","[1697548594696, 1697548596099]"
1338,1338,575,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631623,1697548635894,120,,,"[20, 993, 1249, 57, 547]","[1697548631643, 1697548632636, 1697548633885, 1697548633942, 1697548634489]"
1339,1339,684,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610834,120,,,[312],[1697548609287]
1340,1340,63,8,[],200,llama-7b,128,1,1231.0,1.0,1,A100,1697548596104,1697548597335,120,39.0,1.0,"[97, 1134]","[1697548596201, 1697548597335]"
1341,1341,414,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597337,1697548599938,120,,,"[26, 580, 37]","[1697548597363, 1697548597943, 1697548597980]"
1342,1342,201,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610851,1697548612996,120,,,"[196, 1770]","[1697548611047, 1697548612817]"
1343,1343,164,6,[],200,llama-7b,128,1,2086.0,1.0,1,A100,1697548592073,1697548594159,120,15.0,1.0,"[213, 1873]","[1697548592286, 1697548594159]"
1344,1344,773,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602106,120,,,"[133, 1801]","[1697548600084, 1697548601885]"
1345,1345,146,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664206,120,,,"[285, 1627]","[1697548662152, 1697548663779]"
1346,1346,525,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594164,1697548597360,120,,,"[22, 740, 1420, 72, 58]","[1697548594186, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
1347,1347,898,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653515,1697548657539,120,,,[11],[1697548653526]
1348,1348,558,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615231,120,,,"[20, 2004]","[1697548613026, 1697548615030]"
1349,1349,172,11,[],200,llama-7b,128,1,1724.0,1.0,1,A100,1697548602129,1697548603853,120,19.0,1.0,"[322, 1402]","[1697548602451, 1697548603853]"
1350,1350,884,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599944,120,,,"[190, 2160]","[1697548597557, 1697548599717]"
1351,1351,532,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603858,1697548605267,120,,,"[59, 1153]","[1697548603917, 1697548605070]"
1352,1352,288,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,"[444, 1490]","[1697548600397, 1697548601887]"
1353,1353,916,19,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548615242,1697548617265,120,8.0,1.0,"[41, 1982]","[1697548615283, 1697548617265]"
1354,1354,897,13,[],200,llama-7b,128,1,1947.0,1.0,1,A100,1697548605285,1697548607232,120,9.0,1.0,"[378, 1569]","[1697548605663, 1697548607232]"
1355,1355,325,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608966,120,,,"[41, 1493]","[1697548607278, 1697548608771]"
1356,1356,646,10,[],200,llama-7b,128,1,1724.0,1.0,1,A100,1697548602129,1697548603853,120,14.0,1.0,"[262, 1462]","[1697548602391, 1697548603853]"
1357,1357,346,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617269,1697548620354,120,,,"[6, 681, 1440, 84, 82, 80, 81]","[1697548617275, 1697548617956, 1697548619396, 1697548619480, 1697548619562, 1697548619642, 1697548619723]"
1358,1358,72,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603857,1697548605264,120,,,"[26, 1186]","[1697548603883, 1697548605069]"
1359,1359,62,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610834,120,,,[292],[1697548609267]
1360,1360,771,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610824,120,,,[266],[1697548609239]
1361,1361,419,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548612998,120,,,"[320, 1644]","[1697548611172, 1697548612816]"
1362,1362,524,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588177,1697548589559,120,,,"[72, 1131]","[1697548588249, 1697548589380]"
1363,1363,688,18,[],200,llama-7b,128,1,2185.0,1.0,1,A100,1697548617378,1697548619563,120,345.0,4.0,"[63, 1844, 112, 84, 82]","[1697548617441, 1697548619285, 1697548619397, 1697548619481, 1697548619563]"
1364,1364,494,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612820,1697548615236,120,,,"[52, 666]","[1697548612872, 1697548613538]"
1365,1365,808,0,[],200,llama-7b,128,1,3194.0,1.0,1,A100,1697548575933,1697548579127,120,286.0,2.0,"[317, 2187, 689]","[1697548576250, 1697548578437, 1697548579126]"
1366,1366,228,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602108,120,,,"[100, 1833]","[1697548600051, 1697548601884]"
1367,1367,883,7,[],200,llama-7b,128,1,2021.0,1.0,1,A100,1697548589567,1697548591588,120,563.0,1.0,"[16, 2005]","[1697548589583, 1697548591588]"
1368,1368,192,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548612998,120,,,"[305, 1659]","[1697548611157, 1697548612816]"
1369,1369,304,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,"[38, 1219]","[1697548591632, 1697548592851]"
1370,1370,433,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592064,120,,,[195],[1697548589769]
1371,1371,444,0,[],200,llama-7b,128,1,3447.0,1.0,1,A100,1697548575932,1697548579379,120,457.0,6.0,"[232, 1078, 60, 1824, 93, 92, 68]","[1697548576164, 1697548577242, 1697548577302, 1697548579126, 1697548579219, 1697548579311, 1697548579379]"
1372,1372,770,16,[],200,llama-7b,128,1,2024.0,1.0,1,A100,1697548613007,1697548615031,120,13.0,1.0,"[267, 1757]","[1697548613274, 1697548615031]"
1373,1373,766,7,[],200,llama-7b,128,1,1740.0,1.0,1,A100,1697548594356,1697548596096,120,11.0,1.0,"[221, 1519]","[1697548594577, 1697548596096]"
1374,1374,404,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585963,1697548589557,120,,,"[36, 1213, 1177, 79, 78, 75]","[1697548585999, 1697548587212, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
1375,1375,807,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579382,1697548581082,120,,,"[7, 1675]","[1697548579389, 1697548581064]"
1376,1376,734,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589570,1697548592061,120,,,"[33, 1985]","[1697548589603, 1697548591588]"
1377,1377,165,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594346,120,,,"[69, 2018]","[1697548592141, 1697548594159]"
1378,1378,823,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617367,120,,,"[305, 1716]","[1697548615549, 1697548617265]"
1379,1379,550,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615236,120,,,"[184, 1840]","[1697548613190, 1697548615030]"
1380,1380,199,8,[],200,llama-7b,128,1,1232.0,1.0,1,A100,1697548596102,1697548597334,120,13.0,1.0,"[74, 1158]","[1697548596176, 1697548597334]"
1381,1381,253,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620358,120,,,"[159, 1749, 112, 82, 84, 78, 81]","[1697548617537, 1697548619286, 1697548619398, 1697548619480, 1697548619564, 1697548619642, 1697548619723]"
1382,1382,556,9,[],200,llama-7b,128,1,606.0,1.0,1,A100,1697548597338,1697548597944,120,9.0,1.0,"[111, 495]","[1697548597449, 1697548597944]"
1383,1383,919,10,[],200,llama-7b,128,1,1772.0,1.0,1,A100,1697548597947,1697548599719,120,14.0,1.0,"[9, 1763]","[1697548597956, 1697548599719]"
1384,1384,289,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597368,1697548599942,120,,,"[423, 1926]","[1697548597791, 1697548599717]"
1385,1385,611,19,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548620368,1697548622331,120,14.0,1.0,"[402, 1561]","[1697548620770, 1697548622331]"
1386,1386,650,11,[],200,llama-7b,128,1,1933.0,1.0,1,A100,1697548599953,1697548601886,120,13.0,1.0,"[409, 1524]","[1697548600362, 1697548601886]"
1387,1387,344,11,[],200,llama-7b,128,1,820.0,1.0,1,A100,1697548599721,1697548600541,120,13.0,1.0,"[43, 777]","[1697548599764, 1697548600541]"
1388,1388,234,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583921,120,,,"[379, 2428]","[1697548581473, 1697548583901]"
1389,1389,767,7,[],200,llama-7b,128,1,2086.0,1.0,1,A100,1697548592076,1697548594162,120,11.0,1.0,"[363, 1722]","[1697548592439, 1697548594161]"
1390,1390,41,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624952,120,,,"[37, 1085]","[1697548622372, 1697548623457]"
1391,1391,401,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583925,1697548586198,120,,,"[51, 1982]","[1697548583976, 1697548585958]"
1392,1392,195,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594167,1697548597357,120,,,"[75, 685, 1419, 73, 57]","[1697548594242, 1697548594927, 1697548596346, 1697548596419, 1697548596476]"
1393,1393,769,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[178, 1910]","[1697548592251, 1697548594161]"
1394,1394,79,12,[],200,llama-7b,128,1,785.0,1.0,1,A100,1697548601891,1697548602676,120,12.0,1.0,"[58, 726]","[1697548601949, 1697548602675]"
1395,1395,438,13,[],200,llama-7b,128,1,2389.0,1.0,1,A100,1697548602681,1697548605070,120,9.0,1.0,"[58, 2330]","[1697548602739, 1697548605069]"
1396,1396,399,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624967,1697548627099,120,,,"[97, 1956]","[1697548625064, 1697548627020]"
1397,1397,271,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631890,120,,,"[36, 1979]","[1697548629638, 1697548631617]"
1398,1398,285,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594351,1697548597359,120,,,"[128, 1618, 251, 72, 56]","[1697548594479, 1697548596097, 1697548596348, 1697548596420, 1697548596476]"
1399,1399,788,14,[],200,llama-7b,128,1,665.0,1.0,1,A100,1697548605073,1697548605738,120,31.0,1.0,"[35, 630]","[1697548605108, 1697548605738]"
1400,1400,43,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[340, 1593]","[1697548600292, 1697548601885]"
1401,1401,867,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583921,120,,,"[352, 1364, 109, 102, 88, 89, 86, 84]","[1697548581446, 1697548582810, 1697548582919, 1697548583021, 1697548583109, 1697548583198, 1697548583284, 1697548583368]"
1402,1402,630,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633845,120,,,[247],[1697548632143]
1403,1403,920,12,[],200,llama-7b,128,1,2614.0,1.0,1,A100,1697548605283,1697548607897,120,96.0,4.0,"[83, 1865, 507, 79, 80]","[1697548605366, 1697548607231, 1697548607738, 1697548607817, 1697548607897]"
1404,1404,406,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589575,1697548592064,120,,,"[203, 1813]","[1697548589778, 1697548591591]"
1405,1405,765,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594347,120,,,[166],[1697548592238]
1406,1406,200,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597358,120,,,"[99, 1648, 251, 71, 58]","[1697548594449, 1697548596097, 1697548596348, 1697548596419, 1697548596477]"
1407,1407,496,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622594,120,,,"[196, 1768]","[1697548620564, 1697548622332]"
1408,1408,292,2,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548583926,1697548585959,120,286.0,1.0,"[146, 1887]","[1697548584072, 1697548585959]"
1409,1409,646,3,[],200,llama-7b,128,1,1250.0,1.0,1,A100,1697548585963,1697548587213,120,14.0,1.0,"[57, 1193]","[1697548586020, 1697548587213]"
1410,1410,102,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586197,120,,,"[243, 1782]","[1697548584175, 1697548585957]"
1411,1411,165,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635904,120,,,"[306, 1437]","[1697548634161, 1697548635598]"
1412,1412,77,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548587219,1697548589557,120,,,"[15, 2145]","[1697548587234, 1697548589379]"
1413,1413,530,0,[],200,llama-7b,128,1,1310.0,1.0,1,A100,1697548575931,1697548577241,120,26.0,1.0,"[55, 1255]","[1697548575986, 1697548577241]"
1414,1414,630,8,[],200,llama-7b,128,1,1934.0,1.0,1,A100,1697548599952,1697548601886,120,6.0,1.0,"[221, 1712]","[1697548600173, 1697548601885]"
1415,1415,533,0,[],200,llama-7b,128,1,1370.0,1.0,1,A100,1697548575932,1697548577302,120,216.0,2.0,"[217, 1153]","[1697548576149, 1697548577302]"
1416,1416,864,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548577304,1697548581083,120,,,"[10, 2640, 99, 97, 94, 90, 94, 86, 82]","[1697548577314, 1697548579954, 1697548580053, 1697548580150, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
1417,1417,298,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633845,120,,,[56],[1697548631951]
1418,1418,852,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599945,120,,,"[210, 2140]","[1697548597577, 1697548599717]"
1419,1419,659,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633856,1697548635904,120,,,"[320, 1422]","[1697548634176, 1697548635598]"
1420,1420,282,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,"[434, 1500]","[1697548600387, 1697548601887]"
1421,1421,92,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[346],[1697548636262]
1422,1422,422,27,[],200,llama-7b,128,1,1596.0,1.0,1,A100,1697548637704,1697548639300,120,26.0,1.0,"[294, 1302]","[1697548637998, 1697548639300]"
1423,1423,780,28,[],200,llama-7b,128,1,8842.0,1.0,1,A100,1697548639304,1697548648146,120,85.0,20.0,"[47, 831, 1232, 365, 1260, 584, 1588, 85, 64, 953, 94, 91, 89, 68, 969, 96, 95, 94, 72, 93, 71]","[1697548639351, 1697548640182, 1697548641414, 1697548641779, 1697548643039, 1697548643623, 1697548645211, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646587, 1697548646655, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647981, 1697548648074, 1697548648145]"
1424,1424,699,3,[],200,llama-7b,128,1,2032.0,1.0,1,A100,1697548583925,1697548585957,120,39.0,1.0,"[44, 1988]","[1697548583969, 1697548585957]"
1425,1425,636,11,[],200,llama-7b,128,1,1724.0,1.0,1,A100,1697548602130,1697548603854,120,31.0,1.0,"[337, 1387]","[1697548602467, 1697548603854]"
1426,1426,295,2,[],200,llama-7b,128,1,1829.0,1.0,1,A100,1697548581089,1697548582918,120,52.0,2.0,"[277, 1443, 109]","[1697548581366, 1697548582809, 1697548582918]"
1427,1427,124,4,[],200,llama-7b,128,1,2426.0,1.0,1,A100,1697548585963,1697548588389,120,83.0,2.0,"[31, 2395]","[1697548585994, 1697548588389]"
1428,1428,649,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582921,1697548586199,120,,,"[20, 1484]","[1697548582941, 1697548584425]"
1429,1429,158,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603858,1697548605266,120,,,"[64, 1148]","[1697548603922, 1697548605070]"
1430,1430,372,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581088,1697548583922,120,,,"[164, 1558, 109, 100, 90, 89, 86, 83]","[1697548581252, 1697548582810, 1697548582919, 1697548583019, 1697548583109, 1697548583198, 1697548583284, 1697548583367]"
1431,1431,200,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617366,120,,,"[50, 787]","[1697548615085, 1697548615872]"
1432,1432,515,13,[],200,llama-7b,128,1,1948.0,1.0,1,A100,1697548605283,1697548607231,120,11.0,1.0,"[79, 1869]","[1697548605362, 1697548607231]"
1433,1433,519,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597357,120,,,"[17, 1729, 250, 73, 58]","[1697548594367, 1697548596096, 1697548596346, 1697548596419, 1697548596477]"
1434,1434,121,19,[],200,llama-7b,128,1,756.0,1.0,1,A100,1697548619567,1697548620323,120,13.0,1.0,"[10, 746]","[1697548619577, 1697548620323]"
1435,1435,159,12,[],200,llama-7b,128,1,1970.0,1.0,1,A100,1697548610848,1697548612818,120,31.0,1.0,"[208, 1761]","[1697548611056, 1697548612817]"
1436,1436,909,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617367,120,,,"[300, 1721]","[1697548615544, 1697548617265]"
1437,1437,729,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586199,120,,,[213],[1697548584145]
1438,1438,129,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586212,1697548589557,120,,,"[345, 1616, 217, 78, 79, 77]","[1697548586557, 1697548588173, 1697548588390, 1697548588468, 1697548588547, 1697548588624]"
1439,1439,518,13,[],200,llama-7b,128,1,716.0,1.0,1,A100,1697548612823,1697548613539,120,23.0,1.0,"[82, 634]","[1697548612905, 1697548613539]"
1440,1440,586,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548670393,1697548674836,120,,,"[14, 1184, 2021, 80, 60]","[1697548670407, 1697548671591, 1697548673612, 1697548673692, 1697548673752]"
1441,1441,876,14,[],200,llama-7b,128,1,1533.0,1.0,1,A100,1697548607237,1697548608770,120,11.0,1.0,"[11, 1522]","[1697548607248, 1697548608770]"
1442,1442,872,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613540,1697548617363,120,,,"[6, 2326]","[1697548613546, 1697548615872]"
1443,1443,302,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608773,1697548612996,120,,,"[16, 836, 1243, 569]","[1697548608789, 1697548609625, 1697548610868, 1697548611437]"
1444,1444,300,15,[],200,llama-7b,128,1,1908.0,1.0,1,A100,1697548617377,1697548619285,120,9.0,1.0,"[26, 1882]","[1697548617403, 1697548619285]"
1445,1445,631,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615235,120,,,[164],[1697548613170]
1446,1446,14,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674843,1697548677257,120,,,"[11, 1942]","[1697548674854, 1697548676796]"
1447,1447,375,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677270,1697548679650,120,,,[25],[1697548677295]
1448,1448,647,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608967,120,,,"[302, 1648, 508, 80, 79, 78]","[1697548605582, 1697548607230, 1697548607738, 1697548607818, 1697548607897, 1697548607975]"
1449,1449,677,2,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548583932,1697548585957,120,9.0,1.0,"[260, 1765]","[1697548584192, 1697548585957]"
1450,1450,488,4,[],200,llama-7b,128,1,2016.0,1.0,1,A100,1697548589574,1697548591590,120,6.0,1.0,"[93, 1923]","[1697548589667, 1697548591590]"
1451,1451,744,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624952,120,,,"[138, 1937]","[1697548622741, 1697548624678]"
1452,1452,843,22,[],200,llama-7b,128,1,2210.0,1.0,1,A100,1697548627137,1697548629347,120,14.0,1.0,"[203, 2007]","[1697548627340, 1697548629347]"
1453,1453,41,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599940,120,,,"[326, 2024]","[1697548597693, 1697548599717]"
1454,1454,273,23,[],200,llama-7b,128,1,919.0,1.0,1,A100,1697548629354,1697548630273,120,19.0,1.0,"[64, 855]","[1697548629418, 1697548630273]"
1455,1455,631,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548630277,1697548635893,120,,,"[42, 3566, 57, 547]","[1697548630319, 1697548633885, 1697548633942, 1697548634489]"
1456,1456,400,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602107,120,,,"[30, 1902]","[1697548599981, 1697548601883]"
1457,1457,56,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637696,120,,,[142],[1697548636057]
1458,1458,753,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605263,120,,,"[252, 1471, 263, 77, 76]","[1697548602381, 1697548603852, 1697548604115, 1697548604192, 1697548604268]"
1459,1459,80,2,[],200,llama-7b,128,1,3280.0,1.0,1,A100,1697548583933,1697548587213,120,13.0,1.0,"[365, 2915]","[1697548584298, 1697548587213]"
1460,1460,186,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605284,1697548608966,120,,,"[259, 1686, 510, 78, 79, 78]","[1697548605543, 1697548607229, 1697548607739, 1697548607817, 1697548607896, 1697548607974]"
1461,1461,172,19,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624976,1697548627023,120,19.0,1.0,"[375, 1672]","[1697548625351, 1697548627023]"
1462,1462,622,14,[],200,llama-7b,128,1,542.0,1.0,1,A100,1697548610846,1697548611388,120,20.0,1.0,"[12, 530]","[1697548610858, 1697548611388]"
1463,1463,419,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639646,120,,,"[41, 1555]","[1697548637743, 1697548639298]"
1464,1464,632,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610834,120,,,[200],[1697548609173]
1465,1465,536,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627024,1697548629598,120,,,"[53, 861]","[1697548627077, 1697548627938]"
1466,1466,747,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639649,1697548641378,120,,,[131],[1697548639780]
1467,1467,180,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642999,120,,,[159],[1697548641545]
1468,1468,50,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548611391,1697548613005,120,,,"[18, 1409]","[1697548611409, 1697548612818]"
1469,1469,539,29,[],200,llama-7b,128,1,6214.0,1.0,1,A100,1697548643004,1697548649218,120,83.0,20.0,"[20, 548, 51, 1589, 83, 65, 955, 94, 90, 90, 68, 968, 96, 95, 93, 73, 93, 71, 892, 91, 89]","[1697548643024, 1697548643572, 1697548643623, 1697548645212, 1697548645295, 1697548645360, 1697548646315, 1697548646409, 1697548646499, 1697548646589, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647909, 1697548647982, 1697548648075, 1697548648146, 1697548649038, 1697548649129, 1697548649218]"
1470,1470,64,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548612999,120,,,"[325, 1639]","[1697548611177, 1697548612816]"
1471,1471,868,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631890,120,,,"[330, 1680]","[1697548629938, 1697548631618]"
1472,1472,497,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613019,1697548615233,120,,,"[335, 1678]","[1697548613354, 1697548615032]"
1473,1473,862,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617366,120,,,[294],[1697548615537]
1474,1474,290,18,[],200,llama-7b,128,1,1907.0,1.0,1,A100,1697548617378,1697548619285,120,14.0,1.0,"[182, 1725]","[1697548617560, 1697548619285]"
1475,1475,650,19,[],200,llama-7b,128,1,1032.0,1.0,1,A100,1697548619291,1697548620323,120,13.0,1.0,"[57, 974]","[1697548619348, 1697548620322]"
1476,1476,419,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613009,1697548615234,120,,,"[126, 1897]","[1697548613135, 1697548615032]"
1477,1477,75,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620325,1697548622596,120,,,"[21, 579]","[1697548620346, 1697548620925]"
1478,1478,778,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617364,120,,,"[140, 1883]","[1697548615383, 1697548617266]"
1479,1479,892,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649221,1697548655270,120,,,"[20, 2937, 267, 319, 88, 70, 403, 96, 89, 85, 653, 97, 82, 63, 80]","[1697548649241, 1697548652178, 1697548652445, 1697548652764, 1697548652852, 1697548652922, 1697548653325, 1697548653421, 1697548653510, 1697548653595, 1697548654248, 1697548654345, 1697548654427, 1697548654490, 1697548654570]"
1480,1480,188,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605741,1697548608965,120,,,"[14, 3014]","[1697548605755, 1697548608769]"
1481,1481,294,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655280,1697548657538,120,,,[189],[1697548655469]
1482,1482,295,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631902,1697548633849,120,,,[336],[1697548632238]
1483,1483,648,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657547,1697548659865,120,,,"[292, 1738]","[1697548657839, 1697548659577]"
1484,1484,651,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633856,1697548635904,120,,,"[382, 1360]","[1697548634238, 1697548635598]"
1485,1485,547,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610824,120,,,[215],[1697548609188]
1486,1486,80,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661853,120,,,[41],[1697548659912]
1487,1487,397,11,[],200,llama-7b,128,1,1987.0,1.0,1,A100,1697548602127,1697548604114,120,67.0,2.0,"[151, 1576, 260]","[1697548602278, 1697548603854, 1697548604114]"
1488,1488,70,2,[],200,llama-7b,128,1,2026.0,1.0,1,A100,1697548583933,1697548585959,120,39.0,1.0,"[350, 1676]","[1697548584283, 1697548585959]"
1489,1489,439,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664205,120,,,"[196, 1715]","[1697548662063, 1697548663778]"
1490,1490,499,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548580154,1697548583920,120,,,"[10, 1305, 1448, 102, 89, 88, 87, 83]","[1697548580164, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583366]"
1491,1491,323,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607900,1697548612995,120,,,"[18, 1707, 1243, 569]","[1697548607918, 1697548609625, 1697548610868, 1697548611437]"
1492,1492,588,10,[],200,llama-7b,128,1,1948.0,1.0,1,A100,1697548605283,1697548607231,120,11.0,1.0,"[93, 1855]","[1697548605376, 1697548607231]"
1493,1493,557,8,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597367,1697548599717,120,31.0,1.0,"[311, 2039]","[1697548597678, 1697548599717]"
1494,1494,888,9,[],200,llama-7b,128,1,820.0,1.0,1,A100,1697548599721,1697548600541,120,19.0,1.0,"[36, 783]","[1697548599757, 1697548600540]"
1495,1495,309,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548600544,1697548605272,120,,,"[34, 2096, 1439, 78, 76]","[1697548600578, 1697548602674, 1697548604113, 1697548604191, 1697548604267]"
1496,1496,800,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667373,120,,,"[52, 2198, 82, 67, 85]","[1697548664269, 1697548666467, 1697548666549, 1697548666616, 1697548666701]"
1497,1497,667,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608963,120,,,"[73, 1874, 508, 79, 79, 78]","[1697548605356, 1697548607230, 1697548607738, 1697548607817, 1697548607896, 1697548607974]"
1498,1498,919,11,[],200,llama-7b,128,1,1533.0,1.0,1,A100,1697548607237,1697548608770,120,14.0,1.0,"[61, 1472]","[1697548607298, 1697548608770]"
1499,1499,860,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583931,1697548586198,120,,,"[118, 1909]","[1697548584049, 1697548585958]"
1500,1500,346,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608774,1697548612996,120,,,"[25, 827, 1242, 569]","[1697548608799, 1697548609626, 1697548610868, 1697548611437]"
1501,1501,704,13,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548613006,1697548615031,120,14.0,1.0,"[20, 2004]","[1697548613026, 1697548615030]"
1502,1502,129,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615044,1697548617367,120,,,"[100, 730]","[1697548615144, 1697548615874]"
1503,1503,488,15,[],200,llama-7b,128,1,1906.0,1.0,1,A100,1697548617379,1697548619285,120,6.0,1.0,"[196, 1710]","[1697548617575, 1697548619285]"
1504,1504,820,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619290,1697548620358,120,,,"[54, 978]","[1697548619344, 1697548620322]"
1505,1505,200,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671230,120,,,"[306, 2025, 470, 84, 63, 64, 81]","[1697548667684, 1697548669709, 1697548670179, 1697548670263, 1697548670326, 1697548670390, 1697548670471]"
1506,1506,556,37,[],200,llama-7b,128,1,1834.0,1.0,1,A100,1697548671242,1697548673076,120,9.0,1.0,"[102, 1732]","[1697548671344, 1697548673076]"
1507,1507,253,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622594,120,,,"[216, 1746]","[1697548620586, 1697548622332]"
1508,1508,97,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610832,120,,,[180],[1697548609153]
1509,1509,909,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673080,1697548674841,120,,,"[36, 1203]","[1697548673116, 1697548674319]"
1510,1510,612,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624951,120,,,"[68, 2005]","[1697548622672, 1697548624677]"
1511,1511,339,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548677261,120,,,"[322, 1627]","[1697548675170, 1697548676797]"
1512,1512,36,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624969,1697548627103,120,,,"[72, 1979]","[1697548625041, 1697548627020]"
1513,1513,462,40,[],200,llama-7b,128,1,1812.0,1.0,1,A100,1697548683212,1697548685024,120,52.0,1.0,"[10, 1802]","[1697548683222, 1697548685024]"
1514,1514,816,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548686714,120,,,"[11, 1139]","[1697548685041, 1697548686180]"
1515,1515,367,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629598,120,,,"[234, 2007]","[1697548627340, 1697548629347]"
1516,1516,244,42,[],200,llama-7b,128,1,2871.0,1.0,1,A100,1697548686732,1697548689603,120,9.0,1.0,"[359, 2512]","[1697548687091, 1697548689603]"
1517,1517,700,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677278,1697548679649,120,,,[265],[1697548677543]
1518,1518,459,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610846,1697548612998,120,,,"[17, 525, 49]","[1697548610863, 1697548611388, 1697548611437]"
1519,1519,578,43,[],200,llama-7b,128,1,2244.0,1.0,1,A100,1697548689608,1697548691852,120,31.0,1.0,"[12, 2232]","[1697548689620, 1697548691852]"
1520,1520,220,41,[],200,llama-7b,128,1,2315.0,1.0,1,A100,1697548679657,1697548681972,120,67.0,2.0,"[187, 1845, 283]","[1697548679844, 1697548681689, 1697548681972]"
1521,1521,10,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691859,1697548695714,120,,,"[28, 2257]","[1697548691887, 1697548694144]"
1522,1522,792,14,[],200,llama-7b,128,1,2022.0,1.0,1,A100,1697548613010,1697548615032,120,11.0,1.0,"[279, 1743]","[1697548613289, 1697548615032]"
1523,1523,220,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617365,120,,,"[25, 813]","[1697548615060, 1697548615873]"
1524,1524,721,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631886,120,,,"[61, 1955]","[1697548629663, 1697548631618]"
1525,1525,369,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695726,1697548697835,120,,,"[252, 1706]","[1697548695978, 1697548697684]"
1526,1526,574,16,[],200,llama-7b,128,1,2021.0,1.0,1,A100,1697548617377,1697548619398,120,364.0,2.0,"[83, 1825, 112]","[1697548617460, 1697548619285, 1697548619397]"
1527,1527,124,1,[],200,llama-7b,128,1,2489.0,1.0,1,A100,1697548580429,1697548582918,120,83.0,2.0,"[6, 2483]","[1697548580435, 1697548582918]"
1528,1528,481,2,[],200,llama-7b,128,1,1503.0,1.0,1,A100,1697548582922,1697548584425,120,10.0,1.0,"[26, 1477]","[1697548582948, 1697548584425]"
1529,1529,580,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681974,1697548686705,120,,,"[6, 2063, 1219, 91, 90, 89, 70, 87, 86, 68]","[1697548681980, 1697548684043, 1697548685262, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
1530,1530,835,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548584428,1697548589557,120,,,"[15, 2771, 1175, 79, 78, 75]","[1697548584443, 1697548587214, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
1531,1531,1,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619401,1697548622594,120,,,"[22, 1502]","[1697548619423, 1697548620925]"
1532,1532,269,4,[],200,llama-7b,128,1,2022.0,1.0,1,A100,1697548589568,1697548591590,120,11.0,1.0,"[107, 1915]","[1697548589675, 1697548591590]"
1533,1533,597,5,[],200,llama-7b,128,1,1257.0,1.0,1,A100,1697548591594,1697548592851,120,39.0,1.0,"[45, 1212]","[1697548591639, 1697548592851]"
1534,1534,29,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592854,1697548597360,120,,,"[18, 2054, 1420, 72, 58]","[1697548592872, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
1535,1535,359,18,[],200,llama-7b,128,1,2074.0,1.0,1,A100,1697548622604,1697548624678,120,10.0,1.0,"[146, 1928]","[1697548622750, 1697548624678]"
1536,1536,141,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[174, 1914]","[1697548592247, 1697548594161]"
1537,1537,781,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585963,1697548589556,120,,,"[27, 1225, 1174, 78, 79, 75]","[1697548585990, 1697548587215, 1697548588389, 1697548588467, 1697548588546, 1697548588621]"
1538,1538,326,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610836,120,,,[80],[1697548609053]
1539,1539,655,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610851,1697548612997,120,,,"[224, 1743]","[1697548611075, 1697548612818]"
1540,1540,385,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597372,1697548599943,120,,,"[499, 1847]","[1697548597871, 1697548599718]"
1541,1541,743,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[315, 1616]","[1697548600267, 1697548601883]"
1542,1542,168,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602131,1697548605263,120,,,"[346, 1377, 261, 78, 76]","[1697548602477, 1697548603854, 1697548604115, 1697548604193, 1697548604269]"
1543,1543,614,10,[],200,llama-7b,128,1,1950.0,1.0,1,A100,1697548605279,1697548607229,120,15.0,1.0,"[11, 1939]","[1697548605290, 1697548607229]"
1544,1544,495,7,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548594358,1697548596100,120,13.0,1.0,"[389, 1352]","[1697548594747, 1697548596099]"
1545,1545,47,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607236,1697548608967,120,,,"[52, 1483]","[1697548607288, 1697548608771]"
1546,1546,405,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548612997,120,,,"[394, 2019, 49]","[1697548609369, 1697548611388, 1697548611437]"
1547,1547,766,13,[],200,llama-7b,128,1,2027.0,1.0,1,A100,1697548613006,1697548615033,120,11.0,1.0,"[149, 1878]","[1697548613155, 1697548615033]"
1548,1548,162,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615038,1697548617367,120,,,"[77, 758]","[1697548615115, 1697548615873]"
1549,1549,854,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596102,1697548597358,120,,,"[57, 1176]","[1697548596159, 1697548597335]"
1550,1550,522,15,[],200,llama-7b,128,1,1907.0,1.0,1,A100,1697548617379,1697548619286,120,20.0,1.0,"[201, 1705]","[1697548617580, 1697548619285]"
1551,1551,601,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617367,120,,,"[10, 2012]","[1697548615252, 1697548617264]"
1552,1552,80,16,[],200,llama-7b,128,1,2024.0,1.0,1,A100,1697548613008,1697548615032,120,13.0,1.0,"[127, 1897]","[1697548613135, 1697548615032]"
1553,1553,409,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592061,120,,,"[30, 1990]","[1697548589598, 1697548591588]"
1554,1554,287,9,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597367,1697548599717,120,10.0,1.0,"[282, 2067]","[1697548597649, 1697548599716]"
1555,1555,440,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617366,120,,,"[50, 787]","[1697548615085, 1697548615872]"
1556,1556,617,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599722,1697548602107,120,,,"[65, 753]","[1697548599787, 1697548600540]"
1557,1557,499,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615232,120,,,"[60, 1965]","[1697548613066, 1697548615031]"
1558,1558,509,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610833,120,,,[168],[1697548609141]
1559,1559,828,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617368,120,,,[93],[1697548615335]
1560,1560,35,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617379,1697548620359,120,,,"[266, 1641, 110, 84, 83, 80, 81]","[1697548617645, 1697548619286, 1697548619396, 1697548619480, 1697548619563, 1697548619643, 1697548619724]"
1561,1561,49,11,[],200,llama-7b,128,1,2064.0,1.0,1,A100,1697548602128,1697548604192,120,109.0,3.0,"[243, 1481, 262, 78]","[1697548602371, 1697548603852, 1697548604114, 1697548604192]"
1562,1562,794,18,[],200,llama-7b,128,1,1907.0,1.0,1,A100,1697548617379,1697548619286,120,11.0,1.0,"[168, 1739]","[1697548617547, 1697548619286]"
1563,1563,766,6,[],200,llama-7b,128,1,2088.0,1.0,1,A100,1697548592073,1697548594161,120,11.0,1.0,"[179, 1909]","[1697548592252, 1697548594161]"
1564,1564,200,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594164,1697548597361,120,,,"[51, 711, 1420, 72, 58]","[1697548594215, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
1565,1565,222,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619291,1697548620358,120,,,"[47, 984]","[1697548619338, 1697548620322]"
1566,1566,256,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617386,1697548620355,120,,,"[308, 1593, 110, 84, 82, 81, 81]","[1697548617694, 1697548619287, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619725]"
1567,1567,672,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622597,120,,,"[367, 1594]","[1697548620735, 1697548622329]"
1568,1568,105,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622608,1697548624958,120,,,"[331, 1741]","[1697548622939, 1697548624680]"
1569,1569,403,12,[],200,llama-7b,128,1,3542.0,1.0,1,A100,1697548604195,1697548607737,120,874.0,2.0,"[6, 1536, 2000]","[1697548604201, 1697548605737, 1697548607737]"
1570,1570,763,13,[],200,llama-7b,128,1,1031.0,1.0,1,A100,1697548607741,1697548608772,120,20.0,1.0,"[16, 1015]","[1697548607757, 1697548608772]"
1571,1571,615,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622599,120,,,"[173, 1794]","[1697548620536, 1697548622330]"
1572,1572,187,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608775,1697548612996,120,,,"[39, 812, 1242, 569]","[1697548608814, 1697548609626, 1697548610868, 1697548611437]"
1573,1573,554,8,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597367,1697548599717,120,26.0,1.0,"[205, 2145]","[1697548597572, 1697548599717]"
1574,1574,462,22,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624974,1697548627021,120,52.0,1.0,"[276, 1771]","[1697548625250, 1697548627021]"
1575,1575,817,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627023,1697548629597,120,,,"[7, 908]","[1697548627030, 1697548627938]"
1576,1576,242,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631890,120,,,"[36, 1979]","[1697548629638, 1697548631617]"
1577,1577,519,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615232,120,,,"[25, 1999]","[1697548613031, 1697548615030]"
1578,1578,41,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622608,1697548624959,120,,,"[340, 1732]","[1697548622948, 1697548624680]"
1579,1579,404,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624975,1697548627102,120,,,"[324, 1723]","[1697548625299, 1697548627022]"
1580,1580,878,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617364,120,,,"[114, 1909]","[1697548615357, 1697548617266]"
1581,1581,573,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633849,120,,,[341],[1697548632242]
1582,1582,732,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627105,1697548629599,120,,,"[49, 2183]","[1697548627154, 1697548629337]"
1583,1583,307,17,[],200,llama-7b,128,1,1908.0,1.0,1,A100,1697548617378,1697548619286,120,26.0,1.0,"[104, 1804]","[1697548617482, 1697548619286]"
1584,1584,665,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619290,1697548620357,120,,,"[20, 1012]","[1697548619310, 1697548620322]"
1585,1585,164,24,[],200,llama-7b,128,1,2008.0,1.0,1,A100,1697548629611,1697548631619,120,15.0,1.0,"[346, 1662]","[1697548629957, 1697548631619]"
1586,1586,519,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631623,1697548635895,120,,,"[42, 971, 1249, 57, 548]","[1697548631665, 1697548632636, 1697548633885, 1697548633942, 1697548634490]"
1587,1587,95,19,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548620368,1697548622331,120,12.0,1.0,"[175, 1788]","[1697548620543, 1697548622331]"
1588,1588,540,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624959,120,,,"[73, 1050]","[1697548622408, 1697548623458]"
1589,1589,462,2,[],200,llama-7b,128,1,2026.0,1.0,1,A100,1697548583932,1697548585958,120,52.0,1.0,"[321, 1705]","[1697548584253, 1697548585958]"
1590,1590,228,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624951,120,,,"[19, 1102]","[1697548622354, 1697548623456]"
1591,1591,899,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627101,120,,,"[290, 1757]","[1697548625264, 1697548627021]"
1592,1592,327,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629599,120,,,"[300, 1900]","[1697548627438, 1697548629338]"
1593,1593,811,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585964,1697548589558,120,,,"[102, 1147, 1176, 79, 78, 76]","[1697548586066, 1697548587213, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
1594,1594,327,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657547,1697548659866,120,,,"[303, 1728]","[1697548657850, 1697548659578]"
1595,1595,685,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661856,120,,,[167],[1697548660039]
1596,1596,115,33,[],200,llama-7b,128,1,1911.0,1.0,1,A100,1697548661867,1697548663778,120,13.0,1.0,"[206, 1705]","[1697548662073, 1697548663778]"
1597,1597,348,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581067,1697548583921,120,,,"[27, 1714, 110, 102, 89, 88, 86, 83]","[1697548581094, 1697548582808, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583366]"
1598,1598,239,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592063,120,,,"[164, 1852]","[1697548589738, 1697548591590]"
1599,1599,570,5,[],200,llama-7b,128,1,2087.0,1.0,1,A100,1697548592073,1697548594160,120,18.0,1.0,"[293, 1794]","[1697548592366, 1697548594160]"
1600,1600,2,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594164,1697548597360,120,,,"[7, 755, 1420, 72, 58]","[1697548594171, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
1601,1601,476,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663782,1697548667371,120,,,"[30, 910, 1744, 83, 66, 85]","[1697548663812, 1697548664722, 1697548666466, 1697548666549, 1697548666615, 1697548666700]"
1602,1602,685,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629607,1697548631888,120,,,"[173, 1837]","[1697548629780, 1697548631617]"
1603,1603,584,19,[],200,llama-7b,128,1,2046.0,1.0,1,A100,1697548624974,1697548627020,120,10.0,1.0,"[198, 1848]","[1697548625172, 1697548627020]"
1604,1604,364,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597372,1697548599943,120,,,"[497, 1849]","[1697548597869, 1697548599718]"
1605,1605,707,4,[],200,llama-7b,128,1,2026.0,1.0,1,A100,1697548583933,1697548585959,120,8.0,1.0,"[340, 1686]","[1697548584273, 1697548585959]"
1606,1606,723,8,[],200,llama-7b,128,1,1933.0,1.0,1,A100,1697548599953,1697548601886,120,14.0,1.0,"[404, 1529]","[1697548600357, 1697548601886]"
1607,1607,803,35,[],200,llama-7b,128,1,2331.0,1.0,1,A100,1697548667378,1697548669709,120,20.0,1.0,"[281, 2049]","[1697548667659, 1697548669708]"
1608,1608,132,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585963,1697548589558,120,,,"[91, 1159, 1176, 79, 78, 76]","[1697548586054, 1697548587213, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
1609,1609,922,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548652925,1697548657536,120,,,"[14, 3239]","[1697548652939, 1697548656178]"
1610,1610,349,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657545,1697548659869,120,,,"[128, 1906]","[1697548657673, 1697548659579]"
1611,1611,86,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633847,120,,,[277],[1697548632173]
1612,1612,148,9,[],200,llama-7b,128,1,784.0,1.0,1,A100,1697548601891,1697548602675,120,16.0,1.0,"[53, 731]","[1697548601944, 1697548602675]"
1613,1613,711,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661856,120,,,[310],[1697548660187]
1614,1614,444,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635895,120,,,"[194, 1551]","[1697548634048, 1697548635599]"
1615,1615,139,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661861,1697548664210,120,,,"[32, 859, 228]","[1697548661893, 1697548662752, 1697548662980]"
1616,1616,12,20,[],200,llama-7b,128,1,915.0,1.0,1,A100,1697548627024,1697548627939,120,11.0,1.0,"[39, 875]","[1697548627063, 1697548627938]"
1617,1617,470,34,[],200,llama-7b,128,1,2248.0,1.0,1,A100,1697548664218,1697548666466,120,39.0,2.0,"[276, 1543, 429]","[1697548664494, 1697548666037, 1697548666466]"
1618,1618,231,36,[],200,llama-7b,128,1,1489.0,1.0,1,A100,1697548669714,1697548671203,120,13.0,1.0,"[69, 1420]","[1697548669783, 1697548671203]"
1619,1619,371,21,[],200,llama-7b,128,1,2329.0,1.0,1,A100,1697548627943,1697548630272,120,13.0,1.0,"[34, 2295]","[1697548627977, 1697548630272]"
1620,1620,635,9,[],200,llama-7b,128,1,1746.0,1.0,1,A100,1697548594350,1697548596096,120,23.0,1.0,"[48, 1698]","[1697548594398, 1697548596096]"
1621,1621,585,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671206,1697548674838,120,,,"[37, 1831, 539, 79, 61]","[1697548671243, 1697548673074, 1697548673613, 1697548673692, 1697548673753]"
1622,1622,63,10,[],200,llama-7b,128,1,1232.0,1.0,1,A100,1697548596103,1697548597335,120,39.0,1.0,"[88, 1143]","[1697548596191, 1697548597334]"
1623,1623,7,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[165],[1697548636081]
1624,1624,827,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666468,1697548667370,120,,,"[10, 869]","[1697548666478, 1697548667347]"
1625,1625,485,13,[],200,llama-7b,128,1,2744.0,1.0,1,A100,1697548605073,1697548607817,120,67.0,3.0,"[22, 643, 1999, 79]","[1697548605095, 1697548605738, 1697548607737, 1697548607816]"
1626,1626,845,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607820,1697548612995,120,,,"[6, 1799, 1243, 569]","[1697548607826, 1697548609625, 1697548610868, 1697548611437]"
1627,1627,19,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677258,120,,,"[43, 1909]","[1697548674887, 1697548676796]"
1628,1628,376,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677271,1697548679649,120,,,"[267, 1935]","[1697548677538, 1697548679473]"
1629,1629,367,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637704,1697548639646,120,,,"[341, 1256]","[1697548638045, 1697548639301]"
1630,1630,276,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613005,1697548615231,120,,,[16],[1697548613021]
1631,1631,725,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639650,1697548641380,120,,,[214],[1697548639864]
1632,1632,638,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617368,120,,,"[27, 1996]","[1697548615269, 1697548617265]"
1633,1633,47,30,[],200,llama-7b,128,1,6214.0,1.0,1,A100,1697548643004,1697548649218,120,90.0,20.0,"[14, 554, 52, 1588, 83, 66, 954, 94, 90, 89, 69, 968, 96, 95, 94, 72, 93, 71, 892, 91, 89]","[1697548643018, 1697548643572, 1697548643624, 1697548645212, 1697548645295, 1697548645361, 1697548646315, 1697548646409, 1697548646499, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647982, 1697548648075, 1697548648146, 1697548649038, 1697548649129, 1697548649218]"
1634,1634,39,17,[],200,llama-7b,128,1,1908.0,1.0,1,A100,1697548617379,1697548619287,120,8.0,1.0,"[280, 1627]","[1697548617659, 1697548619286]"
1635,1635,708,40,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679658,1697548681688,120,140.0,1.0,"[326, 1704]","[1697548679984, 1697548681688]"
1636,1636,153,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642997,120,,,[256],[1697548641642]
1637,1637,257,36,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667378,1697548669708,120,14.0,1.0,"[414, 1915]","[1697548667792, 1697548669707]"
1638,1638,139,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548683211,120,,,"[50, 1161]","[1697548681748, 1697548682909]"
1639,1639,881,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,"[268, 1818]","[1697548592341, 1697548594159]"
1640,1640,239,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583922,120,,,"[85, 1634, 109, 102, 89, 89, 86, 83]","[1697548581175, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583198, 1697548583284, 1697548583367]"
1641,1641,401,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585962,1697548589557,120,,,"[43, 1207, 1177, 78, 79, 75]","[1697548586005, 1697548587212, 1697548588389, 1697548588467, 1697548588546, 1697548588621]"
1642,1642,396,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619290,1697548620357,120,,,"[28, 1005]","[1697548619318, 1697548620323]"
1643,1643,759,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592062,120,,,"[331, 1691]","[1697548589899, 1697548591590]"
1644,1644,615,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671228,120,,,"[61, 1430]","[1697548669774, 1697548671204]"
1645,1645,43,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671242,1697548674840,120,,,"[117, 1717, 538, 79, 60]","[1697548671359, 1697548673076, 1697548673614, 1697548673693, 1697548673753]"
1646,1646,493,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683217,1697548686708,120,,,"[340, 1470, 236, 91, 91, 89, 69, 87, 86, 69]","[1697548683557, 1697548685027, 1697548685263, 1697548685354, 1697548685445, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685845]"
1647,1647,673,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583921,120,,,"[46, 1673, 110, 102, 89, 88, 86, 84]","[1697548581135, 1697548582808, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
1648,1648,207,29,[],200,llama-7b,128,1,1789.0,1.0,1,A100,1697548648150,1697548649939,120,10.0,1.0,"[19, 1770]","[1697548648169, 1697548649939]"
1649,1649,560,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649944,1697548655265,120,,,"[29, 3354, 96, 89, 84, 650, 96, 84, 63, 80]","[1697548649973, 1697548653327, 1697548653423, 1697548653512, 1697548653596, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
1650,1650,189,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594347,120,,,"[88, 2000]","[1697548592160, 1697548594160]"
1651,1651,546,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594355,1697548597358,120,,,"[99, 1643, 251, 72, 57]","[1697548594454, 1697548596097, 1697548596348, 1697548596420, 1697548596477]"
1652,1652,747,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620367,1697548622599,120,,,"[100, 1863]","[1697548620467, 1697548622330]"
1653,1653,918,31,[],200,llama-7b,128,1,2144.0,1.0,1,A100,1697548655283,1697548657427,120,23.0,1.0,"[92, 2051]","[1697548655375, 1697548657426]"
1654,1654,177,20,[],200,llama-7b,128,1,2073.0,1.0,1,A100,1697548622604,1697548624677,120,14.0,1.0,"[243, 1830]","[1697548622847, 1697548624677]"
1655,1655,323,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657430,1697548659866,120,,,"[50, 808]","[1697548657480, 1697548658288]"
1656,1656,847,43,[],200,llama-7b,128,1,1985.0,1.0,1,A100,1697548686729,1697548688714,120,10.0,1.0,"[265, 1720]","[1697548686994, 1697548688714]"
1657,1657,614,29,[],200,llama-7b,128,1,2145.0,1.0,1,A100,1697548655280,1697548657425,120,15.0,1.0,"[29, 2116]","[1697548655309, 1697548657425]"
1658,1658,684,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661853,120,,,[280],[1697548660152]
1659,1659,18,30,[],200,llama-7b,128,1,857.0,1.0,1,A100,1697548657431,1697548658288,120,15.0,1.0,"[54, 803]","[1697548657485, 1697548658288]"
1660,1660,535,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624682,1697548627102,120,,,"[46, 1046]","[1697548624728, 1697548625774]"
1661,1661,277,44,[],200,llama-7b,128,1,883.0,1.0,1,A100,1697548688719,1697548689602,120,18.0,1.0,"[42, 841]","[1697548688761, 1697548689602]"
1662,1662,112,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664205,120,,,"[188, 1721]","[1697548662056, 1697548663777]"
1663,1663,480,20,[],200,llama-7b,128,1,598.0,1.0,1,A100,1697548620327,1697548620925,120,26.0,1.0,"[23, 575]","[1697548620350, 1697548620925]"
1664,1664,833,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620928,1697548624951,120,,,"[15, 2513]","[1697548620943, 1697548623456]"
1665,1665,471,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667372,120,,,"[7, 1811, 432, 82, 67, 84]","[1697548664224, 1697548666035, 1697548666467, 1697548666549, 1697548666616, 1697548666700]"
1666,1666,373,39,[],200,llama-7b,128,1,1949.0,1.0,1,A100,1697548674848,1697548676797,120,15.0,1.0,"[327, 1622]","[1697548675175, 1697548676797]"
1667,1667,919,3,[],200,llama-7b,128,1,1719.0,1.0,1,A100,1697548581090,1697548582809,120,14.0,1.0,"[78, 1641]","[1697548581168, 1697548582809]"
1668,1668,258,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624970,1697548627100,120,,,"[196, 1855]","[1697548625166, 1697548627021]"
1669,1669,731,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548676800,1697548679648,120,,,"[21, 1421]","[1697548676821, 1697548678242]"
1670,1670,704,23,[],200,llama-7b,128,1,2200.0,1.0,1,A100,1697548627138,1697548629338,120,14.0,1.0,"[300, 1900]","[1697548627438, 1697548629338]"
1671,1671,156,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683207,120,,,"[306, 1723, 286, 91, 85, 82, 80]","[1697548679964, 1697548681687, 1697548681973, 1697548682064, 1697548682149, 1697548682231, 1697548682311]"
1672,1672,133,24,[],200,llama-7b,128,1,923.0,1.0,1,A100,1697548629349,1697548630272,120,15.0,1.0,"[29, 894]","[1697548629378, 1697548630272]"
1673,1673,865,22,[],200,llama-7b,128,1,2241.0,1.0,1,A100,1697548627106,1697548629347,120,9.0,1.0,"[176, 2065]","[1697548627282, 1697548629347]"
1674,1674,300,23,[],200,llama-7b,128,1,922.0,1.0,1,A100,1697548629351,1697548630273,120,9.0,1.0,"[42, 879]","[1697548629393, 1697548630272]"
1675,1675,658,24,[],200,llama-7b,128,1,2358.0,1.0,1,A100,1697548630278,1697548632636,120,11.0,1.0,"[33, 2325]","[1697548630311, 1697548632636]"
1676,1676,88,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548632640,1697548635904,120,,,"[24, 1767, 59]","[1697548632664, 1697548634431, 1697548634490]"
1677,1677,498,25,[],200,llama-7b,128,1,2359.0,1.0,1,A100,1697548630277,1697548632636,120,9.0,1.0,"[37, 2322]","[1697548630314, 1697548632636]"
1678,1678,438,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637693,120,,,[219],[1697548636135]
1679,1679,767,27,[],200,llama-7b,128,1,393.0,1.0,1,A100,1697548637701,1697548638094,120,11.0,1.0,"[19, 374]","[1697548637720, 1697548638094]"
1680,1680,197,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548638095,1697548639646,120,,,"[21, 1185]","[1697548638116, 1697548639301]"
1681,1681,559,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639652,1697548641375,120,,,[321],[1697548639973]
1682,1682,922,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642999,120,,,[278],[1697548641664]
1683,1683,858,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548632639,1697548635896,120,,,"[15, 1835]","[1697548632654, 1697548634489]"
1684,1684,519,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683213,1697548686708,120,,,"[54, 1758, 239, 90, 91, 89, 69, 87, 86, 68]","[1697548683267, 1697548685025, 1697548685264, 1697548685354, 1697548685445, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
1685,1685,437,31,[],200,llama-7b,128,1,8577.0,1.0,1,A100,1697548643010,1697548651587,120,91.0,29.0,"[145, 1623, 433, 85, 65, 952, 95, 91, 89, 68, 968, 96, 97, 93, 72, 93, 70, 894, 91, 88, 86, 730, 87, 85, 75, 74, 74, 992, 91, 74]","[1697548643155, 1697548644778, 1697548645211, 1697548645296, 1697548645361, 1697548646313, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648145, 1697548649039, 1697548649130, 1697548649218, 1697548649304, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421, 1697548651512, 1697548651586]"
1686,1686,822,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671229,120,,,"[301, 2030, 470, 84, 63, 64, 81]","[1697548667679, 1697548669709, 1697548670179, 1697548670263, 1697548670326, 1697548670390, 1697548670471]"
1687,1687,377,31,[],200,llama-7b,128,1,2408.0,1.0,1,A100,1697548658293,1697548660701,120,13.0,1.0,"[37, 2370]","[1697548658330, 1697548660700]"
1688,1688,257,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637696,120,,,[321],[1697548636237]
1689,1689,616,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637704,1697548639646,120,,,"[346, 1251]","[1697548638050, 1697548639301]"
1690,1690,350,4,[],200,llama-7b,128,1,1089.0,1.0,1,A100,1697548582813,1697548583902,120,216.0,1.0,"[22, 1067]","[1697548582835, 1697548583902]"
1691,1691,223,37,[],200,llama-7b,128,1,1835.0,1.0,1,A100,1697548671243,1697548673078,120,16.0,1.0,"[196, 1638]","[1697548671439, 1697548673077]"
1692,1692,710,5,[],200,llama-7b,128,1,2050.0,1.0,1,A100,1697548583906,1697548585956,120,14.0,1.0,"[30, 2020]","[1697548583936, 1697548585956]"
1693,1693,112,6,[],200,llama-7b,128,1,2428.0,1.0,1,A100,1697548585962,1697548588390,120,16.0,2.0,"[18, 1235, 1175]","[1697548585980, 1697548587215, 1697548588390]"
1694,1694,37,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639654,1697548641377,120,,,[343],[1697548639997]
1695,1695,606,45,[],200,llama-7b,128,1,2244.0,1.0,1,A100,1697548689609,1697548691853,120,9.0,1.0,"[41, 2203]","[1697548689650, 1697548691853]"
1696,1696,39,46,[],200,llama-7b,128,1,2288.0,1.0,1,A100,1697548691855,1697548694143,120,8.0,1.0,"[22, 2266]","[1697548691877, 1697548694143]"
1697,1697,581,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673081,1697548674843,120,,,"[76, 1162]","[1697548673157, 1697548674319]"
1698,1698,738,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548660704,1697548664209,120,,,"[11, 2264]","[1697548660715, 1697548662979]"
1699,1699,395,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642997,120,,,[74],[1697548641458]
1700,1700,465,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588392,1697548592063,120,,,"[15, 1751]","[1697548588407, 1697548590158]"
1701,1701,168,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664221,1697548667372,120,,,"[372, 1446, 428, 84, 66, 85]","[1697548664593, 1697548666039, 1697548666467, 1697548666551, 1697548666617, 1697548666702]"
1702,1702,754,31,[],200,llama-7b,128,1,3493.0,1.0,1,A100,1697548643006,1697548646499,120,88.0,7.0,"[79, 2126, 85, 64, 953, 94, 92]","[1697548643085, 1697548645211, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646499]"
1703,1703,823,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594348,120,,,"[42, 2045]","[1697548592114, 1697548594159]"
1704,1704,45,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610833,120,,,[379],[1697548609354]
1705,1705,80,5,[],200,llama-7b,128,1,774.0,1.0,1,A100,1697548589386,1697548590160,120,13.0,1.0,"[51, 722]","[1697548589437, 1697548590159]"
1706,1706,400,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548694154,1697548697833,120,,,"[20, 2554]","[1697548694174, 1697548696728]"
1707,1707,438,6,[],200,llama-7b,128,1,2689.0,1.0,1,A100,1697548590162,1697548592851,120,9.0,1.0,"[10, 2678]","[1697548590172, 1697548592850]"
1708,1708,522,34,[],200,llama-7b,128,1,648.0,1.0,1,A100,1697548667378,1697548668026,120,20.0,1.0,"[60, 588]","[1697548667438, 1697548668026]"
1709,1709,34,35,[],200,llama-7b,128,1,3166.0,1.0,1,A100,1697548668036,1697548671202,120,12.0,1.0,"[32, 3134]","[1697548668068, 1697548671202]"
1710,1710,403,15,[],200,llama-7b,128,1,592.0,1.0,1,A100,1697548610846,1697548611438,120,874.0,2.0,"[20, 522, 50]","[1697548610866, 1697548611388, 1697548611438]"
1711,1711,250,9,[],200,llama-7b,128,1,1741.0,1.0,1,A100,1697548594356,1697548596097,120,31.0,1.0,"[295, 1446]","[1697548594651, 1697548596097]"
1712,1712,393,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671205,1697548674837,120,,,"[15, 371, 2021, 80, 60]","[1697548671220, 1697548671591, 1697548673612, 1697548673692, 1697548673752]"
1713,1713,56,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617368,120,,,"[325, 1697]","[1697548615569, 1697548617266]"
1714,1714,611,10,[],200,llama-7b,128,1,1231.0,1.0,1,A100,1697548596104,1697548597335,120,14.0,1.0,"[90, 1141]","[1697548596194, 1697548597335]"
1715,1715,127,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597337,1697548599939,120,,,"[37, 570, 36]","[1697548597374, 1697548597944, 1697548597980]"
1716,1716,588,30,[],200,llama-7b,128,1,2377.0,1.0,1,A100,1697548651516,1697548653893,120,11.0,1.0,"[7, 2370]","[1697548651523, 1697548653893]"
1717,1717,18,31,[],200,llama-7b,128,1,2283.0,1.0,1,A100,1697548653897,1697548656180,120,15.0,1.0,"[18, 2265]","[1697548653915, 1697548656180]"
1718,1718,754,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677255,120,,,[113],[1697548674957]
1719,1719,183,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679651,120,,,"[76, 2128]","[1697548677345, 1697548679473]"
1720,1720,463,32,[],200,llama-7b,128,1,2105.0,1.0,1,A100,1697548656182,1697548658287,120,39.0,1.0,"[20, 2085]","[1697548656202, 1697548658287]"
1721,1721,764,16,[],200,llama-7b,128,1,2098.0,1.0,1,A100,1697548611440,1697548613538,120,39.0,1.0,"[15, 2083]","[1697548611455, 1697548613538]"
1722,1722,486,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,"[395, 1537]","[1697548600348, 1697548601885]"
1723,1723,194,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613540,1697548617364,120,,,"[12, 2320]","[1697548613552, 1697548615872]"
1724,1724,415,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617386,1697548620355,120,,,"[298, 1713, 84, 82, 81, 80]","[1697548617684, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619724]"
1725,1725,843,13,[],200,llama-7b,128,1,1727.0,1.0,1,A100,1697548602127,1697548603854,120,14.0,1.0,"[146, 1581]","[1697548602273, 1697548603854]"
1726,1726,643,18,[],200,llama-7b,128,1,1907.0,1.0,1,A100,1697548617378,1697548619285,120,18.0,1.0,"[20, 1886]","[1697548617398, 1697548619284]"
1727,1727,273,14,[],200,llama-7b,128,1,1211.0,1.0,1,A100,1697548603859,1697548605070,120,19.0,1.0,"[76, 1135]","[1697548603935, 1697548605070]"
1728,1728,631,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605074,1697548608962,120,,,"[41, 2622, 80, 79, 78]","[1697548605115, 1697548607737, 1697548607817, 1697548607896, 1697548607974]"
1729,1729,67,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619290,1697548620358,120,,,"[18, 1014]","[1697548619308, 1697548620322]"
1730,1730,517,39,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679659,1697548681689,120,15.0,1.0,"[401, 1629]","[1697548680060, 1697548681689]"
1731,1731,776,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622594,120,,,"[203, 1766]","[1697548620566, 1697548622332]"
1732,1732,426,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622597,120,,,[366],[1697548620736]
1733,1733,195,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589385,1697548592066,120,,,"[42, 732]","[1697548589427, 1697548590159]"
1734,1734,871,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548683211,120,,,"[30, 1181]","[1697548681728, 1697548682909]"
1735,1735,781,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624952,120,,,"[142, 1932]","[1697548622746, 1697548624678]"
1736,1736,327,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610834,120,,,[95],[1697548609068]
1737,1737,646,10,[],200,llama-7b,128,1,2346.0,1.0,1,A100,1697548597372,1697548599718,120,14.0,1.0,"[473, 1873]","[1697548597845, 1697548599718]"
1738,1738,732,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586210,1697548589562,120,,,"[250, 1712, 217, 79, 79, 76]","[1697548586460, 1697548588172, 1697548588389, 1697548588468, 1697548588547, 1697548588623]"
1739,1739,301,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686707,120,,,"[271, 1538, 238, 91, 90, 90, 69, 87, 86, 69]","[1697548683487, 1697548685025, 1697548685263, 1697548685354, 1697548685444, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685845]"
1740,1740,211,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624976,1697548627102,120,,,"[367, 1679]","[1697548625343, 1697548627022]"
1741,1741,181,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[303, 1631]","[1697548600255, 1697548601886]"
1742,1742,535,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602109,1697548605265,120,,,"[26, 1716, 263, 78, 76]","[1697548602135, 1697548603851, 1697548604114, 1697548604192, 1697548604268]"
1743,1743,543,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627143,1697548629598,120,,,"[354, 1850]","[1697548627497, 1697548629347]"
1744,1744,434,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548587220,1697548589558,120,,,"[22, 2137]","[1697548587242, 1697548589379]"
1745,1745,895,11,[],200,llama-7b,128,1,1948.0,1.0,1,A100,1697548605285,1697548607233,120,15.0,1.0,"[398, 1550]","[1697548605683, 1697548607233]"
1746,1746,900,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631889,120,,,"[249, 1760]","[1697548629857, 1697548631617]"
1747,1747,98,1,[],200,llama-7b,128,1,1720.0,1.0,1,A100,1697548581089,1697548582809,120,14.0,1.0,"[14, 1705]","[1697548581103, 1697548582808]"
1748,1748,161,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592066,120,,,[290],[1697548589864]
1749,1749,794,4,[],200,llama-7b,128,1,2016.0,1.0,1,A100,1697548589574,1697548591590,120,11.0,1.0,"[103, 1913]","[1697548589677, 1697548591590]"
1750,1750,517,5,[],200,llama-7b,128,1,2085.0,1.0,1,A100,1697548592076,1697548594161,120,15.0,1.0,"[320, 1765]","[1697548592396, 1697548594161]"
1751,1751,871,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594164,1697548597361,120,,,"[52, 711, 1419, 72, 58]","[1697548594216, 1697548594927, 1697548596346, 1697548596418, 1697548596476]"
1752,1752,319,12,[],200,llama-7b,128,1,1534.0,1.0,1,A100,1697548607238,1697548608772,120,31.0,1.0,"[116, 1417]","[1697548607354, 1697548608771]"
1753,1753,218,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594346,120,,,[10],[1697548591604]
1754,1754,649,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608776,1697548612997,120,,,"[43, 807, 1242, 569]","[1697548608819, 1697548609626, 1697548610868, 1697548611437]"
1755,1755,581,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597357,120,,,"[27, 1719, 251, 72, 58]","[1697548594377, 1697548596096, 1697548596347, 1697548596419, 1697548596477]"
1756,1756,456,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582813,1697548583923,120,,,"[43, 1046]","[1697548582856, 1697548583902]"
1757,1757,301,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597368,1697548599940,120,,,"[372, 1977]","[1697548597740, 1697548599717]"
1758,1758,82,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615231,120,,,"[257, 1767]","[1697548613263, 1697548615030]"
1759,1759,811,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586199,120,,,"[271, 1754]","[1697548584203, 1697548585957]"
1760,1760,361,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592065,120,,,"[215, 1802]","[1697548589789, 1697548591591]"
1761,1761,439,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617369,120,,,"[101, 1922]","[1697548615343, 1697548617265]"
1762,1762,746,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602107,120,,,"[30, 1902]","[1697548599981, 1697548601883]"
1763,1763,758,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548604117,1697548605273,120,,,"[6, 947]","[1697548604123, 1697548605070]"
1764,1764,803,16,[],200,llama-7b,128,1,1901.0,1.0,1,A100,1697548617387,1697548619288,120,20.0,1.0,"[354, 1547]","[1697548617741, 1697548619288]"
1765,1765,849,33,[],200,llama-7b,128,1,1831.0,1.0,1,A100,1697548654349,1697548656180,120,10.0,1.0,"[38, 1793]","[1697548654387, 1697548656180]"
1766,1766,182,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605265,120,,,"[122, 1604, 260, 80, 74]","[1697548602249, 1697548603853, 1697548604113, 1697548604193, 1697548604267]"
1767,1767,718,7,[],200,llama-7b,128,1,2085.0,1.0,1,A100,1697548592076,1697548594161,120,13.0,1.0,"[315, 1769]","[1697548592391, 1697548594160]"
1768,1768,160,13,[],200,llama-7b,128,1,1947.0,1.0,1,A100,1697548605284,1697548607231,120,13.0,1.0,"[313, 1634]","[1697548605597, 1697548607231]"
1769,1769,517,14,[],200,llama-7b,128,1,1534.0,1.0,1,A100,1697548607237,1697548608771,120,15.0,1.0,"[31, 1503]","[1697548607268, 1697548608771]"
1770,1770,243,4,[],200,llama-7b,128,1,2338.0,1.0,1,A100,1697548586210,1697548588548,120,67.0,4.0,"[274, 1689, 217, 78, 79]","[1697548586484, 1697548588173, 1697548588390, 1697548588468, 1697548588547]"
1771,1771,880,15,[],200,llama-7b,128,1,2093.0,1.0,1,A100,1697548608775,1697548610868,120,84.0,2.0,"[34, 816, 1243]","[1697548608809, 1697548609625, 1697548610868]"
1772,1772,116,8,[],200,llama-7b,128,1,762.0,1.0,1,A100,1697548594165,1697548594927,120,23.0,1.0,"[55, 707]","[1697548594220, 1697548594927]"
1773,1773,278,34,[],200,llama-7b,128,1,2106.0,1.0,1,A100,1697548656182,1697548658288,120,13.0,1.0,"[15, 2090]","[1697548656197, 1697548658287]"
1774,1774,469,9,[],200,llama-7b,128,1,2404.0,1.0,1,A100,1697548594930,1697548597334,120,17.0,1.0,"[22, 2382]","[1697548594952, 1697548597334]"
1775,1775,631,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548658291,1697548664203,120,,,"[25, 3580, 57, 1026]","[1697548658316, 1697548661896, 1697548661953, 1697548662979]"
1776,1776,687,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597361,120,,,"[290, 1451, 249, 73, 58]","[1697548594646, 1697548596097, 1697548596346, 1697548596419, 1697548596477]"
1777,1777,682,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615232,120,,,"[35, 1990]","[1697548613041, 1697548615031]"
1778,1778,119,8,[],200,llama-7b,128,1,2346.0,1.0,1,A100,1697548597372,1697548599718,120,31.0,1.0,"[466, 1879]","[1697548597838, 1697548599717]"
1779,1779,573,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588550,1697548592064,120,,,[10],[1697548588560]
1780,1780,449,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599721,1697548602106,120,,,"[53, 767]","[1697548599774, 1697548600541]"
1781,1781,827,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597337,1697548599938,120,,,"[20, 586, 37]","[1697548597357, 1697548597943, 1697548597980]"
1782,1782,2,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[193, 1895]","[1697548592266, 1697548594161]"
1783,1783,359,7,[],200,llama-7b,128,1,1741.0,1.0,1,A100,1697548594358,1697548596099,120,10.0,1.0,"[384, 1357]","[1697548594742, 1697548596099]"
1784,1784,719,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596102,1697548597358,120,,,[50],[1697548596152]
1785,1785,810,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605266,120,,,"[219, 1505, 263, 80, 74]","[1697548602346, 1697548603851, 1697548604114, 1697548604194, 1697548604268]"
1786,1786,147,9,[],200,llama-7b,128,1,2349.0,1.0,1,A100,1697548597367,1697548599716,120,182.0,1.0,"[276, 2073]","[1697548597643, 1697548599716]"
1787,1787,449,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581088,1697548583923,120,,,"[201, 1522, 106, 102, 89, 88, 87, 84]","[1697548581289, 1697548582811, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583367]"
1788,1788,261,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599950,1697548602107,120,,,[21],[1697548599971]
1789,1789,480,10,[],200,llama-7b,128,1,820.0,1.0,1,A100,1697548599721,1697548600541,120,26.0,1.0,"[46, 774]","[1697548599767, 1697548600541]"
1790,1790,398,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583920,120,,,"[347, 1369, 109, 102, 88, 88, 87, 84]","[1697548581441, 1697548582810, 1697548582919, 1697548583021, 1697548583109, 1697548583197, 1697548583284, 1697548583368]"
1791,1791,622,12,[],200,llama-7b,128,1,1726.0,1.0,1,A100,1697548602127,1697548603853,120,20.0,1.0,"[53, 1672]","[1697548602180, 1697548603852]"
1792,1792,835,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548600546,1697548605273,120,,,"[49, 2080, 1438, 78, 76]","[1697548600595, 1697548602675, 1697548604113, 1697548604191, 1697548604267]"
1793,1793,746,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642997,120,,,[61],[1697548641446]
1794,1794,779,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629599,120,,,"[309, 1891]","[1697548627447, 1697548629338]"
1795,1795,235,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605285,1697548608967,120,,,"[296, 2157, 80, 79, 78]","[1697548605581, 1697548607738, 1697548607818, 1697548607897, 1697548607975]"
1796,1796,261,29,[],200,llama-7b,128,1,2205.0,1.0,1,A100,1697548643006,1697548645211,120,874.0,2.0,"[69, 2136]","[1697548643075, 1697548645211]"
1797,1797,621,30,[],200,llama-7b,128,1,6302.0,1.0,1,A100,1697548645213,1697548651515,120,88.0,20.0,"[16, 1846, 551, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86, 732, 86, 84, 76, 74, 73, 1002, 84]","[1697548645229, 1697548647075, 1697548647626, 1697548647722, 1697548647817, 1697548647911, 1697548647982, 1697548648075, 1697548648147, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650036, 1697548650122, 1697548650206, 1697548650282, 1697548650356, 1697548650429, 1697548651431, 1697548651515]"
1798,1798,208,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629607,1697548631889,120,,,"[255, 1755]","[1697548629862, 1697548631617]"
1799,1799,569,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631902,1697548633850,120,,,[163],[1697548632065]
1800,1800,809,2,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548583932,1697548585957,120,16.0,1.0,"[264, 1761]","[1697548584196, 1697548585957]"
1801,1801,240,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585962,1697548589557,120,,,"[33, 1217, 1177, 79, 78, 75]","[1697548585995, 1697548587212, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
1802,1802,317,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581091,1697548583920,120,,,"[273, 1445, 109, 102, 89, 88, 86, 84]","[1697548581364, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
1803,1803,203,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583921,120,,,"[392, 2415]","[1697548581486, 1697548583901]"
1804,1804,564,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586198,120,,,"[264, 1761]","[1697548584196, 1697548585957]"
1805,1805,263,12,[],200,llama-7b,128,1,1947.0,1.0,1,A100,1697548605283,1697548607230,120,15.0,1.0,"[56, 1891]","[1697548605339, 1697548607230]"
1806,1806,894,3,[],200,llama-7b,128,1,1968.0,1.0,1,A100,1697548586204,1697548588172,120,14.0,1.0,"[13, 1954]","[1697548586217, 1697548588171]"
1807,1807,617,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608966,120,,,"[36, 1498]","[1697548607273, 1697548608771]"
1808,1808,691,19,[],200,llama-7b,128,1,1092.0,1.0,1,A100,1697548624682,1697548625774,120,47.0,1.0,"[31, 1061]","[1697548624713, 1697548625774]"
1809,1809,216,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592062,120,,,"[373, 1649]","[1697548589941, 1697548591590]"
1810,1810,121,20,[],200,llama-7b,128,1,2161.0,1.0,1,A100,1697548625776,1697548627937,120,13.0,1.0,"[14, 2147]","[1697548625790, 1697548627937]"
1811,1811,483,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627940,1697548631885,120,,,"[12, 2319]","[1697548627952, 1697548630271]"
1812,1812,837,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633847,120,,,[76],[1697548631977]
1813,1813,572,4,[],200,llama-7b,128,1,2022.0,1.0,1,A100,1697548589568,1697548591590,120,16.0,1.0,"[94, 1927]","[1697548589662, 1697548591589]"
1814,1814,575,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[188, 1900]","[1697548592261, 1697548594161]"
1815,1815,926,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,"[57, 1200]","[1697548591651, 1697548592851]"
1816,1816,421,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586199,120,,,"[321, 1705]","[1697548584253, 1697548585958]"
1817,1817,91,7,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548594351,1697548596098,120,23.0,1.0,"[196, 1551]","[1697548594547, 1697548596098]"
1818,1818,876,16,[],200,llama-7b,128,1,1032.0,1.0,1,A100,1697548619290,1697548620322,120,11.0,1.0,"[43, 989]","[1697548619333, 1697548620322]"
1819,1819,266,23,[],200,llama-7b,128,1,1743.0,1.0,1,A100,1697548633854,1697548635597,120,9.0,1.0,"[90, 1653]","[1697548633944, 1697548635597]"
1820,1820,550,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617366,120,,,"[45, 792]","[1697548615080, 1697548615872]"
1821,1821,884,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583921,120,,,"[359, 1357, 109, 102, 88, 89, 86, 84]","[1697548581453, 1697548582810, 1697548582919, 1697548583021, 1697548583109, 1697548583198, 1697548583284, 1697548583368]"
1822,1822,441,8,[],200,llama-7b,128,1,1232.0,1.0,1,A100,1697548596103,1697548597335,120,6.0,1.0,"[63, 1169]","[1697548596166, 1697548597335]"
1823,1823,708,24,[],200,llama-7b,128,1,905.0,1.0,1,A100,1697548635603,1697548636508,120,140.0,1.0,"[41, 864]","[1697548635644, 1697548636508]"
1824,1824,382,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594348,120,,,"[42, 2045]","[1697548592114, 1697548594159]"
1825,1825,906,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617379,1697548620358,120,,,"[202, 1704, 111, 84, 82, 81, 80]","[1697548617581, 1697548619285, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
1826,1826,868,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586205,1697548589560,120,,,"[58, 1908, 218, 79, 79, 75]","[1697548586263, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
1827,1827,743,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594351,1697548597359,120,,,"[195, 1552, 250, 70, 58]","[1697548594546, 1697548596098, 1697548596348, 1697548596418, 1697548596476]"
1828,1828,337,19,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548620369,1697548622332,120,12.0,1.0,"[200, 1763]","[1697548620569, 1697548622332]"
1829,1829,145,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597368,1697548599941,120,,,"[411, 1939]","[1697548597779, 1697548599718]"
1830,1830,312,2,[],200,llama-7b,128,1,2027.0,1.0,1,A100,1697548583933,1697548585960,120,23.0,1.0,"[356, 1670]","[1697548584289, 1697548585959]"
1831,1831,136,15,[],200,llama-7b,128,1,838.0,1.0,1,A100,1697548615035,1697548615873,120,31.0,1.0,"[35, 803]","[1697548615070, 1697548615873]"
1832,1832,502,10,[],200,llama-7b,128,1,1934.0,1.0,1,A100,1697548599951,1697548601885,120,19.0,1.0,"[189, 1745]","[1697548600140, 1697548601885]"
1833,1833,494,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615876,1697548620353,120,,,"[15, 2065, 1440, 83, 83, 80, 81]","[1697548615891, 1697548617956, 1697548619396, 1697548619479, 1697548619562, 1697548619642, 1697548619723]"
1834,1834,859,11,[],200,llama-7b,128,1,786.0,1.0,1,A100,1697548601890,1697548602676,120,23.0,1.0,"[65, 721]","[1697548601955, 1697548602676]"
1835,1835,364,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620381,1697548622592,120,,,"[409, 1541]","[1697548620790, 1697548622331]"
1836,1836,289,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602679,1697548605264,120,,,"[33, 2358]","[1697548602712, 1697548605070]"
1837,1837,725,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622606,1697548624962,120,,,"[252, 1820]","[1697548622858, 1697548624678]"
1838,1838,1,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635904,120,,,"[325, 1418]","[1697548634180, 1697548635598]"
1839,1839,156,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624976,1697548629596,120,,,"[382, 2579]","[1697548625358, 1697548627937]"
1840,1840,503,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667368,120,,,"[121, 1699, 430, 83, 67, 84]","[1697548664338, 1697548666037, 1697548666467, 1697548666550, 1697548666617, 1697548666701]"
1841,1841,647,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608964,120,,,"[393, 1559, 507, 80, 79, 78]","[1697548605673, 1697548607232, 1697548607739, 1697548607819, 1697548607898, 1697548607976]"
1842,1842,510,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629603,1697548631889,120,,,[10],[1697548629613]
1843,1843,863,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633846,120,,,[61],[1697548631957]
1844,1844,877,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635912,1697548637694,120,,,[47],[1697548635959]
1845,1845,227,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653330,1697548657537,120,,,"[6, 2842]","[1697548653336, 1697548656178]"
1846,1846,163,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610832,120,,,[77],[1697548609050]
1847,1847,17,38,[],200,llama-7b,128,1,648.0,1.0,1,A100,1697548667377,1697548668025,120,23.0,1.0,"[77, 571]","[1697548667454, 1697548668025]"
1848,1848,589,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657545,1697548659868,120,,,"[103, 1930]","[1697548657648, 1697548659578]"
1849,1849,255,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599943,120,,,"[174, 2176]","[1697548597540, 1697548599716]"
1850,1850,240,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627941,1697548631886,120,,,"[23, 2307]","[1697548627964, 1697548630271]"
1851,1851,302,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637700,1697548639636,120,,,"[146, 1454]","[1697548637846, 1697548639300]"
1852,1852,749,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641374,120,,,[36],[1697548639682]
1853,1853,14,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661854,120,,,[285],[1697548660157]
1854,1854,181,29,[],200,llama-7b,128,1,12126.0,1.0,1,A100,1697548641384,1697548653510,120,91.0,39.0,"[17, 336, 42, 1261, 584, 1587, 85, 64, 954, 94, 91, 89, 68, 968, 97, 95, 93, 72, 93, 72, 891, 92, 88, 86, 731, 86, 86, 75, 74, 74, 991, 91, 74, 859, 319, 88, 70, 403, 97, 89]","[1697548641401, 1697548641737, 1697548641779, 1697548643040, 1697548643624, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647721, 1697548647816, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650034, 1697548650120, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651420, 1697548651511, 1697548651585, 1697548652444, 1697548652763, 1697548652851, 1697548652921, 1697548653324, 1697548653421, 1697548653510]"
1855,1855,373,38,[],200,llama-7b,128,1,1910.0,1.0,1,A100,1697548661867,1697548663777,120,15.0,1.0,"[179, 1731]","[1697548662046, 1697548663777]"
1856,1856,584,8,[],200,llama-7b,128,1,1933.0,1.0,1,A100,1697548599952,1697548601885,120,10.0,1.0,"[386, 1547]","[1697548600338, 1697548601885]"
1857,1857,698,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663782,1697548667371,120,,,"[6, 934, 1744, 83, 66, 84]","[1697548663788, 1697548664722, 1697548666466, 1697548666549, 1697548666615, 1697548666699]"
1858,1858,12,9,[],200,llama-7b,128,1,785.0,1.0,1,A100,1697548601890,1697548602675,120,11.0,1.0,"[50, 735]","[1697548601940, 1697548602675]"
1859,1859,690,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633844,120,,,[175],[1697548632071]
1860,1860,370,39,[],200,llama-7b,128,1,3170.0,1.0,1,A100,1697548668033,1697548671203,120,31.0,1.0,"[30, 3139]","[1697548668063, 1697548671202]"
1861,1861,264,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635894,120,,,"[131, 1613]","[1697548633985, 1697548635598]"
1862,1862,728,40,[],200,llama-7b,128,1,1868.0,1.0,1,A100,1697548671206,1697548673074,120,20.0,1.0,"[29, 1839]","[1697548671235, 1697548673074]"
1863,1863,625,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637696,120,,,[132],[1697548636047]
1864,1864,115,25,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548633854,1697548635601,120,13.0,1.0,"[219, 1528]","[1697548634073, 1697548635601]"
1865,1865,887,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597339,1697548599940,120,,,"[98, 507, 36]","[1697548597437, 1697548597944, 1697548597980]"
1866,1866,56,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639639,120,,,"[265, 1331]","[1697548637968, 1697548639299]"
1867,1867,411,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,[271],[1697548592344]
1868,1868,315,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[293, 1641]","[1697548600245, 1697548601886]"
1869,1869,208,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579130,1697548581081,120,,,"[18, 1916]","[1697548579148, 1697548581064]"
1870,1870,675,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622596,120,,,"[23, 1943]","[1697548620385, 1697548622328]"
1871,1871,558,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581093,1697548583921,120,,,"[306, 1411, 108, 102, 89, 89, 86, 84]","[1697548581399, 1697548582810, 1697548582918, 1697548583020, 1697548583109, 1697548583198, 1697548583284, 1697548583368]"
1872,1872,417,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639648,1697548641376,120,,,[112],[1697548639760]
1873,1873,746,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642997,120,,,[55],[1697548641440]
1874,1874,100,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622602,1697548624958,120,,,"[20, 2054]","[1697548622622, 1697548624676]"
1875,1875,528,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617379,1697548620358,120,,,"[188, 1718, 111, 84, 82, 81, 80]","[1697548617567, 1697548619285, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
1876,1876,567,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586198,120,,,"[308, 1718]","[1697548584240, 1697548585958]"
1877,1877,484,31,[],200,llama-7b,128,1,11334.0,1.0,1,A100,1697548643008,1697548654342,120,86.0,36.0,"[282, 3025, 93, 92, 88, 69, 968, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86, 732, 86, 85, 75, 74, 75, 991, 92, 74, 858, 319, 88, 70, 403, 97, 89, 84, 651, 96]","[1697548643290, 1697548646315, 1697548646408, 1697548646500, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650035, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650430, 1697548651421, 1697548651513, 1697548651587, 1697548652445, 1697548652764, 1697548652852, 1697548652922, 1697548653325, 1697548653422, 1697548653511, 1697548653595, 1697548654246, 1697548654342]"
1878,1878,74,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586205,1697548589559,120,,,"[35, 1931, 218, 79, 78, 76]","[1697548586240, 1697548588171, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
1879,1879,879,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599941,120,,,"[103, 2247]","[1697548597469, 1697548599716]"
1880,1880,841,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548654347,1697548657502,120,,,[20],[1697548654367]
1881,1881,433,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592062,120,,,[112],[1697548589686]
1882,1882,267,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657547,1697548659866,120,,,"[328, 1703]","[1697548657875, 1697548659578]"
1883,1883,762,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594348,120,,,"[22, 2065]","[1697548592094, 1697548594159]"
1884,1884,173,27,[],200,llama-7b,128,1,6298.0,1.0,1,A100,1697548643006,1697548649304,120,96.0,20.0,"[78, 1693, 434, 85, 64, 953, 94, 91, 91, 66, 970, 97, 95, 93, 72, 93, 70, 893, 92, 88, 86]","[1697548643084, 1697548644777, 1697548645211, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646589, 1697548646655, 1697548647625, 1697548647722, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648145, 1697548649038, 1697548649130, 1697548649218, 1697548649304]"
1885,1885,775,0,[],200,llama-7b,128,1,1310.0,1.0,1,A100,1697548575932,1697548577242,120,17.0,1.0,"[111, 1199]","[1697548576043, 1697548577242]"
1886,1886,351,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583111,1697548586200,120,,,"[9, 1305]","[1697548583120, 1697548584425]"
1887,1887,304,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[283, 1651]","[1697548600235, 1697548601886]"
1888,1888,656,16,[],200,llama-7b,128,1,1032.0,1.0,1,A100,1697548619290,1697548620322,120,26.0,1.0,"[40, 992]","[1697548619330, 1697548620322]"
1889,1889,628,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661856,120,,,[177],[1697548660049]
1890,1890,207,1,[],200,llama-7b,128,1,1194.0,1.0,1,A100,1697548577245,1697548578439,120,10.0,1.0,"[30, 1164]","[1697548577275, 1697548578439]"
1891,1891,752,9,[],200,llama-7b,128,1,2066.0,1.0,1,A100,1697548602127,1697548604193,120,39.0,3.0,"[53, 1672, 263, 78]","[1697548602180, 1697548603852, 1697548604115, 1697548604193]"
1892,1892,198,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597360,120,,,"[231, 1509, 250, 73, 57]","[1697548594587, 1697548596096, 1697548596346, 1697548596419, 1697548596476]"
1893,1893,59,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620325,1697548622596,120,,,"[16, 584]","[1697548620341, 1697548620925]"
1894,1894,557,8,[],200,llama-7b,128,1,2351.0,1.0,1,A100,1697548597367,1697548599718,120,31.0,1.0,"[397, 1954]","[1697548597764, 1697548599718]"
1895,1895,914,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599723,1697548602107,120,,,"[76, 741]","[1697548599799, 1697548600540]"
1896,1896,416,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622608,1697548624959,120,,,"[270, 1801]","[1697548622878, 1697548624679]"
1897,1897,428,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602109,1697548605264,120,,,"[25, 1717, 263, 78, 76]","[1697548602134, 1697548603851, 1697548604114, 1697548604192, 1697548604268]"
1898,1898,28,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661866,1697548664209,120,,,"[11, 875, 228]","[1697548661877, 1697548662752, 1697548662980]"
1899,1899,782,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608963,120,,,"[42, 1908, 508, 79, 79, 78]","[1697548605322, 1697548607230, 1697548607738, 1697548607817, 1697548607896, 1697548607974]"
1900,1900,183,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548604196,1697548605271,120,,,"[17, 857]","[1697548604213, 1697548605070]"
1901,1901,387,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667370,120,,,"[268, 1551, 429, 85, 65, 84]","[1697548664486, 1697548666037, 1697548666466, 1697548666551, 1697548666616, 1697548666700]"
1902,1902,210,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610832,120,,,[97],[1697548609070]
1903,1903,526,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649310,1697548655270,120,,,"[31, 2838, 266, 320, 88, 69, 403, 97, 88, 85, 650, 96, 85, 62, 80]","[1697548649341, 1697548652179, 1697548652445, 1697548652765, 1697548652853, 1697548652922, 1697548653325, 1697548653422, 1697548653510, 1697548653595, 1697548654245, 1697548654341, 1697548654426, 1697548654488, 1697548654568]"
1904,1904,570,13,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610851,1697548612818,120,18.0,1.0,"[220, 1747]","[1697548611071, 1697548612818]"
1905,1905,745,37,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667378,1697548669708,120,17.0,1.0,"[276, 2054]","[1697548667654, 1697548669708]"
1906,1906,6,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612822,1697548615231,120,,,[70],[1697548612892]
1907,1907,174,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671228,120,,,"[71, 1419]","[1697548669784, 1697548671203]"
1908,1908,533,39,[],200,llama-7b,128,1,2370.0,1.0,1,A100,1697548671244,1697548673614,120,216.0,2.0,"[255, 2114]","[1697548671499, 1697548673613]"
1909,1909,52,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673616,1697548677266,120,,,"[15, 1841]","[1697548673631, 1697548675472]"
1910,1910,884,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657502,120,,,"[265, 1880]","[1697548655546, 1697548657426]"
1911,1911,540,11,[],200,llama-7b,128,1,2692.0,1.0,1,A100,1697548605283,1697548607975,120,140.0,5.0,"[103, 1845, 507, 80, 79, 78]","[1697548605386, 1697548607231, 1697548607738, 1697548607818, 1697548607897, 1697548607975]"
1912,1912,334,15,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548615242,1697548617265,120,15.0,1.0,"[110, 1913]","[1697548615352, 1697548617265]"
1913,1913,694,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617269,1697548620355,120,,,"[31, 2096, 84, 82, 81, 80]","[1697548617300, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
1914,1914,898,12,[],200,llama-7b,128,1,2891.0,1.0,1,A100,1697548607977,1697548610868,120,79.0,2.0,"[6, 2885]","[1697548607983, 1697548610868]"
1915,1915,538,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583931,1697548586197,120,,,"[229, 1796]","[1697548584160, 1697548585956]"
1916,1916,327,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610869,1697548615234,120,,,"[380, 2288]","[1697548611249, 1697548613537]"
1917,1917,114,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620367,1697548622596,120,,,"[297, 1664]","[1697548620664, 1697548622328]"
1918,1918,654,4,[],200,llama-7b,128,1,2342.0,1.0,1,A100,1697548586205,1697548588547,120,47.0,4.0,"[50, 1915, 219, 79, 79]","[1697548586255, 1697548588170, 1697548588389, 1697548588468, 1697548588547]"
1919,1919,656,14,[],200,llama-7b,128,1,2022.0,1.0,1,A100,1697548615243,1697548617265,120,26.0,1.0,"[235, 1787]","[1697548615478, 1697548617265]"
1920,1920,899,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586205,1697548589560,120,,,"[142, 1824, 218, 79, 79, 75]","[1697548586347, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
1921,1921,83,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617269,1697548620355,120,,,"[21, 666, 1440, 84, 82, 80, 81]","[1697548617290, 1697548617956, 1697548619396, 1697548619480, 1697548619562, 1697548619642, 1697548619723]"
1922,1922,444,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622600,120,,,"[187, 1781]","[1697548620550, 1697548622331]"
1923,1923,474,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624958,120,,,[34],[1697548622637]
1924,1924,831,19,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624974,1697548627021,120,11.0,1.0,"[163, 1884]","[1697548625137, 1697548627021]"
1925,1925,802,17,[],200,llama-7b,128,1,2072.0,1.0,1,A100,1697548622608,1697548624680,120,9.0,1.0,"[289, 1782]","[1697548622897, 1697548624679]"
1926,1926,86,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588549,1697548592064,120,,,"[14, 1595]","[1697548588563, 1697548590158]"
1927,1927,203,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624682,1697548627101,120,,,"[21, 1070]","[1697548624703, 1697548625773]"
1928,1928,234,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627026,1697548629598,120,,,"[71, 842]","[1697548627097, 1697548627939]"
1929,1929,413,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[205, 1881]","[1697548592278, 1697548594159]"
1930,1930,561,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[143, 2089]","[1697548627249, 1697548629338]"
1931,1931,918,20,[],200,llama-7b,128,1,2010.0,1.0,1,A100,1697548629608,1697548631618,120,23.0,1.0,"[264, 1746]","[1697548629872, 1697548631618]"
1932,1932,771,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597358,120,,,"[114, 1633, 251, 72, 56]","[1697548594464, 1697548596097, 1697548596348, 1697548596420, 1697548596476]"
1933,1933,329,4,[],200,llama-7b,128,1,2016.0,1.0,1,A100,1697548589574,1697548591590,120,15.0,1.0,"[78, 1937]","[1697548589652, 1697548591589]"
1934,1934,86,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633848,120,,,[292],[1697548632188]
1935,1935,598,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629603,1697548631887,120,,,"[85, 1931]","[1697548629688, 1697548631619]"
1936,1936,551,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592076,1697548594348,120,,,"[305, 1779]","[1697548592381, 1697548594160]"
1937,1937,683,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,[40],[1697548591634]
1938,1938,26,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633844,120,,,[49],[1697548631945]
1939,1939,196,8,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597367,1697548599717,120,13.0,1.0,"[419, 1931]","[1697548597786, 1697548599717]"
1940,1940,380,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633853,1697548635904,120,,,"[47, 590]","[1697548633900, 1697548634490]"
1941,1941,348,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631623,1697548635894,120,,,"[22, 991, 1249, 57, 547]","[1697548631645, 1697548632636, 1697548633885, 1697548633942, 1697548634489]"
1942,1942,739,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[166],[1697548636082]
1943,1943,555,9,[],200,llama-7b,128,1,819.0,1.0,1,A100,1697548599722,1697548600541,120,11.0,1.0,"[87, 732]","[1697548599809, 1697548600541]"
1944,1944,916,10,[],200,llama-7b,128,1,2131.0,1.0,1,A100,1697548600544,1697548602675,120,8.0,1.0,"[30, 2100]","[1697548600574, 1697548602674]"
1945,1945,108,6,[],200,llama-7b,128,1,1998.0,1.0,1,A100,1697548594350,1697548596348,120,182.0,2.0,"[47, 1699, 251]","[1697548594397, 1697548596096, 1697548596347]"
1946,1946,135,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639638,120,,,"[182, 1416]","[1697548637885, 1697548639301]"
1947,1947,445,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635903,120,,,"[291, 1451]","[1697548634146, 1697548635597]"
1948,1948,209,20,[],200,llama-7b,128,1,2073.0,1.0,1,A100,1697548622603,1697548624676,120,20.0,1.0,"[29, 2044]","[1697548622632, 1697548624676]"
1949,1949,319,11,[],200,llama-7b,128,1,2391.0,1.0,1,A100,1697548602679,1697548605070,120,31.0,1.0,"[29, 2362]","[1697548602708, 1697548605070]"
1950,1950,803,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637697,120,,,[151],[1697548636066]
1951,1951,438,7,[],200,llama-7b,128,1,984.0,1.0,1,A100,1697548596352,1697548597336,120,9.0,1.0,"[17, 967]","[1697548596369, 1697548597336]"
1952,1952,539,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624682,1697548627101,120,,,"[16, 1077]","[1697548624698, 1697548625775]"
1953,1953,95,20,[],200,llama-7b,128,1,2055.0,1.0,1,A100,1697548624966,1697548627021,120,12.0,1.0,"[269, 1785]","[1697548625235, 1697548627020]"
1954,1954,496,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641374,120,,,[293],[1697548639948]
1955,1955,429,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627024,1697548629597,120,,,"[31, 883]","[1697548627055, 1697548627938]"
1956,1956,224,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639649,120,,,"[270, 1326]","[1697548637973, 1697548639299]"
1957,1957,583,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639654,1697548641375,120,,,[314],[1697548639968]
1958,1958,897,22,[],200,llama-7b,128,1,2201.0,1.0,1,A100,1697548627138,1697548629339,120,9.0,1.0,"[329, 1872]","[1697548627467, 1697548629339]"
1959,1959,323,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629362,1697548631889,120,,,"[93, 818]","[1697548629455, 1697548630273]"
1960,1960,796,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597337,1697548599938,120,,,"[16, 110, 517]","[1697548597353, 1697548597463, 1697548597980]"
1961,1961,661,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610853,1697548613000,120,,,"[344, 1620]","[1697548611197, 1697548612817]"
1962,1962,677,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633847,120,,,[267],[1697548632168]
1963,1963,232,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602106,120,,,"[229, 1704]","[1697548600181, 1697548601885]"
1964,1964,105,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635894,120,,,"[121, 1623]","[1697548633975, 1697548635598]"
1965,1965,591,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602117,1697548605265,120,,,"[138, 1598, 260, 80, 74]","[1697548602255, 1697548603853, 1697548604113, 1697548604193, 1697548604267]"
1966,1966,557,9,[],200,llama-7b,128,1,2351.0,1.0,1,A100,1697548597366,1697548599717,120,31.0,1.0,"[128, 2223]","[1697548597494, 1697548599717]"
1967,1967,93,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613020,1697548615233,120,,,"[344, 1668]","[1697548613364, 1697548615032]"
1968,1968,920,11,[],200,llama-7b,128,1,2617.0,1.0,1,A100,1697548605280,1697548607897,120,96.0,4.0,"[97, 1854, 507, 79, 80]","[1697548605377, 1697548607231, 1697548607738, 1697548607817, 1697548607897]"
1969,1969,450,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617369,120,,,"[111, 1912]","[1697548615353, 1697548617265]"
1970,1970,806,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617386,1697548620355,120,,,"[350, 1551, 110, 84, 82, 81, 80]","[1697548617736, 1697548619287, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619724]"
1971,1971,231,19,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548620369,1697548622332,120,13.0,1.0,"[190, 1773]","[1697548620559, 1697548622332]"
1972,1972,919,10,[],200,llama-7b,128,1,820.0,1.0,1,A100,1697548599721,1697548600541,120,14.0,1.0,"[51, 769]","[1697548599772, 1697548600541]"
1973,1973,561,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622336,1697548624958,120,,,"[106, 1016]","[1697548622442, 1697548623458]"
1974,1974,341,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607900,1697548612995,120,,,"[16, 1709, 1243, 569]","[1697548607916, 1697548609625, 1697548610868, 1697548611437]"
1975,1975,319,11,[],200,llama-7b,128,1,2131.0,1.0,1,A100,1697548600544,1697548602675,120,31.0,1.0,"[28, 2103]","[1697548600572, 1697548602675]"
1976,1976,678,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602677,1697548605263,120,,,"[11, 2381]","[1697548602688, 1697548605069]"
1977,1977,99,13,[],200,llama-7b,128,1,1950.0,1.0,1,A100,1697548605280,1697548607230,120,10.0,1.0,"[288, 1662]","[1697548605568, 1697548607230]"
1978,1978,700,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615231,120,,,[253],[1697548613259]
1979,1979,457,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607236,1697548608966,120,,,[22],[1697548607258]
1980,1980,128,14,[],200,llama-7b,128,1,2022.0,1.0,1,A100,1697548615243,1697548617265,120,9.0,1.0,"[20, 2002]","[1697548615263, 1697548617265]"
1981,1981,919,21,[],200,llama-7b,128,1,2051.0,1.0,1,A100,1697548624969,1697548627020,120,14.0,1.0,"[75, 1976]","[1697548625044, 1697548627020]"
1982,1982,335,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633847,120,,,[262],[1697548632163]
1983,1983,816,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610836,120,,,[322],[1697548609297]
1984,1984,488,15,[],200,llama-7b,128,1,687.0,1.0,1,A100,1697548617269,1697548617956,120,6.0,1.0,"[26, 661]","[1697548617295, 1697548617956]"
1985,1985,689,26,[],200,llama-7b,128,1,1745.0,1.0,1,A100,1697548633855,1697548635600,120,15.0,1.0,"[198, 1546]","[1697548634053, 1697548635599]"
1986,1986,355,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627024,1697548629597,120,,,"[26, 887]","[1697548627050, 1697548627937]"
1987,1987,117,27,[],200,llama-7b,128,1,2129.0,1.0,1,A100,1697548635603,1697548637732,120,364.0,2.0,"[36, 870, 1222]","[1697548635639, 1697548636509, 1697548637731]"
1988,1988,443,28,[],200,llama-7b,128,1,1566.0,1.0,1,A100,1697548637735,1697548639301,120,19.0,1.0,"[330, 1236]","[1697548638065, 1697548639301]"
1989,1989,804,29,[],200,llama-7b,128,1,879.0,1.0,1,A100,1697548639304,1697548640183,120,20.0,1.0,"[52, 827]","[1697548639356, 1697548640183]"
1990,1990,235,30,[],200,llama-7b,128,1,6466.0,1.0,1,A100,1697548640190,1697548646656,120,161.0,12.0,"[20, 1569, 1261, 584, 1587, 85, 64, 954, 94, 91, 89, 68]","[1697548640210, 1697548641779, 1697548643040, 1697548643624, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656]"
1991,1991,655,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688900,120,,,"[321, 1664]","[1697548687050, 1697548688714]"
1992,1992,714,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629601,1697548631889,120,,,"[22, 1993]","[1697548629623, 1697548631616]"
1993,1993,562,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583921,120,,,"[56, 1664, 109, 102, 89, 88, 86, 84]","[1697548581145, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
1994,1994,113,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633846,120,,,[66],[1697548631967]
1995,1995,87,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691086,120,,,"[53, 1885]","[1697548688960, 1697548690845]"
1996,1996,417,44,[],200,llama-7b,128,1,2151.0,1.0,1,A100,1697548691096,1697548693247,120,17.0,1.0,"[256, 1894]","[1697548691352, 1697548693246]"
1997,1997,775,45,[],200,llama-7b,128,1,892.0,1.0,1,A100,1697548693252,1697548694144,120,17.0,1.0,"[27, 865]","[1697548693279, 1697548694144]"
1998,1998,464,25,[],200,llama-7b,128,1,1745.0,1.0,1,A100,1697548633854,1697548635599,120,12.0,1.0,"[126, 1618]","[1697548633980, 1697548635598]"
1999,1999,890,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583933,1697548586199,120,,,"[302, 1723]","[1697548584235, 1697548585958]"
2000,2000,336,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610849,1697548612998,120,,,"[47, 1919]","[1697548610896, 1697548612815]"
2001,2001,823,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635604,1697548639637,120,,,"[48, 856, 1224, 416]","[1697548635652, 1697548636508, 1697548637732, 1697548638148]"
2002,2002,203,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548694161,1697548697833,120,,,"[18, 2549]","[1697548694179, 1697548696728]"
2003,2003,824,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617959,1697548620356,120,,,"[15, 2347]","[1697548617974, 1697548620321]"
2004,2004,12,2,[],200,llama-7b,128,1,1720.0,1.0,1,A100,1697548581089,1697548582809,120,11.0,1.0,"[54, 1666]","[1697548581143, 1697548582809]"
2005,2005,367,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582814,1697548583922,120,,,"[51, 1037]","[1697548582865, 1697548583902]"
2006,2006,912,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610851,1697548612997,120,,,"[221, 1746]","[1697548611072, 1697548612818]"
2007,2007,724,4,[],200,llama-7b,128,1,3281.0,1.0,1,A100,1697548583933,1697548587214,120,11.0,1.0,"[407, 2874]","[1697548584340, 1697548587214]"
2008,2008,252,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622600,120,,,"[178, 1790]","[1697548620541, 1697548622331]"
2009,2009,697,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615235,120,,,"[169, 1858]","[1697548613175, 1697548615033]"
2010,2010,317,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586210,1697548589562,120,,,"[245, 1717, 217, 79, 79, 77]","[1697548586455, 1697548588172, 1697548588389, 1697548588468, 1697548588547, 1697548588624]"
2011,2011,241,5,[],200,llama-7b,128,1,2158.0,1.0,1,A100,1697548587223,1697548589381,120,19.0,1.0,"[50, 2108]","[1697548587273, 1697548589381]"
2012,2012,678,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592065,120,,,"[220, 1794]","[1697548589794, 1697548591588]"
2013,2013,607,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589385,1697548592065,120,,,"[21, 753]","[1697548589406, 1697548590159]"
2014,2014,106,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[203, 1883]","[1697548592276, 1697548594159]"
2015,2015,35,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[183, 1905]","[1697548592256, 1697548594161]"
2016,2016,612,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622608,1697548624959,120,,,"[274, 1797]","[1697548622882, 1697548624679]"
2017,2017,556,6,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548594356,1697548596098,120,9.0,1.0,"[290, 1451]","[1697548594646, 1697548596097]"
2018,2018,910,7,[],200,llama-7b,128,1,1231.0,1.0,1,A100,1697548596104,1697548597335,120,8.0,1.0,"[95, 1136]","[1697548596199, 1697548597335]"
2019,2019,76,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548639638,120,,,"[355, 1823, 54]","[1697548636271, 1697548638094, 1697548638148]"
2020,2020,339,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597338,1697548599940,120,,,"[101, 505, 36]","[1697548597439, 1697548597944, 1697548597980]"
2021,2021,698,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599950,1697548602108,120,,,"[24, 1909]","[1697548599974, 1697548601883]"
2022,2022,37,19,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624974,1697548627021,120,20.0,1.0,"[290, 1757]","[1697548625264, 1697548627021]"
2023,2023,58,25,[],200,llama-7b,128,1,1743.0,1.0,1,A100,1697548633855,1697548635598,120,15.0,1.0,"[320, 1423]","[1697548634175, 1697548635598]"
2024,2024,305,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610870,1697548613000,120,,,"[369, 1578]","[1697548611239, 1697548612817]"
2025,2025,127,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605266,120,,,"[156, 1571, 260, 79, 75]","[1697548602283, 1697548603854, 1697548604114, 1697548604193, 1697548604268]"
2026,2026,390,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627024,1697548629597,120,,,"[36, 878]","[1697548627060, 1697548627938]"
2027,2027,852,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[54, 2020]","[1697548622657, 1697548624677]"
2028,2028,434,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641374,120,,,[90],[1697548639736]
2029,2029,663,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613008,1697548615237,120,,,[231],[1697548613239]
2030,2030,57,36,[],200,llama-7b,128,1,1818.0,1.0,1,A100,1697548664218,1697548666036,120,13.0,1.0,"[247, 1571]","[1697548664465, 1697548666036]"
2031,2031,368,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627100,120,,,"[197, 1849]","[1697548625171, 1697548627020]"
2032,2032,880,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643003,120,,,[365],[1697548641751]
2033,2033,60,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617370,120,,,[405],[1697548615649]
2034,2034,704,2,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548583926,1697548585959,120,14.0,1.0,"[156, 1877]","[1697548584082, 1697548585959]"
2035,2035,421,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617387,1697548620355,120,,,"[352, 1548, 110, 84, 82, 81, 81]","[1697548617739, 1697548619287, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619725]"
2036,2036,129,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585966,1697548589558,120,,,"[120, 1127, 1176, 79, 78, 76]","[1697548586086, 1697548587213, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
2037,2037,502,37,[],200,llama-7b,128,1,1300.0,1.0,1,A100,1697548666047,1697548667347,120,19.0,1.0,"[32, 1267]","[1697548666079, 1697548667346]"
2038,2038,864,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667350,1697548671225,120,,,"[20, 654, 36, 2117, 85, 64, 64, 80]","[1697548667370, 1697548668024, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670390, 1697548670470]"
2039,2039,489,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592061,120,,,[20],[1697548589588]
2040,2040,24,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603857,1697548605265,120,,,[39],[1697548603896]
2041,2041,846,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594348,120,,,"[67, 2020]","[1697548592139, 1697548594159]"
2042,2042,547,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639649,1697548641378,120,,,[191],[1697548639840]
2043,2043,316,27,[],200,llama-7b,128,1,6293.0,1.0,1,A100,1697548643012,1697548649305,120,86.0,20.0,"[157, 1608, 434, 86, 64, 953, 94, 91, 89, 68, 968, 96, 97, 93, 70, 94, 71, 894, 91, 88, 87]","[1697548643169, 1697548644777, 1697548645211, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647817, 1697548647910, 1697548647980, 1697548648074, 1697548648145, 1697548649039, 1697548649130, 1697548649218, 1697548649305]"
2044,2044,907,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642996,120,,,[41],[1697548641426]
2045,2045,149,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633843,120,,,[18],[1697548631913]
2046,2046,779,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620361,1697548622596,120,,,"[14, 1952]","[1697548620375, 1697548622327]"
2047,2047,512,23,[],200,llama-7b,128,1,1745.0,1.0,1,A100,1697548633854,1697548635599,120,11.0,1.0,"[135, 1610]","[1697548633989, 1697548635599]"
2048,2048,294,39,[],200,llama-7b,128,1,2377.0,1.0,1,A100,1697548671237,1697548673614,120,9.0,2.0,"[35, 2342]","[1697548671272, 1697548673614]"
2049,2049,873,24,[],200,llama-7b,128,1,905.0,1.0,1,A100,1697548635603,1697548636508,120,6.0,1.0,"[46, 859]","[1697548635649, 1697548636508]"
2050,2050,389,25,[],200,llama-7b,128,1,1583.0,1.0,1,A100,1697548636511,1697548638094,120,8.0,1.0,"[6, 1577]","[1697548636517, 1697548638094]"
2051,2051,749,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548638096,1697548639645,120,,,"[5, 1200]","[1697548638101, 1697548639301]"
2052,2052,327,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588175,1697548589559,120,,,"[44, 1160]","[1697548588219, 1697548589379]"
2053,2053,426,29,[],200,llama-7b,128,1,10506.0,1.0,1,A100,1697548643005,1697548653511,120,79.0,36.0,"[54, 2151, 86, 64, 953, 94, 91, 91, 66, 970, 97, 95, 93, 72, 93, 70, 893, 92, 88, 86, 730, 87, 85, 75, 74, 74, 992, 91, 74, 858, 320, 88, 70, 403, 96, 90]","[1697548643059, 1697548645210, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646589, 1697548646655, 1697548647625, 1697548647722, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648145, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421, 1697548651512, 1697548651586, 1697548652444, 1697548652764, 1697548652852, 1697548652922, 1697548653325, 1697548653421, 1697548653511]"
2054,2054,169,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639653,1697548641375,120,,,[300],[1697548639953]
2055,2055,529,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641387,1697548643002,120,,,[354],[1697548641741]
2056,2056,675,28,[],200,llama-7b,128,1,3613.0,1.0,1,A100,1697548649309,1697548652922,120,563.0,5.0,"[16, 2853, 267, 320, 88, 69]","[1697548649325, 1697548652178, 1697548652445, 1697548652765, 1697548652853, 1697548652922]"
2057,2057,356,6,[],200,llama-7b,128,1,1997.0,1.0,1,A100,1697548594351,1697548596348,120,874.0,2.0,"[118, 1628, 251]","[1697548594469, 1697548596097, 1697548596348]"
2058,2058,180,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586201,1697548589559,120,,,"[14, 1956, 218, 79, 78, 76]","[1697548586215, 1697548588171, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
2059,2059,131,40,[],200,llama-7b,128,1,2329.0,1.0,1,A100,1697548667378,1697548669707,120,8.0,1.0,"[170, 2159]","[1697548667548, 1697548669707]"
2060,2060,371,10,[],200,llama-7b,128,1,2390.0,1.0,1,A100,1697548602680,1697548605070,120,13.0,1.0,"[55, 2334]","[1697548602735, 1697548605069]"
2061,2061,522,15,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610850,1697548612817,120,20.0,1.0,"[187, 1780]","[1697548611037, 1697548612817]"
2062,2062,801,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597337,1697548599939,120,,,"[35, 572, 36]","[1697548597372, 1697548597944, 1697548597980]"
2063,2063,164,6,[],200,llama-7b,128,1,2087.0,1.0,1,A100,1697548592073,1697548594160,120,15.0,1.0,"[286, 1801]","[1697548592359, 1697548594160]"
2064,2064,229,10,[],200,llama-7b,128,1,1934.0,1.0,1,A100,1697548599951,1697548601885,120,15.0,1.0,"[204, 1730]","[1697548600155, 1697548601885]"
2065,2065,777,10,[],200,llama-7b,128,1,1935.0,1.0,1,A100,1697548599953,1697548601888,120,9.0,1.0,"[440, 1494]","[1697548600393, 1697548601887]"
2066,2066,591,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601889,1697548605271,120,,,"[26, 760, 1438, 78, 76]","[1697548601915, 1697548602675, 1697548604113, 1697548604191, 1697548604267]"
2067,2067,478,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602677,1697548605263,120,,,"[6, 2386]","[1697548602683, 1697548605069]"
2068,2068,528,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594164,1697548597360,120,,,"[36, 726, 1420, 72, 58]","[1697548594200, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
2069,2069,832,11,[],200,llama-7b,128,1,1953.0,1.0,1,A100,1697548605279,1697548607232,120,15.0,1.0,"[102, 1850]","[1697548605381, 1697548607231]"
2070,2070,264,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608968,120,,,"[71, 1462]","[1697548607308, 1697548608770]"
2071,2071,212,11,[],200,llama-7b,128,1,785.0,1.0,1,A100,1697548601891,1697548602676,120,31.0,1.0,"[59, 726]","[1697548601950, 1697548602676]"
2072,2072,569,12,[],200,llama-7b,128,1,2389.0,1.0,1,A100,1697548602680,1697548605069,120,16.0,1.0,"[38, 2351]","[1697548602718, 1697548605069]"
2073,2073,880,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612822,1697548615231,120,,,"[75, 642]","[1697548612897, 1697548613539]"
2074,2074,0,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605073,1697548608960,120,,,"[25, 639, 2000, 80, 78, 78]","[1697548605098, 1697548605737, 1697548607737, 1697548607817, 1697548607895, 1697548607973]"
2075,2075,623,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610833,120,,,[327],[1697548609302]
2076,2076,310,17,[],200,llama-7b,128,1,2024.0,1.0,1,A100,1697548615242,1697548617266,120,26.0,1.0,"[106, 1917]","[1697548615348, 1697548617265]"
2077,2077,159,41,[],200,llama-7b,128,1,1238.0,1.0,1,A100,1697548673080,1697548674318,120,31.0,1.0,"[46, 1192]","[1697548673126, 1697548674318]"
2078,2078,886,8,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597367,1697548599717,120,17.0,1.0,"[292, 2058]","[1697548597659, 1697548599717]"
2079,2079,26,14,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610848,1697548612815,120,18.0,1.0,"[38, 1929]","[1697548610886, 1697548612815]"
2080,2080,523,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674321,1697548677256,120,,,"[20, 1132]","[1697548674341, 1697548675473]"
2081,2081,473,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635604,1697548639637,120,,,"[63, 841, 1223, 417]","[1697548635667, 1697548636508, 1697548637731, 1697548638148]"
2082,2082,287,9,[],200,llama-7b,128,1,819.0,1.0,1,A100,1697548599721,1697548600540,120,10.0,1.0,"[56, 762]","[1697548599777, 1697548600539]"
2083,2083,852,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677274,1697548679649,120,,,"[297, 1903]","[1697548677571, 1697548679474]"
2084,2084,425,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597337,1697548599939,120,,,"[35, 572, 36]","[1697548597372, 1697548597944, 1697548597980]"
2085,2085,542,30,[],200,llama-7b,128,1,1552.0,1.0,1,A100,1697548653515,1697548655067,120,11.0,1.0,"[6, 1546]","[1697548653521, 1697548655067]"
2086,2086,787,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602107,120,,,"[84, 1849]","[1697548600035, 1697548601884]"
2087,2087,283,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583921,120,,,"[51, 1669, 109, 102, 89, 88, 86, 84]","[1697548581140, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583367]"
2088,2088,901,31,[],200,llama-7b,128,1,1109.0,1.0,1,A100,1697548655072,1697548656181,120,17.0,1.0,"[23, 1085]","[1697548655095, 1697548656180]"
2089,2089,302,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605266,120,,,"[232, 1491, 262, 78, 76]","[1697548602361, 1697548603852, 1697548604114, 1697548604192, 1697548604268]"
2090,2090,332,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610823,120,,,[6],[1697548608979]
2091,2091,297,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548656184,1697548659864,120,,,"[38, 2066]","[1697548656222, 1697548658288]"
2092,2092,642,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548600543,1697548605272,120,,,"[10, 2121, 1439, 78, 76]","[1697548600553, 1697548602674, 1697548604113, 1697548604191, 1697548604267]"
2093,2093,656,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661856,120,,,[114],[1697548659986]
2094,2094,646,11,[],200,llama-7b,128,1,1726.0,1.0,1,A100,1697548602127,1697548603853,120,14.0,1.0,"[58, 1668]","[1697548602185, 1697548603853]"
2095,2095,401,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649221,1697548655269,120,,,"[11, 2945, 267, 320, 88, 70, 403, 96, 89, 84, 654, 97, 82, 63, 80]","[1697548649232, 1697548652177, 1697548652444, 1697548652764, 1697548652852, 1697548652922, 1697548653325, 1697548653421, 1697548653510, 1697548653594, 1697548654248, 1697548654345, 1697548654427, 1697548654490, 1697548654570]"
2096,2096,68,11,[],200,llama-7b,128,1,1946.0,1.0,1,A100,1697548605283,1697548607229,120,12.0,1.0,"[203, 1743]","[1697548605486, 1697548607229]"
2097,2097,75,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603857,1697548605264,120,,,"[24, 1188]","[1697548603881, 1697548605069]"
2098,2098,81,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661866,1697548664210,120,,,"[17, 869, 228]","[1697548661883, 1697548662752, 1697548662980]"
2099,2099,426,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608965,120,,,[16],[1697548607253]
2100,2100,436,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605285,1697548608963,120,,,"[363, 1584, 507, 79, 80, 77]","[1697548605648, 1697548607232, 1697548607739, 1697548607818, 1697548607898, 1697548607975]"
2101,2101,685,15,[],200,llama-7b,128,1,592.0,1.0,1,A100,1697548610846,1697548611438,120,364.0,2.0,"[7, 535, 50]","[1697548610853, 1697548611388, 1697548611438]"
2102,2102,439,35,[],200,llama-7b,128,1,2398.0,1.0,1,A100,1697548664218,1697548666616,120,13.0,4.0,"[249, 1569, 430, 84, 66]","[1697548664467, 1697548666036, 1697548666466, 1697548666550, 1697548666616]"
2103,2103,787,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610834,120,,,[185],[1697548609158]
2104,2104,191,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610847,1697548612998,120,,,"[31, 1936]","[1697548610878, 1697548612814]"
2105,2105,549,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613008,1697548615232,120,,,"[271, 1752]","[1697548613279, 1697548615031]"
2106,2106,802,36,[],200,llama-7b,128,1,728.0,1.0,1,A100,1697548666620,1697548667348,120,9.0,1.0,"[17, 711]","[1697548666637, 1697548667348]"
2107,2107,314,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597362,120,,,"[335, 1407, 248, 74, 57]","[1697548594691, 1697548596098, 1697548596346, 1697548596420, 1697548596477]"
2108,2108,877,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597368,1697548599942,120,,,"[470, 1879]","[1697548597838, 1697548599717]"
2109,2109,205,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667351,1697548671225,120,,,"[28, 645, 36, 2117, 85, 64, 64, 80]","[1697548667379, 1697548668024, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670390, 1697548670470]"
2110,2110,673,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597368,1697548599943,120,,,"[493, 1857]","[1697548597861, 1697548599718]"
2111,2111,757,32,[],200,llama-7b,128,1,2145.0,1.0,1,A100,1697548655280,1697548657425,120,20.0,1.0,"[19, 2126]","[1697548655299, 1697548657425]"
2112,2112,98,9,[],200,llama-7b,128,1,1934.0,1.0,1,A100,1697548599953,1697548601887,120,14.0,1.0,"[429, 1505]","[1697548600382, 1697548601887]"
2113,2113,185,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657429,1697548659865,120,,,"[31, 828]","[1697548657460, 1697548658288]"
2114,2114,451,10,[],200,llama-7b,128,1,784.0,1.0,1,A100,1697548601892,1697548602676,120,286.0,1.0,"[72, 711]","[1697548601964, 1697548602675]"
2115,2115,631,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661854,120,,,[285],[1697548660157]
2116,2116,893,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622595,120,,,"[274, 1684]","[1697548620644, 1697548622328]"
2117,2117,781,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602679,1697548605263,120,,,"[14, 2376]","[1697548602693, 1697548605069]"
2118,2118,927,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586204,1697548589559,120,,,"[19, 1948, 218, 79, 78, 76]","[1697548586223, 1697548588171, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
2119,2119,214,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605285,1697548608961,120,,,"[307, 1638, 509, 79, 79, 78]","[1697548605592, 1697548607230, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
2120,2120,342,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617386,1697548620354,120,,,"[288, 1613, 110, 84, 82, 81, 80]","[1697548617674, 1697548619287, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619724]"
2121,2121,323,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624952,120,,,"[136, 1938]","[1697548622740, 1697548624678]"
2122,2122,573,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610824,120,,,[31],[1697548609004]
2123,2123,681,21,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624974,1697548627021,120,23.0,1.0,"[270, 1777]","[1697548625244, 1697548627021]"
2124,2124,348,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589572,1697548592061,120,,,"[75, 1942]","[1697548589647, 1697548591589]"
2125,2125,82,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627023,1697548629597,120,,,"[17, 898]","[1697548627040, 1697548627938]"
2126,2126,708,6,[],200,llama-7b,128,1,2087.0,1.0,1,A100,1697548592072,1697548594159,120,140.0,1.0,"[12, 2075]","[1697548592084, 1697548594159]"
2127,2127,433,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631890,120,,,[26],[1697548629628]
2128,2128,790,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633849,120,,,[155],[1697548632056]
2129,2129,6,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610847,1697548612995,120,,,[199],[1697548611046]
2130,2130,676,20,[],200,llama-7b,128,1,1961.0,1.0,1,A100,1697548620368,1697548622329,120,19.0,1.0,"[322, 1639]","[1697548620690, 1697548622329]"
2131,2131,220,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635895,120,,,"[204, 1542]","[1697548634058, 1697548635600]"
2132,2132,104,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624951,120,,,"[27, 1094]","[1697548622362, 1697548623456]"
2133,2133,71,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583921,120,,,"[12, 1707, 110, 102, 89, 88, 86, 83]","[1697548581101, 1697548582808, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583283, 1697548583366]"
2134,2134,359,15,[],200,llama-7b,128,1,2028.0,1.0,1,A100,1697548613006,1697548615034,120,10.0,1.0,"[159, 1868]","[1697548613165, 1697548615033]"
2135,2135,458,22,[],200,llama-7b,128,1,2058.0,1.0,1,A100,1697548624962,1697548627020,120,11.0,1.0,"[19, 2039]","[1697548624981, 1697548627020]"
2136,2136,690,16,[],200,llama-7b,128,1,835.0,1.0,1,A100,1697548615038,1697548615873,120,39.0,1.0,"[72, 763]","[1697548615110, 1697548615873]"
2137,2137,118,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615877,1697548620354,120,,,"[20, 2059, 1440, 84, 82, 80, 81]","[1697548615897, 1697548617956, 1697548619396, 1697548619480, 1697548619562, 1697548619642, 1697548619723]"
2138,2138,828,0,[],200,llama-7b,128,1,3448.0,1.0,1,A100,1697548575932,1697548579380,120,182.0,6.0,"[247, 1123, 1824, 93, 92, 68]","[1697548576179, 1697548577302, 1697548579126, 1697548579219, 1697548579311, 1697548579379]"
2139,2139,818,23,[],200,llama-7b,128,1,914.0,1.0,1,A100,1697548627025,1697548627939,120,13.0,1.0,"[55, 859]","[1697548627080, 1697548627939]"
2140,2140,820,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548658289,1697548664203,120,,,"[16, 2395, 1196, 57, 1026]","[1697548658305, 1697548660700, 1697548661896, 1697548661953, 1697548662979]"
2141,2141,432,2,[],200,llama-7b,128,1,2026.0,1.0,1,A100,1697548583932,1697548585958,120,13.0,1.0,"[328, 1698]","[1697548584260, 1697548585958]"
2142,2142,792,3,[],200,llama-7b,128,1,1250.0,1.0,1,A100,1697548585963,1697548587213,120,11.0,1.0,"[79, 1171]","[1697548586042, 1697548587213]"
2143,2143,253,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579383,1697548581082,120,,,"[16, 1665]","[1697548579399, 1697548581064]"
2144,2144,817,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627100,120,,,"[92, 1954]","[1697548625066, 1697548627020]"
2145,2145,223,4,[],200,llama-7b,128,1,2160.0,1.0,1,A100,1697548587220,1697548589380,120,16.0,1.0,"[32, 2128]","[1697548587252, 1697548589380]"
2146,2146,250,34,[],200,llama-7b,128,1,1819.0,1.0,1,A100,1697548664217,1697548666036,120,31.0,1.0,"[53, 1766]","[1697548664270, 1697548666036]"
2147,2147,701,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583922,120,,,"[90, 1630, 108, 103, 88, 89, 86, 83]","[1697548581180, 1697548582810, 1697548582918, 1697548583021, 1697548583109, 1697548583198, 1697548583284, 1697548583367]"
2148,2148,608,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666047,1697548667369,120,,,"[47, 1253]","[1697548666094, 1697548667347]"
2149,2149,577,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589384,1697548592064,120,,,"[17, 758]","[1697548589401, 1697548590159]"
2150,2150,706,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635912,1697548637694,120,,,[52],[1697548635964]
2151,2151,73,11,[],200,llama-7b,128,1,817.0,1.0,1,A100,1697548599724,1697548600541,120,9.0,1.0,"[90, 727]","[1697548599814, 1697548600541]"
2152,2152,430,12,[],200,llama-7b,128,1,2128.0,1.0,1,A100,1697548600547,1697548602675,120,15.0,1.0,"[36, 2091]","[1697548600583, 1697548602674]"
2153,2153,242,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629598,120,,,"[68, 2163]","[1697548627174, 1697548629337]"
2154,2154,107,23,[],200,llama-7b,128,1,447.0,1.0,1,A100,1697548637702,1697548638149,120,216.0,2.0,"[28, 418]","[1697548637730, 1697548638148]"
2155,2155,760,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602679,1697548605263,120,,,"[24, 2366]","[1697548602703, 1697548605069]"
2156,2156,573,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629603,1697548631886,120,,,[80],[1697548629683]
2157,2157,464,24,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548638151,1697548640182,120,12.0,1.0,"[6, 2025]","[1697548638157, 1697548640182]"
2158,2158,1,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633844,120,,,[26],[1697548631921]
2159,2159,189,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605279,1697548608966,120,,,"[222, 1728, 510, 78, 79, 78]","[1697548605501, 1697548607229, 1697548607739, 1697548607817, 1697548607896, 1697548607974]"
2160,2160,819,25,[],200,llama-7b,128,1,1551.0,1.0,1,A100,1697548640186,1697548641737,120,13.0,1.0,"[5, 1546]","[1697548640191, 1697548641737]"
2161,2161,253,26,[],200,llama-7b,128,1,7478.0,1.0,1,A100,1697548641740,1697548649218,120,67.0,20.0,"[46, 1787, 51, 1588, 85, 64, 954, 93, 91, 89, 69, 968, 96, 95, 94, 71, 94, 71, 892, 91, 89]","[1697548641786, 1697548643573, 1697548643624, 1697548645212, 1697548645297, 1697548645361, 1697548646315, 1697548646408, 1697548646499, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648075, 1697548648146, 1697548649038, 1697548649129, 1697548649218]"
2162,2162,359,24,[],200,llama-7b,128,1,1744.0,1.0,1,A100,1697548633854,1697548635598,120,10.0,1.0,"[111, 1633]","[1697548633965, 1697548635598]"
2163,2163,132,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583926,1697548586198,120,,,"[141, 1891]","[1697548584067, 1697548585958]"
2164,2164,905,6,[],200,llama-7b,128,1,2085.0,1.0,1,A100,1697548592076,1697548594161,120,11.0,1.0,"[359, 1726]","[1697548592435, 1697548594161]"
2165,2165,31,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610833,120,,,[90],[1697548609063]
2166,2166,390,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610851,1697548612998,120,,,"[301, 1664]","[1697548611152, 1697548612816]"
2167,2167,546,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610838,120,,,[370],[1697548609345]
2168,2168,475,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622598,120,,,"[88, 1879]","[1697548620450, 1697548622329]"
2169,2169,332,7,[],200,llama-7b,128,1,762.0,1.0,1,A100,1697548594164,1697548594926,120,39.0,1.0,"[7, 755]","[1697548594171, 1697548594926]"
2170,2170,439,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637694,120,,,[57],[1697548635972]
2171,2171,911,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642999,120,,,[98],[1697548641482]
2172,2172,345,29,[],200,llama-7b,128,1,7199.0,1.0,1,A100,1697548643008,1697548650207,120,39.0,20.0,"[294, 2435, 578, 94, 91, 88, 69, 968, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86, 732, 87, 85]","[1697548643302, 1697548645737, 1697548646315, 1697548646409, 1697548646500, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650035, 1697548650122, 1697548650207]"
2173,2173,801,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639653,120,,,"[275, 1321]","[1697548637978, 1697548639299]"
2174,2174,693,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594931,1697548597357,120,,,"[26, 2377]","[1697548594957, 1697548597334]"
2175,2175,124,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599941,120,,,[100],[1697548597466]
2176,2176,207,18,[],200,llama-7b,128,1,1907.0,1.0,1,A100,1697548617378,1697548619285,120,10.0,1.0,"[68, 1839]","[1697548617446, 1697548619285]"
2177,2177,533,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619290,1697548620358,120,,,[50],[1697548619340]
2178,2178,707,30,[],200,llama-7b,128,1,3016.0,1.0,1,A100,1697548650212,1697548653228,120,8.0,1.0,"[38, 2978]","[1697548650250, 1697548653228]"
2179,2179,748,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615232,120,,,[45],[1697548613051]
2180,2180,452,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602107,120,,,"[395, 1537]","[1697548600348, 1697548601885]"
2181,2181,894,20,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548620371,1697548622330,120,14.0,1.0,"[375, 1584]","[1697548620746, 1697548622330]"
2182,2182,135,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653230,1697548657537,120,,,"[9, 2940]","[1697548653239, 1697548656179]"
2183,2183,173,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617369,120,,,"[106, 1917]","[1697548615348, 1697548617265]"
2184,2184,809,11,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548602109,1697548603851,120,16.0,1.0,"[31, 1711]","[1697548602140, 1697548603851]"
2185,2185,240,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603856,1697548605265,120,,,"[37, 1177]","[1697548603893, 1697548605070]"
2186,2186,493,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586204,1697548589559,120,,,"[29, 1938, 218, 79, 78, 76]","[1697548586233, 1697548588171, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
2187,2187,597,13,[],200,llama-7b,128,1,1948.0,1.0,1,A100,1697548605283,1697548607231,120,39.0,1.0,"[78, 1870]","[1697548605361, 1697548607231]"
2188,2188,25,14,[],200,llama-7b,128,1,1533.0,1.0,1,A100,1697548607237,1697548608770,120,12.0,1.0,"[26, 1507]","[1697548607263, 1697548608770]"
2189,2189,536,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617387,1697548620356,120,,,"[373, 1528, 109, 84, 83, 80, 81]","[1697548617760, 1697548619288, 1697548619397, 1697548619481, 1697548619564, 1697548619644, 1697548619725]"
2190,2190,356,15,[],200,llama-7b,128,1,2095.0,1.0,1,A100,1697548608773,1697548610868,120,874.0,2.0,"[21, 831, 1243]","[1697548608794, 1697548609625, 1697548610868]"
2191,2191,714,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610869,1697548613000,120,,,"[375, 1574]","[1697548611244, 1697548612818]"
2192,2192,139,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613010,1697548615235,120,,,"[165, 1859]","[1697548613175, 1697548615034]"
2193,2193,864,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622595,120,,,"[228, 1737]","[1697548620591, 1697548622328]"
2194,2194,229,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639660,1697548641378,120,,,[395],[1697548640055]
2195,2195,296,22,[],200,llama-7b,128,1,2072.0,1.0,1,A100,1697548622606,1697548624678,120,6.0,1.0,"[257, 1815]","[1697548622863, 1697548624678]"
2196,2196,651,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624684,1697548627102,120,,,"[55, 1035]","[1697548624739, 1697548625774]"
2197,2197,502,18,[],200,llama-7b,128,1,2022.0,1.0,1,A100,1697548615244,1697548617266,120,19.0,1.0,"[340, 1682]","[1697548615584, 1697548617266]"
2198,2198,588,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548643003,120,,,[365],[1697548641750]
2199,2199,80,24,[],200,llama-7b,128,1,2209.0,1.0,1,A100,1697548627138,1697548629347,120,13.0,1.0,"[335, 1866]","[1697548627473, 1697548629339]"
2200,2200,10,30,[],200,llama-7b,128,1,3644.0,1.0,1,A100,1697548643012,1697548646656,120,563.0,9.0,"[175, 1590, 434, 86, 64, 953, 94, 91, 89, 68]","[1697548643187, 1697548644777, 1697548645211, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656]"
2201,2201,852,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592061,120,,,"[306, 1715]","[1697548589874, 1697548591589]"
2202,2202,411,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629352,1697548631888,120,,,[46],[1697548629398]
2203,2203,859,19,[],200,llama-7b,128,1,686.0,1.0,1,A100,1697548617270,1697548617956,120,23.0,1.0,"[35, 651]","[1697548617305, 1697548617956]"
2204,2204,765,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633848,120,,,[282],[1697548632178]
2205,2205,849,5,[],200,llama-7b,128,1,1257.0,1.0,1,A100,1697548591595,1697548592852,120,10.0,1.0,"[76, 1181]","[1697548591671, 1697548592852]"
2206,2206,322,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624952,120,,,"[29, 1092]","[1697548622364, 1697548623456]"
2207,2207,466,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659864,120,,,"[197, 1836]","[1697548657743, 1697548659579]"
2208,2208,376,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617959,1697548620357,120,,,"[25, 2338]","[1697548617984, 1697548620322]"
2209,2209,910,7,[],200,llama-7b,128,1,2351.0,1.0,1,A100,1697548597366,1697548599717,120,8.0,1.0,"[108, 2242]","[1697548597474, 1697548599716]"
2210,2210,816,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661852,120,,,[16],[1697548659887]
2211,2211,252,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639649,1697548641377,120,,,[120],[1697548639769]
2212,2212,274,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592855,1697548597360,120,,,"[30, 2041, 1420, 72, 58]","[1697548592885, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
2213,2213,14,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583920,120,,,"[297, 1418, 109, 102, 89, 88, 87, 84]","[1697548581391, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583284, 1697548583368]"
2214,2214,734,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622595,120,,,"[283, 1677]","[1697548620651, 1697548622328]"
2215,2215,245,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664204,120,,,"[121, 1789]","[1697548661988, 1697548663777]"
2216,2216,613,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548643000,120,,,[299],[1697548641684]
2217,2217,603,35,[],200,llama-7b,128,1,1819.0,1.0,1,A100,1697548664217,1697548666036,120,9.0,1.0,"[48, 1771]","[1697548664265, 1697548666036]"
2218,2218,342,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615233,120,,,"[55, 1970]","[1697548613061, 1697548615031]"
2219,2219,134,29,[],200,llama-7b,128,1,6297.0,1.0,1,A100,1697548643008,1697548649305,120,86.0,20.0,"[164, 1605, 434, 86, 64, 953, 94, 91, 89, 68, 968, 96, 97, 92, 72, 93, 71, 894, 91, 89, 86]","[1697548643172, 1697548644777, 1697548645211, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647817, 1697548647909, 1697548647981, 1697548648074, 1697548648145, 1697548649039, 1697548649130, 1697548649219, 1697548649305]"
2220,2220,162,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[54, 2020]","[1697548622657, 1697548624677]"
2221,2221,522,23,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624974,1697548627021,120,20.0,1.0,"[285, 1762]","[1697548625259, 1697548627021]"
2222,2222,492,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649309,1697548655270,120,,,"[21, 2848, 267, 320, 88, 69, 403, 97, 88, 85, 650, 96, 84, 63, 82]","[1697548649330, 1697548652178, 1697548652445, 1697548652765, 1697548652853, 1697548652922, 1697548653325, 1697548653422, 1697548653510, 1697548653595, 1697548654245, 1697548654341, 1697548654425, 1697548654488, 1697548654570]"
2223,2223,876,24,[],200,llama-7b,128,1,913.0,1.0,1,A100,1697548627026,1697548627939,120,11.0,1.0,"[63, 850]","[1697548627089, 1697548627939]"
2224,2224,395,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597361,120,,,"[235, 1505, 250, 73, 58]","[1697548594591, 1697548596096, 1697548596346, 1697548596419, 1697548596477]"
2225,2225,279,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627942,1697548631886,120,,,"[27, 2302]","[1697548627969, 1697548630271]"
2226,2226,721,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599940,120,,,"[321, 2029]","[1697548597688, 1697548599717]"
2227,2227,146,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602106,120,,,"[198, 1736]","[1697548600149, 1697548601885]"
2228,2228,638,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633846,120,,,[67],[1697548631962]
2229,2229,850,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655282,1697548657536,120,,,"[56, 2087]","[1697548655338, 1697548657425]"
2230,2230,70,27,[],200,llama-7b,128,1,578.0,1.0,1,A100,1697548633854,1697548634432,120,39.0,1.0,"[26, 552]","[1697548633880, 1697548634432]"
2231,2231,227,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619292,1697548622593,120,,,"[67, 1565]","[1697548619359, 1697548620924]"
2232,2232,114,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617365,120,,,"[206, 1817]","[1697548615449, 1697548617266]"
2233,2233,508,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602122,1697548605264,120,,,"[28, 1701, 263, 78, 76]","[1697548602150, 1697548603851, 1697548604114, 1697548604192, 1697548604268]"
2234,2234,474,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620357,120,,,"[88, 1931, 84, 83, 80, 80]","[1697548617466, 1697548619397, 1697548619481, 1697548619564, 1697548619644, 1697548619724]"
2235,2235,272,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657545,1697548659868,120,,,"[98, 1935]","[1697548657643, 1697548659578]"
2236,2236,541,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605284,1697548608961,120,,,"[317, 1630, 508, 79, 79, 78]","[1697548605601, 1697548607231, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
2237,2237,866,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608963,120,,,"[57, 1893, 508, 79, 79, 78]","[1697548605337, 1697548607230, 1697548607738, 1697548607817, 1697548607896, 1697548607974]"
2238,2238,828,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622594,120,,,[193],[1697548620556]
2239,2239,457,11,[],200,llama-7b,128,1,2456.0,1.0,1,A100,1697548605283,1697548607739,120,874.0,2.0,"[303, 2152]","[1697548605586, 1697548607738]"
2240,2240,629,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661855,120,,,[305],[1697548660182]
2241,2241,814,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607742,1697548612995,120,,,"[20, 1863, 1243, 569]","[1697548607762, 1697548609625, 1697548610868, 1697548611437]"
2242,2242,31,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664205,120,,,"[194, 1716]","[1697548662061, 1697548663777]"
2243,2243,776,8,[],200,llama-7b,128,1,1991.0,1.0,1,A100,1697548594355,1697548596346,120,67.0,2.0,"[202, 1541, 248]","[1697548594557, 1697548596098, 1697548596346]"
2244,2244,392,35,[],200,llama-7b,128,1,1819.0,1.0,1,A100,1697548664216,1697548666035,120,20.0,1.0,"[18, 1801]","[1697548664234, 1697548666035]"
2245,2245,86,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635904,120,,,"[330, 1413]","[1697548634185, 1697548635598]"
2246,2246,381,14,[],200,llama-7b,128,1,2455.0,1.0,1,A100,1697548605283,1697548607738,120,140.0,2.0,"[68, 1879, 508]","[1697548605351, 1697548607230, 1697548607738]"
2247,2247,448,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548639638,120,,,"[345, 1833, 54]","[1697548636261, 1697548638094, 1697548638148]"
2248,2248,273,6,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548594356,1697548596098,120,19.0,1.0,"[300, 1441]","[1697548594656, 1697548596097]"
2249,2249,909,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617364,120,,,"[15, 822]","[1697548615050, 1697548615872]"
2250,2250,605,7,[],200,llama-7b,128,1,1232.0,1.0,1,A100,1697548596103,1697548597335,120,8.0,1.0,"[86, 1145]","[1697548596189, 1697548597334]"
2251,2251,732,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607743,1697548612995,120,,,"[19, 1862, 1244, 569]","[1697548607762, 1697548609624, 1697548610868, 1697548611437]"
2252,2252,33,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597337,1697548599939,120,,,"[21, 585, 37]","[1697548597358, 1697548597943, 1697548597980]"
2253,2253,396,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602106,120,,,"[206, 1727]","[1697548600158, 1697548601885]"
2254,2254,49,31,[],200,llama-7b,128,1,2828.0,1.0,1,A100,1697548651516,1697548654344,120,109.0,3.0,"[20, 2357, 355, 96]","[1697548651536, 1697548653893, 1697548654248, 1697548654344]"
2255,2255,750,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602121,1697548605264,120,,,"[24, 1706, 263, 78, 76]","[1697548602145, 1697548603851, 1697548604114, 1697548604192, 1697548604268]"
2256,2256,406,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548654347,1697548657501,120,,,"[13, 1819]","[1697548654360, 1697548656179]"
2257,2257,765,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657547,1697548659866,120,,,[308],[1697548657855]
2258,2258,807,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639651,1697548641379,120,,,[203],[1697548639854]
2259,2259,339,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620357,120,,,"[97, 1810, 113, 83, 83, 80, 80]","[1697548617475, 1697548619285, 1697548619398, 1697548619481, 1697548619564, 1697548619644, 1697548619724]"
2260,2260,165,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661852,120,,,[193],[1697548660065]
2261,2261,232,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642998,120,,,[96],[1697548641482]
2262,2262,756,2,[],200,llama-7b,128,1,2032.0,1.0,1,A100,1697548583925,1697548585957,120,19.0,1.0,"[39, 1993]","[1697548583964, 1697548585957]"
2263,2263,186,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585962,1697548589557,120,,,"[27, 1226, 1175, 77, 79, 75]","[1697548585989, 1697548587215, 1697548588390, 1697548588467, 1697548588546, 1697548588621]"
2264,2264,590,28,[],200,llama-7b,128,1,7199.0,1.0,1,A100,1697548643008,1697548650207,120,88.0,20.0,"[292, 2437, 578, 94, 91, 88, 69, 968, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86, 732, 87, 85]","[1697548643300, 1697548645737, 1697548646315, 1697548646409, 1697548646500, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650035, 1697548650122, 1697548650207]"
2265,2265,886,29,[],200,llama-7b,128,1,1766.0,1.0,1,A100,1697548643012,1697548644778,120,17.0,1.0,"[254, 1512]","[1697548643266, 1697548644778]"
2266,2266,524,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664204,120,,,"[111, 1799]","[1697548661978, 1697548663777]"
2267,2267,293,30,[],200,llama-7b,128,1,6734.0,1.0,1,A100,1697548644780,1697548651514,120,91.0,20.0,"[12, 2283, 550, 96, 96, 93, 72, 93, 71, 891, 92, 88, 87, 731, 87, 85, 75, 73, 75, 991, 92]","[1697548644792, 1697548647075, 1697548647625, 1697548647721, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649304, 1697548650035, 1697548650122, 1697548650207, 1697548650282, 1697548650355, 1697548650430, 1697548651421, 1697548651513]"
2268,2268,882,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664216,1697548667369,120,,,"[224, 1595, 431, 84, 65, 85]","[1697548664440, 1697548666035, 1697548666466, 1697548666550, 1697548666615, 1697548666700]"
2269,2269,671,19,[],200,llama-7b,128,1,1961.0,1.0,1,A100,1697548620367,1697548622328,120,12.0,1.0,"[12, 1949]","[1697548620379, 1697548622328]"
2270,2270,887,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548577244,1697548581083,120,,,"[22, 1173, 687, 93, 92, 70, 86, 85, 501, 96, 95, 90, 94, 86, 82]","[1697548577266, 1697548578439, 1697548579126, 1697548579219, 1697548579311, 1697548579381, 1697548579467, 1697548579552, 1697548580053, 1697548580149, 1697548580244, 1697548580334, 1697548580428, 1697548580514, 1697548580596]"
2271,2271,100,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624952,120,,,"[42, 1080]","[1697548622377, 1697548623457]"
2272,2272,304,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620326,1697548622595,120,,,"[14, 585]","[1697548620340, 1697548620925]"
2273,2273,811,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581094,1697548583921,120,,,"[364, 1352, 109, 102, 89, 88, 86, 84]","[1697548581458, 1697548582810, 1697548582919, 1697548583021, 1697548583110, 1697548583198, 1697548583284, 1697548583368]"
2274,2274,653,31,[],200,llama-7b,128,1,3055.0,1.0,1,A100,1697548651516,1697548654571,120,96.0,6.0,"[14, 2363, 355, 95, 84, 63, 80]","[1697548651530, 1697548653893, 1697548654248, 1697548654343, 1697548654427, 1697548654490, 1697548654570]"
2275,2275,225,17,[],200,llama-7b,128,1,686.0,1.0,1,A100,1697548617270,1697548617956,120,23.0,1.0,"[45, 641]","[1697548617315, 1697548617956]"
2276,2276,104,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548652925,1697548657537,120,,,"[16, 3237]","[1697548652941, 1697548656178]"
2277,2277,757,1,[],200,llama-7b,128,1,1511.0,1.0,1,A100,1697548578444,1697548579955,120,20.0,1.0,"[74, 1437]","[1697548578518, 1697548579955]"
2278,2278,579,18,[],200,llama-7b,128,1,2362.0,1.0,1,A100,1697548617960,1697548620322,120,19.0,1.0,"[35, 2327]","[1697548617995, 1697548620322]"
2279,2279,236,2,[],200,llama-7b,128,1,3281.0,1.0,1,A100,1697548583933,1697548587214,120,8.0,1.0,"[404, 2876]","[1697548584337, 1697548587213]"
2280,2280,848,17,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548620362,1697548622329,120,47.0,1.0,"[78, 1889]","[1697548620440, 1697548622329]"
2281,2281,454,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657545,1697548659865,120,,,"[136, 1898]","[1697548657681, 1697548659579]"
2282,2282,490,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671228,120,,,"[66, 1423]","[1697548669779, 1697548671202]"
2283,2283,186,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548579958,1697548583920,120,,,"[30, 1481, 1448, 102, 89, 88, 87, 83]","[1697548579988, 1697548581469, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583366]"
2284,2284,567,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548587220,1697548589558,120,,,"[30, 2130]","[1697548587250, 1697548589380]"
2285,2285,784,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661852,120,,,[203],[1697548660075]
2286,2286,12,19,[],200,llama-7b,128,1,599.0,1.0,1,A100,1697548620326,1697548620925,120,11.0,1.0,"[19, 580]","[1697548620345, 1697548620925]"
2287,2287,213,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664207,120,,,"[297, 1614]","[1697548662165, 1697548663779]"
2288,2288,848,42,[],200,llama-7b,128,1,3072.0,1.0,1,A100,1697548671246,1697548674318,120,47.0,1.0,"[391, 2681]","[1697548671637, 1697548674318]"
2289,2289,276,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674322,1697548677257,120,,,[35],[1697548674357]
2290,2290,370,20,[],200,llama-7b,128,1,2527.0,1.0,1,A100,1697548620929,1697548623456,120,31.0,1.0,"[20, 2507]","[1697548620949, 1697548623456]"
2291,2291,924,4,[],200,llama-7b,128,1,2020.0,1.0,1,A100,1697548589568,1697548591588,120,9.0,1.0,"[40, 1980]","[1697548589608, 1697548591588]"
2292,2292,606,44,[],200,llama-7b,128,1,2205.0,1.0,1,A100,1697548677270,1697548679475,120,9.0,1.0,"[183, 2022]","[1697548677453, 1697548679475]"
2293,2293,35,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679478,1697548683206,120,,,"[41, 838, 1615, 90, 86, 82, 81]","[1697548679519, 1697548680357, 1697548681972, 1697548682062, 1697548682148, 1697548682230, 1697548682311]"
2294,2294,353,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,"[25, 1232]","[1697548591619, 1697548592851]"
2295,2295,574,33,[],200,llama-7b,128,1,2249.0,1.0,1,A100,1697548664218,1697548666467,120,364.0,2.0,"[258, 1560, 430]","[1697548664476, 1697548666036, 1697548666466]"
2296,2296,8,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666470,1697548667371,120,,,"[9, 868]","[1697548666479, 1697548667347]"
2297,2297,712,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597358,120,,,"[32, 1714, 251, 72, 58]","[1697548594382, 1697548596096, 1697548596347, 1697548596419, 1697548596477]"
2298,2298,111,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599940,120,,,[332],[1697548597699]
2299,2299,392,46,[],200,llama-7b,128,1,1809.0,1.0,1,A100,1697548683216,1697548685025,120,20.0,1.0,"[241, 1568]","[1697548683457, 1697548685025]"
2300,2300,337,35,[],200,llama-7b,128,1,2328.0,1.0,1,A100,1697548667379,1697548669707,120,12.0,1.0,"[393, 1935]","[1697548667772, 1697548669707]"
2301,2301,832,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639649,1697548641377,120,,,[125],[1697548639774]
2302,2302,474,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,[330],[1697548600282]
2303,2303,380,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612821,1697548615235,120,,,[40],[1697548612861]
2304,2304,261,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548643000,120,,,[181],[1697548641565]
2305,2305,752,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685029,1697548686720,120,,,"[22, 1128]","[1697548685051, 1697548686179]"
2306,2306,590,29,[],200,llama-7b,128,1,7193.0,1.0,1,A100,1697548643014,1697548650207,120,88.0,20.0,"[335, 2388, 578, 94, 91, 89, 68, 968, 96, 95, 94, 71, 94, 71, 891, 92, 88, 86, 732, 87, 85]","[1697548643349, 1697548645737, 1697548646315, 1697548646409, 1697548646500, 1697548646589, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648075, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650035, 1697548650122, 1697548650207]"
2307,2307,828,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602109,1697548605264,120,,,"[18, 1987, 78, 76]","[1697548602127, 1697548604114, 1697548604192, 1697548604268]"
2308,2308,268,48,[],200,llama-7b,128,1,2872.0,1.0,1,A100,1697548686732,1697548689604,120,19.0,1.0,"[405, 2467]","[1697548687137, 1697548689604]"
2309,2309,257,10,[],200,llama-7b,128,1,1949.0,1.0,1,A100,1697548605283,1697548607232,120,14.0,1.0,"[172, 1777]","[1697548605455, 1697548607232]"
2310,2310,627,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548689608,1697548693447,120,,,"[17, 2227]","[1697548689625, 1697548691852]"
2311,2311,924,12,[],200,llama-7b,128,1,1952.0,1.0,1,A100,1697548605280,1697548607232,120,9.0,1.0,"[373, 1579]","[1697548605653, 1697548607232]"
2312,2312,616,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607240,1697548608968,120,,,"[129, 1402]","[1697548607369, 1697548608771]"
2313,2313,15,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610833,120,,,[302],[1697548609277]
2314,2314,55,50,[],200,llama-7b,128,1,1973.0,1.0,1,A100,1697548693459,1697548695432,120,12.0,1.0,"[28, 1944]","[1697548693487, 1697548695431]"
2315,2315,413,51,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695438,1697548697835,120,,,"[54, 1236]","[1697548695492, 1697548696728]"
2316,2316,764,7,[],200,llama-7b,128,1,1740.0,1.0,1,A100,1697548594356,1697548596096,120,39.0,1.0,"[226, 1514]","[1697548594582, 1697548596096]"
2317,2317,278,8,[],200,llama-7b,128,1,1235.0,1.0,1,A100,1697548596100,1697548597335,120,13.0,1.0,"[39, 1195]","[1697548596139, 1697548597334]"
2318,2318,352,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607238,1697548608968,120,,,"[109, 1424]","[1697548607347, 1697548608771]"
2319,2319,636,9,[],200,llama-7b,128,1,605.0,1.0,1,A100,1697548597339,1697548597944,120,31.0,1.0,"[102, 503]","[1697548597441, 1697548597944]"
2320,2320,68,10,[],200,llama-7b,128,1,1774.0,1.0,1,A100,1697548597945,1697548599719,120,12.0,1.0,"[14, 1760]","[1697548597959, 1697548599719]"
2321,2321,430,11,[],200,llama-7b,128,1,817.0,1.0,1,A100,1697548599724,1697548600541,120,15.0,1.0,"[92, 725]","[1697548599816, 1697548600541]"
2322,2322,707,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610833,120,,,[369],[1697548609344]
2323,2323,759,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548600543,1697548605271,120,,,"[19, 2113, 1438, 78, 76]","[1697548600562, 1697548602675, 1697548604113, 1697548604191, 1697548604267]"
2324,2324,133,15,[],200,llama-7b,128,1,1964.0,1.0,1,A100,1697548610853,1697548612817,120,15.0,1.0,"[382, 1582]","[1697548611235, 1697548612817]"
2325,2325,791,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610824,120,,,[220],[1697548609193]
2326,2326,103,2,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548583932,1697548585957,120,15.0,1.0,"[253, 1772]","[1697548584185, 1697548585957]"
2327,2327,462,23,[],200,llama-7b,128,1,2046.0,1.0,1,A100,1697548624974,1697548627020,120,52.0,1.0,"[207, 1839]","[1697548625181, 1697548627020]"
2328,2328,491,16,[],200,llama-7b,128,1,719.0,1.0,1,A100,1697548612819,1697548613538,120,14.0,1.0,"[17, 702]","[1697548612836, 1697548613538]"
2329,2329,184,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608965,120,,,"[188, 1762, 506, 79, 78, 79]","[1697548605471, 1697548607233, 1697548607739, 1697548607818, 1697548607896, 1697548607975]"
2330,2330,821,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613541,1697548617364,120,,,"[10, 2321]","[1697548613551, 1697548615872]"
2331,2331,460,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585963,1697548589557,120,,,"[46, 1203, 1177, 79, 78, 75]","[1697548586009, 1697548587212, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
2332,2332,426,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605279,1697548608963,120,,,"[33, 2426, 79, 79, 78]","[1697548605312, 1697548607738, 1697548607817, 1697548607896, 1697548607974]"
2333,2333,254,18,[],200,llama-7b,128,1,1908.0,1.0,1,A100,1697548617378,1697548619286,120,58.0,1.0,"[276, 1632]","[1697548617654, 1697548619286]"
2334,2334,615,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619288,1697548620357,120,,,"[13, 1021]","[1697548619301, 1697548620322]"
2335,2335,818,4,[],200,llama-7b,128,1,2016.0,1.0,1,A100,1697548589573,1697548591589,120,13.0,1.0,"[64, 1951]","[1697548589637, 1697548591588]"
2336,2336,820,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627023,1697548629597,120,,,"[20, 895]","[1697548627043, 1697548627938]"
2337,2337,253,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631887,120,,,"[75, 1935]","[1697548629683, 1697548631618]"
2338,2338,544,14,[],200,llama-7b,128,1,2413.0,1.0,1,A100,1697548608975,1697548611388,120,26.0,1.0,"[409, 2004]","[1697548609384, 1697548611388]"
2339,2339,45,20,[],200,llama-7b,128,1,1960.0,1.0,1,A100,1697548620371,1697548622331,120,19.0,1.0,"[395, 1564]","[1697548620766, 1697548622330]"
2340,2340,248,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,"[48, 1209]","[1697548591642, 1697548592851]"
2341,2341,578,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633846,120,,,[252],[1697548632153]
2342,2342,191,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610848,1697548612998,120,,,"[43, 1924]","[1697548610891, 1697548612815]"
2343,2343,897,15,[],200,llama-7b,128,1,1428.0,1.0,1,A100,1697548611390,1697548612818,120,9.0,1.0,"[14, 1414]","[1697548611404, 1697548612818]"
2344,2344,544,16,[],200,llama-7b,128,1,2014.0,1.0,1,A100,1697548613019,1697548615033,120,26.0,1.0,"[340, 1673]","[1697548613359, 1697548615032]"
2345,2345,576,6,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548594350,1697548596097,120,14.0,1.0,"[88, 1659]","[1697548594438, 1697548596097]"
2346,2346,7,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635895,120,,,"[199, 1546]","[1697548634053, 1697548635599]"
2347,2347,907,17,[],200,llama-7b,128,1,837.0,1.0,1,A100,1697548615036,1697548615873,120,10.0,1.0,"[69, 768]","[1697548615105, 1697548615873]"
2348,2348,398,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624951,120,,,"[17, 1104]","[1697548622352, 1697548623456]"
2349,2349,336,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615875,1697548620353,120,,,"[11, 2069, 1441, 83, 83, 80, 80]","[1697548615886, 1697548617955, 1697548619396, 1697548619479, 1697548619562, 1697548619642, 1697548619722]"
2350,2350,366,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635912,1697548637693,120,,,[42],[1697548635954]
2351,2351,7,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596102,1697548597359,120,,,"[79, 1153]","[1697548596181, 1697548597334]"
2352,2352,720,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639638,120,,,"[250, 1346]","[1697548637953, 1697548639299]"
2353,2353,364,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597372,1697548599943,120,,,"[487, 1859]","[1697548597859, 1697548599718]"
2354,2354,695,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622596,120,,,"[23, 1943]","[1697548620385, 1697548622328]"
2355,2355,328,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612822,1697548615237,120,,,"[64, 652]","[1697548612886, 1697548613538]"
2356,2356,718,9,[],200,llama-7b,128,1,1933.0,1.0,1,A100,1697548599952,1697548601885,120,13.0,1.0,"[348, 1585]","[1697548600300, 1697548601885]"
2357,2357,215,20,[],200,llama-7b,128,1,2072.0,1.0,1,A100,1697548622608,1697548624680,120,12.0,1.0,"[330, 1742]","[1697548622938, 1697548624680]"
2358,2358,152,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601888,1697548605272,120,,,"[8, 779, 1438, 78, 76]","[1697548601896, 1697548602675, 1697548604113, 1697548604191, 1697548604267]"
2359,2359,153,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639647,1697548641375,120,,,[95],[1697548639742]
2360,2360,660,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617369,120,,,"[345, 1677]","[1697548615589, 1697548617266]"
2361,2361,483,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642998,120,,,[83],[1697548641467]
2362,2362,723,22,[],200,llama-7b,128,1,2053.0,1.0,1,A100,1697548624967,1697548627020,120,14.0,1.0,"[72, 1981]","[1697548625039, 1697548627020]"
2363,2363,844,32,[],200,llama-7b,128,1,1769.0,1.0,1,A100,1697548643008,1697548644777,120,10.0,1.0,"[57, 1712]","[1697548643065, 1697548644777]"
2364,2364,91,18,[],200,llama-7b,128,1,1902.0,1.0,1,A100,1697548617387,1697548619289,120,23.0,1.0,"[377, 1525]","[1697548617764, 1697548619289]"
2365,2365,451,19,[],200,llama-7b,128,1,1632.0,1.0,1,A100,1697548619292,1697548620924,120,286.0,1.0,"[62, 1570]","[1697548619354, 1697548620924]"
2366,2366,154,23,[],200,llama-7b,128,1,912.0,1.0,1,A100,1697548627027,1697548627939,120,13.0,1.0,"[67, 845]","[1697548627094, 1697548627939]"
2367,2367,481,11,[],200,llama-7b,128,1,1948.0,1.0,1,A100,1697548605283,1697548607231,120,10.0,1.0,"[89, 1859]","[1697548605372, 1697548607231]"
2368,2368,805,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620927,1697548624950,120,,,"[7, 2521]","[1697548620934, 1697548623455]"
2369,2369,842,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608967,120,,,"[110, 1424]","[1697548607347, 1697548608771]"
2370,2370,268,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610836,120,,,[399],[1697548609374]
2371,2371,273,33,[],200,llama-7b,128,1,2296.0,1.0,1,A100,1697548644780,1697548647076,120,19.0,1.0,"[19, 2276]","[1697548644799, 1697548647075]"
2372,2372,626,14,[],200,llama-7b,128,1,1964.0,1.0,1,A100,1697548610852,1697548612816,120,10.0,1.0,"[310, 1654]","[1697548611162, 1697548612816]"
2373,2373,779,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627101,120,,,"[281, 1766]","[1697548625255, 1697548627021]"
2374,2374,51,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612822,1697548615237,120,,,"[69, 647]","[1697548612891, 1697548613538]"
2375,2375,797,7,[],200,llama-7b,128,1,2071.0,1.0,1,A100,1697548592855,1697548594926,120,26.0,1.0,"[25, 2046]","[1697548592880, 1697548594926]"
2376,2376,628,34,[],200,llama-7b,128,1,4508.0,1.0,1,A100,1697548647079,1697548651587,120,732.0,10.0,"[38, 2918, 87, 85, 75, 74, 74, 999, 84, 74]","[1697548647117, 1697548650035, 1697548650122, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651429, 1697548651513, 1697548651587]"
2377,2377,53,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548651590,1697548655269,120,,,"[6, 3470]","[1697548651596, 1697548655066]"
2378,2378,204,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[163, 2072]","[1697548627269, 1697548629341]"
2379,2379,791,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548651590,1697548655270,120,,,[6],[1697548651596]
2380,2380,499,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657535,120,,,"[40, 2104]","[1697548655321, 1697548657425]"
2381,2381,407,41,[],200,llama-7b,128,1,2201.0,1.0,1,A100,1697548677273,1697548679474,120,16.0,1.0,"[268, 1933]","[1697548677541, 1697548679474]"
2382,2382,585,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635917,1697548637696,120,,,[320],[1697548636237]
2383,2383,913,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639646,120,,,"[53, 1543]","[1697548637755, 1697548639298]"
2384,2384,217,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655279,1697548657503,120,,,"[25, 2121]","[1697548655304, 1697548657425]"
2385,2385,344,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639654,1697548641383,120,,,[282],[1697548639936]
2386,2386,861,37,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548657545,1697548659578,120,10.0,1.0,"[113, 1920]","[1697548657658, 1697548659578]"
2387,2387,698,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642998,120,,,[262],[1697548641648]
2388,2388,293,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664206,120,,,"[43, 1076, 1195, 57, 1027]","[1697548659625, 1697548660701, 1697548661896, 1697548661953, 1697548662980]"
2389,2389,678,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605073,1697548608961,120,,,"[27, 638, 1999, 80, 78, 78]","[1697548605100, 1697548605738, 1697548607737, 1697548607817, 1697548607895, 1697548607973]"
2390,2390,36,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671227,120,,,"[114, 2214, 472, 84, 65, 64, 80]","[1697548667492, 1697548669706, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
2391,2391,905,16,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610851,1697548612818,120,11.0,1.0,"[229, 1738]","[1697548611080, 1697548612818]"
2392,2392,786,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631889,120,,,"[269, 1741]","[1697548629877, 1697548631618]"
2393,2393,338,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612823,1697548615231,120,,,"[79, 637]","[1697548612902, 1697548613539]"
2394,2394,575,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657544,1697548659867,120,,,"[24, 2009]","[1697548657568, 1697548659577]"
2395,2395,367,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671242,1697548674841,120,,,"[217, 1616, 538, 79, 61]","[1697548671459, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
2396,2396,8,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661856,120,,,[172],[1697548660044]
2397,2397,754,2,[],200,llama-7b,128,1,2278.0,1.0,1,A100,1697548581089,1697548583367,120,88.0,7.0,"[253, 1575, 102, 89, 88, 87, 84]","[1697548581342, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583367]"
2398,2398,652,39,[],200,llama-7b,128,1,1820.0,1.0,1,A100,1697548664218,1697548666038,120,14.0,1.0,"[335, 1485]","[1697548664553, 1697548666038]"
2399,2399,724,38,[],200,llama-7b,128,1,3394.0,1.0,1,A100,1697548674848,1697548678242,120,11.0,1.0,"[394, 3000]","[1697548675242, 1697548678242]"
2400,2400,52,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666047,1697548667370,120,,,"[66, 1233]","[1697548666113, 1697548667346]"
2401,2401,184,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583370,1697548586200,120,,,"[20, 1035]","[1697548583390, 1697548584425]"
2402,2402,402,41,[],200,llama-7b,128,1,3092.0,1.0,1,A100,1697548667379,1697548670471,120,457.0,6.0,"[373, 1954, 471, 85, 64, 65, 80]","[1697548667752, 1697548669706, 1697548670177, 1697548670262, 1697548670326, 1697548670391, 1697548670471]"
2403,2403,248,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594346,120,,,"[78, 2010]","[1697548592150, 1697548594160]"
2404,2404,338,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664206,120,,,"[208, 1702]","[1697548662076, 1697548663778]"
2405,2405,538,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586209,1697548589561,120,,,"[173, 1789, 218, 79, 79, 75]","[1697548586382, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
2406,2406,401,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624952,120,,,"[171, 1901]","[1697548622775, 1697548624676]"
2407,2407,758,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624962,1697548627103,120,,,"[24, 2034]","[1697548624986, 1697548627020]"
2408,2408,896,5,[],200,llama-7b,128,1,2017.0,1.0,1,A100,1697548589574,1697548591591,120,15.0,1.0,"[199, 1818]","[1697548589773, 1697548591591]"
2409,2409,155,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548678245,1697548683205,120,,,"[34, 2077, 1615, 92, 85, 81, 81]","[1697548678279, 1697548680356, 1697548681971, 1697548682063, 1697548682148, 1697548682229, 1697548682310]"
2410,2410,410,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591595,1697548594347,120,,,"[34, 1222]","[1697548591629, 1697548592851]"
2411,2411,191,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627139,1697548629599,120,,,"[339, 1861]","[1697548627478, 1697548629339]"
2412,2412,554,24,[],200,llama-7b,128,1,2009.0,1.0,1,A100,1697548629611,1697548631620,120,26.0,1.0,"[356, 1653]","[1697548629967, 1697548631620]"
2413,2413,771,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594351,1697548597359,120,,,"[128, 1618, 251, 72, 56]","[1697548594479, 1697548596097, 1697548596348, 1697548596420, 1697548596476]"
2414,2414,882,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631624,1697548635894,120,,,"[29, 982, 1250, 57, 547]","[1697548631653, 1697548632635, 1697548633885, 1697548633942, 1697548634489]"
2415,2415,375,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583927,1697548586197,120,,,[223],[1697548584150]
2416,2416,200,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599944,120,,,"[205, 2145]","[1697548597572, 1697548599717]"
2417,2417,699,37,[],200,llama-7b,128,1,1821.0,1.0,1,A100,1697548664217,1697548666038,120,39.0,1.0,"[218, 1603]","[1697548664435, 1697548666038]"
2418,2418,616,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664725,1697548667372,120,,,"[11, 2609]","[1697548664736, 1697548667345]"
2419,2419,705,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586205,1697548589559,120,,,"[48, 2136, 79, 79, 75]","[1697548586253, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
2420,2420,126,18,[],200,llama-7b,128,1,2022.0,1.0,1,A100,1697548615244,1697548617266,120,19.0,1.0,"[338, 1684]","[1697548615582, 1697548617266]"
2421,2421,307,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637693,120,,,[234],[1697548636150]
2422,2422,512,40,[],200,llama-7b,128,1,1809.0,1.0,1,A100,1697548683216,1697548685025,120,11.0,1.0,"[231, 1577]","[1697548683447, 1697548685024]"
2423,2423,667,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637700,1697548639636,120,,,"[149, 1451]","[1697548637849, 1697548639300]"
2424,2424,425,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548634435,1697548635902,120,,,"[34, 1130]","[1697548634469, 1697548635599]"
2425,2425,92,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641379,120,,,[16],[1697548639662]
2426,2426,754,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635912,1697548637693,120,,,[37],[1697548635949]
2427,2427,480,19,[],200,llama-7b,128,1,685.0,1.0,1,A100,1697548617271,1697548617956,120,26.0,1.0,"[66, 619]","[1697548617337, 1697548617956]"
2428,2428,841,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548686720,120,,,[26],[1697548685056]
2429,2429,836,20,[],200,llama-7b,128,1,2363.0,1.0,1,A100,1697548617959,1697548620322,120,11.0,1.0,"[20, 2343]","[1697548617979, 1697548620322]"
2430,2430,179,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637700,1697548639637,120,,,"[154, 1446]","[1697548637854, 1697548639300]"
2431,2431,124,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666048,1697548667370,120,,,[93],[1697548666141]
2432,2432,539,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639648,1697548641376,120,,,[107],[1697548639755]
2433,2433,483,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667377,1697548671227,120,,,"[105, 2224, 472, 84, 65, 64, 80]","[1697548667482, 1697548669706, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
2434,2434,748,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599939,120,,,[326],[1697548597693]
2435,2435,900,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642998,120,,,[264],[1697548641649]
2436,2436,144,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602107,120,,,"[16, 1916]","[1697548599967, 1697548601883]"
2437,2437,504,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605263,120,,,"[252, 1471, 263, 77, 76]","[1697548602381, 1697548603852, 1697548604115, 1697548604192, 1697548604268]"
2438,2438,236,21,[],200,llama-7b,128,1,599.0,1.0,1,A100,1697548620326,1697548620925,120,8.0,1.0,"[25, 574]","[1697548620351, 1697548620925]"
2439,2439,271,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686732,1697548691085,120,,,"[406, 2466]","[1697548687138, 1697548689604]"
2440,2440,453,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642996,120,,,[37],[1697548641421]
2441,2441,332,33,[],200,llama-7b,128,1,568.0,1.0,1,A100,1697548643004,1697548643572,120,39.0,1.0,"[21, 547]","[1697548643025, 1697548643572]"
2442,2442,298,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610824,120,,,[278],[1697548609252]
2443,2443,628,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693448,120,,,[147],[1697548691243]
2444,2444,662,34,[],200,llama-7b,128,1,6632.0,1.0,1,A100,1697548643575,1697548650207,120,83.0,20.0,"[6, 2156, 578, 94, 91, 89, 68, 968, 96, 95, 94, 71, 94, 71, 891, 92, 88, 86, 732, 87, 85]","[1697548643581, 1697548645737, 1697548646315, 1697548646409, 1697548646500, 1697548646589, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648075, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650035, 1697548650122, 1697548650207]"
2445,2445,628,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610848,1697548612998,120,,,[299],[1697548611147]
2446,2446,597,22,[],200,llama-7b,128,1,2525.0,1.0,1,A100,1697548620931,1697548623456,120,39.0,1.0,"[23, 2502]","[1697548620954, 1697548623456]"
2447,2447,864,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605279,1697548608962,120,,,"[26, 1924, 508, 80, 79, 78]","[1697548605305, 1697548607229, 1697548607737, 1697548607817, 1697548607896, 1697548607974]"
2448,2448,57,44,[],200,llama-7b,128,1,1973.0,1.0,1,A100,1697548693459,1697548695432,120,13.0,1.0,"[100, 1873]","[1697548693559, 1697548695432]"
2449,2449,28,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548623461,1697548627100,120,,,"[15, 2298]","[1697548623476, 1697548625774]"
2450,2450,416,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695436,1697548697834,120,,,"[11, 1281]","[1697548695447, 1697548696728]"
2451,2451,783,30,[],200,llama-7b,128,1,1772.0,1.0,1,A100,1697548643006,1697548644778,120,286.0,1.0,"[137, 1634]","[1697548643143, 1697548644777]"
2452,2452,215,31,[],200,llama-7b,128,1,2293.0,1.0,1,A100,1697548644782,1697548647075,120,12.0,1.0,"[50, 2243]","[1697548644832, 1697548647075]"
2453,2453,463,4,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548586210,1697548588173,120,39.0,1.0,"[269, 1694]","[1697548586479, 1697548588173]"
2454,2454,377,3,[],200,llama-7b,128,1,1962.0,1.0,1,A100,1697548586210,1697548588172,120,13.0,1.0,"[249, 1713]","[1697548586459, 1697548588172]"
2455,2455,738,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588175,1697548589558,120,,,[34],[1697548588209]
2456,2456,822,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588176,1697548589559,120,,,"[68, 1136]","[1697548588244, 1697548589380]"
2457,2457,163,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592061,120,,,"[316, 1705]","[1697548589884, 1697548591589]"
2458,2458,574,32,[],200,llama-7b,128,1,2957.0,1.0,1,A100,1697548647078,1697548650035,120,364.0,2.0,"[20, 2841, 96]","[1697548647098, 1697548649939, 1697548650035]"
2459,2459,298,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610824,120,,,[37],[1697548609010]
2460,2460,522,6,[],200,llama-7b,128,1,2087.0,1.0,1,A100,1697548592072,1697548594159,120,20.0,1.0,"[37, 2050]","[1697548592109, 1697548594159]"
2461,2461,627,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610850,1697548613000,120,,,"[133, 1834]","[1697548610983, 1697548612817]"
2462,2462,209,21,[],200,llama-7b,128,1,2075.0,1.0,1,A100,1697548622604,1697548624679,120,20.0,1.0,"[161, 1913]","[1697548622765, 1697548624678]"
2463,2463,928,33,[],200,llama-7b,128,1,3187.0,1.0,1,A100,1697548650040,1697548653227,120,20.0,1.0,"[16, 3171]","[1697548650056, 1697548653227]"
2464,2464,51,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613011,1697548615234,120,,,"[134, 1888]","[1697548613145, 1697548615033]"
2465,2465,49,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622599,120,,,"[112, 1855]","[1697548620475, 1697548622330]"
2466,2466,357,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653230,1697548657537,120,,,"[13, 2935]","[1697548653243, 1697548656178]"
2467,2467,655,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673617,1697548677265,120,,,"[10, 1845]","[1697548673627, 1697548675472]"
2468,2468,567,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624681,1697548627101,120,,,"[7, 1086]","[1697548624688, 1697548625774]"
2469,2469,800,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659865,120,,,[200],[1697548657746]
2470,2470,753,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666046,1697548667369,120,,,"[57, 1244]","[1697548666103, 1697548667347]"
2471,2471,411,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617368,120,,,[315],[1697548615559]
2472,2472,880,7,[],200,llama-7b,128,1,2182.0,1.0,1,A100,1697548594164,1697548596346,120,84.0,2.0,"[27, 735, 1420]","[1697548594191, 1697548594926, 1697548596346]"
2473,2473,254,6,[],200,llama-7b,128,1,2015.0,1.0,1,A100,1697548589574,1697548591589,120,58.0,1.0,"[270, 1744]","[1697548589844, 1697548591588]"
2474,2474,765,17,[],200,llama-7b,128,1,2018.0,1.0,1,A100,1697548617379,1697548619397,120,84.0,2.0,"[260, 1757]","[1697548617639, 1697548619396]"
2475,2475,378,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622608,1697548624959,120,,,[342],[1697548622950]
2476,2476,474,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599942,120,,,[484],[1697548597851]
2477,2477,733,21,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624974,1697548627021,120,31.0,1.0,"[183, 1864]","[1697548625157, 1697548627021]"
2478,2478,198,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619399,1697548622594,120,,,"[14, 1511]","[1697548619413, 1697548620924]"
2479,2479,183,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671229,120,,,"[301, 2030, 470, 84, 63, 64, 81]","[1697548667679, 1697548669709, 1697548670179, 1697548670263, 1697548670326, 1697548670390, 1697548670471]"
2480,2480,158,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627024,1697548629598,120,,,"[46, 868]","[1697548627070, 1697548627938]"
2481,2481,280,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596348,1697548597359,120,,,"[6, 981]","[1697548596354, 1697548597335]"
2482,2482,639,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599942,120,,,"[417, 1933]","[1697548597784, 1697548599717]"
2483,2483,551,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589573,1697548592061,120,,,"[69, 1947]","[1697548589642, 1697548591589]"
2484,2484,68,10,[],200,llama-7b,128,1,1933.0,1.0,1,A100,1697548599953,1697548601886,120,12.0,1.0,"[390, 1542]","[1697548600343, 1697548601885]"
2485,2485,528,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[13, 2060]","[1697548622616, 1697548624676]"
2486,2486,888,20,[],200,llama-7b,128,1,2049.0,1.0,1,A100,1697548624971,1697548627020,120,19.0,1.0,"[83, 1966]","[1697548625054, 1697548627020]"
2487,2487,51,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610832,120,,,[288],[1697548609262]
2488,2488,916,29,[],200,llama-7b,128,1,3017.0,1.0,1,A100,1697548650210,1697548653227,120,8.0,1.0,"[29, 2988]","[1697548650239, 1697548653227]"
2489,2489,806,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583922,120,,,"[169, 1551, 109, 100, 90, 89, 86, 82]","[1697548581259, 1697548582810, 1697548582919, 1697548583019, 1697548583109, 1697548583198, 1697548583284, 1697548583366]"
2490,2490,496,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610851,1697548612996,120,,,"[191, 1775]","[1697548611042, 1697548612817]"
2491,2491,675,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583931,1697548586199,120,,,"[133, 1895]","[1697548584064, 1697548585959]"
2492,2492,314,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627023,1697548629597,120,,,"[15, 900]","[1697548627038, 1697548627938]"
2493,2493,863,13,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610848,1697548612815,120,10.0,1.0,"[234, 1733]","[1697548611082, 1697548612815]"
2494,2494,673,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631887,120,,,"[165, 1848]","[1697548629773, 1697548631621]"
2495,2495,76,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586210,1697548589557,120,,,"[260, 1702, 217, 79, 79, 77]","[1697548586470, 1697548588172, 1697548588389, 1697548588468, 1697548588547, 1697548588624]"
2496,2496,101,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633848,120,,,[134],[1697548632035]
2497,2497,348,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653230,1697548657537,120,,,"[14, 2935]","[1697548653244, 1697548656179]"
2498,2498,856,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615233,120,,,"[65, 1960]","[1697548613071, 1697548615031]"
2499,2499,294,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612820,1697548615236,120,,,[51],[1697548612871]
2500,2500,654,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617368,120,,,"[310, 1711]","[1697548615554, 1697548617265]"
2501,2501,173,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617386,1697548620355,120,,,"[303, 1598, 110, 84, 82, 81, 80]","[1697548617689, 1697548619287, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619724]"
2502,2502,677,3,[],200,llama-7b,128,1,1247.0,1.0,1,A100,1697548585966,1697548587213,120,9.0,1.0,"[118, 1129]","[1697548586084, 1697548587213]"
2503,2503,533,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622599,120,,,[107],[1697548620470]
2504,2504,706,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657544,1697548659868,120,,,"[102, 1932]","[1697548657646, 1697548659578]"
2505,2505,887,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622608,1697548624959,120,,,"[294, 1777]","[1697548622902, 1697548624679]"
2506,2506,140,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661852,120,,,[203],[1697548660075]
2507,2507,317,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627101,120,,,"[301, 1747]","[1697548625275, 1697548627022]"
2508,2508,671,20,[],200,llama-7b,128,1,2241.0,1.0,1,A100,1697548627106,1697548629347,120,12.0,1.0,"[153, 2080]","[1697548627259, 1697548629339]"
2509,2509,74,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629351,1697548631887,120,,,"[44, 877]","[1697548629395, 1697548630272]"
2510,2510,434,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633846,120,,,[247],[1697548632148]
2511,2511,296,4,[],200,llama-7b,128,1,2017.0,1.0,1,A100,1697548589574,1697548591591,120,6.0,1.0,"[179, 1838]","[1697548589753, 1697548591591]"
2512,2512,714,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596349,1697548597359,120,,,"[15, 971]","[1697548596364, 1697548597335]"
2513,2513,143,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599943,120,,,"[125, 2225]","[1697548597491, 1697548599716]"
2514,2514,660,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591595,1697548594348,120,,,"[71, 1186]","[1697548591666, 1697548592852]"
2515,2515,63,9,[],200,llama-7b,128,1,785.0,1.0,1,A100,1697548601890,1697548602675,120,39.0,1.0,"[34, 750]","[1697548601924, 1697548602674]"
2516,2516,791,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635896,120,,,[290],[1697548634145]
2517,2517,589,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,"[415, 1518]","[1697548600368, 1697548601886]"
2518,2518,220,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[355],[1697548636271]
2519,2519,17,10,[],200,llama-7b,128,1,1725.0,1.0,1,A100,1697548602127,1697548603852,120,23.0,1.0,"[229, 1496]","[1697548602356, 1697548603852]"
2520,2520,375,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603856,1697548605265,120,,,[35],[1697548603891]
2521,2521,579,25,[],200,llama-7b,128,1,1597.0,1.0,1,A100,1697548637704,1697548639301,120,19.0,1.0,"[331, 1265]","[1697548638035, 1697548639300]"
2522,2522,734,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608965,120,,,"[207, 1739, 510, 78, 79, 78]","[1697548605490, 1697548607229, 1697548607739, 1697548607817, 1697548607896, 1697548607974]"
2523,2523,907,26,[],200,llama-7b,128,1,878.0,1.0,1,A100,1697548639305,1697548640183,120,10.0,1.0,"[53, 825]","[1697548639358, 1697548640183]"
2524,2524,336,27,[],200,llama-7b,128,1,5174.0,1.0,1,A100,1697548640186,1697548645360,120,58.0,7.0,"[10, 1541, 41, 1262, 584, 1587, 85, 64]","[1697548640196, 1697548641737, 1697548641778, 1697548643040, 1697548643624, 1697548645211, 1697548645296, 1697548645360]"
2525,2525,695,28,[],200,llama-7b,128,1,8060.0,1.0,1,A100,1697548645363,1697548653423,120,92.0,20.0,"[11, 3064, 600, 92, 88, 86, 730, 87, 85, 75, 74, 74, 1001, 81, 76, 858, 320, 88, 69, 404, 97]","[1697548645374, 1697548648438, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651430, 1697548651511, 1697548651587, 1697548652445, 1697548652765, 1697548652853, 1697548652922, 1697548653326, 1697548653423]"
2526,2526,91,6,[],200,llama-7b,128,1,1744.0,1.0,1,A100,1697548594355,1697548596099,120,23.0,1.0,"[346, 1398]","[1697548594701, 1697548596099]"
2527,2527,831,1,[],200,llama-7b,128,1,1720.0,1.0,1,A100,1697548581090,1697548582810,120,11.0,1.0,"[296, 1423]","[1697548581386, 1697548582809]"
2528,2528,419,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596103,1697548597358,120,,,"[46, 1186]","[1697548596149, 1697548597335]"
2529,2529,774,8,[],200,llama-7b,128,1,2351.0,1.0,1,A100,1697548597367,1697548599718,120,8.0,1.0,"[382, 1969]","[1697548597749, 1697548599718]"
2530,2530,200,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599722,1697548602107,120,,,"[79, 739]","[1697548599801, 1697548600540]"
2531,2531,232,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582814,1697548583922,120,,,"[37, 1051]","[1697548582851, 1697548583902]"
2532,2532,558,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605265,120,,,"[113, 1613, 260, 80, 76]","[1697548602240, 1697548603853, 1697548604113, 1697548604193, 1697548604269]"
2533,2533,586,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583933,1697548589557,120,,,"[412, 2869, 1175, 79, 78, 75]","[1697548584345, 1697548587214, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
2534,2534,162,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610833,120,,,[317],[1697548609292]
2535,2535,702,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548623460,1697548627099,120,,,"[11, 2303]","[1697548623471, 1697548625774]"
2536,2536,586,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605265,120,,,"[63, 1663, 262, 78, 76]","[1697548602190, 1697548603853, 1697548604115, 1697548604193, 1697548604269]"
2537,2537,850,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592076,1697548594348,120,,,"[313, 1771]","[1697548592389, 1697548594160]"
2538,2538,331,14,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548613008,1697548615033,120,26.0,1.0,"[142, 1883]","[1697548613150, 1697548615033]"
2539,2539,682,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615036,1697548617366,120,,,"[59, 778]","[1697548615095, 1697548615873]"
2540,2540,740,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617367,120,,,"[307, 1714]","[1697548615551, 1697548617265]"
2541,2541,133,22,[],200,llama-7b,128,1,2232.0,1.0,1,A100,1697548627105,1697548629337,120,15.0,1.0,"[55, 2177]","[1697548627160, 1697548629337]"
2542,2542,643,2,[],200,llama-7b,128,1,3281.0,1.0,1,A100,1697548583932,1697548587213,120,18.0,1.0,"[361, 2919]","[1697548584293, 1697548587212]"
2543,2543,487,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629346,1697548631886,120,,,[16],[1697548629362]
2544,2544,846,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633847,120,,,[75],[1697548631976]
2545,2545,111,16,[],200,llama-7b,128,1,2264.0,1.0,1,A100,1697548617379,1697548619643,120,79.0,5.0,"[198, 1819, 84, 82, 81]","[1697548617577, 1697548619396, 1697548619480, 1697548619562, 1697548619643]"
2546,2546,469,17,[],200,llama-7b,128,1,1277.0,1.0,1,A100,1697548619647,1697548620924,120,17.0,1.0,"[17, 1260]","[1697548619664, 1697548620924]"
2547,2547,75,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548587220,1697548589557,120,,,"[20, 2139]","[1697548587240, 1697548589379]"
2548,2548,271,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635895,120,,,"[208, 1537]","[1697548634063, 1697548635600]"
2549,2549,407,4,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548589568,1697548591591,120,16.0,1.0,"[371, 1651]","[1697548589939, 1697548591590]"
2550,2550,284,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597361,120,,,"[334, 1656, 74, 57]","[1697548594690, 1697548596346, 1697548596420, 1697548596477]"
2551,2551,168,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617386,1697548620354,120,,,"[283, 1618, 110, 83, 83, 81, 80]","[1697548617669, 1697548619287, 1697548619397, 1697548619480, 1697548619563, 1697548619644, 1697548619724]"
2552,2552,602,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635917,1697548637694,120,,,[263],[1697548636180]
2553,2553,834,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620927,1697548624950,120,,,"[6, 2522]","[1697548620933, 1697548623455]"
2554,2554,33,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637701,1697548639637,120,,,"[155, 1444]","[1697548637856, 1697548639300]"
2555,2555,570,5,[],200,llama-7b,128,1,990.0,1.0,1,A100,1697548588391,1697548589381,120,18.0,1.0,"[7, 983]","[1697548588398, 1697548589381]"
2556,2556,394,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639659,1697548641376,120,,,[323],[1697548639982]
2557,2557,305,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[345, 1588]","[1697548600297, 1697548601885]"
2558,2558,752,29,[],200,llama-7b,128,1,1656.0,1.0,1,A100,1697548641384,1697548643040,120,39.0,3.0,"[27, 326, 42, 1261]","[1697548641411, 1697548641737, 1697548641779, 1697548643040]"
2559,2559,616,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597372,1697548599943,120,,,"[504, 1842]","[1697548597876, 1697548599718]"
2560,2560,266,30,[],200,llama-7b,128,1,2694.0,1.0,1,A100,1697548643043,1697548645737,120,9.0,1.0,"[314, 2380]","[1697548643357, 1697548645737]"
2561,2561,783,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610839,120,,,[365],[1697548609340]
2562,2562,45,10,[],200,llama-7b,128,1,1934.0,1.0,1,A100,1697548599953,1697548601887,120,19.0,1.0,"[424, 1510]","[1697548600377, 1697548601887]"
2563,2563,620,31,[],200,llama-7b,128,1,4466.0,1.0,1,A100,1697548645740,1697548650206,120,100.0,8.0,"[18, 2680, 600, 92, 88, 87, 729, 87, 85]","[1697548645758, 1697548648438, 1697548649038, 1697548649130, 1697548649218, 1697548649305, 1697548650034, 1697548650121, 1697548650206]"
2564,2564,526,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622597,120,,,"[73, 1893]","[1697548620435, 1697548622328]"
2565,2565,188,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610850,1697548612995,120,,,"[186, 1781]","[1697548611036, 1697548612817]"
2566,2566,549,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615231,120,,,"[15, 2009]","[1697548613021, 1697548615030]"
2567,2567,404,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601890,1697548605272,120,,,"[35, 750, 1439, 77, 77]","[1697548601925, 1697548602675, 1697548604114, 1697548604191, 1697548604268]"
2568,2568,909,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617369,120,,,"[96, 1927]","[1697548615338, 1697548617265]"
2569,2569,337,17,[],200,llama-7b,128,1,1901.0,1.0,1,A100,1697548617387,1697548619288,120,12.0,1.0,"[312, 1588]","[1697548617699, 1697548619287]"
2570,2570,50,32,[],200,llama-7b,128,1,3303.0,1.0,1,A100,1697548650209,1697548653512,120,90.0,4.0,"[20, 2998, 101, 96, 88]","[1697548650229, 1697548653227, 1697548653328, 1697548653424, 1697548653512]"
2571,2571,687,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619291,1697548620358,120,,,"[44, 987]","[1697548619335, 1697548620322]"
2572,2572,43,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624953,120,,,"[244, 1829]","[1697548622848, 1697548624677]"
2573,2573,89,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622598,120,,,"[392, 1570]","[1697548620760, 1697548622330]"
2574,2574,568,21,[],200,llama-7b,128,1,1090.0,1.0,1,A100,1697548624684,1697548625774,120,11.0,1.0,"[53, 1037]","[1697548624737, 1697548625774]"
2575,2575,13,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679646,120,,,"[342, 3051]","[1697548675190, 1697548678241]"
2576,2576,754,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608964,120,,,"[164, 2292, 79, 79, 78]","[1697548605447, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
2577,2577,400,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627101,120,,,"[208, 1838]","[1697548625182, 1697548627020]"
2578,2578,447,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622608,1697548624959,120,,,[346],[1697548622954]
2579,2579,763,21,[],200,llama-7b,128,1,2200.0,1.0,1,A100,1697548627138,1697548629338,120,20.0,1.0,"[242, 1957]","[1697548627380, 1697548629337]"
2580,2580,811,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627100,120,,,"[202, 1844]","[1697548625176, 1697548627020]"
2581,2581,928,22,[],200,llama-7b,128,1,2159.0,1.0,1,A100,1697548625779,1697548627938,120,20.0,1.0,"[42, 2117]","[1697548625821, 1697548627938]"
2582,2582,353,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627941,1697548631886,120,,,"[21, 2309]","[1697548627962, 1697548630271]"
2583,2583,241,22,[],200,llama-7b,128,1,2233.0,1.0,1,A100,1697548627105,1697548629338,120,19.0,1.0,"[79, 2154]","[1697548627184, 1697548629338]"
2584,2584,407,33,[],200,llama-7b,128,1,1551.0,1.0,1,A100,1697548653516,1697548655067,120,16.0,1.0,"[10, 1541]","[1697548653526, 1697548655067]"
2585,2585,686,23,[],200,llama-7b,128,1,921.0,1.0,1,A100,1697548629350,1697548630271,120,31.0,1.0,"[34, 887]","[1697548629384, 1697548630271]"
2586,2586,714,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633847,120,,,[80],[1697548631981]
2587,2587,111,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548630274,1697548635893,120,,,"[20, 3591, 56, 548]","[1697548630294, 1697548633885, 1697548633941, 1697548634489]"
2588,2588,772,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655070,1697548657503,120,,,"[23, 1087]","[1697548655093, 1697548656180]"
2589,2589,67,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661877,1697548664208,120,,,"[363, 1538]","[1697548662240, 1697548663778]"
2590,2590,184,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610832,120,,,[173],[1697548609146]
2591,2591,467,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635910,1697548637693,120,,,[37],[1697548635947]
2592,2592,115,25,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548633857,1697548635599,120,13.0,1.0,"[386, 1356]","[1697548634243, 1697548635599]"
2593,2593,425,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667370,120,,,"[242, 1576, 430, 84, 65, 85]","[1697548664460, 1697548666036, 1697548666466, 1697548666550, 1697548666615, 1697548666700]"
2594,2594,513,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610851,1697548612998,120,,,"[286, 1678]","[1697548611137, 1697548612815]"
2595,2595,375,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683209,120,,,[379],[1697548680036]
2596,2596,784,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671230,120,,,"[369, 1963, 469, 83, 64, 65, 80]","[1697548667747, 1697548669710, 1697548670179, 1697548670262, 1697548670326, 1697548670391, 1697548670471]"
2597,2597,820,41,[],200,llama-7b,128,1,2629.0,1.0,1,A100,1697548683216,1697548685845,120,161.0,9.0,"[266, 1543, 238, 91, 90, 89, 70, 87, 86, 69]","[1697548683482, 1697548685025, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685845]"
2598,2598,564,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631888,120,,,"[229, 1779]","[1697548629837, 1697548631616]"
2599,2599,245,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685847,1697548688898,120,,,"[15, 1681]","[1697548685862, 1697548687543]"
2600,2600,183,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583923,120,,,"[198, 1524, 106, 102, 89, 88, 87, 84]","[1697548581287, 1697548582811, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583367]"
2601,2601,80,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633844,120,,,[179],[1697548632075]
2602,2602,437,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635894,120,,,"[140, 1605]","[1697548633994, 1697548635599]"
2603,2603,209,38,[],200,llama-7b,128,1,1834.0,1.0,1,A100,1697548671243,1697548673077,120,20.0,1.0,"[121, 1712]","[1697548671364, 1697548673076]"
2604,2604,242,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627943,1697548631886,120,,,"[31, 2297]","[1697548627974, 1697548630271]"
2605,2605,102,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585963,1697548589557,120,,,"[77, 1173, 1176, 79, 78, 75]","[1697548586040, 1697548587213, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
2606,2606,601,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[13, 1924]","[1697548688920, 1697548690844]"
2607,2607,796,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635912,1697548637693,120,,,[243],[1697548636155]
2608,2608,765,42,[],200,llama-7b,128,1,2495.0,1.0,1,A100,1697548679477,1697548681972,120,84.0,2.0,"[16, 2479]","[1697548679493, 1697548681972]"
2609,2609,190,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681974,1697548686705,120,,,"[10, 2059, 1219, 91, 90, 89, 70, 87, 86, 68]","[1697548681984, 1697548684043, 1697548685262, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
2610,2610,30,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693452,120,,,"[343, 1809]","[1697548691439, 1697548693248]"
2611,2611,537,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583931,1697548586199,120,,,"[209, 1816]","[1697548584140, 1697548585956]"
2612,2612,224,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637700,1697548639646,120,,,"[70, 1529]","[1697548637770, 1697548639299]"
2613,2613,582,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641376,120,,,[337],[1697548639992]
2614,2614,391,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695717,120,,,[297],[1697548693761]
2615,2615,553,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686722,1697548688899,120,,,"[14, 1976]","[1697548686736, 1697548688712]"
2616,2616,461,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589576,1697548592063,120,,,[375],[1697548589951]
2617,2617,912,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643001,120,,,[349],[1697548641735]
2618,2618,721,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,"[48, 1911]","[1697548695772, 1697548697683]"
2619,2619,341,28,[],200,llama-7b,128,1,6300.0,1.0,1,A100,1697548643004,1697548649304,120,87.0,20.0,"[36, 1736, 436, 84, 64, 955, 94, 91, 89, 68, 968, 97, 94, 94, 72, 93, 71, 892, 91, 89, 85]","[1697548643040, 1697548644776, 1697548645212, 1697548645296, 1697548645360, 1697548646315, 1697548646409, 1697548646500, 1697548646589, 1697548646657, 1697548647625, 1697548647722, 1697548647816, 1697548647910, 1697548647982, 1697548648075, 1697548648146, 1697548649038, 1697548649129, 1697548649218, 1697548649303]"
2620,2620,882,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691086,120,,,"[66, 1870]","[1697548688975, 1697548690845]"
2621,2621,819,5,[],200,llama-7b,128,1,2088.0,1.0,1,A100,1697548592073,1697548594161,120,13.0,1.0,"[190, 1898]","[1697548592263, 1697548594161]"
2622,2622,869,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586205,1697548589560,120,,,"[73, 1893, 218, 79, 79, 75]","[1697548586278, 1697548588171, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
2623,2623,835,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622608,1697548624958,120,,,"[275, 1796]","[1697548622883, 1697548624679]"
2624,2624,255,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594168,1697548597361,120,,,"[67, 692, 1419, 72, 58]","[1697548594235, 1697548594927, 1697548596346, 1697548596418, 1697548596476]"
2625,2625,584,7,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597367,1697548599717,120,10.0,1.0,"[331, 2019]","[1697548597698, 1697548599717]"
2626,2626,314,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693450,120,,,"[191, 1958]","[1697548691288, 1697548693246]"
2627,2627,13,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599720,1697548602106,120,,,"[22, 798]","[1697548599742, 1697548600540]"
2628,2628,103,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610824,120,,,[26],[1697548608999]
2629,2629,457,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610847,1697548612997,120,,,[238],[1697548611085]
2630,2630,416,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635603,1697548639636,120,,,"[19, 887, 1222, 417]","[1697548635622, 1697548636509, 1697548637731, 1697548638148]"
2631,2631,671,19,[],200,llama-7b,128,1,2022.0,1.0,1,A100,1697548615244,1697548617266,120,12.0,1.0,"[320, 1702]","[1697548615564, 1697548617266]"
2632,2632,363,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605266,120,,,"[214, 1510, 263, 79, 75]","[1697548602341, 1697548603851, 1697548604114, 1697548604193, 1697548604268]"
2633,2633,39,36,[],200,llama-7b,128,1,1299.0,1.0,1,A100,1697548666047,1697548667346,120,8.0,1.0,"[71, 1228]","[1697548666118, 1697548667346]"
2634,2634,96,20,[],200,llama-7b,128,1,687.0,1.0,1,A100,1697548617269,1697548617956,120,31.0,1.0,"[11, 676]","[1697548617280, 1697548617956]"
2635,2635,369,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667351,1697548671225,120,,,"[43, 631, 35, 2117, 85, 64, 64, 80]","[1697548667394, 1697548668025, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670390, 1697548670470]"
2636,2636,430,3,[],200,llama-7b,128,1,1253.0,1.0,1,A100,1697548585962,1697548587215,120,15.0,1.0,"[12, 1241]","[1697548585974, 1697548587215]"
2637,2637,452,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617960,1697548620357,120,,,"[29, 2333]","[1697548617989, 1697548620322]"
2638,2638,727,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674839,120,,,"[400, 2680]","[1697548671638, 1697548674318]"
2639,2639,808,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622596,120,,,"[306, 1655]","[1697548620674, 1697548622329]"
2640,2640,240,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624952,120,,,"[131, 1942]","[1697548622735, 1697548624677]"
2641,2641,152,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677257,120,,,"[214, 1737]","[1697548675059, 1697548676796]"
2642,2642,723,10,[],200,llama-7b,128,1,1947.0,1.0,1,A100,1697548605283,1697548607230,120,14.0,1.0,"[288, 1659]","[1697548605571, 1697548607230]"
2643,2643,508,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679645,120,,,"[76, 2128]","[1697548677345, 1697548679473]"
2644,2644,40,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671226,120,,,"[65, 582, 35, 2117, 85, 64, 64, 80]","[1697548667443, 1697548668025, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670390, 1697548670470]"
2645,2645,865,41,[],200,llama-7b,128,1,2032.0,1.0,1,A100,1697548679657,1697548681689,120,9.0,1.0,"[118, 1914]","[1697548679775, 1697548681689]"
2646,2646,385,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548686705,120,,,"[65, 2280, 1219, 91, 90, 89, 70, 87, 86, 68]","[1697548681763, 1697548684043, 1697548685262, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
2647,2647,151,11,[],200,llama-7b,128,1,1534.0,1.0,1,A100,1697548607237,1697548608771,120,39.0,1.0,"[101, 1433]","[1697548607338, 1697548608771]"
2648,2648,484,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608774,1697548612996,120,,,"[30, 2064, 569]","[1697548608804, 1697548610868, 1697548611437]"
2649,2649,775,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641379,120,,,[20],[1697548639666]
2650,2650,747,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686719,1697548688902,120,,,"[238, 1755]","[1697548686957, 1697548688712]"
2651,2651,556,18,[],200,llama-7b,128,1,2073.0,1.0,1,A100,1697548622606,1697548624679,120,9.0,1.0,"[267, 1806]","[1697548622873, 1697548624679]"
2652,2652,291,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642997,120,,,[66],[1697548641452]
2653,2653,915,19,[],200,llama-7b,128,1,1092.0,1.0,1,A100,1697548624682,1697548625774,120,182.0,1.0,"[36, 1056]","[1697548624718, 1697548625774]"
2654,2654,729,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,[171],[1697548627277]
2655,2655,176,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688910,1697548691087,120,,,[240],[1697548689150]
2656,2656,343,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548625778,1697548629597,120,,,"[33, 2127]","[1697548625811, 1697548627938]"
2657,2657,702,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631890,120,,,"[51, 1964]","[1697548629653, 1697548631617]"
2658,2658,652,29,[],200,llama-7b,128,1,1770.0,1.0,1,A100,1697548643007,1697548644777,120,14.0,1.0,"[73, 1697]","[1697548643080, 1697548644777]"
2659,2659,218,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631900,1697548633847,120,,,[263],[1697548632163]
2660,2660,575,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627104,120,,,"[82, 1964]","[1697548625056, 1697548627020]"
2661,2661,595,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610835,120,,,[195],[1697548609168]
2662,2662,79,30,[],200,llama-7b,128,1,2293.0,1.0,1,A100,1697548644782,1697548647075,120,12.0,1.0,"[47, 2246]","[1697548644829, 1697548647075]"
2663,2663,3,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629598,120,,,"[242, 1957]","[1697548627380, 1697548629337]"
2664,2664,20,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610851,1697548612997,120,,,"[215, 1752]","[1697548611066, 1697548612818]"
2665,2665,576,23,[],200,llama-7b,128,1,1746.0,1.0,1,A100,1697548633853,1697548635599,120,14.0,1.0,"[190, 1556]","[1697548634043, 1697548635599]"
2666,2666,202,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596348,1697548597359,120,,,"[6, 981]","[1697548596354, 1697548597335]"
2667,2667,6,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635605,1697548639637,120,,,"[67, 2059, 417]","[1697548635672, 1697548637731, 1697548638148]"
2668,2668,348,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613010,1697548615232,120,,,"[273, 1748]","[1697548613283, 1697548615031]"
2669,2669,357,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631888,120,,,"[234, 1775]","[1697548629842, 1697548631617]"
2670,2670,561,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599940,120,,,"[316, 2034]","[1697548597683, 1697548599717]"
2671,2671,711,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617368,120,,,"[31, 1992]","[1697548615273, 1697548617265]"
2672,2672,915,11,[],200,llama-7b,128,1,1934.0,1.0,1,A100,1697548599952,1697548601886,120,182.0,1.0,"[218, 1715]","[1697548600170, 1697548601885]"
2673,2673,437,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548647078,1697548655268,120,,,"[29, 2832, 96, 86, 86, 75, 74, 74, 991, 91, 74, 860, 319, 89, 69, 404, 97, 88, 84, 650, 97, 84, 62, 80]","[1697548647107, 1697548649939, 1697548650035, 1697548650121, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651421, 1697548651512, 1697548651586, 1697548652446, 1697548652765, 1697548652854, 1697548652923, 1697548653327, 1697548653424, 1697548653512, 1697548653596, 1697548654246, 1697548654343, 1697548654427, 1697548654489, 1697548654569]"
2674,2674,364,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639653,1697548641374,120,,,[293],[1697548639946]
2675,2675,345,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601891,1697548605272,120,,,"[54, 730, 1439, 78, 76]","[1697548601945, 1697548602675, 1697548604114, 1697548604192, 1697548604268]"
2676,2676,143,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617381,1697548620354,120,,,"[288, 1618, 110, 84, 82, 81, 80]","[1697548617669, 1697548619287, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619724]"
2677,2677,793,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605285,1697548608964,120,,,"[403, 2051, 80, 79, 78]","[1697548605688, 1697548607739, 1697548607819, 1697548607898, 1697548607976]"
2678,2678,502,17,[],200,llama-7b,128,1,1968.0,1.0,1,A100,1697548620362,1697548622330,120,19.0,1.0,"[393, 1575]","[1697548620755, 1697548622330]"
2679,2679,790,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655280,1697548657503,120,,,"[31, 2114]","[1697548655311, 1697548657425]"
2680,2680,195,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657547,1697548659865,120,,,"[307, 1724]","[1697548657854, 1697548659578]"
2681,2681,827,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624958,120,,,"[49, 1073]","[1697548622384, 1697548623457]"
2682,2682,552,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661854,120,,,[280],[1697548660152]
2683,2683,252,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627100,120,,,"[271, 1776]","[1697548625245, 1697548627021]"
2684,2684,544,24,[],200,llama-7b,128,1,1743.0,1.0,1,A100,1697548633855,1697548635598,120,26.0,1.0,"[316, 1427]","[1697548634171, 1697548635598]"
2685,2685,307,37,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667378,1697548669708,120,26.0,1.0,"[261, 2069]","[1697548667639, 1697548669708]"
2686,2686,232,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661855,120,,,[107],[1697548659978]
2687,2687,670,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671227,120,,,"[36, 1454]","[1697548669749, 1697548671203]"
2688,2688,897,23,[],200,llama-7b,128,1,2232.0,1.0,1,A100,1697548627106,1697548629338,120,9.0,1.0,"[88, 2144]","[1697548627194, 1697548629338]"
2689,2689,589,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664207,120,,,"[303, 1609]","[1697548662170, 1697548663779]"
2690,2690,162,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615234,120,,,"[154, 1873]","[1697548613160, 1697548615033]"
2691,2691,80,41,[],200,llama-7b,128,1,2195.0,1.0,1,A100,1697548677278,1697548679473,120,13.0,1.0,"[190, 2005]","[1697548677468, 1697548679473]"
2692,2692,70,39,[],200,llama-7b,128,1,1835.0,1.0,1,A100,1697548671242,1697548673077,120,39.0,1.0,"[141, 1694]","[1697548671383, 1697548673077]"
2693,2693,410,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679477,1697548683205,120,,,"[16, 864, 1615, 91, 84, 83, 80]","[1697548679493, 1697548680357, 1697548681972, 1697548682063, 1697548682147, 1697548682230, 1697548682310]"
2694,2694,538,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674839,120,,,"[410, 2670]","[1697548671648, 1697548674318]"
2695,2695,430,40,[],200,llama-7b,128,1,1238.0,1.0,1,A100,1697548673081,1697548674319,120,15.0,1.0,"[83, 1155]","[1697548673164, 1697548674319]"
2696,2696,22,38,[],200,llama-7b,128,1,1819.0,1.0,1,A100,1697548664217,1697548666036,120,16.0,1.0,"[234, 1585]","[1697548664451, 1697548666036]"
2697,2697,536,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583919,120,,,"[260, 1460, 108, 102, 89, 89, 86, 84]","[1697548581349, 1697548582809, 1697548582917, 1697548583019, 1697548583108, 1697548583197, 1697548583283, 1697548583367]"
2698,2698,786,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674324,1697548677257,120,,,"[48, 1101]","[1697548674372, 1697548675473]"
2699,2699,426,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592066,120,,,[286],[1697548589854]
2700,2700,107,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548587220,1697548589557,120,,,[27],[1697548587247]
2701,2701,913,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599722,1697548602107,120,,,"[67, 751]","[1697548599789, 1697548600540]"
2702,2702,784,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592076,1697548594348,120,,,"[373, 1713]","[1697548592449, 1697548594162]"
2703,2703,552,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592065,120,,,"[281, 1739]","[1697548589849, 1697548591588]"
2704,2704,214,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679648,120,,,"[184, 2021]","[1697548677453, 1697548679474]"
2705,2705,911,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592076,1697548594348,120,,,"[368, 1717]","[1697548592444, 1697548594161]"
2706,2706,863,39,[],200,llama-7b,128,1,1952.0,1.0,1,A100,1697548674845,1697548676797,120,10.0,1.0,"[131, 1821]","[1697548674976, 1697548676797]"
2707,2707,333,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597361,120,,,"[300, 1441, 249, 73, 58]","[1697548594656, 1697548596097, 1697548596346, 1697548596419, 1697548596477]"
2708,2708,569,43,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679657,1697548681687,120,16.0,1.0,"[232, 1798]","[1697548679889, 1697548681687]"
2709,2709,292,40,[],200,llama-7b,128,1,1442.0,1.0,1,A100,1697548676800,1697548678242,120,286.0,1.0,"[6, 1436]","[1697548676806, 1697548678242]"
2710,2710,653,41,[],200,llama-7b,128,1,4065.0,1.0,1,A100,1697548678245,1697548682310,120,96.0,6.0,"[36, 2075, 1615, 92, 85, 81, 81]","[1697548678281, 1697548680356, 1697548681971, 1697548682063, 1697548682148, 1697548682229, 1697548682310]"
2711,2711,891,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583931,1697548586198,120,,,"[108, 1919]","[1697548584039, 1697548585958]"
2712,2712,899,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548683211,120,,,"[45, 1166]","[1697548681743, 1697548682909]"
2713,2713,421,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602677,1697548605263,120,,,"[6, 2386]","[1697548602683, 1697548605069]"
2714,2714,84,42,[],200,llama-7b,128,1,1729.0,1.0,1,A100,1697548682315,1697548684044,120,26.0,1.0,"[11, 1717]","[1697548682326, 1697548684043]"
2715,2715,331,45,[],200,llama-7b,128,1,1810.0,1.0,1,A100,1697548683216,1697548685026,120,26.0,1.0,"[272, 1538]","[1697548683488, 1697548685026]"
2716,2716,316,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586204,1697548589559,120,,,"[21, 1946, 218, 79, 78, 76]","[1697548586225, 1697548588171, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
2717,2717,690,46,[],200,llama-7b,128,1,2510.0,1.0,1,A100,1697548685033,1697548687543,120,39.0,1.0,"[73, 2437]","[1697548685106, 1697548687543]"
2718,2718,416,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548684047,1697548686709,120,,,"[11, 2121]","[1697548684058, 1697548686179]"
2719,2719,123,47,[],200,llama-7b,128,1,2054.0,1.0,1,A100,1697548687548,1697548689602,120,14.0,1.0,"[32, 2022]","[1697548687580, 1697548689602]"
2720,2720,770,44,[],200,llama-7b,128,1,2871.0,1.0,1,A100,1697548686732,1697548689603,120,13.0,1.0,"[349, 2522]","[1697548687081, 1697548689603]"
2721,2721,782,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608966,120,,,"[276, 1674, 510, 77, 79, 78]","[1697548605556, 1697548607230, 1697548607740, 1697548607817, 1697548607896, 1697548607974]"
2722,2722,674,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592062,120,,,"[106, 1910]","[1697548589680, 1697548591590]"
2723,2723,729,11,[],200,llama-7b,128,1,2665.0,1.0,1,A100,1697548605072,1697548607737,120,874.0,2.0,"[16, 2649]","[1697548605088, 1697548607737]"
2724,2724,200,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548689608,1697548693448,120,,,"[7, 2237]","[1697548689615, 1697548691852]"
2725,2725,124,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653427,1697548657538,120,,,[20],[1697548653447]
2726,2726,558,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693458,1697548695717,120,,,"[14, 1959]","[1697548693472, 1697548695431]"
2727,2727,563,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548689608,1697548693448,120,,,"[22, 2222]","[1697548689630, 1697548691852]"
2728,2728,922,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695718,120,,,"[90, 1883]","[1697548693549, 1697548695432]"
2729,2729,156,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607741,1697548612995,120,,,"[16, 1868, 1243, 568]","[1697548607757, 1697548609625, 1697548610868, 1697548611436]"
2730,2730,347,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,[130],[1697548695854]
2731,2731,296,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610835,120,,,[322],[1697548609297]
2732,2732,654,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610847,1697548612998,120,,,"[26, 1942]","[1697548610873, 1697548612815]"
2733,2733,454,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657547,1697548659865,120,,,"[234, 1796]","[1697548657781, 1697548659577]"
2734,2734,488,13,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548613008,1697548615033,120,6.0,1.0,"[132, 1892]","[1697548613140, 1697548615032]"
2735,2735,915,47,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695724,1697548697683,120,182.0,1.0,"[141, 1818]","[1697548695865, 1697548697683]"
2736,2736,846,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615037,1697548617366,120,,,"[63, 773]","[1697548615100, 1697548615873]"
2737,2737,276,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617380,1697548620354,120,,,"[299, 1718, 84, 82, 81, 80]","[1697548617679, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619724]"
2738,2738,493,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610851,1697548612996,120,,,"[201, 1765]","[1697548611052, 1697548612817]"
2739,2739,851,15,[],200,llama-7b,128,1,2024.0,1.0,1,A100,1697548613009,1697548615033,120,23.0,1.0,"[136, 1888]","[1697548613145, 1697548615033]"
2740,2740,634,16,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548620371,1697548622330,120,13.0,1.0,"[390, 1569]","[1697548620761, 1697548622330]"
2741,2741,279,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615036,1697548617366,120,,,"[54, 782]","[1697548615090, 1697548615872]"
2742,2742,812,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661854,120,,,[290],[1697548660162]
2743,2743,639,17,[],200,llama-7b,128,1,2345.0,1.0,1,A100,1697548617379,1697548619724,120,100.0,6.0,"[286, 1621, 111, 83, 83, 81, 80]","[1697548617665, 1697548619286, 1697548619397, 1697548619480, 1697548619563, 1697548619644, 1697548619724]"
2744,2744,240,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664205,120,,,"[185, 1724]","[1697548662053, 1697548663777]"
2745,2745,691,36,[],200,llama-7b,128,1,1490.0,1.0,1,A100,1697548669713,1697548671203,120,47.0,1.0,"[41, 1449]","[1697548669754, 1697548671203]"
2746,2746,66,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619727,1697548622595,120,,,"[18, 1180]","[1697548619745, 1697548620925]"
2747,2747,599,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664216,1697548667373,120,,,"[13, 1806, 432, 82, 67, 84]","[1697548664229, 1697548666035, 1697548666467, 1697548666549, 1697548666616, 1697548666700]"
2748,2748,395,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624952,120,,,"[151, 1923]","[1697548622755, 1697548624678]"
2749,2749,121,37,[],200,llama-7b,128,1,1868.0,1.0,1,A100,1697548671207,1697548673075,120,13.0,1.0,"[47, 1821]","[1697548671254, 1697548673075]"
2750,2750,754,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624968,1697548627103,120,,,[68],[1697548625036]
2751,2751,28,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667379,1697548671231,120,,,"[392, 1935, 472, 84, 65, 64, 80]","[1697548667771, 1697548669706, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
2752,2752,474,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673081,1697548674843,120,,,[86],[1697548673167]
2753,2753,373,13,[],200,llama-7b,128,1,1964.0,1.0,1,A100,1697548610852,1697548612816,120,15.0,1.0,"[290, 1673]","[1697548611142, 1697548612815]"
2754,2754,834,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548677255,120,,,"[113, 1836]","[1697548674961, 1697548676797]"
2755,2755,187,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629599,120,,,"[330, 1871]","[1697548627468, 1697548629339]"
2756,2756,115,16,[],200,llama-7b,128,1,2098.0,1.0,1,A100,1697548611440,1697548613538,120,13.0,1.0,"[7, 2090]","[1697548611447, 1697548613537]"
2757,2757,113,5,[],200,llama-7b,128,1,1203.0,1.0,1,A100,1697548588177,1697548589380,120,13.0,1.0,"[82, 1121]","[1697548588259, 1697548589380]"
2758,2758,542,22,[],200,llama-7b,128,1,2014.0,1.0,1,A100,1697548629607,1697548631621,120,11.0,1.0,"[159, 1855]","[1697548629766, 1697548631621]"
2759,2759,237,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679649,120,,,"[13, 2191]","[1697548677282, 1697548679473]"
2760,2760,656,14,[],200,llama-7b,128,1,1948.0,1.0,1,A100,1697548605284,1697548607232,120,26.0,1.0,"[370, 1578]","[1697548605654, 1697548607232]"
2761,2761,871,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631624,1697548635894,120,,,"[31, 980, 1250, 57, 547]","[1697548631655, 1697548632635, 1697548633885, 1697548633942, 1697548634489]"
2762,2762,903,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617364,120,,,"[121, 1902]","[1697548615363, 1697548617265]"
2763,2763,83,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608968,120,,,"[76, 1457]","[1697548607313, 1697548608770]"
2764,2764,469,17,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548613542,1697548615872,120,17.0,1.0,"[15, 2315]","[1697548613557, 1697548615872]"
2765,2765,296,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637693,120,,,[229],[1697548636145]
2766,2766,831,18,[],200,llama-7b,128,1,2080.0,1.0,1,A100,1697548615876,1697548617956,120,11.0,1.0,"[11, 2069]","[1697548615887, 1697548617956]"
2767,2767,598,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683208,120,,,"[336, 1694, 285, 91, 85, 82, 80]","[1697548679994, 1697548681688, 1697548681973, 1697548682064, 1697548682149, 1697548682231, 1697548682311]"
2768,2768,232,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617958,1697548620356,120,,,"[11, 2353]","[1697548617969, 1697548620322]"
2769,2769,235,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624962,1697548627103,120,,,[18],[1697548624980]
2770,2770,663,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605265,120,,,"[143, 1843, 80, 75]","[1697548602270, 1697548604113, 1697548604193, 1697548604268]"
2771,2771,593,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622599,120,,,"[119, 1848]","[1697548620482, 1697548622330]"
2772,2772,593,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629598,120,,,"[247, 1952]","[1697548627385, 1697548629337]"
2773,2773,29,42,[],200,llama-7b,128,1,2391.0,1.0,1,A100,1697548683213,1697548685604,120,161.0,6.0,"[53, 1759, 239, 90, 91, 88, 70]","[1697548683266, 1697548685025, 1697548685264, 1697548685354, 1697548685445, 1697548685533, 1697548685603]"
2774,2774,24,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622608,1697548624958,120,,,[345],[1697548622953]
2775,2775,94,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605285,1697548608964,120,,,"[408, 1540, 507, 79, 79, 78]","[1697548605693, 1697548607233, 1697548607740, 1697548607819, 1697548607898, 1697548607976]"
2776,2776,378,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627101,120,,,[310],[1697548625284]
2777,2777,455,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610823,120,,,[213],[1697548609186]
2778,2778,20,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548650211,1697548655267,120,,,"[34, 2982, 101, 96, 89, 84, 650, 96, 84, 62, 81]","[1697548650245, 1697548653227, 1697548653328, 1697548653424, 1697548653513, 1697548653597, 1697548654247, 1697548654343, 1697548654427, 1697548654489, 1697548654570]"
2779,2779,382,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685606,1697548688904,120,,,"[10, 1927]","[1697548685616, 1697548687543]"
2780,2780,377,31,[],200,llama-7b,128,1,2144.0,1.0,1,A100,1697548655282,1697548657426,120,13.0,1.0,"[83, 2061]","[1697548655365, 1697548657426]"
2781,2781,173,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657544,1697548659867,120,,,"[18, 2014]","[1697548657562, 1697548659576]"
2782,2782,731,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627105,1697548629599,120,,,"[64, 2168]","[1697548627169, 1697548629337]"
2783,2783,530,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659878,1697548661856,120,,,[361],[1697548660239]
2784,2784,736,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657438,1697548659867,120,,,"[100, 2038]","[1697548657538, 1697548659576]"
2785,2785,740,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688910,1697548691087,120,,,"[245, 1690]","[1697548689155, 1697548690845]"
2786,2786,165,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661855,120,,,[104],[1697548659976]
2787,2787,779,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610849,1697548612999,120,,,"[94, 1872]","[1697548610943, 1697548612815]"
2788,2788,254,45,[],200,llama-7b,128,1,2153.0,1.0,1,A100,1697548691095,1697548693248,120,58.0,1.0,"[168, 1985]","[1697548691263, 1697548693248]"
2789,2789,708,3,[],200,llama-7b,128,1,1964.0,1.0,1,A100,1697548586208,1697548588172,120,140.0,1.0,"[158, 1806]","[1697548586366, 1697548588172]"
2790,2790,112,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588176,1697548589559,120,,,"[53, 1150]","[1697548588229, 1697548589379]"
2791,2791,881,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664211,120,,,"[71, 814, 229]","[1697548661938, 1697548662752, 1697548662981]"
2792,2792,614,46,[],200,llama-7b,128,1,890.0,1.0,1,A100,1697548693254,1697548694144,120,15.0,1.0,"[64, 826]","[1697548693318, 1697548694144]"
2793,2793,45,47,[],200,llama-7b,128,1,2574.0,1.0,1,A100,1697548694154,1697548696728,120,19.0,1.0,"[20, 2554]","[1697548694174, 1697548696728]"
2794,2794,132,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631885,120,,,"[339, 1672]","[1697548629947, 1697548631619]"
2795,2795,497,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664207,120,,,"[307, 1604]","[1697548662175, 1697548663779]"
2796,2796,493,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631902,1697548633848,120,,,[291],[1697548632193]
2797,2797,854,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635896,120,,,"[286, 1461]","[1697548634141, 1697548635602]"
2798,2798,311,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664219,1697548667371,120,,,"[332, 1486, 430, 84, 65, 85]","[1697548664551, 1697548666037, 1697548666467, 1697548666551, 1697548666616, 1697548666701]"
2799,2799,285,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637696,120,,,[136],[1697548636051]
2800,2800,853,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667371,120,,,"[325, 1494, 429, 85, 65, 85]","[1697548664543, 1697548666037, 1697548666466, 1697548666551, 1697548666616, 1697548666701]"
2801,2801,730,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639646,120,,,"[295, 1302]","[1697548637998, 1697548639300]"
2802,2802,151,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639650,1697548641380,120,,,[219],[1697548639869]
2803,2803,510,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642999,120,,,[154],[1697548641540]
2804,2804,868,31,[],200,llama-7b,128,1,7199.0,1.0,1,A100,1697548643008,1697548650207,120,85.0,20.0,"[287, 2442, 578, 93, 92, 88, 69, 968, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86, 732, 86, 86]","[1697548643295, 1697548645737, 1697548646315, 1697548646408, 1697548646500, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650035, 1697548650121, 1697548650207]"
2805,2805,281,36,[],200,llama-7b,128,1,2331.0,1.0,1,A100,1697548667379,1697548669710,120,23.0,1.0,"[363, 1968]","[1697548667742, 1697548669710]"
2806,2806,209,13,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548613009,1697548615032,120,20.0,1.0,"[285, 1738]","[1697548613294, 1697548615032]"
2807,2807,541,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.90 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548578443,1697548581084,120,,,"[44, 1467, 100, 96, 94, 102, 82, 86, 82]","[1697548578487, 1697548579954, 1697548580054, 1697548580150, 1697548580244, 1697548580346, 1697548580428, 1697548580514, 1697548580596]"
2808,2808,635,37,[],200,llama-7b,128,1,1489.0,1.0,1,A100,1697548669715,1697548671204,120,23.0,1.0,"[93, 1395]","[1697548669808, 1697548671203]"
2809,2809,563,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617364,120,,,"[14, 823]","[1697548615049, 1697548615872]"
2810,2810,824,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637700,1697548639637,120,,,"[164, 1436]","[1697548637864, 1697548639300]"
2811,2811,233,21,[],200,llama-7b,128,1,2058.0,1.0,1,A100,1697548624962,1697548627020,120,6.0,1.0,"[40, 2018]","[1697548625002, 1697548627020]"
2812,2812,161,32,[],200,llama-7b,128,1,3929.0,1.0,1,A100,1697548646501,1697548650430,120,109.0,7.0,"[6, 3528, 86, 85, 75, 74, 75]","[1697548646507, 1697548650035, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650430]"
2813,2813,926,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620358,120,,,"[167, 1741, 110, 83, 85, 78, 81]","[1697548617545, 1697548619286, 1697548619396, 1697548619479, 1697548619564, 1697548619642, 1697548619723]"
2814,2814,537,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581091,1697548583920,120,,,"[290, 1428, 109, 102, 89, 88, 87, 84]","[1697548581381, 1697548582809, 1697548582918, 1697548583020, 1697548583109, 1697548583197, 1697548583284, 1697548583368]"
2815,2815,905,8,[],200,llama-7b,128,1,1741.0,1.0,1,A100,1697548594356,1697548596097,120,11.0,1.0,"[295, 1446]","[1697548594651, 1697548596097]"
2816,2816,255,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639656,1697548641378,120,,,[403],[1697548640059]
2817,2817,519,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548650433,1697548655268,120,,,"[15, 3445, 354, 96, 84, 63, 80]","[1697548650448, 1697548653893, 1697548654247, 1697548654343, 1697548654427, 1697548654490, 1697548654570]"
2818,2818,124,30,[],200,llama-7b,128,1,2205.0,1.0,1,A100,1697548643006,1697548645211,120,83.0,2.0,"[84, 2121]","[1697548643090, 1697548645211]"
2819,2819,482,31,[],200,llama-7b,128,1,8206.0,1.0,1,A100,1697548645216,1697548653422,120,91.0,20.0,"[18, 3204, 600, 92, 88, 86, 732, 85, 85, 76, 74, 73, 1002, 84, 72, 858, 320, 88, 69, 404, 96]","[1697548645234, 1697548648438, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650036, 1697548650121, 1697548650206, 1697548650282, 1697548650356, 1697548650429, 1697548651431, 1697548651515, 1697548651587, 1697548652445, 1697548652765, 1697548652853, 1697548652922, 1697548653326, 1697548653422]"
2820,2820,587,28,[],200,llama-7b,128,1,2184.0,1.0,1,A100,1697548641388,1697548643572,120,13.0,1.0,"[383, 1801]","[1697548641771, 1697548643572]"
2821,2821,673,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695718,120,,,"[95, 1878]","[1697548693554, 1697548695432]"
2822,2822,197,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633853,1697548635893,120,,,"[103, 1641]","[1697548633956, 1697548635597]"
2823,2823,98,48,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695725,1697548697684,120,14.0,1.0,"[243, 1716]","[1697548695968, 1697548697684]"
2824,2824,878,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655283,1697548657538,120,,,"[190, 1952]","[1697548655473, 1697548657425]"
2825,2825,685,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624968,1697548627100,120,,,"[101, 1951]","[1697548625069, 1697548627020]"
2826,2826,303,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659865,120,,,"[236, 1795]","[1697548657782, 1697548659577]"
2827,2827,659,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659878,1697548664208,120,,,"[375, 2499, 229]","[1697548660253, 1697548662752, 1697548662981]"
2828,2828,58,37,[],200,llama-7b,128,1,1819.0,1.0,1,A100,1697548664218,1697548666037,120,15.0,1.0,"[266, 1552]","[1697548664484, 1697548666036]"
2829,2829,216,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633847,120,,,[86],[1697548631982]
2830,2830,574,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635894,120,,,"[193, 1551]","[1697548634048, 1697548635599]"
2831,2831,110,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629598,120,,,"[227, 1972]","[1697548627365, 1697548629337]"
2832,2832,89,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635910,1697548637692,120,,,[24],[1697548635934]
2833,2833,438,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610832,120,,,[375],[1697548609350]
2834,2834,439,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631886,120,,,"[76, 1940]","[1697548629678, 1697548631618]"
2835,2835,419,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666047,1697548667369,120,,,"[51, 1249]","[1697548666098, 1697548667347]"
2836,2836,798,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633849,120,,,[154],[1697548632055]
2837,2837,782,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671228,120,,,"[180, 2149, 471, 85, 64, 64, 80]","[1697548667558, 1697548669707, 1697548670178, 1697548670263, 1697548670327, 1697548670391, 1697548670471]"
2838,2838,797,15,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610850,1697548612817,120,26.0,1.0,"[191, 1776]","[1697548611041, 1697548612817]"
2839,2839,449,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639638,120,,,"[186, 1412]","[1697548637889, 1697548639301]"
2840,2840,192,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612822,1697548615236,120,,,"[59, 657]","[1697548612881, 1697548613538]"
2841,2841,803,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639650,1697548641378,120,,,[134],[1697548639784]
2842,2842,553,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617369,120,,,"[343, 1679]","[1697548615587, 1697548617266]"
2843,2843,1,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653427,1697548657539,120,,,"[30, 2722]","[1697548653457, 1697548656179]"
2844,2844,913,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617386,1697548620355,120,,,"[298, 1603, 110, 84, 82, 81, 80]","[1697548617684, 1697548619287, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619724]"
2845,2845,227,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635904,120,,,"[16, 562, 58]","[1697548633870, 1697548634432, 1697548634490]"
2846,2846,361,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657547,1697548659865,120,,,"[297, 1734]","[1697548657844, 1697548659578]"
2847,2847,341,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622599,120,,,"[118, 1850]","[1697548620480, 1697548622330]"
2848,2848,586,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[340],[1697548636256]
2849,2849,699,20,[],200,llama-7b,128,1,2074.0,1.0,1,A100,1697548622602,1697548624676,120,39.0,1.0,"[9, 2065]","[1697548622611, 1697548624676]"
2850,2850,914,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639638,120,,,"[236, 1362]","[1697548637939, 1697548639301]"
2851,2851,721,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661855,120,,,[92],[1697548659963]
2852,2852,761,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548670476,1697548674837,120,,,"[11, 1103, 2022, 80, 60]","[1697548670487, 1697548671590, 1697548673612, 1697548673692, 1697548673752]"
2853,2853,100,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624682,1697548627101,120,,,"[26, 1066]","[1697548624708, 1697548625774]"
2854,2854,235,28,[],200,llama-7b,128,1,6333.0,1.0,1,A100,1697548641388,1697548647721,120,161.0,12.0,"[382, 1853, 1588, 86, 64, 954, 93, 91, 89, 69, 968, 96]","[1697548641770, 1697548643623, 1697548645211, 1697548645297, 1697548645361, 1697548646315, 1697548646408, 1697548646499, 1697548646588, 1697548646657, 1697548647625, 1697548647721]"
2855,2855,562,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,"[414, 1519]","[1697548600367, 1697548601886]"
2856,2856,146,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664203,120,,,"[81, 804, 229]","[1697548661948, 1697548662752, 1697548662981]"
2857,2857,343,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639648,1697548641377,120,,,[116],[1697548639764]
2858,2858,458,22,[],200,llama-7b,128,1,2209.0,1.0,1,A100,1697548627139,1697548629348,120,11.0,1.0,"[353, 1855]","[1697548627492, 1697548629347]"
2859,2859,702,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642998,120,,,[78],[1697548641462]
2860,2860,593,31,[],200,llama-7b,128,1,4852.0,1.0,1,A100,1697548646660,1697548651512,120,335.0,9.0,"[11, 3267, 97, 86, 86, 74, 74, 75, 991, 91]","[1697548646671, 1697548649938, 1697548650035, 1697548650121, 1697548650207, 1697548650281, 1697548650355, 1697548650430, 1697548651421, 1697548651512]"
2861,2861,131,31,[],200,llama-7b,128,1,1771.0,1.0,1,A100,1697548643006,1697548644777,120,8.0,1.0,"[73, 1698]","[1697548643079, 1697548644777]"
2862,2862,506,36,[],200,llama-7b,128,1,1819.0,1.0,1,A100,1697548664217,1697548666036,120,16.0,1.0,"[42, 1777]","[1697548664259, 1697548666036]"
2863,2863,492,32,[],200,llama-7b,128,1,6734.0,1.0,1,A100,1697548644780,1697548651514,120,47.0,20.0,"[32, 2264, 550, 95, 96, 93, 72, 93, 71, 891, 93, 88, 86, 732, 86, 85, 75, 74, 74, 1000, 84]","[1697548644812, 1697548647076, 1697548647626, 1697548647721, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648146, 1697548649037, 1697548649130, 1697548649218, 1697548649304, 1697548650036, 1697548650122, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651430, 1697548651514]"
2864,2864,189,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677259,120,,,"[104, 1848]","[1697548674948, 1697548676796]"
2865,2865,819,23,[],200,llama-7b,128,1,915.0,1.0,1,A100,1697548629358,1697548630273,120,13.0,1.0,"[92, 823]","[1697548629450, 1697548630273]"
2866,2866,555,44,[],200,llama-7b,128,1,2195.0,1.0,1,A100,1697548677278,1697548679473,120,11.0,1.0,"[205, 1990]","[1697548677483, 1697548679473]"
2867,2867,885,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679477,1697548683205,120,,,"[32, 848, 1614, 91, 85, 83, 81]","[1697548679509, 1697548680357, 1697548681971, 1697548682062, 1697548682147, 1697548682230, 1697548682311]"
2868,2868,21,32,[],200,llama-7b,128,1,2376.0,1.0,1,A100,1697548651517,1697548653893,120,15.0,1.0,"[20, 2356]","[1697548651537, 1697548653893]"
2869,2869,351,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653896,1697548657538,120,,,"[14, 2270]","[1697548653910, 1697548656180]"
2870,2870,245,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548630276,1697548635893,120,,,"[15, 2344, 1250, 57, 547]","[1697548630291, 1697548632635, 1697548633885, 1697548633942, 1697548634489]"
2871,2871,709,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657549,1697548659866,120,,,"[351, 1679]","[1697548657900, 1697548659579]"
2872,2872,137,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661853,120,,,[208],[1697548660080]
2873,2873,496,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664205,120,,,"[176, 1734]","[1697548662043, 1697548663777]"
2874,2874,12,37,[],200,llama-7b,128,1,1818.0,1.0,1,A100,1697548664217,1697548666035,120,11.0,1.0,"[230, 1588]","[1697548664447, 1697548666035]"
2875,2875,314,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683213,1697548686708,120,,,"[29, 1782, 239, 91, 90, 89, 70, 87, 86, 68]","[1697548683242, 1697548685024, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
2876,2876,837,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674839,120,,,"[116, 1722, 538, 79, 60]","[1697548671354, 1697548673076, 1697548673614, 1697548673693, 1697548673753]"
2877,2877,366,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666045,1697548667368,120,,,"[24, 1277]","[1697548666069, 1697548667346]"
2878,2878,53,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613020,1697548615233,120,,,"[354, 1658]","[1697548613374, 1697548615032]"
2879,2879,729,39,[],200,llama-7b,128,1,684.0,1.0,1,A100,1697548667377,1697548668061,120,874.0,2.0,"[87, 597]","[1697548667464, 1697548668061]"
2880,2880,158,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548668062,1697548671226,120,,,"[6, 3134]","[1697548668068, 1697548671202]"
2881,2881,160,21,[],200,llama-7b,128,1,2015.0,1.0,1,A100,1697548629603,1697548631618,120,13.0,1.0,"[65, 1950]","[1697548629668, 1697548631618]"
2882,2882,521,22,[],200,llama-7b,128,1,1012.0,1.0,1,A100,1697548631625,1697548632637,120,18.0,1.0,"[65, 947]","[1697548631690, 1697548632637]"
2883,2883,389,24,[],200,llama-7b,128,1,2232.0,1.0,1,A100,1697548627105,1697548629337,120,8.0,1.0,"[44, 2188]","[1697548627149, 1697548629337]"
2884,2884,530,45,[],200,llama-7b,128,1,2150.0,1.0,1,A100,1697548691097,1697548693247,120,26.0,1.0,"[269, 1881]","[1697548691366, 1697548693247]"
2885,2885,673,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688901,120,,,"[311, 1674]","[1697548687040, 1697548688714]"
2886,2886,517,41,[],200,llama-7b,128,1,3072.0,1.0,1,A100,1697548671246,1697548674318,120,15.0,1.0,"[397, 2675]","[1697548671643, 1697548674318]"
2887,2887,743,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629347,1697548631887,120,,,"[12, 912]","[1697548629359, 1697548630271]"
2888,2888,889,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693252,1697548695715,120,,,"[37, 854]","[1697548693289, 1697548694143]"
2889,2889,851,23,[],200,llama-7b,128,1,1789.0,1.0,1,A100,1697548632642,1697548634431,120,23.0,1.0,"[28, 1761]","[1697548632670, 1697548634431]"
2890,2890,225,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610833,120,,,[183],[1697548609156]
2891,2891,584,15,[],200,llama-7b,128,1,1964.0,1.0,1,A100,1697548610852,1697548612816,120,10.0,1.0,"[295, 1668]","[1697548611147, 1697548612815]"
2892,2892,849,42,[],200,llama-7b,128,1,1151.0,1.0,1,A100,1697548674322,1697548675473,120,10.0,1.0,"[44, 1107]","[1697548674366, 1697548675473]"
2893,2893,276,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548634436,1697548639635,120,,,"[43, 3252, 417]","[1697548634479, 1697548637731, 1697548638148]"
2894,2894,285,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697836,120,,,"[37, 1922]","[1697548695761, 1697548697683]"
2895,2895,274,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548675476,1697548679647,120,,,"[7, 2758]","[1697548675483, 1697548678241]"
2896,2896,633,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683208,120,,,"[124, 1908, 282, 92, 85, 81, 81]","[1697548679781, 1697548681689, 1697548681971, 1697548682063, 1697548682148, 1697548682229, 1697548682310]"
2897,2897,9,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612821,1697548615235,120,,,"[26, 691]","[1697548612847, 1697548613538]"
2898,2898,178,11,[],200,llama-7b,128,1,1948.0,1.0,1,A100,1697548605284,1697548607232,120,11.0,1.0,"[375, 1573]","[1697548605659, 1697548607232]"
2899,2899,94,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688911,1697548691088,120,,,"[340, 1613]","[1697548689251, 1697548690864]"
2900,2900,625,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607238,1697548608968,120,,,"[121, 1412]","[1697548607359, 1697548608771]"
2901,2901,452,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693453,120,,,"[68, 2083]","[1697548691164, 1697548693247]"
2902,2902,54,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548612997,120,,,"[389, 2024, 49]","[1697548609364, 1697548611388, 1697548611437]"
2903,2903,717,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633845,120,,,[50],[1697548631946]
2904,2904,629,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639661,1697548641379,120,,,[408],[1697548640069]
2905,2905,363,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617366,120,,,"[296, 1726]","[1697548615539, 1697548617265]"
2906,2906,60,26,[],200,llama-7b,128,1,12035.0,1.0,1,A100,1697548641386,1697548653421,120,93.0,36.0,"[375, 1863, 1587, 85, 64, 954, 94, 91, 89, 68, 968, 97, 95, 93, 72, 93, 72, 891, 92, 88, 86, 731, 86, 86, 75, 74, 74, 992, 91, 74, 858, 320, 88, 69, 404, 96]","[1697548641761, 1697548643624, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647721, 1697548647816, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650034, 1697548650120, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421, 1697548651512, 1697548651586, 1697548652444, 1697548652764, 1697548652852, 1697548652921, 1697548653325, 1697548653421]"
2907,2907,81,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548654573,1697548657502,120,,,"[8, 1599]","[1697548654581, 1697548656180]"
2908,2908,142,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635895,120,,,"[213, 1532]","[1697548634068, 1697548635600]"
2909,2909,435,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657548,1697548659866,120,,,"[321, 1709]","[1697548657869, 1697548659578]"
2910,2910,791,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661852,120,,,[193],[1697548660065]
2911,2911,693,18,[],200,llama-7b,128,1,2017.0,1.0,1,A100,1697548617379,1697548619396,120,67.0,2.0,"[190, 1716, 111]","[1697548617569, 1697548619285, 1697548619396]"
2912,2912,190,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664211,120,,,"[76, 809, 229]","[1697548661943, 1697548662752, 1697548662981]"
2913,2913,421,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653427,1697548657538,120,,,"[29, 2723]","[1697548653456, 1697548656179]"
2914,2914,122,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619399,1697548622594,120,,,"[12, 1514]","[1697548619411, 1697548620925]"
2915,2915,551,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664219,1697548667371,120,,,"[322, 1496, 430, 84, 65, 85]","[1697548664541, 1697548666037, 1697548666467, 1697548666551, 1697548666616, 1697548666701]"
2916,2916,487,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622602,1697548624958,120,,,[14],[1697548622616]
2917,2917,846,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624968,1697548627100,120,,,"[169, 1884]","[1697548625137, 1697548627021]"
2918,2918,750,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657547,1697548659865,120,,,"[298, 1733]","[1697548657845, 1697548659578]"
2919,2919,245,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627137,1697548629598,120,,,"[218, 1982]","[1697548627355, 1697548629337]"
2920,2920,184,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661854,120,,,[67],[1697548659938]
2921,2921,702,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622336,1697548624961,120,,,"[62, 1059]","[1697548622398, 1697548623457]"
2922,2922,309,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602123,1697548605266,120,,,"[145, 1585, 260, 80, 75]","[1697548602268, 1697548603853, 1697548604113, 1697548604193, 1697548604268]"
2923,2923,519,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617367,120,,,"[298, 1724]","[1697548615541, 1697548617265]"
2924,2924,768,43,[],200,llama-7b,128,1,2389.0,1.0,1,A100,1697548683215,1697548685604,120,47.0,6.0,"[137, 1674, 236, 93, 88, 89, 72]","[1697548683352, 1697548685026, 1697548685262, 1697548685355, 1697548685443, 1697548685532, 1697548685604]"
2925,2925,494,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664204,120,,,"[121, 1789]","[1697548661988, 1697548663777]"
2926,2926,690,8,[],200,llama-7b,128,1,2351.0,1.0,1,A100,1697548597367,1697548599718,120,39.0,1.0,"[378, 1972]","[1697548597745, 1697548599717]"
2927,2927,538,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664204,120,,,"[115, 1794]","[1697548661983, 1697548663777]"
2928,2928,824,34,[],200,llama-7b,128,1,2398.0,1.0,1,A100,1697548664219,1697548666617,120,58.0,4.0,"[347, 1472, 429, 85, 65]","[1697548664566, 1697548666038, 1697548666467, 1697548666552, 1697548666617]"
2929,2929,670,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608964,120,,,"[177, 1772, 507, 79, 79, 78]","[1697548605460, 1697548607232, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
2930,2930,101,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610824,120,,,[27],[1697548609000]
2931,2931,458,13,[],200,llama-7b,128,1,1968.0,1.0,1,A100,1697548610847,1697548612815,120,11.0,1.0,"[24, 1944]","[1697548610871, 1697548612815]"
2932,2932,897,31,[],200,llama-7b,128,1,1820.0,1.0,1,A100,1697548664217,1697548666037,120,9.0,1.0,"[146, 1674]","[1697548664363, 1697548666037]"
2933,2933,816,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612820,1697548615234,120,,,"[17, 701]","[1697548612837, 1697548613538]"
2934,2934,326,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666047,1697548667369,120,,,"[31, 1268]","[1697548666078, 1697548667346]"
2935,2935,42,18,[],200,llama-7b,128,1,1901.0,1.0,1,A100,1697548617386,1697548619287,120,10.0,1.0,"[269, 1631]","[1697548617655, 1697548619286]"
2936,2936,217,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617365,120,,,"[208, 1815]","[1697548615451, 1697548617266]"
2937,2937,653,33,[],200,llama-7b,128,1,3095.0,1.0,1,A100,1697548667377,1697548670472,120,96.0,6.0,"[410, 1920, 471, 84, 65, 64, 81]","[1697548667787, 1697548669707, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670472]"
2938,2938,377,39,[],200,llama-7b,128,1,1305.0,1.0,1,A100,1697548666041,1697548667346,120,13.0,1.0,"[17, 1287]","[1697548666058, 1697548667345]"
2939,2939,914,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667379,1697548671231,120,,,"[397, 1931, 471, 84, 65, 64, 80]","[1697548667776, 1697548669707, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
2940,2940,120,9,[],200,llama-7b,128,1,818.0,1.0,1,A100,1697548599723,1697548600541,120,17.0,1.0,"[88, 730]","[1697548599811, 1697548600541]"
2941,2941,401,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619291,1697548620358,120,,,"[58, 973]","[1697548619349, 1697548620322]"
2942,2942,456,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548600543,1697548605271,120,,,"[11, 2120, 1439, 78, 76]","[1697548600554, 1697548602674, 1697548604113, 1697548604191, 1697548604267]"
2943,2943,84,34,[],200,llama-7b,128,1,1115.0,1.0,1,A100,1697548670476,1697548671591,120,26.0,1.0,"[18, 1097]","[1697548670494, 1697548671591]"
2944,2944,574,16,[],200,llama-7b,128,1,2019.0,1.0,1,A100,1697548617378,1697548619397,120,364.0,2.0,"[72, 1835, 112]","[1697548617450, 1697548619285, 1697548619397]"
2945,2945,441,35,[],200,llama-7b,128,1,2725.0,1.0,1,A100,1697548671594,1697548674319,120,6.0,1.0,"[73, 2652]","[1697548671667, 1697548674319]"
2946,2946,573,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589567,1697548592060,120,,,[20],[1697548589587]
2947,2947,799,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674322,1697548677257,120,,,"[34, 1116]","[1697548674356, 1697548675472]"
2948,2948,343,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674840,120,,,"[199, 1632, 538, 80, 61]","[1697548671442, 1697548673074, 1697548673612, 1697548673692, 1697548673753]"
2949,2949,229,37,[],200,llama-7b,128,1,2201.0,1.0,1,A100,1697548677273,1697548679474,120,15.0,1.0,"[107, 2094]","[1697548677380, 1697548679474]"
2950,2950,213,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597361,120,,,"[310, 1431, 249, 73, 58]","[1697548594666, 1697548596097, 1697548596346, 1697548596419, 1697548596477]"
2951,2951,674,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679479,1697548683206,120,,,"[44, 834, 1615, 91, 84, 83, 81]","[1697548679523, 1697548680357, 1697548681972, 1697548682063, 1697548682147, 1697548682230, 1697548682311]"
2952,2952,3,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619400,1697548622594,120,,,"[6, 1519]","[1697548619406, 1697548620925]"
2953,2953,673,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674846,1697548677258,120,,,"[228, 1722]","[1697548675074, 1697548676796]"
2954,2954,98,40,[],200,llama-7b,128,1,2196.0,1.0,1,A100,1697548677278,1697548679474,120,14.0,1.0,"[275, 1921]","[1697548677553, 1697548679474]"
2955,2955,15,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592063,120,,,"[386, 1636]","[1697548589954, 1697548591590]"
2956,2956,378,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,[303],[1697548592376]
2957,2957,362,18,[],200,llama-7b,128,1,2074.0,1.0,1,A100,1697548622604,1697548624678,120,14.0,1.0,"[248, 1825]","[1697548622852, 1697548624677]"
2958,2958,739,6,[],200,llama-7b,128,1,1741.0,1.0,1,A100,1697548594356,1697548596097,120,216.0,1.0,"[230, 1510]","[1697548594586, 1697548596096]"
2959,2959,813,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605284,1697548608966,120,,,"[282, 1664, 510, 77, 80, 77]","[1697548605566, 1697548607230, 1697548607740, 1697548607817, 1697548607897, 1697548607974]"
2960,2960,257,7,[],200,llama-7b,128,1,1231.0,1.0,1,A100,1697548596103,1697548597334,120,14.0,1.0,"[84, 1147]","[1697548596187, 1697548597334]"
2961,2961,807,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624682,1697548627101,120,,,"[16, 1077]","[1697548624698, 1697548625775]"
2962,2962,610,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597337,1697548599939,120,,,"[41, 566, 36]","[1697548597378, 1697548597944, 1697548597980]"
2963,2963,243,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610832,120,,,[277],[1697548609251]
2964,2964,452,41,[],200,llama-7b,128,1,2669.0,1.0,1,A100,1697548679479,1697548682148,120,216.0,4.0,"[45, 833, 1615, 91, 85]","[1697548679524, 1697548680357, 1697548681972, 1697548682063, 1697548682148]"
2965,2965,35,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[288, 1646]","[1697548600240, 1697548601886]"
2966,2966,597,13,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610849,1697548612816,120,39.0,1.0,"[89, 1877]","[1697548610938, 1697548612815]"
2967,2967,237,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629598,120,,,"[232, 1967]","[1697548627370, 1697548629337]"
2968,2968,801,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637694,120,,,[255],[1697548636170]
2969,2969,394,10,[],200,llama-7b,128,1,1724.0,1.0,1,A100,1697548602128,1697548603852,120,11.0,1.0,"[243, 1481]","[1697548602371, 1697548603852]"
2970,2970,232,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637701,1697548639645,120,,,"[24, 370, 54]","[1697548637725, 1697548638095, 1697548638149]"
2971,2971,106,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683212,1697548686707,120,,,"[10, 1802, 239, 91, 90, 89, 70, 87, 86, 68]","[1697548683222, 1697548685024, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
2972,2972,79,14,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548613006,1697548615031,120,12.0,1.0,"[40, 1985]","[1697548613046, 1697548615031]"
2973,2973,811,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682150,1697548686706,120,,,"[6, 1888, 1218, 91, 90, 89, 70, 87, 86, 68]","[1697548682156, 1697548684044, 1697548685262, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
2974,2974,593,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639648,1697548641376,120,,,[102],[1697548639750]
2975,2975,920,29,[],200,llama-7b,128,1,2240.0,1.0,1,A100,1697548641384,1697548643624,120,96.0,4.0,"[27, 326, 42, 1261, 584]","[1697548641411, 1697548641737, 1697548641779, 1697548643040, 1697548643624]"
2976,2976,348,30,[],200,llama-7b,128,1,7886.0,1.0,1,A100,1697548643627,1697548651513,120,91.0,20.0,"[6, 3442, 550, 96, 96, 93, 71, 94, 71, 891, 92, 88, 87, 731, 87, 85, 75, 73, 75, 991, 92]","[1697548643633, 1697548647075, 1697548647625, 1697548647721, 1697548647817, 1697548647910, 1697548647981, 1697548648075, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649304, 1697548650035, 1697548650122, 1697548650207, 1697548650282, 1697548650355, 1697548650430, 1697548651421, 1697548651513]"
2977,2977,439,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617365,120,,,"[30, 808]","[1697548615065, 1697548615873]"
2978,2978,701,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548651516,1697548655268,120,,,"[11, 2366, 354, 96, 84, 63, 80]","[1697548651527, 1697548653893, 1697548654247, 1697548654343, 1697548654427, 1697548654490, 1697548654570]"
2979,2979,707,22,[],200,llama-7b,128,1,2360.0,1.0,1,A100,1697548630276,1697548632636,120,8.0,1.0,"[30, 2330]","[1697548630306, 1697548632636]"
2980,2980,62,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622336,1697548624970,120,,,"[67, 1055]","[1697548622403, 1697548623458]"
2981,2981,462,40,[],200,llama-7b,128,1,1989.0,1.0,1,A100,1697548686723,1697548688712,120,52.0,1.0,"[215, 1774]","[1697548686938, 1697548688712]"
2982,2982,136,32,[],200,llama-7b,128,1,2145.0,1.0,1,A100,1697548655281,1697548657426,120,31.0,1.0,"[281, 1864]","[1697548655562, 1697548657426]"
2983,2983,799,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617379,1697548620358,120,,,"[178, 1728, 111, 84, 82, 81, 80]","[1697548617557, 1697548619285, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
2984,2984,819,41,[],200,llama-7b,128,1,886.0,1.0,1,A100,1697548688716,1697548689602,120,13.0,1.0,"[30, 856]","[1697548688746, 1697548689602]"
2985,2985,547,3,[],200,llama-7b,128,1,2027.0,1.0,1,A100,1697548583931,1697548585958,120,12.0,1.0,"[128, 1899]","[1697548584059, 1697548585958]"
2986,2986,878,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585961,1697548589556,120,,,"[9, 1244, 1176, 78, 78, 75]","[1697548585970, 1697548587214, 1697548588390, 1697548588468, 1697548588546, 1697548588621]"
2987,2987,392,18,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624976,1697548627023,120,20.0,1.0,"[377, 1670]","[1697548625353, 1697548627023]"
2988,2988,493,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657430,1697548659865,120,,,"[31, 826]","[1697548657461, 1697548658287]"
2989,2989,750,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627025,1697548629598,120,,,"[50, 863]","[1697548627075, 1697548627938]"
2990,2990,221,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548689609,1697548693449,120,,,"[36, 2207]","[1697548689645, 1697548691852]"
2991,2991,180,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629603,1697548631886,120,,,"[70, 1945]","[1697548629673, 1697548631618]"
2992,2992,578,43,[],200,llama-7b,128,1,1974.0,1.0,1,A100,1697548693459,1697548695433,120,31.0,1.0,"[115, 1859]","[1697548693574, 1697548695433]"
2993,2993,22,12,[],200,llama-7b,128,1,1946.0,1.0,1,A100,1697548605284,1697548607230,120,16.0,1.0,"[254, 1691]","[1697548605538, 1697548607229]"
2994,2994,826,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661853,120,,,[46],[1697548659917]
2995,2995,6,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695437,1697548697834,120,,,[25],[1697548695462]
2996,2996,203,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622597,120,,,"[317, 1644]","[1697548620685, 1697548622329]"
2997,2997,256,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664207,120,,,"[294, 1617]","[1697548662162, 1697548663779]"
2998,2998,468,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607236,1697548608965,120,,,"[17, 1517]","[1697548607253, 1697548608770]"
2999,2999,538,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633844,120,,,[36],[1697548631931]
3000,3000,473,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674841,120,,,"[192, 1642, 537, 78, 61]","[1697548671435, 1697548673077, 1697548673614, 1697548673692, 1697548673753]"
3001,3001,767,5,[],200,llama-7b,128,1,1256.0,1.0,1,A100,1697548591597,1697548592853,120,11.0,1.0,"[84, 1171]","[1697548591681, 1697548592852]"
3002,3002,832,36,[],200,llama-7b,128,1,1949.0,1.0,1,A100,1697548674848,1697548676797,120,15.0,1.0,"[296, 1653]","[1697548675144, 1697548676797]"
3003,3003,257,37,[],200,llama-7b,128,1,1442.0,1.0,1,A100,1697548676800,1697548678242,120,14.0,1.0,"[31, 1411]","[1697548676831, 1697548678242]"
3004,3004,610,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667368,120,,,"[121, 1699, 430, 82, 68, 84]","[1697548664338, 1697548666037, 1697548666467, 1697548666549, 1697548666617, 1697548666701]"
3005,3005,561,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624953,120,,,"[181, 1892]","[1697548622785, 1697548624677]"
3006,3006,915,19,[],200,llama-7b,128,1,2054.0,1.0,1,A100,1697548624968,1697548627022,120,182.0,1.0,"[302, 1752]","[1697548625270, 1697548627022]"
3007,3007,620,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548678246,1697548683205,120,,,"[40, 2070, 1615, 92, 85, 81, 81]","[1697548678286, 1697548680356, 1697548681971, 1697548682063, 1697548682148, 1697548682229, 1697548682310]"
3008,3008,340,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627026,1697548629598,120,,,"[76, 2235]","[1697548627102, 1697548629337]"
3009,3009,191,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592856,1697548597360,120,,,"[34, 2036, 1420, 72, 58]","[1697548592890, 1697548594926, 1697548596346, 1697548596418, 1697548596476]"
3010,3010,35,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667377,1697548671226,120,,,"[92, 556, 36, 2117, 84, 65, 64, 80]","[1697548667469, 1697548668025, 1697548668061, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
3011,3011,547,7,[],200,llama-7b,128,1,2349.0,1.0,1,A100,1697548597368,1697548599717,120,12.0,1.0,"[315, 2034]","[1697548597683, 1697548599717]"
3012,3012,905,8,[],200,llama-7b,128,1,820.0,1.0,1,A100,1697548599721,1697548600541,120,11.0,1.0,"[70, 749]","[1697548599791, 1697548600540]"
3013,3013,308,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548600545,1697548605272,120,,,"[34, 2095, 1439, 78, 76]","[1697548600579, 1697548602674, 1697548604113, 1697548604191, 1697548604267]"
3014,3014,667,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608964,120,,,"[159, 1790, 507, 79, 79, 78]","[1697548605442, 1697548607232, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
3015,3015,891,22,[],200,llama-7b,128,1,637.0,1.0,1,A100,1697548633853,1697548634490,120,52.0,2.0,"[35, 543, 59]","[1697548633888, 1697548634431, 1697548634490]"
3016,3016,413,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548634493,1697548639635,120,,,"[9, 2007, 1222, 417]","[1697548634502, 1697548636509, 1697548637731, 1697548638148]"
3017,3017,770,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641379,120,,,[15],[1697548639661]
3018,3018,202,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641388,1697548655265,120,,,"[387, 1797, 51, 1588, 86, 64, 954, 93, 91, 89, 69, 968, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86, 731, 87, 85, 75, 74, 74, 992, 91, 74, 858, 320, 88, 69, 404, 96, 89, 84, 651, 96, 85, 62, 80]","[1697548641775, 1697548643572, 1697548643623, 1697548645211, 1697548645297, 1697548645361, 1697548646315, 1697548646408, 1697548646499, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421, 1697548651512, 1697548651586, 1697548652444, 1697548652764, 1697548652852, 1697548652921, 1697548653325, 1697548653421, 1697548653510, 1697548653594, 1697548654245, 1697548654341, 1697548654426, 1697548654488, 1697548654568]"
3019,3019,99,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610824,120,,,[272],[1697548609246]
3020,3020,655,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639649,120,,,"[270, 1326]","[1697548637973, 1697548639299]"
3021,3021,393,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674838,120,,,"[342, 1497, 537, 79, 61]","[1697548671580, 1697548673077, 1697548673614, 1697548673693, 1697548673754]"
3022,3022,454,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610847,1697548612996,120,,,"[210, 1761]","[1697548611057, 1697548612818]"
3023,3023,333,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617377,1697548620355,120,,,"[15, 1892, 113, 83, 83, 80, 81]","[1697548617392, 1697548619284, 1697548619397, 1697548619480, 1697548619563, 1697548619643, 1697548619724]"
3024,3024,813,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615234,120,,,"[154, 1873]","[1697548613160, 1697548615033]"
3025,3025,842,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677256,120,,,"[137, 1815]","[1697548674982, 1697548676797]"
3026,3026,324,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617367,120,,,"[304, 1718]","[1697548615547, 1697548617265]"
3027,3027,18,21,[],200,llama-7b,128,1,2010.0,1.0,1,A100,1697548629608,1697548631618,120,15.0,1.0,"[269, 1741]","[1697548629877, 1697548631618]"
3028,3028,272,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677268,1697548679650,120,,,"[37, 2168]","[1697548677305, 1697548679473]"
3029,3029,878,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613014,1697548615233,120,,,"[331, 1687]","[1697548613345, 1697548615032]"
3030,3030,87,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639659,1697548641376,120,,,[334],[1697548639993]
3031,3031,373,22,[],200,llama-7b,128,1,1012.0,1.0,1,A100,1697548631624,1697548632636,120,15.0,1.0,"[11, 1001]","[1697548631635, 1697548632636]"
3032,3032,633,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683207,120,,,"[321, 1709, 285, 91, 85, 82, 80]","[1697548679979, 1697548681688, 1697548681973, 1697548682064, 1697548682149, 1697548682231, 1697548682311]"
3033,3033,226,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594930,1697548597357,120,,,"[17, 2386]","[1697548594947, 1697548597333]"
3034,3034,448,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642998,120,,,[269],[1697548641654]
3035,3035,308,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617364,120,,,"[117, 1906]","[1697548615359, 1697548617265]"
3036,3036,731,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548632639,1697548635903,120,,,"[6, 1786, 58]","[1697548632645, 1697548634431, 1697548634489]"
3037,3037,62,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683213,1697548686708,120,,,"[49, 1763, 239, 90, 90, 89, 70, 87, 86, 68]","[1697548683262, 1697548685025, 1697548685264, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
3038,3038,250,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637696,120,,,[127],[1697548636042]
3039,3039,666,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617377,1697548620356,120,,,"[23, 1885, 112, 83, 83, 80, 81]","[1697548617400, 1697548619285, 1697548619397, 1697548619480, 1697548619563, 1697548619643, 1697548619724]"
3040,3040,587,9,[],200,llama-7b,128,1,2351.0,1.0,1,A100,1697548597366,1697548599717,120,13.0,1.0,"[322, 2029]","[1697548597688, 1697548599717]"
3041,3041,895,28,[],200,llama-7b,128,1,1769.0,1.0,1,A100,1697548643008,1697548644777,120,15.0,1.0,"[172, 1597]","[1697548643180, 1697548644777]"
3042,3042,320,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548644780,1697548655267,120,,,"[29, 2817, 95, 96, 94, 71, 93, 71, 891, 93, 88, 86, 731, 87, 85, 75, 74, 74, 1000, 84, 73, 858, 319, 89, 69, 403, 97, 89, 84, 651, 96, 84, 63, 80]","[1697548644809, 1697548647626, 1697548647721, 1697548647817, 1697548647911, 1697548647982, 1697548648075, 1697548648146, 1697548649037, 1697548649130, 1697548649218, 1697548649304, 1697548650035, 1697548650122, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651430, 1697548651514, 1697548651587, 1697548652445, 1697548652764, 1697548652853, 1697548652922, 1697548653325, 1697548653422, 1697548653511, 1697548653595, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
3043,3043,913,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599721,1697548602107,120,,,"[60, 759]","[1697548599781, 1697548600540]"
3044,3044,389,43,[],200,llama-7b,128,1,1984.0,1.0,1,A100,1697548686729,1697548688713,120,8.0,1.0,"[108, 1876]","[1697548686837, 1697548688713]"
3045,3045,558,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655283,1697548657535,120,,,"[278, 1865]","[1697548655561, 1697548657426]"
3046,3046,381,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617369,120,,,"[395, 1628]","[1697548615639, 1697548617267]"
3047,3047,341,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602128,1697548605265,120,,,"[117, 1608, 260, 80, 74]","[1697548602245, 1697548603853, 1697548604113, 1697548604193, 1697548604267]"
3048,3048,743,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688716,1697548691086,120,,,"[25, 861]","[1697548688741, 1697548689602]"
3049,3049,613,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639637,120,,,"[171, 1427]","[1697548637874, 1697548639301]"
3050,3050,886,27,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548657545,1697548659578,120,17.0,1.0,"[111, 1922]","[1697548657656, 1697548659578]"
3051,3051,311,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664205,120,,,"[38, 1081, 1195, 57, 1027]","[1697548659620, 1697548660701, 1697548661896, 1697548661953, 1697548662980]"
3052,3052,671,29,[],200,llama-7b,128,1,1820.0,1.0,1,A100,1697548664219,1697548666039,120,12.0,1.0,"[362, 1458]","[1697548664581, 1697548666039]"
3053,3053,103,30,[],200,llama-7b,128,1,1298.0,1.0,1,A100,1697548666049,1697548667347,120,15.0,1.0,"[100, 1198]","[1697548666149, 1697548667347]"
3054,3054,136,7,[],200,llama-7b,128,1,763.0,1.0,1,A100,1697548594164,1697548594927,120,31.0,1.0,"[47, 715]","[1697548594211, 1697548594926]"
3055,3055,462,31,[],200,llama-7b,128,1,672.0,1.0,1,A100,1697548667353,1697548668025,120,52.0,1.0,"[42, 630]","[1697548667395, 1697548668025]"
3056,3056,587,8,[],200,llama-7b,128,1,2404.0,1.0,1,A100,1697548594930,1697548597334,120,13.0,1.0,"[10, 2394]","[1697548594940, 1697548597334]"
3057,3057,171,45,[],200,llama-7b,128,1,2152.0,1.0,1,A100,1697548691096,1697548693248,120,6.0,1.0,"[301, 1851]","[1697548691397, 1697548693248]"
3058,3058,718,25,[],200,llama-7b,128,1,906.0,1.0,1,A100,1697548635603,1697548636509,120,13.0,1.0,"[16, 890]","[1697548635619, 1697548636509]"
3059,3059,18,9,[],200,llama-7b,128,1,606.0,1.0,1,A100,1697548597338,1697548597944,120,15.0,1.0,"[30, 576]","[1697548597368, 1697548597944]"
3060,3060,794,32,[],200,llama-7b,128,1,3169.0,1.0,1,A100,1697548668033,1697548671202,120,11.0,1.0,"[27, 3142]","[1697548668060, 1697548671202]"
3061,3061,41,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641377,120,,,[390],[1697548640045]
3062,3062,118,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548636511,1697548639638,120,,,"[10, 1573, 54]","[1697548636521, 1697548638094, 1697548638148]"
3063,3063,401,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548643000,120,,,[294],[1697548641679]
3064,3064,855,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643000,120,,,[293],[1697548641679]
3065,3065,219,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671205,1697548674837,120,,,"[7, 379, 2021, 80, 60]","[1697548671212, 1697548671591, 1697548673612, 1697548673692, 1697548673752]"
3066,3066,574,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633844,120,,,[41],[1697548631936]
3067,3067,755,28,[],200,llama-7b,128,1,7344.0,1.0,1,A100,1697548643012,1697548650356,120,286.0,25.0,"[239, 1527, 434, 85, 64, 953, 94, 91, 89, 68, 968, 96, 96, 94, 71, 93, 72, 891, 92, 88, 89, 729, 86, 85, 75, 74]","[1697548643251, 1697548644778, 1697548645212, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649306, 1697548650035, 1697548650121, 1697548650206, 1697548650281, 1697548650355]"
3068,3068,674,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657536,120,,,"[296, 1850]","[1697548655577, 1697548657427]"
3069,3069,534,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693252,1697548695715,120,,,"[42, 850]","[1697548693294, 1697548694144]"
3070,3070,4,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635903,120,,,"[32, 545, 59]","[1697548633886, 1697548634431, 1697548634490]"
3071,3071,288,28,[],200,llama-7b,128,1,6300.0,1.0,1,A100,1697548643004,1697548649304,120,93.0,20.0,"[45, 1728, 433, 86, 64, 955, 94, 91, 89, 66, 970, 97, 94, 94, 72, 93, 71, 892, 92, 88, 86]","[1697548643049, 1697548644777, 1697548645210, 1697548645296, 1697548645360, 1697548646315, 1697548646409, 1697548646500, 1697548646589, 1697548646655, 1697548647625, 1697548647722, 1697548647816, 1697548647910, 1697548647982, 1697548648075, 1697548648146, 1697548649038, 1697548649130, 1697548649218, 1697548649304]"
3072,3072,104,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657545,1697548659864,120,,,"[193, 1841]","[1697548657738, 1697548659579]"
3073,3073,475,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639652,1697548641382,120,,,[242],[1697548639894]
3074,3074,895,47,[],200,llama-7b,128,1,2414.0,1.0,1,A100,1697548695726,1697548698140,120,15.0,1.0,"[351, 2062]","[1697548696077, 1697548698139]"
3075,3075,462,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661856,120,,,[111],[1697548659983]
3076,3076,834,28,[],200,llama-7b,128,1,7830.0,1.0,1,A100,1697548641388,1697548649218,120,85.0,20.0,"[377, 1807, 52, 1587, 85, 65, 953, 94, 91, 89, 69, 968, 96, 95, 93, 72, 93, 72, 891, 92, 88]","[1697548641765, 1697548643572, 1697548643624, 1697548645211, 1697548645296, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217]"
3077,3077,734,29,[],200,llama-7b,128,1,4016.0,1.0,1,A100,1697548649309,1697548653325,120,100.0,6.0,"[7, 2862, 267, 320, 87, 70, 403]","[1697548649316, 1697548652178, 1697548652445, 1697548652765, 1697548652852, 1697548652922, 1697548653325]"
3078,3078,578,34,[],200,llama-7b,128,1,1953.0,1.0,1,A100,1697548674844,1697548676797,120,31.0,1.0,"[118, 1835]","[1697548674962, 1697548676797]"
3079,3079,796,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664207,120,,,"[312, 1597]","[1697548662180, 1697548663777]"
3080,3080,3,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548676802,1697548679648,120,,,"[29, 1410]","[1697548676831, 1697548678241]"
3081,3081,361,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683208,120,,,"[122, 1910, 283, 91, 85, 83, 79]","[1697548679779, 1697548681689, 1697548681972, 1697548682063, 1697548682148, 1697548682231, 1697548682310]"
3082,3082,892,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583931,1697548586198,120,,,"[113, 1914]","[1697548584044, 1697548585958]"
3083,3083,695,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683212,1697548686708,120,,,"[20, 1792, 239, 91, 90, 89, 70, 87, 86, 68]","[1697548683232, 1697548685024, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
3084,3084,227,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667369,120,,,"[141, 1679, 431, 82, 67, 85]","[1697548664358, 1697548666037, 1697548666468, 1697548666550, 1697548666617, 1697548666702]"
3085,3085,585,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667377,1697548671227,120,,,"[165, 2165, 471, 85, 64, 64, 80]","[1697548667542, 1697548669707, 1697548670178, 1697548670263, 1697548670327, 1697548670391, 1697548670471]"
3086,3086,610,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649221,1697548655269,120,,,"[15, 2941, 267, 320, 88, 70, 402, 97, 89, 84, 654, 97, 82, 63, 80]","[1697548649236, 1697548652177, 1697548652444, 1697548652764, 1697548652852, 1697548652922, 1697548653324, 1697548653421, 1697548653510, 1697548653594, 1697548654248, 1697548654345, 1697548654427, 1697548654490, 1697548654570]"
3087,3087,127,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688900,120,,,"[263, 1721]","[1697548686992, 1697548688713]"
3088,3088,486,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691090,120,,,"[232, 1704]","[1697548689141, 1697548690845]"
3089,3089,320,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548586205,1697548589560,120,,,"[75, 2109, 79, 79, 75]","[1697548586280, 1697548588389, 1697548588468, 1697548588547, 1697548588622]"
3090,3090,159,30,[],200,llama-7b,128,1,2849.0,1.0,1,A100,1697548653330,1697548656179,120,31.0,1.0,"[11, 2838]","[1697548653341, 1697548656179]"
3091,3091,11,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674836,120,,,"[230, 1602, 538, 79, 61]","[1697548671473, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
3092,3092,844,40,[],200,llama-7b,128,1,2153.0,1.0,1,A100,1697548691096,1697548693249,120,10.0,1.0,"[172, 1980]","[1697548691268, 1697548693248]"
3093,3093,264,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693252,1697548695717,120,,,"[62, 830]","[1697548693314, 1697548694144]"
3094,3094,517,31,[],200,llama-7b,128,1,2104.0,1.0,1,A100,1697548656184,1697548658288,120,15.0,1.0,"[36, 2068]","[1697548656220, 1697548658288]"
3095,3095,365,37,[],200,llama-7b,128,1,1953.0,1.0,1,A100,1697548674843,1697548676796,120,23.0,1.0,"[26, 1927]","[1697548674869, 1697548676796]"
3096,3096,873,32,[],200,llama-7b,128,1,2408.0,1.0,1,A100,1697548658292,1697548660700,120,6.0,1.0,"[29, 2379]","[1697548658321, 1697548660700]"
3097,3097,711,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697834,120,,,"[150, 1808]","[1697548695874, 1697548697682]"
3098,3098,304,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548660703,1697548664208,120,,,"[7, 2042, 227]","[1697548660710, 1697548662752, 1697548662979]"
3099,3099,130,28,[],200,llama-7b,128,1,2146.0,1.0,1,A100,1697548655281,1697548657427,120,14.0,1.0,"[301, 1845]","[1697548655582, 1697548657427]"
3100,3100,484,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657431,1697548659867,120,,,[64],[1697548657495]
3101,3101,633,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667371,120,,,"[283, 1536, 429, 85, 65, 85]","[1697548664501, 1697548666037, 1697548666466, 1697548666551, 1697548666616, 1697548666701]"
3102,3102,702,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649310,1697548655271,120,,,"[49, 2819, 268, 319, 89, 69, 403, 96, 89, 84, 651, 96, 84, 63, 80]","[1697548649359, 1697548652178, 1697548652446, 1697548652765, 1697548652854, 1697548652923, 1697548653326, 1697548653422, 1697548653511, 1697548653595, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
3103,3103,67,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667381,1697548671225,120,,,"[485, 1843, 470, 84, 64, 64, 81]","[1697548667866, 1697548669709, 1697548670179, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
3104,3104,674,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592062,120,,,"[111, 1905]","[1697548589685, 1697548591590]"
3105,3105,105,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,"[301, 1786]","[1697548592374, 1697548594160]"
3106,3106,601,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597357,120,,,"[14, 1731, 251, 73, 58]","[1697548594364, 1697548596095, 1697548596346, 1697548596419, 1697548596477]"
3107,3107,831,37,[],200,llama-7b,128,1,1299.0,1.0,1,A100,1697548666047,1697548667346,120,11.0,1.0,"[66, 1233]","[1697548666113, 1697548667346]"
3108,3108,32,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599943,120,,,"[186, 2165]","[1697548597552, 1697548599717]"
3109,3109,847,13,[],200,llama-7b,128,1,2024.0,1.0,1,A100,1697548613007,1697548615031,120,10.0,1.0,"[54, 1970]","[1697548613061, 1697548615031]"
3110,3110,874,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548587221,1697548589558,120,,,"[38, 2121]","[1697548587259, 1697548589380]"
3111,3111,263,38,[],200,llama-7b,128,1,675.0,1.0,1,A100,1697548667350,1697548668025,120,15.0,1.0,"[24, 651]","[1697548667374, 1697548668025]"
3112,3112,275,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617365,120,,,"[30, 808]","[1697548615065, 1697548615873]"
3113,3113,916,10,[],200,llama-7b,128,1,1724.0,1.0,1,A100,1697548602129,1697548603853,120,8.0,1.0,"[312, 1412]","[1697548602441, 1697548603853]"
3114,3114,303,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592061,120,,,"[301, 1720]","[1697548589869, 1697548591589]"
3115,3115,574,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635908,1697548637692,120,,,[29],[1697548635937]
3116,3116,129,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655292,1697548659864,120,,,"[348, 2647]","[1697548655640, 1697548658287]"
3117,3117,317,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603857,1697548605266,120,,,"[50, 1163]","[1697548603907, 1697548605070]"
3118,3118,393,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599953,1697548602108,120,,,"[430, 1504]","[1697548600383, 1697548601887]"
3119,3119,4,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639638,120,,,"[176, 1422]","[1697548637879, 1697548639301]"
3120,3120,458,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661856,120,,,[371],[1697548660248]
3121,3121,661,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594347,120,,,"[102, 1986]","[1697548592174, 1697548594160]"
3122,3122,817,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664210,120,,,"[22, 863, 228]","[1697548661889, 1697548662752, 1697548662980]"
3123,3123,634,15,[],200,llama-7b,128,1,1908.0,1.0,1,A100,1697548617378,1697548619286,120,13.0,1.0,"[164, 1744]","[1697548617542, 1697548619286]"
3124,3124,362,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639661,1697548641379,120,,,[413],[1697548640074]
3125,3125,56,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619290,1697548620358,120,,,"[30, 1003]","[1697548619320, 1697548620323]"
3126,3126,754,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605263,120,,,"[328, 1658, 78, 76]","[1697548602457, 1697548604115, 1697548604193, 1697548604269]"
3127,3127,245,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667372,120,,,"[360, 1461, 428, 85, 65, 84]","[1697548664578, 1697548666039, 1697548666467, 1697548666552, 1697548666617, 1697548666701]"
3128,3128,719,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643000,120,,,[169],[1697548641555]
3129,3129,156,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605279,1697548608962,120,,,"[364, 1588, 508, 79, 79, 78]","[1697548605643, 1697548607231, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
3130,3130,150,29,[],200,llama-7b,128,1,3303.0,1.0,1,A100,1697548643012,1697548646315,120,216.0,2.0,"[259, 3044]","[1697548643271, 1697548646315]"
3131,3131,598,30,[],200,llama-7b,128,1,5104.0,1.0,1,A100,1697548646317,1697548651421,120,345.0,12.0,"[7, 2114, 600, 92, 88, 87, 730, 86, 85, 75, 74, 75, 991]","[1697548646324, 1697548648438, 1697548649038, 1697548649130, 1697548649218, 1697548649305, 1697548650035, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650430, 1697548651421]"
3132,3132,242,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615231,120,,,"[253, 1771]","[1697548613259, 1697548615030]"
3133,3133,90,7,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548594350,1697548596097,120,19.0,1.0,"[93, 1654]","[1697548594443, 1697548596097]"
3134,3134,513,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610833,120,,,[178],[1697548609151]
3135,3135,448,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596100,1697548597357,120,,,"[22, 1212]","[1697548596122, 1697548597334]"
3136,3136,602,14,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548615242,1697548617265,120,15.0,1.0,"[37, 1986]","[1697548615279, 1697548617265]"
3137,3137,526,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637696,120,,,[326],[1697548636242]
3138,3138,778,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597368,1697548599942,120,,,"[412, 1937]","[1697548597780, 1697548599717]"
3139,3139,886,27,[],200,llama-7b,128,1,1597.0,1.0,1,A100,1697548637703,1697548639300,120,17.0,1.0,"[280, 1317]","[1697548637983, 1697548639300]"
3140,3140,30,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617269,1697548620354,120,,,"[6, 680, 1441, 84, 82, 80, 81]","[1697548617275, 1697548617955, 1697548619396, 1697548619480, 1697548619562, 1697548619642, 1697548619723]"
3141,3141,207,10,[],200,llama-7b,128,1,1933.0,1.0,1,A100,1697548599952,1697548601885,120,10.0,1.0,"[343, 1590]","[1697548600295, 1697548601885]"
3142,3142,864,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548612999,120,,,"[330, 1634]","[1697548611182, 1697548612816]"
3143,3143,602,34,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667378,1697548669708,120,15.0,1.0,"[266, 2064]","[1697548667644, 1697548669708]"
3144,3144,566,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601890,1697548605273,120,,,"[20, 765, 1438, 78, 76]","[1697548601910, 1697548602675, 1697548604113, 1697548604191, 1697548604267]"
3145,3145,33,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669714,1697548671228,120,,,"[60, 1430]","[1697548669774, 1697548671204]"
3146,3146,912,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661869,1697548664205,120,,,"[182, 1726]","[1697548662051, 1697548663777]"
3147,3147,87,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548650209,1697548655266,120,,,"[15, 3003, 100, 96, 89, 84, 651, 96, 84, 62, 80]","[1697548650224, 1697548653227, 1697548653327, 1697548653423, 1697548653512, 1697548653596, 1697548654247, 1697548654343, 1697548654427, 1697548654489, 1697548654569]"
3148,3148,294,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613014,1697548615233,120,,,[330],[1697548613344]
3149,3149,365,36,[],200,llama-7b,128,1,3080.0,1.0,1,A100,1697548671238,1697548674318,120,23.0,1.0,"[353, 2727]","[1697548671591, 1697548674318]"
3150,3150,719,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674321,1697548677256,120,,,[15],[1697548674336]
3151,3151,344,36,[],200,llama-7b,128,1,1819.0,1.0,1,A100,1697548664217,1697548666036,120,13.0,1.0,"[43, 1776]","[1697548664260, 1697548666036]"
3152,3152,58,45,[],200,llama-7b,128,1,1812.0,1.0,1,A100,1697548683213,1697548685025,120,15.0,1.0,"[39, 1773]","[1697548683252, 1697548685025]"
3153,3153,419,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548686710,120,,,"[6, 1143]","[1697548685036, 1697548686179]"
3154,3154,669,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666047,1697548667370,120,,,"[71, 1228]","[1697548666118, 1697548667346]"
3155,3155,363,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622597,120,,,"[75, 1891]","[1697548620437, 1697548622328]"
3156,3156,651,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617365,120,,,"[213, 1810]","[1697548615456, 1697548617266]"
3157,3157,749,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688899,120,,,"[245, 1739]","[1697548686974, 1697548688713]"
3158,3158,100,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671228,120,,,"[190, 2139, 472, 84, 64, 64, 81]","[1697548667568, 1697548669707, 1697548670179, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
3159,3159,182,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691086,120,,,"[52, 1886]","[1697548688959, 1697548690845]"
3160,3160,56,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620358,120,,,"[109, 1799, 112, 83, 83, 80, 79]","[1697548617487, 1697548619286, 1697548619398, 1697548619481, 1697548619564, 1697548619644, 1697548619723]"
3161,3161,441,36,[],200,llama-7b,128,1,2142.0,1.0,1,A100,1697548655283,1697548657425,120,6.0,1.0,"[191, 1951]","[1697548655474, 1697548657425]"
3162,3162,430,11,[],200,llama-7b,128,1,786.0,1.0,1,A100,1697548601890,1697548602676,120,15.0,1.0,"[44, 741]","[1697548601934, 1697548602675]"
3163,3163,541,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693450,120,,,"[246, 1904]","[1697548691342, 1697548693246]"
3164,3164,684,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589576,1697548592062,120,,,"[312, 1701]","[1697548589888, 1697548591589]"
3165,3165,895,50,[],200,llama-7b,128,1,1970.0,1.0,1,A100,1697548693464,1697548695434,120,15.0,1.0,"[374, 1596]","[1697548693838, 1697548695434]"
3166,3166,782,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695717,120,,,"[322, 1648]","[1697548693786, 1697548695434]"
3167,3167,319,51,[],200,llama-7b,128,1,1290.0,1.0,1,A100,1697548695438,1697548696728,120,31.0,1.0,"[44, 1246]","[1697548695482, 1697548696728]"
3168,3168,612,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627137,1697548629598,120,,,"[228, 1972]","[1697548627365, 1697548629337]"
3169,3169,800,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657430,1697548659866,120,,,[60],[1697548657490]
3170,3170,229,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661855,120,,,[310],[1697548660187]
3171,3171,217,51,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697834,120,,,"[135, 1824]","[1697548695859, 1697548697683]"
3172,3172,679,39,[],200,llama-7b,128,1,1905.0,1.0,1,A100,1697548661874,1697548663779,120,15.0,1.0,"[376, 1529]","[1697548662250, 1697548663779]"
3173,3173,40,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631888,120,,,"[239, 1770]","[1697548629847, 1697548631617]"
3174,3174,907,18,[],200,llama-7b,128,1,1968.0,1.0,1,A100,1697548620363,1697548622331,120,10.0,1.0,"[122, 1846]","[1697548620485, 1697548622331]"
3175,3175,138,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548636511,1697548639639,120,,,"[15, 1568, 54]","[1697548636526, 1697548638094, 1697548638148]"
3176,3176,405,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633845,120,,,[189],[1697548632085]
3177,3177,417,17,[],200,llama-7b,128,1,1958.0,1.0,1,A100,1697548620370,1697548622328,120,17.0,1.0,"[269, 1689]","[1697548620639, 1697548622328]"
3178,3178,601,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631886,120,,,"[61, 1955]","[1697548629663, 1697548631618]"
3179,3179,707,40,[],200,llama-7b,128,1,675.0,1.0,1,A100,1697548667350,1697548668025,120,8.0,1.0,"[30, 644]","[1697548667380, 1697548668024]"
3180,3180,455,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624962,1697548627102,120,,,"[30, 2028]","[1697548624992, 1697548627020]"
3181,3181,337,19,[],200,llama-7b,128,1,1121.0,1.0,1,A100,1697548622338,1697548623459,120,12.0,1.0,"[109, 1012]","[1697548622447, 1697548623459]"
3182,3182,26,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633844,120,,,[184],[1697548632080]
3183,3183,111,40,[],200,llama-7b,128,1,2916.0,1.0,1,A100,1697548663784,1697548666700,120,79.0,5.0,"[48, 2634, 83, 67, 84]","[1697548663832, 1697548666466, 1697548666549, 1697548666616, 1697548666700]"
3184,3184,775,18,[],200,llama-7b,128,1,1122.0,1.0,1,A100,1697548622335,1697548623457,120,17.0,1.0,"[58, 1064]","[1697548622393, 1697548623457]"
3185,3185,217,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624976,1697548627102,120,,,"[365, 1681]","[1697548625341, 1697548627022]"
3186,3186,365,18,[],200,llama-7b,128,1,1122.0,1.0,1,A100,1697548622335,1697548623457,120,23.0,1.0,"[32, 1090]","[1697548622367, 1697548623457]"
3187,3187,699,20,[],200,llama-7b,128,1,2313.0,1.0,1,A100,1697548623462,1697548625775,120,39.0,1.0,"[34, 2278]","[1697548623496, 1697548625774]"
3188,3188,100,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548625777,1697548629597,120,,,"[29, 2132]","[1697548625806, 1697548627938]"
3189,3189,577,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[158, 2083]","[1697548627264, 1697548629347]"
3190,3190,809,22,[],200,llama-7b,128,1,2200.0,1.0,1,A100,1697548627138,1697548629338,120,16.0,1.0,"[247, 1953]","[1697548627385, 1697548629338]"
3191,3191,4,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594348,120,,,"[22, 2064]","[1697548592094, 1697548594158]"
3192,3192,384,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633853,1697548635903,120,,,"[40, 538, 59]","[1697548633893, 1697548634431, 1697548634490]"
3193,3193,205,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548623459,1697548627099,120,,,"[7, 2307]","[1697548623466, 1697548625773]"
3194,3194,239,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629349,1697548631887,120,,,"[14, 909]","[1697548629363, 1697548630272]"
3195,3195,135,23,[],200,llama-7b,128,1,1850.0,1.0,1,A100,1697548632639,1697548634489,120,52.0,2.0,"[11, 1780, 59]","[1697548632650, 1697548634430, 1697548634489]"
3196,3196,919,11,[],200,llama-7b,128,1,1953.0,1.0,1,A100,1697548605280,1697548607233,120,14.0,1.0,"[398, 1555]","[1697548605678, 1697548607233]"
3197,3197,571,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633844,120,,,[9],[1697548631904]
3198,3198,323,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607238,1697548608968,120,,,"[114, 1419]","[1697548607352, 1697548608771]"
3199,3199,743,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[160],[1697548636076]
3200,3200,4,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635893,120,,,"[100, 1643]","[1697548633954, 1697548635597]"
3201,3201,668,18,[],200,llama-7b,128,1,2454.0,1.0,1,A100,1697548617269,1697548619723,120,109.0,6.0,"[16, 671, 1440, 84, 82, 80, 81]","[1697548617285, 1697548617956, 1697548619396, 1697548619480, 1697548619562, 1697548619642, 1697548619723]"
3202,3202,732,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612821,1697548615235,120,,,"[45, 673]","[1697548612866, 1697548613539]"
3203,3203,495,24,[],200,llama-7b,128,1,2017.0,1.0,1,A100,1697548634492,1697548636509,120,13.0,1.0,"[8, 2008]","[1697548634500, 1697548636508]"
3204,3204,150,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639645,120,,,[332],[1697548638035]
3205,3205,363,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635909,1697548637693,120,,,[33],[1697548635942]
3206,3206,508,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639651,1697548641374,120,,,[292],[1697548639943]
3207,3207,720,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639637,120,,,"[168, 1429]","[1697548637871, 1697548639300]"
3208,3208,866,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641387,1697548643002,120,,,[349],[1697548641736]
3209,3209,142,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639653,1697548641382,120,,,[240],[1697548639893]
3210,3210,826,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610846,120,,,[289],[1697548609262]
3211,3211,788,12,[],200,llama-7b,128,1,2130.0,1.0,1,A100,1697548600545,1697548602675,120,31.0,1.0,"[24, 2106]","[1697548600569, 1697548602675]"
3212,3212,160,15,[],200,llama-7b,128,1,2024.0,1.0,1,A100,1697548615243,1697548617267,120,13.0,1.0,"[218, 1806]","[1697548615461, 1697548617267]"
3213,3213,849,25,[],200,llama-7b,128,1,1583.0,1.0,1,A100,1697548636512,1697548638095,120,10.0,1.0,"[14, 1568]","[1697548636526, 1697548638094]"
3214,3214,288,30,[],200,llama-7b,128,1,6293.0,1.0,1,A100,1697548643012,1697548649305,120,93.0,20.0,"[224, 1541, 434, 86, 64, 953, 94, 91, 89, 68, 968, 96, 95, 94, 72, 93, 72, 893, 91, 89, 86]","[1697548643236, 1697548644777, 1697548645211, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649039, 1697548649130, 1697548649219, 1697548649305]"
3215,3215,918,3,[],200,llama-7b,128,1,2027.0,1.0,1,A100,1697548583932,1697548585959,120,23.0,1.0,"[346, 1681]","[1697548584278, 1697548585959]"
3216,3216,520,16,[],200,llama-7b,128,1,685.0,1.0,1,A100,1697548617271,1697548617956,120,11.0,1.0,"[66, 619]","[1697548617337, 1697548617956]"
3217,3217,563,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674837,120,,,"[246, 1591, 538, 80, 60]","[1697548671484, 1697548673075, 1697548673613, 1697548673693, 1697548673753]"
3218,3218,274,26,[],200,llama-7b,128,1,8403.0,1.0,1,A100,1697548638095,1697548646498,120,364.0,11.0,"[16, 2071, 1232, 364, 1261, 584, 1587, 85, 64, 954, 94, 91]","[1697548638111, 1697548640182, 1697548641414, 1697548641778, 1697548643039, 1697548643623, 1697548645210, 1697548645295, 1697548645359, 1697548646313, 1697548646407, 1697548646498]"
3219,3219,36,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617959,1697548620357,120,,,"[15, 2347]","[1697548617974, 1697548620321]"
3220,3220,345,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585965,1697548589558,120,,,"[111, 1137, 1176, 79, 78, 76]","[1697548586076, 1697548587213, 1697548588389, 1697548588468, 1697548588546, 1697548588622]"
3221,3221,917,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674843,1697548677257,120,,,"[9, 1944]","[1697548674852, 1697548676796]"
3222,3222,604,27,[],200,llama-7b,128,1,3706.0,1.0,1,A100,1697548646501,1697548650207,120,161.0,4.0,"[13, 3424, 97, 86, 85]","[1697548646514, 1697548649938, 1697548650035, 1697548650121, 1697548650206]"
3223,3223,342,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677270,1697548679646,120,,,"[95, 2109]","[1697548677365, 1697548679474]"
3224,3224,702,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683205,120,,,"[206, 1827, 282, 91, 85, 82, 80]","[1697548679863, 1697548681690, 1697548681972, 1697548682063, 1697548682148, 1697548682230, 1697548682310]"
3225,3225,710,5,[],200,llama-7b,128,1,2021.0,1.0,1,A100,1697548589567,1697548591588,120,14.0,1.0,"[8, 2013]","[1697548589575, 1697548591588]"
3226,3226,36,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548650209,1697548655267,120,,,"[25, 2993, 101, 96, 88, 84, 651, 96, 84, 62, 81]","[1697548650234, 1697548653227, 1697548653328, 1697548653424, 1697548653512, 1697548653596, 1697548654247, 1697548654343, 1697548654427, 1697548654489, 1697548654570]"
3227,3227,397,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622594,120,,,"[198, 1771]","[1697548620561, 1697548622332]"
3228,3228,515,24,[],200,llama-7b,128,1,2328.0,1.0,1,A100,1697548627944,1697548630272,120,11.0,1.0,"[35, 2293]","[1697548627979, 1697548630272]"
3229,3229,66,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671206,1697548674838,120,,,"[40, 1829, 538, 79, 61]","[1697548671246, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
3230,3230,753,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622606,1697548624959,120,,,"[271, 1802]","[1697548622877, 1697548624679]"
3231,3231,874,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548630276,1697548635893,120,,,"[33, 2327, 1248, 58, 547]","[1697548630309, 1697548632636, 1697548633884, 1697548633942, 1697548634489]"
3232,3232,669,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671231,120,,,"[399, 1930, 471, 84, 65, 64, 80]","[1697548667777, 1697548669707, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
3233,3233,112,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,"[28, 1229]","[1697548591622, 1697548592851]"
3234,3234,469,7,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548594351,1697548596098,120,17.0,1.0,"[190, 1557]","[1697548594541, 1697548596098]"
3235,3235,181,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627103,120,,,"[77, 1969]","[1697548625051, 1697548627020]"
3236,3236,824,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596100,1697548597357,120,,,"[12, 1222]","[1697548596112, 1697548597334]"
3237,3237,535,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629598,120,,,"[237, 1962]","[1697548627375, 1697548629337]"
3238,3238,301,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548650211,1697548655268,120,,,"[38, 2978, 101, 96, 89, 84, 650, 96, 84, 63, 80]","[1697548650249, 1697548653227, 1697548653328, 1697548653424, 1697548653513, 1697548653597, 1697548654247, 1697548654343, 1697548654427, 1697548654490, 1697548654570]"
3239,3239,397,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677258,120,,,"[38, 1914]","[1697548674882, 1697548676796]"
3240,3240,865,22,[],200,llama-7b,128,1,2012.0,1.0,1,A100,1697548629608,1697548631620,120,9.0,1.0,"[139, 1873]","[1697548629747, 1697548631620]"
3241,3241,759,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677270,1697548679650,120,,,"[65, 2138]","[1697548677335, 1697548679473]"
3242,3242,250,9,[],200,llama-7b,128,1,2350.0,1.0,1,A100,1697548597367,1697548599717,120,31.0,1.0,"[296, 2054]","[1697548597663, 1697548599717]"
3243,3243,608,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599722,1697548602107,120,,,"[84, 734]","[1697548599806, 1697548600540]"
3244,3244,9,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605264,120,,,"[28, 1696, 264, 77, 77]","[1697548602155, 1697548603851, 1697548604115, 1697548604192, 1697548604269]"
3245,3245,695,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605279,1697548608966,120,,,"[261, 1689, 510, 78, 79, 78]","[1697548605540, 1697548607229, 1697548607739, 1697548607817, 1697548607896, 1697548607974]"
3246,3246,373,12,[],200,llama-7b,128,1,1952.0,1.0,1,A100,1697548605279,1697548607231,120,15.0,1.0,"[66, 1885]","[1697548605345, 1697548607230]"
3247,3247,189,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683206,120,,,"[306, 1723, 286, 91, 85, 82, 80]","[1697548679964, 1697548681687, 1697548681973, 1697548682064, 1697548682149, 1697548682231, 1697548682311]"
3248,3248,634,33,[],200,llama-7b,128,1,2145.0,1.0,1,A100,1697548655282,1697548657427,120,13.0,1.0,"[167, 1978]","[1697548655449, 1697548657427]"
3249,3249,543,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683218,1697548686708,120,,,"[350, 1459, 236, 91, 91, 89, 69, 87, 86, 69]","[1697548683568, 1697548685027, 1697548685263, 1697548685354, 1697548685445, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685845]"
3250,3250,62,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657432,1697548659867,120,,,"[103, 2041]","[1697548657535, 1697548659576]"
3251,3251,129,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608978,1697548612997,120,,,"[401, 2008, 51]","[1697548609379, 1697548611387, 1697548611438]"
3252,3252,487,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613009,1697548615237,120,,,[239],[1697548613248]
3253,3253,731,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608967,120,,,"[46, 1488]","[1697548607283, 1697548608771]"
3254,3254,61,43,[],200,llama-7b,128,1,1984.0,1.0,1,A100,1697548686731,1697548688715,120,9.0,1.0,"[329, 1655]","[1697548687060, 1697548688715]"
3255,3255,560,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627025,1697548629598,120,,,[62],[1697548627087]
3256,3256,161,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610833,120,,,[297],[1697548609272]
3257,3257,819,15,[],200,llama-7b,128,1,2020.0,1.0,1,A100,1697548615246,1697548617266,120,13.0,1.0,"[331, 1689]","[1697548615577, 1697548617266]"
3258,3258,422,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659878,1697548661856,120,,,[314],[1697548660192]
3259,3259,515,15,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548610850,1697548612816,120,11.0,1.0,"[113, 1853]","[1697548610963, 1697548612816]"
3260,3260,28,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612822,1697548615236,120,,,"[55, 661]","[1697548612877, 1697548613538]"
3261,3261,250,16,[],200,llama-7b,128,1,685.0,1.0,1,A100,1697548617271,1697548617956,120,31.0,1.0,"[44, 641]","[1697548617315, 1697548617956]"
3262,3262,922,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629607,1697548631888,120,,,"[229, 1781]","[1697548629836, 1697548631617]"
3263,3263,375,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597945,1697548602106,120,,,[7],[1697548597952]
3264,3264,352,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633848,120,,,[148],[1697548632044]
3265,3265,709,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633857,1697548635904,120,,,"[391, 1351]","[1697548634248, 1697548635599]"
3266,3266,137,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548639638,120,,,"[360, 1818, 54]","[1697548636276, 1697548638094, 1697548638148]"
3267,3267,414,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688718,1697548691085,120,,,"[50, 834]","[1697548688768, 1697548689602]"
3268,3268,772,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664204,120,,,"[100, 1809]","[1697548661968, 1697548663777]"
3269,3269,729,11,[],200,llama-7b,128,1,2005.0,1.0,1,A100,1697548602109,1697548604114,120,874.0,2.0,"[12, 1993]","[1697548602121, 1697548604114]"
3270,3270,584,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639649,1697548641377,120,,,[121],[1697548639770]
3271,3271,156,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548604116,1697548605274,120,,,"[6, 948]","[1697548604122, 1697548605070]"
3272,3272,201,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664216,1697548667372,120,,,"[8, 1811, 432, 82, 67, 84]","[1697548664224, 1697548666035, 1697548666467, 1697548666549, 1697548666616, 1697548666700]"
3273,3273,12,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643000,120,,,[179],[1697548641565]
3274,3274,604,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617958,1697548620356,120,,,"[6, 2357]","[1697548617964, 1697548620321]"
3275,3275,531,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671229,120,,,"[271, 2059, 471, 84, 65, 64, 80]","[1697548667649, 1697548669708, 1697548670179, 1697548670263, 1697548670328, 1697548670392, 1697548670472]"
3276,3276,370,29,[],200,llama-7b,128,1,1767.0,1.0,1,A100,1697548643011,1697548644778,120,31.0,1.0,"[134, 1633]","[1697548643145, 1697548644778]"
3277,3277,895,39,[],200,llama-7b,128,1,1838.0,1.0,1,A100,1697548671238,1697548673076,120,15.0,1.0,"[256, 1582]","[1697548671494, 1697548673076]"
3278,3278,772,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693452,120,,,"[353, 1799]","[1697548691449, 1697548693248]"
3279,3279,325,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673080,1697548674841,120,,,"[26, 1213]","[1697548673106, 1697548674319]"
3280,3280,33,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620369,1697548622593,120,,,"[416, 1546]","[1697548620785, 1697548622331]"
3281,3281,683,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679646,120,,,[388],[1697548675236]
3282,3282,485,13,[],200,llama-7b,128,1,2535.0,1.0,1,A100,1697548605284,1697548607819,120,67.0,3.0,"[374, 1574, 507, 79]","[1697548605658, 1697548607232, 1697548607739, 1697548607818]"
3283,3283,387,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622606,1697548624961,120,,,"[256, 1816]","[1697548622862, 1697548624678]"
3284,3284,846,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607820,1697548612995,120,,,"[6, 1799, 1243, 569]","[1697548607826, 1697548609625, 1697548610868, 1697548611437]"
3285,3285,730,30,[],200,llama-7b,128,1,4523.0,1.0,1,A100,1697548644781,1697548649304,120,364.0,12.0,"[41, 2254, 550, 95, 96, 94, 71, 93, 72, 890, 93, 88, 86]","[1697548644822, 1697548647076, 1697548647626, 1697548647721, 1697548647817, 1697548647911, 1697548647982, 1697548648075, 1697548648147, 1697548649037, 1697548649130, 1697548649218, 1697548649304]"
3286,3286,835,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624975,1697548627102,120,,,"[363, 1684]","[1697548625338, 1697548627022]"
3287,3287,266,21,[],200,llama-7b,128,1,2208.0,1.0,1,A100,1697548627139,1697548629347,120,9.0,1.0,"[348, 1860]","[1697548627487, 1697548629347]"
3288,3288,627,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629353,1697548631888,120,,,"[55, 865]","[1697548629408, 1697548630273]"
3289,3289,277,15,[],200,llama-7b,128,1,2024.0,1.0,1,A100,1697548613006,1697548615030,120,18.0,1.0,"[30, 1994]","[1697548613036, 1697548615030]"
3290,3290,637,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617366,120,,,"[40, 798]","[1697548615075, 1697548615873]"
3291,3291,38,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620357,120,,,"[162, 1746, 110, 83, 85, 78, 81]","[1697548617540, 1697548619286, 1697548619396, 1697548619479, 1697548619564, 1697548619642, 1697548619723]"
3292,3292,108,42,[],200,llama-7b,128,1,2315.0,1.0,1,A100,1697548679657,1697548681972,120,182.0,2.0,"[42, 1988, 285]","[1697548679699, 1697548681687, 1697548681972]"
3293,3293,129,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649315,1697548655271,120,,,"[45, 2818, 268, 319, 89, 69, 403, 97, 88, 85, 650, 96, 84, 63, 80]","[1697548649360, 1697548652178, 1697548652446, 1697548652765, 1697548652854, 1697548652923, 1697548653326, 1697548653423, 1697548653511, 1697548653596, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
3294,3294,392,18,[],200,llama-7b,128,1,1960.0,1.0,1,A100,1697548620372,1697548622332,120,20.0,1.0,"[414, 1545]","[1697548620786, 1697548622331]"
3295,3295,56,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633845,120,,,[242],[1697548632138]
3296,3296,206,46,[],200,llama-7b,128,1,1970.0,1.0,1,A100,1697548693464,1697548695434,120,16.0,1.0,"[317, 1653]","[1697548693781, 1697548695434]"
3297,3297,435,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681975,1697548686705,120,,,"[15, 2054, 1218, 91, 90, 89, 70, 87, 86, 68]","[1697548681990, 1697548684044, 1697548685262, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
3298,3298,382,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635904,120,,,"[95, 1648]","[1697548633949, 1697548635597]"
3299,3299,746,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622336,1697548624959,120,,,"[72, 1050]","[1697548622408, 1697548623458]"
3300,3300,566,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695439,1697548697836,120,,,"[68, 1221]","[1697548695507, 1697548696728]"
3301,3301,177,20,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624975,1697548627022,120,14.0,1.0,"[314, 1733]","[1697548625289, 1697548627022]"
3302,3302,737,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637695,120,,,[274],[1697548636190]
3303,3303,483,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655276,1697548657536,120,,,"[305, 1846]","[1697548655581, 1697548657427]"
3304,3304,166,26,[],200,llama-7b,128,1,1597.0,1.0,1,A100,1697548637703,1697548639300,120,14.0,1.0,"[290, 1307]","[1697548637993, 1697548639300]"
3305,3305,846,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657544,1697548659868,120,,,"[97, 1937]","[1697548657641, 1697548659578]"
3306,3306,527,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639303,1697548655272,120,,,"[23, 2088, 364, 1261, 584, 1587, 86, 64, 953, 94, 91, 89, 68, 969, 96, 95, 94, 71, 94, 71, 891, 93, 88, 86, 731, 86, 85, 76, 74, 74, 991, 91, 74, 858, 320, 88, 70, 403, 97, 89, 84, 651, 96, 84, 63, 80]","[1697548639326, 1697548641414, 1697548641778, 1697548643039, 1697548643623, 1697548645210, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646587, 1697548646655, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647980, 1697548648074, 1697548648145, 1697548649036, 1697548649129, 1697548649217, 1697548649303, 1697548650034, 1697548650120, 1697548650205, 1697548650281, 1697548650355, 1697548650429, 1697548651420, 1697548651511, 1697548651585, 1697548652443, 1697548652763, 1697548652851, 1697548652921, 1697548653324, 1697548653421, 1697548653510, 1697548653594, 1697548654245, 1697548654341, 1697548654425, 1697548654488, 1697548654568]"
3307,3307,274,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661855,120,,,[305],[1697548660182]
3308,3308,299,4,[],200,llama-7b,128,1,2015.0,1.0,1,A100,1697548589574,1697548591589,120,14.0,1.0,"[305, 1710]","[1697548589879, 1697548591589]"
3309,3309,660,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,"[20, 1237]","[1697548591614, 1697548592851]"
3310,3310,792,44,[],200,llama-7b,128,1,1985.0,1.0,1,A100,1697548686729,1697548688714,120,11.0,1.0,"[258, 1726]","[1697548686987, 1697548688713]"
3311,3311,538,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627024,1697548629597,120,,,"[34, 880]","[1697548627058, 1697548627938]"
3312,3312,225,45,[],200,llama-7b,128,1,884.0,1.0,1,A100,1697548688718,1697548689602,120,23.0,1.0,"[38, 846]","[1697548688756, 1697548689602]"
3313,3313,85,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597359,120,,,"[109, 1638, 251, 72, 57]","[1697548594459, 1697548596097, 1697548596348, 1697548596420, 1697548596477]"
3314,3314,866,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629607,1697548631887,120,,,"[171, 1838]","[1697548629778, 1697548631616]"
3315,3315,234,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627100,120,,,"[97, 1949]","[1697548625071, 1697548627020]"
3316,3316,663,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617368,120,,,[96],[1697548615339]
3317,3317,584,46,[],200,llama-7b,128,1,2244.0,1.0,1,A100,1697548689609,1697548691853,120,10.0,1.0,"[31, 2212]","[1697548689640, 1697548691852]"
3318,3318,93,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617387,1697548620356,120,,,"[382, 1520, 108, 84, 83, 80, 81]","[1697548617769, 1697548619289, 1697548619397, 1697548619481, 1697548619564, 1697548619644, 1697548619725]"
3319,3319,592,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627137,1697548629598,120,,,"[218, 1982]","[1697548627355, 1697548629337]"
3320,3320,300,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633848,120,,,[282],[1697548632183]
3321,3321,654,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635904,120,,,"[21, 557, 58]","[1697548633875, 1697548634432, 1697548634490]"
3322,3322,103,47,[],200,llama-7b,128,1,2288.0,1.0,1,A100,1697548691855,1697548694143,120,15.0,1.0,"[7, 2281]","[1697548691862, 1697548694143]"
3323,3323,457,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548694146,1697548697835,120,,,[13],[1697548694159]
3324,3324,84,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637692,120,,,[219],[1697548636135]
3325,3325,151,29,[],200,llama-7b,128,1,2870.0,1.0,1,A100,1697548650358,1697548653228,120,39.0,1.0,"[6, 2863]","[1697548650364, 1697548653227]"
3326,3326,22,22,[],200,llama-7b,128,1,2016.0,1.0,1,A100,1697548629603,1697548631619,120,16.0,1.0,"[85, 1931]","[1697548629688, 1697548631619]"
3327,3327,438,26,[],200,llama-7b,128,1,1597.0,1.0,1,A100,1697548637702,1697548639299,120,9.0,1.0,"[38, 1559]","[1697548637740, 1697548639299]"
3328,3328,18,29,[],200,llama-7b,128,1,3499.0,1.0,1,A100,1697548643576,1697548647075,120,15.0,1.0,"[15, 3484]","[1697548643591, 1697548647075]"
3329,3329,886,27,[],200,llama-7b,128,1,879.0,1.0,1,A100,1697548639304,1697548640183,120,17.0,1.0,"[57, 822]","[1697548639361, 1697548640183]"
3330,3330,378,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548647078,1697548655267,120,,,"[19, 2938, 86, 86, 75, 73, 75, 991, 91, 74, 860, 319, 88, 70, 403, 98, 88, 84, 650, 97, 84, 62, 80]","[1697548647097, 1697548650035, 1697548650121, 1697548650207, 1697548650282, 1697548650355, 1697548650430, 1697548651421, 1697548651512, 1697548651586, 1697548652446, 1697548652765, 1697548652853, 1697548652923, 1697548653326, 1697548653424, 1697548653512, 1697548653596, 1697548654246, 1697548654343, 1697548654427, 1697548654489, 1697548654569]"
3331,3331,317,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548640186,1697548655265,120,,,"[15, 1536, 42, 1261, 584, 1587, 85, 64, 954, 94, 91, 89, 68, 968, 96, 96, 93, 72, 93, 72, 891, 92, 88, 86, 731, 86, 86, 75, 74, 74, 991, 91, 74, 859, 319, 88, 70, 403, 97, 89, 84, 651, 96, 84, 63, 80]","[1697548640201, 1697548641737, 1697548641779, 1697548643040, 1697548643624, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647816, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650034, 1697548650120, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651420, 1697548651511, 1697548651585, 1697548652444, 1697548652763, 1697548652851, 1697548652921, 1697548653324, 1697548653421, 1697548653510, 1697548653594, 1697548654245, 1697548654341, 1697548654425, 1697548654488, 1697548654568]"
3332,3332,622,39,[],200,llama-7b,128,1,1678.0,1.0,1,A100,1697548668032,1697548669710,120,20.0,1.0,"[26, 1652]","[1697548668058, 1697548669710]"
3333,3333,437,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594355,1697548597362,120,,,"[382, 1362, 248, 73, 57]","[1697548594737, 1697548596099, 1697548596347, 1697548596420, 1697548596477]"
3334,3334,134,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592062,120,,,"[98, 1918]","[1697548589672, 1697548591590]"
3335,3335,227,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[29, 2044]","[1697548622632, 1697548624676]"
3336,3336,492,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594346,120,,,"[72, 2015]","[1697548592144, 1697548594159]"
3337,3337,210,40,[],200,llama-7b,128,1,2370.0,1.0,1,A100,1697548671244,1697548673614,120,140.0,2.0,"[326, 1507, 537]","[1697548671570, 1697548673077, 1697548673614]"
3338,3338,583,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627100,120,,,"[173, 1874]","[1697548625147, 1697548627021]"
3339,3339,15,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[142, 2090]","[1697548627248, 1697548629338]"
3340,3340,851,6,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548594350,1697548596097,120,23.0,1.0,"[38, 1708]","[1697548594388, 1697548596096]"
3341,3341,372,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631886,120,,,"[56, 1959]","[1697548629658, 1697548631617]"
3342,3342,706,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633846,120,,,[257],[1697548632158]
3343,3343,280,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596103,1697548597359,120,,,"[93, 1138]","[1697548596196, 1697548597334]"
3344,3344,924,12,[],200,llama-7b,128,1,1949.0,1.0,1,A100,1697548605283,1697548607232,120,9.0,1.0,"[167, 1782]","[1697548605450, 1697548607232]"
3345,3345,610,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599941,120,,,"[113, 2237]","[1697548597479, 1697548599716]"
3346,3346,131,23,[],200,llama-7b,128,1,1746.0,1.0,1,A100,1697548633855,1697548635601,120,8.0,1.0,"[223, 1523]","[1697548634078, 1697548635601]"
3347,3347,407,14,[],200,llama-7b,128,1,2024.0,1.0,1,A100,1697548615242,1697548617266,120,16.0,1.0,"[126, 1898]","[1697548615368, 1697548617266]"
3348,3348,491,24,[],200,llama-7b,128,1,904.0,1.0,1,A100,1697548635604,1697548636508,120,14.0,1.0,"[65, 839]","[1697548635669, 1697548636508]"
3349,3349,23,31,[],200,llama-7b,128,1,2460.0,1.0,1,A100,1697548651433,1697548653893,120,26.0,1.0,"[8, 2451]","[1697548651441, 1697548653892]"
3350,3350,382,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653896,1697548657538,120,,,"[7, 2276]","[1697548653903, 1697548656179]"
3351,3351,323,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608968,120,,,"[66, 1467]","[1697548607303, 1697548608770]"
3352,3352,736,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657550,1697548659866,120,,,"[335, 1694]","[1697548657885, 1697548659579]"
3353,3353,38,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602108,120,,,"[125, 1808]","[1697548600076, 1697548601884]"
3354,3354,682,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610835,120,,,[364],[1697548609339]
3355,3355,144,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633850,120,,,[164],[1697548632060]
3356,3356,667,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[49, 2025]","[1697548622652, 1697548624677]"
3357,3357,149,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677270,1697548679646,120,,,"[80, 2123]","[1697548677350, 1697548679473]"
3358,3358,110,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548612999,120,,,"[310, 1654]","[1697548611162, 1697548612816]"
3359,3359,848,25,[],200,llama-7b,128,1,1584.0,1.0,1,A100,1697548636510,1697548638094,120,47.0,1.0,"[6, 1578]","[1697548636516, 1697548638094]"
3360,3360,71,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624975,1697548627101,120,,,"[308, 1739]","[1697548625283, 1697548627022]"
3361,3361,277,26,[],200,llama-7b,128,1,1205.0,1.0,1,A100,1697548638096,1697548639301,120,18.0,1.0,"[10, 1195]","[1697548638106, 1697548639301]"
3362,3362,395,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605264,120,,,"[317, 1407, 262, 77, 77]","[1697548602446, 1697548603853, 1697548604115, 1697548604192, 1697548604269]"
3363,3363,497,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633857,1697548635904,120,,,"[329, 1412]","[1697548634186, 1697548635598]"
3364,3364,429,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629599,120,,,"[305, 1895]","[1697548627443, 1697548629338]"
3365,3365,503,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679656,1697548683207,120,,,"[37, 1994, 285, 91, 85, 83, 80]","[1697548679693, 1697548681687, 1697548681972, 1697548682063, 1697548682148, 1697548682231, 1697548682311]"
3366,3366,783,21,[],200,llama-7b,128,1,2009.0,1.0,1,A100,1697548629611,1697548631620,120,286.0,1.0,"[351, 1658]","[1697548629962, 1697548631620]"
3367,3367,468,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613007,1697548615236,120,,,"[232, 1791]","[1697548613239, 1697548615030]"
3368,3368,208,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631626,1697548635895,120,,,"[54, 957, 1248, 57, 548]","[1697548631680, 1697548632637, 1697548633885, 1697548633942, 1697548634490]"
3369,3369,827,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617369,120,,,"[335, 1687]","[1697548615579, 1697548617266]"
3370,3370,568,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637695,120,,,[279],[1697548636195]
3371,3371,231,18,[],200,llama-7b,128,1,1901.0,1.0,1,A100,1697548617387,1697548619288,120,13.0,1.0,"[358, 1543]","[1697548617745, 1697548619288]"
3372,3372,859,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637695,120,,,[265],[1697548636180]
3373,3373,586,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619290,1697548620358,120,,,"[25, 1008]","[1697548619315, 1697548620323]"
3374,3374,896,24,[],200,llama-7b,128,1,1596.0,1.0,1,A100,1697548637703,1697548639299,120,15.0,1.0,"[60, 1536]","[1697548637763, 1697548639299]"
3375,3375,332,25,[],200,llama-7b,128,1,879.0,1.0,1,A100,1697548639304,1697548640183,120,39.0,1.0,"[39, 840]","[1697548639343, 1697548640183]"
3376,3376,15,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622593,120,,,"[186, 1777]","[1697548620554, 1697548622331]"
3377,3377,499,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639656,1697548641378,120,,,[408],[1697548640064]
3378,3378,369,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622606,1697548624958,120,,,"[266, 1806]","[1697548622872, 1697548624678]"
3379,3379,691,26,[],200,llama-7b,128,1,1551.0,1.0,1,A100,1697548640186,1697548641737,120,47.0,1.0,"[19, 1532]","[1697548640205, 1697548641737]"
3380,3380,312,2,[],200,llama-7b,128,1,1722.0,1.0,1,A100,1697548581088,1697548582810,120,23.0,1.0,"[151, 1571]","[1697548581239, 1697548582810]"
3381,3381,288,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617368,120,,,"[36, 1987]","[1697548615278, 1697548617265]"
3382,3382,857,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642998,120,,,[76],[1697548641462]
3383,3383,675,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548582819,1697548583923,120,,,"[47, 1036]","[1697548582866, 1697548583902]"
3384,3384,24,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612821,1697548615236,120,,,[46],[1697548612867]
3385,3385,103,4,[],200,llama-7b,128,1,2026.0,1.0,1,A100,1697548583933,1697548585959,120,15.0,1.0,"[348, 1678]","[1697548584281, 1697548585959]"
3386,3386,436,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548585963,1697548589557,120,,,"[41, 1208, 1177, 79, 78, 75]","[1697548586004, 1697548587212, 1697548588389, 1697548588468, 1697548588546, 1697548588621]"
3387,3387,731,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627100,120,,,"[187, 1860]","[1697548625161, 1697548627021]"
3388,3388,262,28,[],200,llama-7b,128,1,1765.0,1.0,1,A100,1697548643012,1697548644777,120,39.0,1.0,"[172, 1593]","[1697548643184, 1697548644777]"
3389,3389,642,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617381,1697548620359,120,,,"[205, 1700, 110, 84, 82, 81, 80]","[1697548617586, 1697548619286, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
3390,3390,616,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548644786,1697548655267,120,,,"[55, 2234, 551, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86, 732, 86, 85, 75, 74, 73, 994, 92, 72, 858, 319, 89, 69, 404, 96, 89, 84, 651, 96, 84, 63, 80]","[1697548644841, 1697548647075, 1697548647626, 1697548647722, 1697548647817, 1697548647911, 1697548647982, 1697548648075, 1697548648147, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650036, 1697548650122, 1697548650207, 1697548650282, 1697548650356, 1697548650429, 1697548651423, 1697548651515, 1697548651587, 1697548652445, 1697548652764, 1697548652853, 1697548652922, 1697548653326, 1697548653422, 1697548653511, 1697548653595, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
3391,3391,752,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603857,1697548605266,120,,,"[49, 1164]","[1697548603906, 1697548605070]"
3392,3392,243,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686720,1697548688899,120,,,"[262, 1731]","[1697548686982, 1697548688713]"
3393,3393,795,6,[],200,llama-7b,128,1,2016.0,1.0,1,A100,1697548589573,1697548591589,120,12.0,1.0,"[77, 1939]","[1697548589650, 1697548591589]"
3394,3394,159,12,[],200,llama-7b,128,1,1946.0,1.0,1,A100,1697548605283,1697548607229,120,31.0,1.0,"[213, 1733]","[1697548605496, 1697548607229]"
3395,3395,518,13,[],200,llama-7b,128,1,1535.0,1.0,1,A100,1697548607236,1697548608771,120,23.0,1.0,"[57, 1478]","[1697548607293, 1697548608771]"
3396,3396,247,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627139,1697548629598,120,,,"[348, 1852]","[1697548627487, 1697548629339]"
3397,3397,876,14,[],200,llama-7b,128,1,851.0,1.0,1,A100,1697548608775,1697548609626,120,11.0,1.0,"[39, 812]","[1697548608814, 1697548609626]"
3398,3398,608,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629607,1697548631887,120,,,"[163, 1851]","[1697548629770, 1697548631621]"
3399,3399,474,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589384,1697548592065,120,,,[23],[1697548589407]
3400,3400,220,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591595,1697548594347,120,,,"[69, 1188]","[1697548591664, 1697548592852]"
3401,3401,834,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594348,120,,,"[7, 2079]","[1697548592079, 1697548594158]"
3402,3402,259,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594349,1697548597357,120,,,"[10, 1736, 251, 73, 57]","[1697548594359, 1697548596095, 1697548596346, 1697548596419, 1697548596476]"
3403,3403,282,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683205,120,,,"[212, 1821, 283, 90, 85, 82, 80]","[1697548679869, 1697548681690, 1697548681973, 1697548682063, 1697548682148, 1697548682230, 1697548682310]"
3404,3404,574,8,[],200,llama-7b,128,1,1998.0,1.0,1,A100,1697548594350,1697548596348,120,364.0,2.0,"[98, 1649, 251]","[1697548594448, 1697548596097, 1697548596348]"
3405,3405,472,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643000,120,,,[184],[1697548641570]
3406,3406,589,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599941,120,,,"[105, 2245]","[1697548597471, 1697548599716]"
3407,3407,4,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596349,1697548597359,120,,,"[10, 976]","[1697548596359, 1697548597335]"
3408,3408,829,30,[],200,llama-7b,128,1,2725.0,1.0,1,A100,1697548643012,1697548645737,120,20.0,1.0,"[275, 2450]","[1697548643287, 1697548645737]"
3409,3409,265,31,[],200,llama-7b,128,1,7682.0,1.0,1,A100,1697548645742,1697548653424,120,86.0,20.0,"[23, 2673, 600, 92, 88, 86, 730, 87, 85, 75, 74, 75, 991, 91, 74, 859, 320, 88, 69, 404, 97]","[1697548645765, 1697548648438, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650430, 1697548651421, 1697548651512, 1697548651586, 1697548652445, 1697548652765, 1697548652853, 1697548652922, 1697548653326, 1697548653423]"
3410,3410,44,30,[],200,llama-7b,128,1,2144.0,1.0,1,A100,1697548655281,1697548657425,120,12.0,1.0,"[20, 2124]","[1697548655301, 1697548657425]"
3411,3411,14,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[310, 1624]","[1697548600262, 1697548601886]"
3412,3412,399,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657429,1697548659865,120,,,"[41, 818]","[1697548657470, 1697548658288]"
3413,3413,760,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661855,120,,,[102],[1697548659973]
3414,3414,452,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597365,1697548599940,120,,,"[94, 485, 36]","[1697548597459, 1697548597944, 1697548597980]"
3415,3415,625,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653427,1697548657538,120,,,"[25, 2727]","[1697548653452, 1697548656179]"
3416,3416,140,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657549,1697548659866,120,,,"[341, 1689]","[1697548657890, 1697548659579]"
3417,3417,162,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661869,1697548664204,120,,,"[114, 1794]","[1697548661983, 1697548663777]"
3418,3418,500,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661854,120,,,[87],[1697548659958]
3419,3419,812,11,[],200,llama-7b,128,1,1932.0,1.0,1,A100,1697548599952,1697548601884,120,16.0,1.0,"[333, 1599]","[1697548600285, 1697548601884]"
3420,3420,244,12,[],200,llama-7b,128,1,786.0,1.0,1,A100,1697548601889,1697548602675,120,9.0,1.0,"[25, 761]","[1697548601914, 1697548602675]"
3421,3421,520,34,[],200,llama-7b,128,1,1820.0,1.0,1,A100,1697548664217,1697548666037,120,11.0,1.0,"[150, 1670]","[1697548664367, 1697548666037]"
3422,3422,850,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661866,1697548664208,120,,,"[379, 1533]","[1697548662245, 1697548663778]"
3423,3423,877,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666047,1697548667370,120,,,"[76, 1223]","[1697548666123, 1697548667346]"
3424,3424,279,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667368,120,,,"[131, 1689, 431, 82, 67, 84]","[1697548664348, 1697548666037, 1697548666468, 1697548666550, 1697548666617, 1697548666701]"
3425,3425,601,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602680,1697548605263,120,,,"[33, 2356]","[1697548602713, 1697548605069]"
3426,3426,709,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683209,120,,,"[387, 2863]","[1697548680045, 1697548682908]"
3427,3427,928,14,[],200,llama-7b,128,1,1950.0,1.0,1,A100,1697548605279,1697548607229,120,20.0,1.0,"[8, 1942]","[1697548605287, 1697548607229]"
3428,3428,353,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607236,1697548608966,120,,,"[22, 1512]","[1697548607258, 1697548608770]"
3429,3429,467,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589568,1697548592062,120,,,"[368, 1654]","[1697548589936, 1697548591590]"
3430,3430,742,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617376,1697548620356,120,,,"[10, 1898, 113, 83, 83, 80, 81]","[1697548617386, 1697548619284, 1697548619397, 1697548619480, 1697548619563, 1697548619643, 1697548619724]"
3431,3431,173,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620369,1697548622593,120,,,"[412, 1550]","[1697548620781, 1697548622331]"
3432,3432,638,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671226,120,,,"[71, 576, 35, 2117, 85, 64, 65, 79]","[1697548667449, 1697548668025, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670391, 1697548670470]"
3433,3433,711,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610832,120,,,[293],[1697548609267]
3434,3434,825,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594347,120,,,"[175, 1914]","[1697548592247, 1697548594161]"
3435,3435,534,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622606,1697548624959,120,,,"[261, 1811]","[1697548622867, 1697548624678]"
3436,3436,317,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657544,1697548659867,120,,,"[24, 2009]","[1697548657568, 1697548659577]"
3437,3437,255,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597361,120,,,"[310, 1432, 248, 73, 58]","[1697548594666, 1697548596098, 1697548596346, 1697548596419, 1697548596477]"
3438,3438,766,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661854,120,,,[290],[1697548660162]
3439,3439,196,32,[],200,llama-7b,128,1,1909.0,1.0,1,A100,1697548661868,1697548663777,120,13.0,1.0,"[105, 1804]","[1697548661973, 1697548663777]"
3440,3440,144,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610846,1697548612998,120,,,"[254, 1715]","[1697548611100, 1697548612815]"
3441,3441,506,18,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548613007,1697548615032,120,16.0,1.0,"[69, 1956]","[1697548613076, 1697548615032]"
3442,3442,554,33,[],200,llama-7b,128,1,940.0,1.0,1,A100,1697548663782,1697548664722,120,26.0,1.0,"[26, 914]","[1697548663808, 1697548664722]"
3443,3443,399,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657536,120,,,"[35, 2109]","[1697548655316, 1697548657425]"
3444,3444,904,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664725,1697548667373,120,,,"[17, 2603]","[1697548664742, 1697548667345]"
3445,3445,335,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667381,1697548671226,120,,,"[507, 1821, 470, 84, 64, 65, 80]","[1697548667888, 1697548669709, 1697548670179, 1697548670263, 1697548670327, 1697548670392, 1697548670472]"
3446,3446,835,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617366,120,,,"[44, 794]","[1697548615079, 1697548615873]"
3447,3447,663,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671242,1697548674839,120,,,"[49, 2323, 79, 60]","[1697548671291, 1697548673614, 1697548673693, 1697548673753]"
3448,3448,44,38,[],200,llama-7b,128,1,1839.0,1.0,1,A100,1697548671237,1697548673076,120,12.0,1.0,"[49, 1790]","[1697548671286, 1697548673076]"
3449,3449,401,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673083,1697548677261,120,,,"[89, 2300]","[1697548673172, 1697548675472]"
3450,3450,98,37,[],200,llama-7b,128,1,3393.0,1.0,1,A100,1697548674848,1697548678241,120,14.0,1.0,"[337, 3056]","[1697548675185, 1697548678241]"
3451,3451,761,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677279,1697548679651,120,,,"[364, 1832]","[1697548677643, 1697548679475]"
3452,3452,186,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683208,120,,,"[393, 1638, 284, 91, 84, 83, 81]","[1697548680051, 1697548681689, 1697548681973, 1697548682064, 1697548682148, 1697548682231, 1697548682312]"
3453,3453,457,38,[],200,llama-7b,128,1,3728.0,1.0,1,A100,1697548678244,1697548681972,120,874.0,2.0,"[15, 3712]","[1697548678259, 1697548681971]"
3454,3454,756,30,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548657547,1697548659578,120,19.0,1.0,"[289, 1741]","[1697548657836, 1697548659577]"
3455,3455,264,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620358,120,,,"[102, 1806, 112, 83, 83, 80, 80]","[1697548617480, 1697548619286, 1697548619398, 1697548619481, 1697548619564, 1697548619644, 1697548619724]"
3456,3456,181,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664205,120,,,"[39, 1080, 1195, 57, 1027]","[1697548659621, 1697548660701, 1697548661896, 1697548661953, 1697548662980]"
3457,3457,633,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664207,120,,,"[305, 1607]","[1697548662172, 1697548663779]"
3458,3458,628,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667374,120,,,"[63, 2187, 82, 67, 85]","[1697548664280, 1697548666467, 1697548666549, 1697548666616, 1697548666701]"
3459,3459,815,39,[],200,llama-7b,128,1,3468.0,1.0,1,A100,1697548681975,1697548685443,120,52.0,4.0,"[14, 2055, 1218, 91, 90]","[1697548681989, 1697548684044, 1697548685262, 1697548685353, 1697548685443]"
3460,3460,33,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667369,120,,,"[228, 1590, 431, 84, 65, 85]","[1697548664445, 1697548666035, 1697548666466, 1697548666550, 1697548666615, 1697548666700]"
3461,3461,240,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685446,1697548688903,120,,,"[7, 2089]","[1697548685453, 1697548687542]"
3462,3462,542,42,[],200,llama-7b,128,1,1811.0,1.0,1,A100,1697548683216,1697548685027,120,11.0,1.0,"[342, 1469]","[1697548683558, 1697548685027]"
3463,3463,871,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548688902,120,,,"[41, 2471]","[1697548685071, 1697548687542]"
3464,3464,510,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653229,1697548655272,120,,,[7],[1697548653236]
3465,3465,567,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688910,1697548691087,120,,,"[241, 1694]","[1697548689151, 1697548690845]"
3466,3466,390,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671228,120,,,"[195, 2134, 472, 84, 64, 65, 80]","[1697548667573, 1697548669707, 1697548670179, 1697548670263, 1697548670327, 1697548670392, 1697548670472]"
3467,3467,615,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622596,120,,,"[295, 1663]","[1697548620665, 1697548622328]"
3468,3468,694,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548676799,1697548679648,120,,,[17],[1697548676816]
3469,3469,302,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691087,120,,,"[330, 1614]","[1697548689239, 1697548690853]"
3470,3470,662,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693452,120,,,"[345, 1806]","[1697548691442, 1697548693248]"
3471,3471,125,39,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679658,1697548681688,120,13.0,1.0,"[336, 1694]","[1697548679994, 1697548681688]"
3472,3472,94,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695715,120,,,"[192, 1781]","[1697548693652, 1697548695433]"
3473,3473,871,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657535,120,,,"[271, 1874]","[1697548655552, 1697548657426]"
3474,3474,489,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548686704,120,,,"[60, 3504, 91, 90, 89, 70, 87, 86, 68]","[1697548681758, 1697548685262, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
3475,3475,842,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661853,120,,,[270],[1697548660142]
3476,3476,836,21,[],200,llama-7b,128,1,2016.0,1.0,1,A100,1697548629602,1697548631618,120,11.0,1.0,"[66, 1950]","[1697548629668, 1697548631618]"
3477,3477,448,47,[],200,llama-7b,128,1,2662.0,1.0,1,A100,1697548695726,1697548698388,120,335.0,12.0,"[337, 2076, 28, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22]","[1697548696063, 1697548698139, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388]"
3478,3478,456,31,[],200,llama-7b,128,1,7683.0,1.0,1,A100,1697548646660,1697548654343,120,90.0,20.0,"[11, 3267, 97, 86, 85, 75, 74, 75, 991, 91, 74, 860, 319, 88, 70, 403, 97, 89, 83, 651, 97]","[1697548646671, 1697548649938, 1697548650035, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650430, 1697548651421, 1697548651512, 1697548651586, 1697548652446, 1697548652765, 1697548652853, 1697548652923, 1697548653326, 1697548653423, 1697548653512, 1697548653595, 1697548654246, 1697548654343]"
3479,3479,51,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671227,120,,,"[36, 1454]","[1697548669749, 1697548671203]"
3480,3480,385,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674839,120,,,"[352, 1488, 536, 79, 61]","[1697548671590, 1697548673078, 1697548673614, 1697548673693, 1697548673754]"
3481,3481,565,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673617,1697548677266,120,,,"[19, 1837]","[1697548673636, 1697548675473]"
3482,3482,384,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622596,120,,,"[314, 1645]","[1697548620684, 1697548622329]"
3483,3483,282,28,[],200,llama-7b,128,1,8841.0,1.0,1,A100,1697548639304,1697548648145,120,87.0,20.0,"[37, 842, 1231, 365, 1261, 583, 1587, 86, 64, 953, 94, 91, 89, 68, 969, 96, 95, 94, 71, 94, 71]","[1697548639341, 1697548640183, 1697548641414, 1697548641779, 1697548643040, 1697548643623, 1697548645210, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646587, 1697548646655, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647980, 1697548648074, 1697548648145]"
3484,3484,775,18,[],200,llama-7b,128,1,1962.0,1.0,1,A100,1697548620369,1697548622331,120,17.0,1.0,"[402, 1560]","[1697548620771, 1697548622331]"
3485,3485,238,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674847,1697548677258,120,,,"[25, 1924]","[1697548674872, 1697548676796]"
3486,3486,738,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548677260,120,,,[246],[1697548675094]
3487,3487,206,19,[],200,llama-7b,128,1,1123.0,1.0,1,A100,1697548622335,1697548623458,120,16.0,1.0,"[67, 1056]","[1697548622402, 1697548623458]"
3488,3488,898,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608972,1697548610824,120,,,[37],[1697548609009]
3489,3489,599,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677279,1697548679650,120,,,"[304, 1892]","[1697548677583, 1697548679475]"
3490,3490,531,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548623460,1697548627099,120,,,"[11, 2303]","[1697548623471, 1697548625774]"
3491,3491,28,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683206,120,,,"[216, 1816, 282, 91, 86, 81, 80]","[1697548679874, 1697548681690, 1697548681972, 1697548682063, 1697548682149, 1697548682230, 1697548682310]"
3492,3492,320,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548612998,120,,,[285],[1697548611137]
3493,3493,767,15,[],200,llama-7b,128,1,686.0,1.0,1,A100,1697548617270,1697548617956,120,11.0,1.0,"[50, 636]","[1697548617320, 1697548617956]"
3494,3494,391,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683215,1697548686705,120,,,"[150, 1897, 91, 90, 90, 69, 87, 86, 69]","[1697548683365, 1697548685262, 1697548685353, 1697548685443, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
3495,3495,195,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617960,1697548620357,120,,,"[30, 2332]","[1697548617990, 1697548620322]"
3496,3496,531,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622596,120,,,"[300, 1659]","[1697548620670, 1697548622329]"
3497,3497,745,45,[],200,llama-7b,128,1,1984.0,1.0,1,A100,1697548686729,1697548688713,120,17.0,1.0,"[55, 1929]","[1697548686784, 1697548688713]"
3498,3498,890,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627137,1697548629598,120,,,"[208, 2001]","[1697548627345, 1697548629346]"
3499,3499,167,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677273,1697548679647,120,,,"[112, 2089]","[1697548677385, 1697548679474]"
3500,3500,890,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[39, 2034]","[1697548622642, 1697548624676]"
3501,3501,523,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683208,120,,,"[89, 1941, 285, 91, 85, 83, 81]","[1697548679746, 1697548681687, 1697548681972, 1697548682063, 1697548682148, 1697548682231, 1697548682312]"
3502,3502,170,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661857,120,,,[188],[1697548660060]
3503,3503,145,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688716,1697548691086,120,,,"[17, 869]","[1697548688733, 1697548689602]"
3504,3504,784,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602680,1697548605264,120,,,"[27, 2363]","[1697548602707, 1697548605070]"
3505,3505,453,39,[],200,llama-7b,128,1,1834.0,1.0,1,A100,1697548671243,1697548673077,120,26.0,1.0,"[145, 1689]","[1697548671388, 1697548673077]"
3506,3506,724,27,[],200,llama-7b,128,1,879.0,1.0,1,A100,1697548639304,1697548640183,120,11.0,1.0,"[42, 836]","[1697548639346, 1697548640182]"
3507,3507,413,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613007,1697548615232,120,,,"[49, 1975]","[1697548613056, 1697548615031]"
3508,3508,813,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548654347,1697548657501,120,,,"[13, 1819]","[1697548654360, 1697548656179]"
3509,3509,814,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673081,1697548674841,120,,,"[30, 1208]","[1697548673111, 1697548674319]"
3510,3510,771,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617368,120,,,"[93, 1930]","[1697548615335, 1697548617265]"
3511,3511,152,28,[],200,llama-7b,128,1,8851.0,1.0,1,A100,1697548640186,1697548649037,120,87.0,20.0,"[9, 1542, 41, 1262, 584, 1587, 85, 64, 954, 94, 91, 89, 68, 968, 96, 95, 94, 72, 93, 72, 891]","[1697548640195, 1697548641737, 1697548641778, 1697548643040, 1697548643624, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649037]"
3512,3512,198,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620358,120,,,"[172, 1734, 112, 84, 82, 81, 80]","[1697548617550, 1697548619284, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
3513,3513,244,41,[],200,llama-7b,128,1,1949.0,1.0,1,A100,1697548674848,1697548676797,120,9.0,1.0,"[297, 1652]","[1697548675145, 1697548676797]"
3514,3514,249,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657547,1697548659865,120,,,"[302, 1729]","[1697548657849, 1697548659578]"
3515,3515,576,42,[],200,llama-7b,128,1,1442.0,1.0,1,A100,1697548676800,1697548678242,120,14.0,1.0,"[27, 1414]","[1697548676827, 1697548678241]"
3516,3516,7,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548678245,1697548683205,120,,,"[25, 2086, 1616, 91, 85, 81, 81]","[1697548678270, 1697548680356, 1697548681972, 1697548682063, 1697548682148, 1697548682229, 1697548682310]"
3517,3517,608,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661852,120,,,[24],[1697548659895]
3518,3518,529,17,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548620368,1697548622331,120,10.0,1.0,"[170, 1793]","[1697548620538, 1697548622331]"
3519,3519,886,18,[],200,llama-7b,128,1,1122.0,1.0,1,A100,1697548622335,1697548623457,120,17.0,1.0,"[39, 1083]","[1697548622374, 1697548623457]"
3520,3520,183,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605284,1697548608967,120,,,"[289, 1657, 508, 79, 80, 78]","[1697548605573, 1697548607230, 1697548607738, 1697548607817, 1697548607897, 1697548607975]"
3521,3521,319,19,[],200,llama-7b,128,1,2314.0,1.0,1,A100,1697548623460,1697548625774,120,31.0,1.0,"[6, 2308]","[1697548623466, 1697548625774]"
3522,3522,675,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548625777,1697548629597,120,,,"[34, 2127]","[1697548625811, 1697548627938]"
3523,3523,74,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631890,120,,,"[289, 1721]","[1697548629897, 1697548631618]"
3524,3524,509,29,[],200,llama-7b,128,1,3723.0,1.0,1,A100,1697548649041,1697548652764,120,286.0,3.0,"[21, 3115, 267, 320]","[1697548649062, 1697548652177, 1697548652444, 1697548652764]"
3525,3525,868,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548652768,1697548655271,120,,,"[6, 2292]","[1697548652774, 1697548655066]"
3526,3526,428,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633848,120,,,[287],[1697548632183]
3527,3527,884,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683213,1697548686709,120,,,"[59, 1753, 239, 90, 91, 89, 69, 87, 86, 68]","[1697548683272, 1697548685025, 1697548685264, 1697548685354, 1697548685445, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
3528,3528,788,23,[],200,llama-7b,128,1,1746.0,1.0,1,A100,1697548633855,1697548635601,120,31.0,1.0,"[228, 1518]","[1697548634083, 1697548635601]"
3529,3529,219,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635604,1697548639637,120,,,"[53, 851, 1224, 416]","[1697548635657, 1697548636508, 1697548637732, 1697548638148]"
3530,3530,578,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639648,1697548641377,120,,,[117],[1697548639765]
3531,3531,911,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642997,120,,,[69],[1697548641453]
3532,3532,335,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548643004,1697548655266,120,,,"[31, 537, 52, 1588, 84, 64, 955, 94, 91, 89, 68, 968, 96, 95, 94, 72, 93, 71, 892, 91, 89, 85, 731, 87, 85, 75, 74, 74, 992, 91, 74, 858, 320, 88, 70, 403, 96, 89, 85, 650, 96, 85, 63, 80]","[1697548643035, 1697548643572, 1697548643624, 1697548645212, 1697548645296, 1697548645360, 1697548646315, 1697548646409, 1697548646500, 1697548646589, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647982, 1697548648075, 1697548648146, 1697548649038, 1697548649129, 1697548649218, 1697548649303, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421, 1697548651512, 1697548651586, 1697548652444, 1697548652764, 1697548652852, 1697548652922, 1697548653325, 1697548653421, 1697548653510, 1697548653595, 1697548654245, 1697548654341, 1697548654426, 1697548654489, 1697548654569]"
3533,3533,543,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608977,1697548612997,120,,,"[397, 2013, 50]","[1697548609374, 1697548611387, 1697548611437]"
3534,3534,110,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594348,120,,,"[27, 2059]","[1697548592099, 1697548594158]"
3535,3535,283,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686728,1697548688900,120,,,"[117, 1868]","[1697548686845, 1697548688713]"
3536,3536,464,7,[],200,llama-7b,128,1,1741.0,1.0,1,A100,1697548594355,1697548596096,120,12.0,1.0,"[217, 1524]","[1697548594572, 1697548596096]"
3537,3537,643,47,[],200,llama-7b,128,1,1936.0,1.0,1,A100,1697548688910,1697548690846,120,18.0,1.0,"[246, 1690]","[1697548689156, 1697548690846]"
3538,3538,832,9,[],200,llama-7b,128,1,1933.0,1.0,1,A100,1697548599953,1697548601886,120,15.0,1.0,"[400, 1533]","[1697548600353, 1697548601886]"
3539,3539,257,10,[],200,llama-7b,128,1,784.0,1.0,1,A100,1697548601891,1697548602675,120,14.0,1.0,"[63, 721]","[1697548601954, 1697548602675]"
3540,3540,71,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690857,1697548693451,120,,,"[28, 967]","[1697548690885, 1697548691852]"
3541,3541,901,15,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548613006,1697548615031,120,17.0,1.0,"[35, 1990]","[1697548613041, 1697548615031]"
3542,3542,911,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596101,1697548597357,120,,,"[33, 1200]","[1697548596134, 1697548597334]"
3543,3543,329,16,[],200,llama-7b,128,1,837.0,1.0,1,A100,1697548615036,1697548615873,120,15.0,1.0,"[59, 778]","[1697548615095, 1697548615873]"
3544,3544,343,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597366,1697548599941,120,,,"[393, 1959]","[1697548597759, 1697548599718]"
3545,3545,659,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615883,1697548620354,120,,,"[19, 2053, 1441, 84, 82, 80, 81]","[1697548615902, 1697548617955, 1697548619396, 1697548619480, 1697548619562, 1697548619642, 1697548619723]"
3546,3546,85,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622596,120,,,"[28, 1938]","[1697548620390, 1697548622328]"
3547,3547,431,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548697835,120,,,"[389, 2874]","[1697548693853, 1697548696727]"
3548,3548,608,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548591594,1697548594347,120,,,"[62, 1195]","[1697548591656, 1697548592851]"
3549,3549,9,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597360,120,,,"[225, 1515, 250, 73, 57]","[1697548594581, 1697548596096, 1697548596346, 1697548596419, 1697548596476]"
3550,3550,289,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639645,120,,,"[51, 1546]","[1697548637753, 1697548639299]"
3551,3551,447,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624952,120,,,[171],[1697548622775]
3552,3552,363,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599941,120,,,"[408, 1943]","[1697548597775, 1697548599718]"
3553,3553,326,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629350,1697548631887,120,,,"[23, 899]","[1697548629373, 1697548630272]"
3554,3554,805,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624970,1697548627099,120,,,"[91, 1959]","[1697548625061, 1697548627020]"
3555,3555,906,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635603,1697548639636,120,,,"[24, 882, 1222, 417]","[1697548635627, 1697548636509, 1697548637731, 1697548638148]"
3556,3556,722,10,[],200,llama-7b,128,1,1934.0,1.0,1,A100,1697548599952,1697548601886,120,39.0,1.0,"[227, 1707]","[1697548600179, 1697548601886]"
3557,3557,334,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639654,1697548641375,120,,,[308],[1697548639962]
3558,3558,783,30,[],200,llama-7b,128,1,2665.0,1.0,1,A100,1697548653515,1697548656180,120,286.0,1.0,"[6, 2658]","[1697548653521, 1697548656179]"
3559,3559,694,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643000,120,,,[174],[1697548641560]
3560,3560,218,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548656182,1697548659864,120,,,[23],[1697548656205]
3561,3561,236,21,[],200,llama-7b,128,1,2200.0,1.0,1,A100,1697548627137,1697548629337,120,8.0,1.0,"[213, 1987]","[1697548627350, 1697548629337]"
3562,3562,759,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620369,1697548622594,120,,,"[220, 1743]","[1697548620589, 1697548622332]"
3563,3563,572,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661852,120,,,[31],[1697548659902]
3564,3564,184,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[49, 2025]","[1697548622652, 1697548624677]"
3565,3565,540,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627101,120,,,"[280, 1767]","[1697548625254, 1697548627021]"
3566,3566,868,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627105,1697548629598,120,,,"[40, 2192]","[1697548627145, 1697548629337]"
3567,3567,461,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631889,120,,,[244],[1697548629852]
3568,3568,2,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631890,120,,,"[51, 1965]","[1697548629653, 1697548631618]"
3569,3569,299,24,[],200,llama-7b,128,1,2014.0,1.0,1,A100,1697548629607,1697548631621,120,14.0,1.0,"[149, 1864]","[1697548629756, 1697548631620]"
3570,3570,663,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631625,1697548635895,120,,,"[60, 2200, 57, 547]","[1697548631685, 1697548633885, 1697548633942, 1697548634489]"
3571,3571,819,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631898,1697548633849,120,,,[300],[1697548632198]
3572,3572,441,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610834,120,,,[327],[1697548609302]
3573,3573,308,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671228,120,,,"[257, 2072, 472, 84, 65, 64, 80]","[1697548667635, 1697548669707, 1697548670179, 1697548670263, 1697548670328, 1697548670392, 1697548670472]"
3574,3574,101,42,[],200,llama-7b,128,1,1809.0,1.0,1,A100,1697548683215,1697548685024,120,13.0,1.0,"[223, 1586]","[1697548683438, 1697548685024]"
3575,3575,665,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674841,120,,,"[211, 1620, 539, 79, 61]","[1697548671454, 1697548673074, 1697548673613, 1697548673692, 1697548673753]"
3576,3576,466,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548686721,120,,,"[16, 1134]","[1697548685046, 1697548686180]"
3577,3577,394,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637695,120,,,[65],[1697548635981]
3578,3578,826,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686731,1697548691084,120,,,"[339, 2532]","[1697548687070, 1697548689602]"
3579,3579,254,45,[],200,llama-7b,128,1,2151.0,1.0,1,A100,1697548691095,1697548693246,120,58.0,1.0,"[42, 2109]","[1697548691137, 1697548693246]"
3580,3580,50,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624975,1697548627101,120,,,"[294, 1752]","[1697548625269, 1697548627021]"
3581,3581,877,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686722,1697548688900,120,,,"[51, 1939]","[1697548686773, 1697548688712]"
3582,3582,68,38,[],200,llama-7b,128,1,1950.0,1.0,1,A100,1697548674846,1697548676796,120,12.0,1.0,"[224, 1726]","[1697548675070, 1697548676796]"
3583,3583,280,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691086,120,,,"[62, 1876]","[1697548688969, 1697548690845]"
3584,3584,366,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637694,120,,,[244],[1697548636160]
3585,3585,639,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693451,120,,,"[276, 1875]","[1697548691372, 1697548693247]"
3586,3586,423,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548676799,1697548679648,120,,,"[27, 1416]","[1697548676826, 1697548678242]"
3587,3587,64,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695715,120,,,"[198, 1776]","[1697548693658, 1697548695434]"
3588,3588,423,47,[],200,llama-7b,128,1,2819.0,1.0,1,A100,1697548695726,1697548698545,120,84.0,20.0,"[346, 2067, 28, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 20, 19, 19, 19, 18, 19, 20]","[1697548696072, 1697548698139, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388, 1697548698411, 1697548698431, 1697548698450, 1697548698469, 1697548698488, 1697548698506, 1697548698525, 1697548698545]"
3589,3589,781,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683206,120,,,"[291, 1739, 286, 90, 86, 81, 81]","[1697548679948, 1697548681687, 1697548681973, 1697548682063, 1697548682149, 1697548682230, 1697548682311]"
3590,3590,720,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637700,1697548639635,120,,,"[75, 1524]","[1697548637775, 1697548639299]"
3591,3591,150,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641374,120,,,[46],[1697548639692]
3592,3592,479,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643001,120,,,[303],[1697548641689]
3593,3593,206,41,[],200,llama-7b,128,1,1811.0,1.0,1,A100,1697548683215,1697548685026,120,16.0,1.0,"[277, 1534]","[1697548683492, 1697548685026]"
3594,3594,834,31,[],200,llama-7b,128,1,6290.0,1.0,1,A100,1697548643013,1697548649303,120,85.0,20.0,"[254, 1511, 434, 85, 64, 953, 94, 91, 89, 69, 967, 96, 96, 94, 71, 93, 72, 891, 92, 88, 86]","[1697548643267, 1697548644778, 1697548645212, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646657, 1697548647624, 1697548647720, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303]"
3595,3595,569,42,[],200,llama-7b,128,1,2512.0,1.0,1,A100,1697548685030,1697548687542,120,16.0,1.0,"[56, 2456]","[1697548685086, 1697548687542]"
3596,3596,53,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667379,1697548671232,120,,,"[402, 1926, 471, 84, 65, 64, 80]","[1697548667781, 1697548669707, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
3597,3597,303,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657544,1697548659868,120,,,"[94, 1940]","[1697548657638, 1697548659578]"
3598,3598,446,20,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548620369,1697548622328,120,26.0,1.0,"[277, 1682]","[1697548620646, 1697548622328]"
3599,3599,743,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[64, 2010]","[1697548622667, 1697548624677]"
3600,3600,650,13,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548613007,1697548615032,120,13.0,1.0,"[74, 1951]","[1697548613081, 1697548615032]"
3601,3601,518,23,[],200,llama-7b,128,1,2009.0,1.0,1,A100,1697548629611,1697548631620,120,23.0,1.0,"[351, 1658]","[1697548629962, 1697548631620]"
3602,3602,875,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631626,1697548635901,120,,,"[72, 939, 1248, 57, 547]","[1697548631698, 1697548632637, 1697548633885, 1697548633942, 1697548634489]"
3603,3603,176,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627101,120,,,[286],[1697548625260]
3604,3604,82,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677279,1697548679650,120,,,"[297, 1898]","[1697548677576, 1697548679474]"
3605,3605,78,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615037,1697548617366,120,,,"[73, 763]","[1697548615110, 1697548615873]"
3606,3606,537,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627137,1697548629598,120,,,"[233, 1967]","[1697548627370, 1697548629337]"
3607,3607,282,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[161],[1697548636077]
3608,3608,641,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639645,120,,,"[337, 1260]","[1697548638040, 1697548639300]"
3609,3609,436,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683207,120,,,"[316, 1714, 285, 91, 85, 82, 80]","[1697548679974, 1697548681688, 1697548681973, 1697548682064, 1697548682149, 1697548682231, 1697548682311]"
3610,3610,54,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631890,120,,,"[279, 1731]","[1697548629887, 1697548631618]"
3611,3611,69,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639650,1697548641378,120,,,[188],[1697548639838]
3612,3612,719,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624952,120,,,[141],[1697548622745]
3613,3613,420,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548643003,120,,,[370],[1697548641755]
3614,3614,779,29,[],200,llama-7b,128,1,4614.0,1.0,1,A100,1697548643010,1697548647624,120,563.0,10.0,"[167, 1600, 434, 86, 64, 953, 94, 91, 89, 68, 968]","[1697548643177, 1697548644777, 1697548645211, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624]"
3615,3615,674,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655280,1697548657503,120,,,"[11, 2133]","[1697548655291, 1697548657424]"
3616,3616,179,30,[],200,llama-7b,128,1,2580.0,1.0,1,A100,1697548647627,1697548650207,120,161.0,4.0,"[7, 2305, 97, 86, 85]","[1697548647634, 1697548649939, 1697548650036, 1697548650122, 1697548650207]"
3617,3617,147,18,[],200,llama-7b,128,1,2053.0,1.0,1,A100,1697548624968,1697548627021,120,182.0,1.0,"[115, 1938]","[1697548625083, 1697548627021]"
3618,3618,540,31,[],200,llama-7b,128,1,3387.0,1.0,1,A100,1697548650210,1697548653597,120,140.0,5.0,"[34, 2983, 101, 96, 88, 85]","[1697548650244, 1697548653227, 1697548653328, 1697548653424, 1697548653512, 1697548653597]"
3619,3619,501,19,[],200,llama-7b,128,1,916.0,1.0,1,A100,1697548627023,1697548627939,120,19.0,1.0,"[25, 890]","[1697548627048, 1697548627938]"
3620,3620,103,30,[],200,llama-7b,128,1,2032.0,1.0,1,A100,1697548657547,1697548659579,120,15.0,1.0,"[327, 1704]","[1697548657874, 1697548659578]"
3621,3621,318,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624969,1697548627100,120,,,"[183, 1869]","[1697548625152, 1697548627021]"
3622,3622,797,44,[],200,llama-7b,128,1,1812.0,1.0,1,A100,1697548683212,1697548685024,120,26.0,1.0,"[15, 1797]","[1697548683227, 1697548685024]"
3623,3623,408,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633849,120,,,[150],[1697548632051]
3624,3624,463,31,[],200,llama-7b,128,1,1119.0,1.0,1,A100,1697548659583,1697548660702,120,39.0,1.0,"[83, 1036]","[1697548659666, 1697548660702]"
3625,3625,790,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548660705,1697548664209,120,,,"[18, 2029, 227]","[1697548660723, 1697548662752, 1697548662979]"
3626,3626,763,23,[],200,llama-7b,128,1,1743.0,1.0,1,A100,1697548633855,1697548635598,120,20.0,1.0,"[295, 1447]","[1697548634150, 1697548635597]"
3627,3627,903,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653600,1697548657538,120,,,"[7, 2573]","[1697548653607, 1697548656180]"
3628,3628,192,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635603,1697548639636,120,,,"[11, 894, 1223, 417]","[1697548635614, 1697548636508, 1697548637731, 1697548638148]"
3629,3629,674,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629599,120,,,"[319, 1881]","[1697548627457, 1697548629338]"
3630,3630,227,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548688902,120,,,"[51, 2461]","[1697548685081, 1697548687542]"
3631,3631,219,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667371,120,,,"[330, 1489, 430, 84, 65, 85]","[1697548664548, 1697548666037, 1697548666467, 1697548666551, 1697548666616, 1697548666701]"
3632,3632,559,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691087,120,,,"[130, 1814]","[1697548689039, 1697548690853]"
3633,3633,332,33,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548657549,1697548659580,120,39.0,1.0,"[390, 1640]","[1697548657939, 1697548659579]"
3634,3634,778,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659584,1697548664207,120,,,"[80, 1037, 1195, 58, 1027]","[1697548659664, 1697548660701, 1697548661896, 1697548661954, 1697548662981]"
3635,3635,918,47,[],200,llama-7b,128,1,2152.0,1.0,1,A100,1697548691094,1697548693246,120,23.0,1.0,"[8, 2143]","[1697548691102, 1697548693245]"
3636,3636,8,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664207,120,,,"[309, 1600]","[1697548662177, 1697548663777]"
3637,3637,99,21,[],200,llama-7b,128,1,2011.0,1.0,1,A100,1697548629608,1697548631619,120,10.0,1.0,"[334, 1677]","[1697548629942, 1697548631619]"
3638,3638,579,34,[],200,llama-7b,128,1,2331.0,1.0,1,A100,1697548667378,1697548669709,120,19.0,1.0,"[291, 2040]","[1697548667669, 1697548669709]"
3639,3639,553,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639650,1697548641379,120,,,[204],[1697548639854]
3640,3640,7,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669714,1697548671228,120,,,"[55, 1435]","[1697548669769, 1697548671204]"
3641,3641,337,36,[],200,llama-7b,128,1,3073.0,1.0,1,A100,1697548671245,1697548674318,120,12.0,1.0,"[355, 2718]","[1697548671600, 1697548674318]"
3642,3642,203,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667369,120,,,"[170, 1650, 429, 84, 65, 85]","[1697548664387, 1697548666037, 1697548666466, 1697548666550, 1697548666615, 1697548666700]"
3643,3643,428,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631624,1697548635895,120,,,"[54, 959, 1248, 57, 548]","[1697548631678, 1697548632637, 1697548633885, 1697548633942, 1697548634490]"
3644,3644,863,20,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548627942,1697548630272,120,10.0,1.0,"[30, 2300]","[1697548627972, 1697548630272]"
3645,3645,695,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674321,1697548677255,120,,,"[11, 1141]","[1697548674332, 1697548675473]"
3646,3646,787,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637693,120,,,[224],[1697548636140]
3647,3647,556,36,[],200,llama-7b,128,1,2329.0,1.0,1,A100,1697548667377,1697548669706,120,9.0,1.0,"[97, 2232]","[1697548667474, 1697548669706]"
3648,3648,916,37,[],200,llama-7b,128,1,1490.0,1.0,1,A100,1697548669713,1697548671203,120,8.0,1.0,"[16, 1473]","[1697548669729, 1697548671202]"
3649,3649,263,21,[],200,llama-7b,128,1,2360.0,1.0,1,A100,1697548630276,1697548632636,120,15.0,1.0,"[23, 2336]","[1697548630299, 1697548632635]"
3650,3650,884,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548643001,120,,,[191],[1697548641575]
3651,3651,348,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671206,1697548674837,120,,,"[24, 1844, 539, 79, 61]","[1697548671230, 1697548673074, 1697548673613, 1697548673692, 1697548673753]"
3652,3652,223,24,[],200,llama-7b,128,1,1596.0,1.0,1,A100,1697548637703,1697548639299,120,16.0,1.0,"[255, 1341]","[1697548637958, 1697548639299]"
3653,3653,316,27,[],200,llama-7b,128,1,7193.0,1.0,1,A100,1697548643014,1697548650207,120,86.0,20.0,"[323, 2400, 578, 94, 91, 88, 69, 968, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86, 732, 87, 85]","[1697548643337, 1697548645737, 1697548646315, 1697548646409, 1697548646500, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650035, 1697548650122, 1697548650207]"
3654,3654,622,22,[],200,llama-7b,128,1,1792.0,1.0,1,A100,1697548632639,1697548634431,120,20.0,1.0,"[6, 1786]","[1697548632645, 1697548634431]"
3655,3655,680,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677256,120,,,"[132, 1820]","[1697548674977, 1697548676797]"
3656,3656,55,23,[],200,llama-7b,128,1,1165.0,1.0,1,A100,1697548634435,1697548635600,120,12.0,1.0,"[39, 1125]","[1697548634474, 1697548635599]"
3657,3657,297,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657536,120,,,"[59, 2085]","[1697548655340, 1697548657425]"
3658,3658,111,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679648,120,,,[188],[1697548677457]
3659,3659,465,41,[],200,llama-7b,128,1,2406.0,1.0,1,A100,1697548679657,1697548682063,120,364.0,3.0,"[222, 1811, 283, 90]","[1697548679879, 1697548681690, 1697548681973, 1697548682063]"
3660,3660,124,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677268,1697548679650,120,,,[29],[1697548677297]
3661,3661,824,42,[],200,llama-7b,128,1,3378.0,1.0,1,A100,1697548682066,1697548685444,120,58.0,4.0,"[6, 1972, 1218, 91, 90]","[1697548682072, 1697548684044, 1697548685262, 1697548685353, 1697548685443]"
3662,3662,483,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683206,120,,,"[301, 1728, 286, 91, 85, 82, 80]","[1697548679959, 1697548681687, 1697548681973, 1697548682064, 1697548682149, 1697548682231, 1697548682311]"
3663,3663,629,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657549,1697548659866,120,,,"[345, 1685]","[1697548657894, 1697548659579]"
3664,3664,56,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661854,120,,,[290],[1697548660167]
3665,3665,840,40,[],200,llama-7b,128,1,1812.0,1.0,1,A100,1697548683213,1697548685025,120,17.0,1.0,"[34, 1777]","[1697548683247, 1697548685024]"
3666,3666,414,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664206,120,,,"[277, 1634]","[1697548662145, 1697548663779]"
3667,3667,249,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685447,1697548688904,120,,,"[6, 2089]","[1697548685453, 1697548687542]"
3668,3668,773,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667369,120,,,"[237, 1581, 430, 84, 65, 85]","[1697548664455, 1697548666036, 1697548666466, 1697548666550, 1697548666615, 1697548666700]"
3669,3669,239,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548686723,120,,,"[11, 1139]","[1697548685041, 1697548686180]"
3670,3670,755,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605280,1697548608965,120,,,"[196, 1757, 506, 79, 78, 79]","[1697548605476, 1697548607233, 1697548607739, 1697548607818, 1697548607896, 1697548607975]"
3671,3671,599,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686732,1697548691085,120,,,"[343, 2527]","[1697548687075, 1697548689602]"
3672,3672,670,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548650209,1697548655267,120,,,"[16, 3002, 101, 95, 89, 84, 651, 96, 84, 62, 80]","[1697548650225, 1697548653227, 1697548653328, 1697548653423, 1697548653512, 1697548653596, 1697548654247, 1697548654343, 1697548654427, 1697548654489, 1697548654569]"
3673,3673,580,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688910,1697548691088,120,,,"[260, 1683]","[1697548689170, 1697548690853]"
3674,3674,11,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693449,120,,,"[152, 2000]","[1697548691248, 1697548693248]"
3675,3675,198,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667377,1697548671226,120,,,"[80, 568, 35, 2118, 84, 65, 64, 80]","[1697548667457, 1697548668025, 1697548668060, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
3676,3676,580,25,[],200,llama-7b,128,1,8842.0,1.0,1,A100,1697548639303,1697548648145,120,88.0,20.0,"[30, 850, 1231, 365, 1261, 583, 1587, 85, 65, 953, 94, 91, 89, 68, 969, 96, 95, 94, 71, 94, 71]","[1697548639333, 1697548640183, 1697548641414, 1697548641779, 1697548643040, 1697548643623, 1697548645210, 1697548645295, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646587, 1697548646655, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647980, 1697548648074, 1697548648145]"
3677,3677,372,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695718,120,,,"[85, 1888]","[1697548693544, 1697548695432]"
3678,3678,726,47,[],200,llama-7b,128,1,3178.0,1.0,1,A100,1697548695727,1697548698905,120,67.0,47.0,"[346, 2066, 28, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 20, 19, 19, 19, 19, 18, 20, 14, 13, 13, 14, 13, 13, 14, 13, 13, 14, 13, 13, 14, 13, 13, 13, 14, 13, 13, 14, 13, 13, 14, 13, 13, 13, 14]","[1697548696073, 1697548698139, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388, 1697548698411, 1697548698431, 1697548698450, 1697548698469, 1697548698488, 1697548698507, 1697548698525, 1697548698545, 1697548698559, 1697548698572, 1697548698585, 1697548698599, 1697548698612, 1697548698625, 1697548698639, 1697548698652, 1697548698665, 1697548698679, 1697548698692, 1697548698705, 1697548698719, 1697548698732, 1697548698745, 1697548698758, 1697548698772, 1697548698785, 1697548698798, 1697548698812, 1697548698825, 1697548698838, 1697548698852, 1697548698865, 1697548698878, 1697548698891, 1697548698905]"
3679,3679,24,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691095,1697548693453,120,,,[33],[1697548691128]
3680,3680,100,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655283,1697548657502,120,,,"[268, 1875]","[1697548655551, 1697548657426]"
3681,3681,458,30,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548657544,1697548659577,120,11.0,1.0,"[38, 1995]","[1697548657582, 1697548659577]"
3682,3682,387,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693463,1697548695716,120,,,"[283, 1686]","[1697548693746, 1697548695432]"
3683,3683,616,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602683,1697548605264,120,,,"[58, 2328]","[1697548602741, 1697548605069]"
3684,3684,183,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610823,120,,,[207],[1697548609181]
3685,3685,786,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664206,120,,,"[57, 1062, 1195, 57, 1027]","[1697548659639, 1697548660701, 1697548661896, 1697548661953, 1697548662980]"
3686,3686,745,45,[],200,llama-7b,128,1,1958.0,1.0,1,A100,1697548695725,1697548697683,120,17.0,1.0,"[213, 1745]","[1697548695938, 1697548697683]"
3687,3687,529,37,[],200,llama-7b,128,1,3072.0,1.0,1,A100,1697548671247,1697548674319,120,10.0,1.0,"[411, 2661]","[1697548671658, 1697548674319]"
3688,3688,12,12,[],200,llama-7b,128,1,1950.0,1.0,1,A100,1697548605280,1697548607230,120,11.0,1.0,"[72, 1878]","[1697548605352, 1697548607230]"
3689,3689,512,13,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610848,1697548612815,120,11.0,1.0,"[39, 1928]","[1697548610887, 1697548612815]"
3690,3690,872,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612820,1697548615235,120,,,"[37, 681]","[1697548612857, 1697548613538]"
3691,3691,8,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686723,1697548688901,120,,,"[142, 1849]","[1697548686865, 1697548688714]"
3692,3692,588,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635917,1697548637696,120,,,[283],[1697548636200]
3693,3693,358,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691086,120,,,"[76, 1861]","[1697548688985, 1697548690846]"
3694,3694,851,23,[],200,llama-7b,128,1,579.0,1.0,1,A100,1697548633853,1697548634432,120,23.0,1.0,"[30, 549]","[1697548633883, 1697548634432]"
3695,3695,304,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617369,120,,,"[395, 1627]","[1697548615639, 1697548617266]"
3696,3696,20,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639645,120,,,"[46, 1551]","[1697548637748, 1697548639299]"
3697,3697,618,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639653,1697548641374,120,,,[288],[1697548639941]
3698,3698,717,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693452,120,,,"[348, 1804]","[1697548691444, 1697548693248]"
3699,3699,687,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633846,120,,,[76],[1697548631971]
3700,3700,52,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642999,120,,,[283],[1697548641669]
3701,3701,279,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548634435,1697548635903,120,,,"[19, 1145]","[1697548634454, 1697548635599]"
3702,3702,117,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695714,120,,,"[178, 1795]","[1697548693638, 1697548695433]"
3703,3703,406,32,[],200,llama-7b,128,1,2353.0,1.0,1,A100,1697548643008,1697548645361,120,244.0,4.0,"[159, 1611, 433, 86, 64]","[1697548643167, 1697548644778, 1697548645211, 1697548645297, 1697548645361]"
3704,3704,630,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637696,120,,,[137],[1697548636052]
3705,3705,481,47,[],200,llama-7b,128,1,1958.0,1.0,1,A100,1697548695725,1697548697683,120,10.0,1.0,"[223, 1735]","[1697548695948, 1697548697683]"
3706,3706,113,26,[],200,llama-7b,128,1,1743.0,1.0,1,A100,1697548633856,1697548635599,120,13.0,1.0,"[325, 1417]","[1697548634181, 1697548635598]"
3707,3707,558,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635603,1697548639636,120,,,"[26, 880, 1222, 417]","[1697548635629, 1697548636509, 1697548637731, 1697548638148]"
3708,3708,912,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641379,120,,,[21],[1697548639667]
3709,3709,60,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639638,120,,,[246],[1697548637949]
3710,3710,418,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639656,1697548641378,120,,,[398],[1697548640054]
3711,3711,346,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642999,120,,,[164],[1697548641550]
3712,3712,621,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581089,1697548583923,120,,,"[250, 1469, 109, 102, 89, 88, 87, 84]","[1697548581339, 1697548582808, 1697548582917, 1697548583019, 1697548583108, 1697548583196, 1697548583283, 1697548583367]"
3713,3713,703,30,[],200,llama-7b,128,1,568.0,1.0,1,A100,1697548643004,1697548643572,120,12.0,1.0,"[30, 538]","[1697548643034, 1697548643572]"
3714,3714,136,41,[],200,llama-7b,128,1,1679.0,1.0,1,A100,1697548668031,1697548669710,120,31.0,1.0,"[14, 1665]","[1697548668045, 1697548669710]"
3715,3715,136,31,[],200,llama-7b,128,1,3499.0,1.0,1,A100,1697548643576,1697548647075,120,31.0,1.0,"[15, 3484]","[1697548643591, 1697548647075]"
3716,3716,763,33,[],200,llama-7b,128,1,3073.0,1.0,1,A100,1697548645365,1697548648438,120,20.0,1.0,"[17, 3056]","[1697548645382, 1697548648438]"
3717,3717,748,28,[],200,llama-7b,128,1,6337.0,1.0,1,A100,1697548641384,1697548647721,120,182.0,14.0,"[17, 378, 1261, 584, 1587, 85, 64, 954, 94, 91, 89, 68, 968, 97]","[1697548641401, 1697548641779, 1697548643040, 1697548643624, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647721]"
3718,3718,466,32,[],200,llama-7b,128,1,7266.0,1.0,1,A100,1697548647077,1697548654343,120,457.0,20.0,"[15, 2846, 97, 86, 86, 74, 74, 75, 991, 91, 74, 860, 319, 88, 70, 403, 98, 88, 83, 651, 97]","[1697548647092, 1697548649938, 1697548650035, 1697548650121, 1697548650207, 1697548650281, 1697548650355, 1697548650430, 1697548651421, 1697548651512, 1697548651586, 1697548652446, 1697548652765, 1697548652853, 1697548652923, 1697548653326, 1697548653424, 1697548653512, 1697548653595, 1697548654246, 1697548654343]"
3719,3719,493,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669715,1697548671229,120,,,"[104, 1385]","[1697548669819, 1697548671204]"
3720,3720,853,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674840,120,,,"[131, 1707, 538, 79, 60]","[1697548671369, 1697548673076, 1697548673614, 1697548673693, 1697548673753]"
3721,3721,679,22,[],200,llama-7b,128,1,922.0,1.0,1,A100,1697548629350,1697548630272,120,15.0,1.0,"[38, 884]","[1697548629388, 1697548630272]"
3722,3722,108,23,[],200,llama-7b,128,1,3610.0,1.0,1,A100,1697548630275,1697548633885,120,182.0,2.0,"[6, 2354, 1250]","[1697548630281, 1697548632635, 1697548633885]"
3723,3723,365,8,[],200,llama-7b,128,1,1742.0,1.0,1,A100,1697548594358,1697548596100,120,23.0,1.0,"[390, 1352]","[1697548594748, 1697548596100]"
3724,3724,462,24,[],200,llama-7b,128,1,1711.0,1.0,1,A100,1697548633888,1697548635599,120,52.0,1.0,"[351, 1360]","[1697548634239, 1697548635599]"
3725,3725,49,3,[],200,llama-7b,128,1,4535.0,1.0,1,A100,1697548583933,1697548588468,120,109.0,3.0,"[409, 2872, 1175, 79]","[1697548584342, 1697548587214, 1697548588389, 1697548588468]"
3726,3726,405,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588471,1697548592064,120,,,"[13, 1674]","[1697548588484, 1697548590158]"
3727,3727,722,9,[],200,llama-7b,128,1,1232.0,1.0,1,A100,1697548596102,1697548597334,120,39.0,1.0,"[82, 1150]","[1697548596184, 1697548597334]"
3728,3728,127,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597337,1697548599939,120,,,"[26, 581, 36]","[1697548597363, 1697548597944, 1697548597980]"
3729,3729,541,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592065,120,,,"[275, 1739]","[1697548589849, 1697548591588]"
3730,3730,737,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[170, 1918]","[1697548592243, 1697548594161]"
3731,3731,482,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602106,120,,,"[196, 1738]","[1697548600147, 1697548601885]"
3732,3732,900,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592074,1697548594348,120,,,"[280, 1805]","[1697548592354, 1697548594159]"
3733,3733,252,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679645,120,,,"[317, 3076]","[1697548675165, 1697548678241]"
3734,3734,822,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635604,1697548639637,120,,,"[55, 850, 1222, 417]","[1697548635659, 1697548636509, 1697548637731, 1697548638148]"
3735,3735,254,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639647,1697548641375,120,,,[90],[1697548639737]
3736,3736,42,19,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548620371,1697548622330,120,10.0,1.0,"[323, 1635]","[1697548620694, 1697548622329]"
3737,3737,246,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635903,120,,,"[301, 1442]","[1697548634156, 1697548635598]"
3738,3738,167,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594351,1697548597359,120,,,"[123, 1623, 251, 72, 56]","[1697548594474, 1697548596097, 1697548596348, 1697548596420, 1697548596476]"
3739,3739,840,12,[],200,llama-7b,128,1,1726.0,1.0,1,A100,1697548602127,1697548603853,120,17.0,1.0,"[137, 1589]","[1697548602264, 1697548603853]"
3740,3740,265,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603857,1697548605264,120,,,"[14, 1198]","[1697548603871, 1697548605069]"
3741,3741,626,14,[],200,llama-7b,128,1,1952.0,1.0,1,A100,1697548605280,1697548607232,120,10.0,1.0,"[359, 1592]","[1697548605639, 1697548607231]"
3742,3742,27,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607239,1697548608968,120,,,"[123, 1409]","[1697548607362, 1697548608771]"
3743,3743,396,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624951,120,,,"[22, 1099]","[1697548622357, 1697548623456]"
3744,3744,385,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610833,120,,,[389],[1697548609364]
3745,3745,743,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610847,1697548612996,120,,,"[214, 1757]","[1697548611061, 1697548612818]"
3746,3746,173,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613009,1697548615234,120,,,"[146, 1878]","[1697548613155, 1697548615033]"
3747,3747,528,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599941,120,,,"[407, 1944]","[1697548597774, 1697548599718]"
3748,3748,531,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617366,120,,,"[240, 1782]","[1697548615483, 1697548617265]"
3749,3749,606,45,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548679657,1697548681688,120,9.0,1.0,"[93, 1938]","[1697548679750, 1697548681688]"
3750,3750,758,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624968,1697548627103,120,,,"[34, 2018]","[1697548625002, 1697548627020]"
3751,3751,39,46,[],200,llama-7b,128,1,1211.0,1.0,1,A100,1697548681698,1697548682909,120,8.0,1.0,"[15, 1196]","[1697548681713, 1697548682909]"
3752,3752,882,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[308, 1626]","[1697548600260, 1697548601886]"
3753,3753,188,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627139,1697548629599,120,,,"[343, 1857]","[1697548627482, 1697548629339]"
3754,3754,312,9,[],200,llama-7b,128,1,1724.0,1.0,1,A100,1697548602129,1697548603853,120,23.0,1.0,"[257, 1466]","[1697548602386, 1697548603852]"
3755,3755,302,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589575,1697548592064,120,,,"[188, 1828]","[1697548589763, 1697548591591]"
3756,3756,397,47,[],200,llama-7b,128,1,2351.0,1.0,1,A100,1697548682912,1697548685263,120,67.0,2.0,"[14, 1118, 1219]","[1697548682926, 1697548684044, 1697548685263]"
3757,3757,549,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631884,120,,,"[349, 1662]","[1697548629957, 1697548631619]"
3758,3758,357,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633844,120,,,[11],[1697548631906]
3759,3759,661,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[198, 1890]","[1697548592271, 1697548594161]"
3760,3760,879,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631902,1697548633847,120,,,[271],[1697548632173]
3761,3761,928,6,[],200,llama-7b,128,1,774.0,1.0,1,A100,1697548589386,1697548590160,120,20.0,1.0,"[55, 719]","[1697548589441, 1697548590160]"
3762,3762,691,44,[],200,llama-7b,128,1,1937.0,1.0,1,A100,1697548688908,1697548690845,120,47.0,1.0,"[66, 1871]","[1697548688974, 1697548690845]"
3763,3763,641,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548603857,1697548605265,120,,,"[45, 1168]","[1697548603902, 1697548605070]"
3764,3764,122,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690859,1697548693451,120,,,"[31, 962]","[1697548690890, 1697548691852]"
3765,3765,715,25,[],200,llama-7b,128,1,578.0,1.0,1,A100,1697548633854,1697548634432,120,20.0,1.0,"[12, 566]","[1697548633866, 1697548634432]"
3766,3766,118,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548634435,1697548635904,120,,,"[9, 1155]","[1697548634444, 1697548635599]"
3767,3767,86,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594355,1697548597360,120,,,"[286, 1456, 249, 73, 58]","[1697548594641, 1697548596097, 1697548596346, 1697548596419, 1697548596477]"
3768,3768,305,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635904,120,,,"[44, 533, 59]","[1697548633898, 1697548634431, 1697548634490]"
3769,3769,247,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548612998,120,,,"[290, 1673]","[1697548611142, 1697548612815]"
3770,3770,481,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637696,120,,,[145],[1697548636061]
3771,3771,475,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695714,120,,,"[193, 1781]","[1697548693653, 1697548695434]"
3772,3772,188,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629350,1697548631887,120,,,"[24, 898]","[1697548629374, 1697548630272]"
3773,3773,835,47,[],200,llama-7b,128,1,2819.0,1.0,1,A100,1697548695726,1697548698545,120,87.0,20.0,"[312, 2101, 28, 22, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 19, 19, 19, 18, 19, 20]","[1697548696038, 1697548698139, 1697548698167, 1697548698189, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388, 1697548698410, 1697548698431, 1697548698450, 1697548698469, 1697548698488, 1697548698506, 1697548698525, 1697548698545]"
3774,3774,447,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597368,1697548599941,120,,,[401],[1697548597769]
3775,3775,374,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602128,1697548605266,120,,,"[218, 1505, 263, 79, 75]","[1697548602346, 1697548603851, 1697548604114, 1697548604193, 1697548604268]"
3776,3776,659,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637694,120,,,[249],[1697548636165]
3777,3777,838,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639639,120,,,"[251, 1345]","[1697548637954, 1697548639299]"
3778,3778,734,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608963,120,,,"[152, 1796, 507, 80, 79, 78]","[1697548605435, 1697548607231, 1697548607738, 1697548607818, 1697548607897, 1697548607975]"
3779,3779,546,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633846,120,,,[70],[1697548631966]
3780,3780,263,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639648,1697548641378,120,,,[137],[1697548639785]
3781,3781,876,24,[],200,llama-7b,128,1,578.0,1.0,1,A100,1697548633854,1697548634432,120,11.0,1.0,"[6, 572]","[1697548633860, 1697548634432]"
3782,3782,623,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642998,120,,,[151],[1697548641535]
3783,3783,19,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548643006,1697548655266,120,,,"[141, 1631, 433, 85, 65, 952, 94, 91, 90, 68, 970, 94, 97, 93, 72, 93, 70, 893, 92, 88, 86, 730, 87, 85, 75, 74, 74, 992, 91, 74, 858, 320, 88, 70, 403, 96, 90, 84, 650, 96, 85, 63, 80]","[1697548643147, 1697548644778, 1697548645211, 1697548645296, 1697548645361, 1697548646313, 1697548646407, 1697548646498, 1697548646588, 1697548646656, 1697548647626, 1697548647720, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648145, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421, 1697548651512, 1697548651586, 1697548652444, 1697548652764, 1697548652852, 1697548652922, 1697548653325, 1697548653421, 1697548653511, 1697548653595, 1697548654245, 1697548654341, 1697548654426, 1697548654489, 1697548654569]"
3784,3784,305,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548634435,1697548635904,120,,,"[34, 1130]","[1697548634469, 1697548635599]"
3785,3785,166,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610833,120,,,[105],[1697548609078]
3786,3786,494,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610853,1697548613000,120,,,"[382, 1582]","[1697548611235, 1697548612817]"
3787,3787,663,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637696,120,,,[331],[1697548636247]
3788,3788,899,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.00 GiB is free. Process 1412106 has 33.39 GiB memory in use. Of the allocated memory 23.20 GiB is allocated by PyTorch, and 8.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548581090,1697548583922,120,,,"[187, 1534, 108, 100, 91, 86, 87, 84]","[1697548581277, 1697548582811, 1697548582919, 1697548583019, 1697548583110, 1697548583196, 1697548583283, 1697548583367]"
3789,3789,92,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639646,120,,,"[58, 1539]","[1697548637760, 1697548639299]"
3790,3790,450,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641376,120,,,[333],[1697548639988]
3791,3791,780,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642999,120,,,[274],[1697548641659]
3792,3792,323,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583932,1697548586198,120,,,"[228, 1796]","[1697548584160, 1697548585956]"
3793,3793,679,5,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548586210,1697548588173,120,15.0,1.0,"[332, 1631]","[1697548586542, 1697548588173]"
3794,3794,748,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637701,1697548639650,120,,,"[7, 441]","[1697548637708, 1697548638149]"
3795,3795,178,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639661,1697548641378,120,,,[398],[1697548640059]
3796,3796,531,29,[],200,llama-7b,128,1,7653.0,1.0,1,A100,1697548641385,1697548649038,120,52.0,20.0,"[21, 331, 42, 1261, 584, 1587, 85, 64, 954, 94, 91, 89, 68, 968, 97, 95, 93, 72, 93, 72, 891]","[1697548641406, 1697548641737, 1697548641779, 1697548643040, 1697548643624, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647721, 1697548647816, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649037]"
3797,3797,414,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671242,1697548674841,120,,,"[212, 1621, 538, 79, 61]","[1697548671454, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
3798,3798,45,22,[],200,llama-7b,128,1,2075.0,1.0,1,A100,1697548622604,1697548624679,120,19.0,1.0,"[157, 1917]","[1697548622761, 1697548624678]"
3799,3799,750,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674837,120,,,"[236, 1596, 538, 80, 60]","[1697548671479, 1697548673075, 1697548673613, 1697548673693, 1697548673753]"
3800,3800,773,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674846,1697548677259,120,,,"[244, 1706]","[1697548675090, 1697548676796]"
3801,3801,176,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679646,120,,,[91],[1697548677360]
3802,3802,530,37,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679657,1697548681687,120,26.0,1.0,"[78, 1952]","[1697548679735, 1697548681687]"
3803,3803,594,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548647726,1697548655268,120,,,"[14, 3129, 561, 83, 74, 857, 319, 88, 70, 406, 97, 88, 84, 651, 97, 83, 62, 80]","[1697548647740, 1697548650869, 1697548651430, 1697548651513, 1697548651587, 1697548652444, 1697548652763, 1697548652851, 1697548652921, 1697548653327, 1697548653424, 1697548653512, 1697548653596, 1697548654247, 1697548654344, 1697548654427, 1697548654489, 1697548654569]"
3804,3804,889,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548686704,120,,,"[60, 2285, 1218, 92, 90, 89, 70, 87, 86, 68]","[1697548681758, 1697548684043, 1697548685261, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
3805,3805,675,12,[],200,llama-7b,128,1,2692.0,1.0,1,A100,1697548605283,1697548607975,120,563.0,5.0,"[187, 1762, 507, 79, 77, 80]","[1697548605470, 1697548607232, 1697548607739, 1697548607818, 1697548607895, 1697548607975]"
3806,3806,104,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607977,1697548612996,120,,,"[6, 1642, 1243, 569]","[1697548607983, 1697548609625, 1697548610868, 1697548611437]"
3807,3807,445,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599939,120,,,"[297, 2053]","[1697548597664, 1697548599717]"
3808,3808,318,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686721,1697548688899,120,,,"[43, 1948]","[1697548686764, 1697548688712]"
3809,3809,370,39,[],200,llama-7b,128,1,1838.0,1.0,1,A100,1697548671238,1697548673076,120,31.0,1.0,"[43, 1795]","[1697548671281, 1697548673076]"
3810,3810,6,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548651517,1697548655269,120,,,"[24, 2707, 96, 83, 63, 80]","[1697548651541, 1697548654248, 1697548654344, 1697548654427, 1697548654490, 1697548654570]"
3811,3811,731,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673081,1697548674843,120,,,"[78, 1160]","[1697548673159, 1697548674319]"
3812,3812,798,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599944,120,,,[200],[1697548597567]
3813,3813,363,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657535,120,,,"[33, 2111]","[1697548655314, 1697548657425]"
3814,3814,674,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[18, 1919]","[1697548688925, 1697548690844]"
3815,3815,229,8,[],200,llama-7b,128,1,1935.0,1.0,1,A100,1697548599953,1697548601888,120,15.0,1.0,"[449, 1485]","[1697548600402, 1697548601887]"
3816,3816,440,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620356,120,,,"[17, 1889, 113, 83, 83, 80, 81]","[1697548617395, 1697548619284, 1697548619397, 1697548619480, 1697548619563, 1697548619643, 1697548619724]"
3817,3817,717,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657544,1697548659868,120,,,"[43, 1990]","[1697548657587, 1697548659577]"
3818,3818,77,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691095,1697548693452,120,,,"[23, 2128]","[1697548691118, 1697548693246]"
3819,3819,504,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693451,120,,,"[281, 1870]","[1697548691377, 1697548693247]"
3820,3820,861,48,[],200,llama-7b,128,1,1974.0,1.0,1,A100,1697548693459,1697548695433,120,10.0,1.0,"[106, 1868]","[1697548693565, 1697548695433]"
3821,3821,291,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695439,1697548697836,120,,,[63],[1697548695502]
3822,3822,880,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592072,1697548594348,120,,,"[63, 2024]","[1697548592135, 1697548594159]"
3823,3823,151,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661855,120,,,[300],[1697548660177]
3824,3824,636,29,[],200,llama-7b,128,1,1789.0,1.0,1,A100,1697548648150,1697548649939,120,31.0,1.0,"[19, 1770]","[1697548648169, 1697548649939]"
3825,3825,343,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693252,1697548695715,120,,,"[22, 870]","[1697548693274, 1697548694144]"
3826,3826,509,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664204,120,,,"[100, 1809]","[1697548661968, 1697548663777]"
3827,3827,704,10,[],200,llama-7b,128,1,1933.0,1.0,1,A100,1697548599953,1697548601886,120,14.0,1.0,"[419, 1514]","[1697548600372, 1697548601886]"
3828,3828,310,6,[],200,llama-7b,128,1,1741.0,1.0,1,A100,1697548594356,1697548596097,120,26.0,1.0,"[280, 1461]","[1697548594636, 1697548596097]"
3829,3829,66,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649942,1697548655272,120,,,"[16, 2221, 267, 320, 88, 69, 404, 96, 88, 85, 650, 96, 84, 63, 80]","[1697548649958, 1697548652179, 1697548652446, 1697548652766, 1697548652854, 1697548652923, 1697548653327, 1697548653423, 1697548653511, 1697548653596, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
3830,3830,839,38,[],200,llama-7b,128,1,2483.0,1.0,1,A100,1697548664219,1697548666702,120,58.0,5.0,"[367, 1453, 428, 85, 65, 85]","[1697548664586, 1697548666039, 1697548666467, 1697548666552, 1697548666617, 1697548666702]"
3831,3831,135,11,[],200,llama-7b,128,1,2225.0,1.0,1,A100,1697548601889,1697548604114,120,52.0,2.0,"[31, 755, 1439]","[1697548601920, 1697548602675, 1697548604114]"
3832,3832,463,12,[],200,llama-7b,128,1,952.0,1.0,1,A100,1697548604118,1697548605070,120,39.0,1.0,"[14, 938]","[1697548604132, 1697548605070]"
3833,3833,664,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596102,1697548597358,120,,,"[60, 1173]","[1697548596162, 1697548597335]"
3834,3834,271,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666704,1697548671225,120,,,"[21, 747, 588, 2117, 85, 64, 64, 80]","[1697548666725, 1697548667472, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670390, 1697548670470]"
3835,3835,89,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599939,120,,,"[302, 2048]","[1697548597669, 1697548599717]"
3836,3836,814,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605073,1697548608961,120,,,"[32, 633, 1999, 80, 79, 78]","[1697548605105, 1697548605738, 1697548607737, 1697548607817, 1697548607896, 1697548607974]"
3837,3837,424,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655282,1697548657501,120,,,"[201, 1942]","[1697548655483, 1697548657425]"
3838,3838,448,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599950,1697548602108,120,,,"[14, 1919]","[1697548599964, 1697548601883]"
3839,3839,243,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610823,120,,,[17],[1697548608990]
3840,3840,788,32,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548657544,1697548659577,120,31.0,1.0,"[28, 2005]","[1697548657572, 1697548659577]"
3841,3841,780,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605266,120,,,"[227, 1495, 263, 78, 76]","[1697548602356, 1697548603851, 1697548604114, 1697548604192, 1697548604268]"
3842,3842,305,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664205,120,,,"[34, 1085, 1195, 57, 1027]","[1697548659616, 1697548660701, 1697548661896, 1697548661953, 1697548662980]"
3843,3843,601,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610846,1697548612997,120,,,"[7, 534, 50]","[1697548610853, 1697548611387, 1697548611437]"
3844,3844,37,16,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548613007,1697548615032,120,20.0,1.0,"[69, 1956]","[1697548613076, 1697548615032]"
3845,3845,214,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605285,1697548608963,120,,,"[379, 1568, 507, 80, 79, 78]","[1697548605664, 1697548607232, 1697548607739, 1697548607819, 1697548607898, 1697548607976]"
3846,3846,367,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615036,1697548617367,120,,,"[64, 773]","[1697548615100, 1697548615873]"
3847,3847,625,40,[],200,llama-7b,128,1,2376.0,1.0,1,A100,1697548671237,1697548673613,120,364.0,2.0,"[24, 1814, 538]","[1697548671261, 1697548673075, 1697548673613]"
3848,3848,194,34,[],200,llama-7b,128,1,6049.0,1.0,1,A100,1697548648441,1697548654490,120,335.0,16.0,"[20, 2408, 562, 82, 74, 857, 320, 88, 69, 407, 97, 87, 84, 651, 97, 83, 63]","[1697548648461, 1697548650869, 1697548651431, 1697548651513, 1697548651587, 1697548652444, 1697548652764, 1697548652852, 1697548652921, 1697548653328, 1697548653425, 1697548653512, 1697548653596, 1697548654247, 1697548654344, 1697548654427, 1697548654490]"
3849,3849,381,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639649,1697548641377,120,,,[126],[1697548639775]
3850,3850,372,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607236,1697548608966,120,,,"[27, 1507]","[1697548607263, 1697548608770]"
3851,3851,155,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601890,1697548605272,120,,,"[29, 756, 1438, 78, 76]","[1697548601919, 1697548602675, 1697548604113, 1697548604191, 1697548604267]"
3852,3852,578,7,[],200,llama-7b,128,1,2349.0,1.0,1,A100,1697548597368,1697548599717,120,31.0,1.0,"[428, 1921]","[1697548597796, 1697548599717]"
3853,3853,732,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610824,120,,,[217],[1697548609191]
3854,3854,726,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548623460,1697548627100,120,,,"[21, 2293]","[1697548623481, 1697548625774]"
3855,3855,159,20,[],200,llama-7b,128,1,2200.0,1.0,1,A100,1697548627138,1697548629338,120,31.0,1.0,"[320, 1880]","[1697548627458, 1697548629338]"
3856,3856,483,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605284,1697548608966,120,,,"[277, 1669, 510, 77, 80, 77]","[1697548605561, 1697548607230, 1697548607740, 1697548607817, 1697548607897, 1697548607974]"
3857,3857,909,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599720,1697548602106,120,,,"[29, 791]","[1697548599749, 1697548600540]"
3858,3858,165,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610849,1697548612999,120,,,"[114, 1853]","[1697548610963, 1697548612816]"
3859,3859,846,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608972,1697548610823,120,,,[7],[1697548608979]
3860,3860,495,16,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548613009,1697548615034,120,13.0,1.0,"[156, 1868]","[1697548613165, 1697548615033]"
3861,3861,637,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548654493,1697548657502,120,,,"[6, 1681]","[1697548654499, 1697548656180]"
3862,3862,271,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610846,1697548612997,120,,,"[13, 529, 49]","[1697548610859, 1697548611388, 1697548611437]"
3863,3863,337,9,[],200,llama-7b,128,1,1725.0,1.0,1,A100,1697548602127,1697548603852,120,12.0,1.0,"[33, 1692]","[1697548602160, 1697548603852]"
3864,3864,66,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657548,1697548659865,120,,,"[322, 1708]","[1697548657870, 1697548659578]"
3865,3865,518,21,[],200,llama-7b,128,1,920.0,1.0,1,A100,1697548629353,1697548630273,120,23.0,1.0,"[60, 860]","[1697548629413, 1697548630273]"
3866,3866,849,17,[],200,llama-7b,128,1,836.0,1.0,1,A100,1697548615038,1697548615874,120,10.0,1.0,"[106, 730]","[1697548615144, 1697548615874]"
3867,3867,691,10,[],200,llama-7b,128,1,1213.0,1.0,1,A100,1697548603857,1697548605070,120,47.0,1.0,"[55, 1158]","[1697548603912, 1697548605070]"
3868,3868,425,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661853,120,,,[65],[1697548659936]
3869,3869,118,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605071,1697548608961,120,,,"[14, 653, 1999, 80, 78, 78]","[1697548605085, 1697548605738, 1697548607737, 1697548607817, 1697548607895, 1697548607973]"
3870,3870,278,18,[],200,llama-7b,128,1,2078.0,1.0,1,A100,1697548615878,1697548617956,120,13.0,1.0,"[23, 2055]","[1697548615901, 1697548617956]"
3871,3871,628,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613008,1697548615237,120,,,[246],[1697548613254]
3872,3872,194,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685607,1697548688904,120,,,"[19, 1917]","[1697548685626, 1697548687543]"
3873,3873,633,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617959,1697548620357,120,,,"[20, 2343]","[1697548617979, 1697548620322]"
3874,3874,476,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610825,120,,,[68],[1697548609041]
3875,3875,681,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610836,120,,,[384],[1697548609359]
3876,3876,555,45,[],200,llama-7b,128,1,1935.0,1.0,1,A100,1697548688910,1697548690845,120,11.0,1.0,"[236, 1699]","[1697548689146, 1697548690845]"
3877,3877,65,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622596,120,,,"[289, 1669]","[1697548620659, 1697548622328]"
3878,3878,420,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597360,120,,,"[220, 1520, 250, 72, 58]","[1697548594576, 1697548596096, 1697548596346, 1697548596418, 1697548596476]"
3879,3879,924,13,[],200,llama-7b,128,1,1966.0,1.0,1,A100,1697548610850,1697548612816,120,9.0,1.0,"[118, 1848]","[1697548610968, 1697548612816]"
3880,3880,512,21,[],200,llama-7b,128,1,2074.0,1.0,1,A100,1697548622604,1697548624678,120,11.0,1.0,"[132, 1942]","[1697548622736, 1697548624678]"
3881,3881,111,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610851,1697548612997,120,,,[226],[1697548611077]
3882,3882,872,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624682,1697548627102,120,,,"[52, 1040]","[1697548624734, 1697548625774]"
3883,3883,357,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612821,1697548615236,120,,,"[55, 662]","[1697548612876, 1697548613538]"
3884,3884,884,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690857,1697548693456,120,,,"[30, 965]","[1697548690887, 1697548691852]"
3885,3885,297,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629599,120,,,"[310, 1890]","[1697548627448, 1697548629338]"
3886,3886,318,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695718,120,,,"[337, 1633]","[1697548693801, 1697548695434]"
3887,3887,859,20,[],200,llama-7b,128,1,1907.0,1.0,1,A100,1697548617379,1697548619286,120,23.0,1.0,"[270, 1637]","[1697548617649, 1697548619286]"
3888,3888,672,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695725,1697548697835,120,,,"[240, 1719]","[1697548695965, 1697548697684]"
3889,3889,715,15,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548615244,1697548617267,120,20.0,1.0,"[400, 1623]","[1697548615644, 1697548617267]"
3890,3890,599,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637695,120,,,[120],[1697548636036]
3891,3891,657,24,[],200,llama-7b,128,1,2008.0,1.0,1,A100,1697548629611,1697548631619,120,10.0,1.0,"[341, 1667]","[1697548629952, 1697548631619]"
3892,3892,140,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617269,1697548620355,120,,,"[26, 661, 1440, 84, 82, 81, 80]","[1697548617295, 1697548617956, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
3893,3893,289,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619290,1697548620358,120,,,"[38, 994]","[1697548619328, 1697548620322]"
3894,3894,85,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631624,1697548635893,120,,,"[6, 1006, 1248, 58, 547]","[1697548631630, 1697548632636, 1697548633884, 1697548633942, 1697548634489]"
3895,3895,362,7,[],200,llama-7b,128,1,2686.0,1.0,1,A100,1697548590165,1697548592851,120,14.0,1.0,"[22, 2664]","[1697548590187, 1697548592851]"
3896,3896,699,21,[],200,llama-7b,128,1,2017.0,1.0,1,A100,1697548629603,1697548631620,120,39.0,1.0,"[148, 1869]","[1697548629751, 1697548631620]"
3897,3897,215,13,[],200,llama-7b,128,1,2389.0,1.0,1,A100,1697548602680,1697548605069,120,12.0,1.0,"[42, 2347]","[1697548602722, 1697548605069]"
3898,3898,499,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622597,120,,,"[375, 1585]","[1697548620745, 1697548622330]"
3899,3899,476,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635603,1697548639636,120,,,"[29, 877, 1222, 417]","[1697548635632, 1697548636509, 1697548637731, 1697548638148]"
3900,3900,100,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631624,1697548635895,120,,,"[49, 963, 1249, 57, 548]","[1697548631673, 1697548632636, 1697548633885, 1697548633942, 1697548634490]"
3901,3901,632,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686705,120,,,"[144, 1666, 236, 91, 90, 90, 69, 87, 86, 69]","[1697548683360, 1697548685026, 1697548685262, 1697548685353, 1697548685443, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
3902,3902,209,30,[],200,llama-7b,128,1,1765.0,1.0,1,A100,1697548643012,1697548644777,120,20.0,1.0,"[170, 1595]","[1697548643182, 1697548644777]"
3903,3903,570,31,[],200,llama-7b,128,1,2295.0,1.0,1,A100,1697548644781,1697548647076,120,18.0,1.0,"[36, 2259]","[1697548644817, 1697548647076]"
3904,3904,926,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548647079,1697548655268,120,,,"[38, 2822, 96, 87, 85, 75, 74, 74, 1000, 83, 73, 860, 319, 89, 69, 404, 97, 88, 84, 651, 96, 84, 62, 80]","[1697548647117, 1697548649939, 1697548650035, 1697548650122, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651430, 1697548651513, 1697548651586, 1697548652446, 1697548652765, 1697548652854, 1697548652923, 1697548653327, 1697548653424, 1697548653512, 1697548653596, 1697548654247, 1697548654343, 1697548654427, 1697548654489, 1697548654569]"
3905,3905,414,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635907,1697548637692,120,,,[13],[1697548635920]
3906,3906,850,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613011,1697548615236,120,,,"[174, 1845]","[1697548613185, 1697548615030]"
3907,3907,405,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[132, 2100]","[1697548627238, 1697548629338]"
3908,3908,835,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641374,120,,,[35],[1697548639681]
3909,3909,261,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643002,120,,,[359],[1697548641745]
3910,3910,88,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639646,120,,,"[33, 1563]","[1697548637735, 1697548639298]"
3911,3911,759,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629601,1697548631890,120,,,"[27, 1988]","[1697548629628, 1697548631616]"
3912,3912,773,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639639,120,,,"[246, 1350]","[1697548637949, 1697548639299]"
3913,3913,615,29,[],200,llama-7b,128,1,6294.0,1.0,1,A100,1697548643012,1697548649306,120,93.0,20.0,"[231, 1534, 434, 86, 64, 953, 94, 91, 89, 68, 968, 96, 96, 94, 71, 93, 72, 893, 91, 89, 86]","[1697548643243, 1697548644777, 1697548645211, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649039, 1697548649130, 1697548649219, 1697548649305]"
3914,3914,85,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548687545,1697548691085,120,,,"[15, 2044]","[1697548687560, 1697548689604]"
3915,3915,275,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617369,120,,,"[348, 1674]","[1697548615592, 1697548617266]"
3916,3916,444,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693451,120,,,"[251, 1899]","[1697548691347, 1697548693246]"
3917,3917,387,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617369,120,,,"[330, 1692]","[1697548615574, 1697548617266]"
3918,3918,187,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633845,120,,,[55],[1697548631951]
3919,3919,805,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695716,120,,,"[209, 1762]","[1697548693669, 1697548695431]"
3920,3920,635,17,[],200,llama-7b,128,1,1901.0,1.0,1,A100,1697548617387,1697548619288,120,23.0,1.0,"[367, 1534]","[1697548617754, 1697548619288]"
3921,3921,748,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617381,1697548620359,120,,,"[263, 1753, 83, 82, 81, 81]","[1697548617644, 1697548619397, 1697548619480, 1697548619562, 1697548619643, 1697548619724]"
3922,3922,202,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639654,1697548641375,120,,,[304],[1697548639958]
3923,3923,108,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588177,1697548589560,120,,,"[76, 1127]","[1697548588253, 1697548589380]"
3924,3924,556,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642997,120,,,[205],[1697548641590]
3925,3925,440,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589574,1697548592065,120,,,"[265, 1749]","[1697548589839, 1697548591588]"
3926,3926,534,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673080,1697548674840,120,,,"[15, 1224]","[1697548673095, 1697548674319]"
3927,3927,918,30,[],200,llama-7b,128,1,1770.0,1.0,1,A100,1697548643008,1697548644778,120,23.0,1.0,"[144, 1626]","[1697548643152, 1697548644778]"
3928,3928,894,40,[],200,llama-7b,128,1,1952.0,1.0,1,A100,1697548674845,1697548676797,120,14.0,1.0,"[220, 1731]","[1697548675065, 1697548676796]"
3929,3929,138,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686706,120,,,"[256, 1553, 238, 91, 90, 89, 70, 87, 86, 68]","[1697548683472, 1697548685025, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
3930,3930,799,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594347,120,,,"[102, 1985]","[1697548592175, 1697548594160]"
3931,3931,318,31,[],200,llama-7b,128,1,3202.0,1.0,1,A100,1697548644780,1697548647982,120,6.0,6.0,"[17, 2278, 550, 96, 96, 93, 72]","[1697548644797, 1697548647075, 1697548647625, 1697548647721, 1697548647817, 1697548647910, 1697548647982]"
3932,3932,380,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631623,1697548635895,120,,,"[40, 2222, 57, 548]","[1697548631663, 1697548633885, 1697548633942, 1697548634490]"
3933,3933,63,18,[],200,llama-7b,128,1,1633.0,1.0,1,A100,1697548619292,1697548620925,120,39.0,1.0,"[75, 1557]","[1697548619367, 1697548620924]"
3934,3934,398,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620929,1697548624950,120,,,"[15, 2512]","[1697548620944, 1697548623456]"
3935,3935,739,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635917,1697548637694,120,,,[248],[1697548636165]
3936,3936,232,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594350,1697548597358,120,,,"[43, 1703, 251, 72, 58]","[1697548594393, 1697548596096, 1697548596347, 1697548596419, 1697548596477]"
3937,3937,255,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637701,1697548639636,120,,,"[82, 1516]","[1697548637783, 1697548639299]"
3938,3938,758,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624966,1697548627099,120,,,"[93, 1961]","[1697548625059, 1697548627020]"
3939,3939,849,41,[],200,llama-7b,128,1,1992.0,1.0,1,A100,1697548686720,1697548688712,120,10.0,1.0,"[15, 1977]","[1697548686735, 1697548688712]"
3940,3940,612,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641378,120,,,[14],[1697548639660]
3941,3941,186,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627137,1697548629598,120,,,"[208, 1992]","[1697548627345, 1697548629337]"
3942,3942,42,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642998,120,,,[92],[1697548641477]
3943,3943,248,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688716,1697548691086,120,,,"[27, 859]","[1697548688743, 1697548689602]"
3944,3944,400,28,[],200,llama-7b,128,1,3493.0,1.0,1,A100,1697548643005,1697548646498,120,123.0,7.0,"[50, 1721, 434, 86, 64, 953, 94, 91]","[1697548643055, 1697548644776, 1697548645210, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498]"
3945,3945,591,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599938,120,,,"[281, 2068]","[1697548597648, 1697548599716]"
3946,3946,162,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679647,120,,,"[404, 2990]","[1697548675252, 1697548678242]"
3947,3947,809,21,[],200,llama-7b,128,1,1123.0,1.0,1,A100,1697548622336,1697548623459,120,16.0,1.0,"[101, 1022]","[1697548622437, 1697548623459]"
3948,3948,893,30,[],200,llama-7b,128,1,4380.0,1.0,1,A100,1697548649041,1697548653421,120,335.0,10.0,"[12, 1816, 561, 84, 73, 857, 320, 88, 69, 403, 97]","[1697548649053, 1697548650869, 1697548651430, 1697548651514, 1697548651587, 1697548652444, 1697548652764, 1697548652852, 1697548652921, 1697548653324, 1697548653421]"
3949,3949,238,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548623462,1697548627100,120,,,"[29, 2283]","[1697548623491, 1697548625774]"
3950,3950,523,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683208,120,,,"[108, 1923, 284, 91, 85, 83, 81]","[1697548679765, 1697548681688, 1697548681972, 1697548682063, 1697548682148, 1697548682231, 1697548682312]"
3951,3951,877,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686706,120,,,"[246, 1563, 238, 91, 90, 89, 70, 87, 86, 68]","[1697548683462, 1697548685025, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
3952,3952,570,23,[],200,llama-7b,128,1,2232.0,1.0,1,A100,1697548627106,1697548629338,120,18.0,1.0,"[133, 2099]","[1697548627239, 1697548629338]"
3953,3953,586,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601890,1697548605273,120,,,"[45, 740, 1439, 78, 76]","[1697548601935, 1697548602675, 1697548604114, 1697548604192, 1697548604268]"
3954,3954,1,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629352,1697548631888,120,,,"[51, 869]","[1697548629403, 1697548630272]"
3955,3955,757,29,[],200,llama-7b,128,1,3436.0,1.0,1,A100,1697548646502,1697548649938,120,20.0,1.0,"[18, 3418]","[1697548646520, 1697548649938]"
3956,3956,159,30,[],200,llama-7b,128,1,2236.0,1.0,1,A100,1697548649943,1697548652179,120,31.0,1.0,"[25, 2211]","[1697548649968, 1697548652179]"
3957,3957,355,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633845,120,,,[61],[1697548631956]
3958,3958,520,31,[],200,llama-7b,128,1,1713.0,1.0,1,A100,1697548652181,1697548653894,120,11.0,1.0,"[19, 1693]","[1697548652200, 1697548653893]"
3959,3959,362,36,[],200,llama-7b,128,1,1820.0,1.0,1,A100,1697548664217,1697548666037,120,14.0,1.0,"[150, 1670]","[1697548664367, 1697548666037]"
3960,3960,434,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695718,120,,,"[327, 1642]","[1697548693791, 1697548695433]"
3961,3961,876,32,[],200,llama-7b,128,1,2282.0,1.0,1,A100,1697548653897,1697548656179,120,11.0,1.0,"[28, 2254]","[1697548653925, 1697548656179]"
3962,3962,409,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635603,1697548639637,120,,,"[51, 854, 1224, 416]","[1697548635654, 1697548636508, 1697548637732, 1697548638148]"
3963,3963,794,43,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695723,1697548697682,120,11.0,1.0,"[12, 1947]","[1697548695735, 1697548697682]"
3964,3964,7,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608963,120,,,"[75, 1872, 508, 79, 79, 78]","[1697548605358, 1697548607230, 1697548607738, 1697548607817, 1697548607896, 1697548607974]"
3965,3965,294,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653427,1697548657538,120,,,[24],[1697548653451]
3966,3966,361,44,[],200,llama-7b,128,1,2474.0,1.0,1,A100,1697548683215,1697548685689,120,67.0,7.0,"[160, 1652, 235, 91, 91, 89, 69, 87]","[1697548683375, 1697548685027, 1697548685262, 1697548685353, 1697548685444, 1697548685533, 1697548685602, 1697548685689]"
3967,3967,855,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639648,1697548641379,120,,,[200],[1697548639848]
3968,3968,651,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657549,1697548659866,120,,,"[393, 1637]","[1697548657942, 1697548659579]"
3969,3969,280,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642996,120,,,[32],[1697548641416]
3970,3970,701,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697834,120,,,"[145, 1813]","[1697548695869, 1697548697682]"
3971,3971,278,44,[],200,llama-7b,128,1,1985.0,1.0,1,A100,1697548686729,1697548688714,120,13.0,1.0,"[143, 1842]","[1697548686872, 1697548688714]"
3972,3972,638,27,[],200,llama-7b,128,1,6300.0,1.0,1,A100,1697548643004,1697548649304,120,88.0,20.0,"[41, 1731, 434, 86, 64, 955, 94, 91, 89, 68, 968, 97, 94, 94, 72, 93, 71, 892, 91, 89, 86]","[1697548643045, 1697548644776, 1697548645210, 1697548645296, 1697548645360, 1697548646315, 1697548646409, 1697548646500, 1697548646589, 1697548646657, 1697548647625, 1697548647722, 1697548647816, 1697548647910, 1697548647982, 1697548648075, 1697548648146, 1697548649038, 1697548649129, 1697548649218, 1697548649304]"
3973,3973,636,45,[],200,llama-7b,128,1,884.0,1.0,1,A100,1697548688719,1697548689603,120,31.0,1.0,"[54, 830]","[1697548688773, 1697548689603]"
3974,3974,64,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548689609,1697548693449,120,,,"[46, 2198]","[1697548689655, 1697548691853]"
3975,3975,715,45,[],200,llama-7b,128,1,1851.0,1.0,1,A100,1697548685693,1697548687544,120,20.0,1.0,"[6, 1845]","[1697548685699, 1697548687544]"
3976,3976,423,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695718,120,,,"[45, 1928]","[1697548693504, 1697548695432]"
3977,3977,752,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,"[57, 1902]","[1697548695781, 1697548697683]"
3978,3978,366,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637695,120,,,[122],[1697548636037]
3979,3979,145,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548687547,1697548691085,120,,,"[29, 2026]","[1697548687576, 1697548689602]"
3980,3980,71,28,[],200,llama-7b,128,1,5033.0,1.0,1,A100,1697548649309,1697548654342,120,364.0,11.0,"[30, 2839, 267, 320, 88, 69, 403, 97, 88, 85, 650, 96]","[1697548649339, 1697548652178, 1697548652445, 1697548652765, 1697548652853, 1697548652922, 1697548653325, 1697548653422, 1697548653510, 1697548653595, 1697548654245, 1697548654341]"
3981,3981,726,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639645,120,,,"[290, 1307]","[1697548637993, 1697548639300]"
3982,3982,253,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666620,1697548667370,120,,,"[6, 721]","[1697548666626, 1697548667347]"
3983,3983,125,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641377,120,,,[347],[1697548640002]
3984,3984,97,26,[],200,llama-7b,128,1,6194.0,1.0,1,A100,1697548648150,1697548654344,120,6.0,20.0,"[13, 1776, 97, 86, 85, 75, 74, 74, 993, 90, 74, 856, 320, 88, 70, 406, 97, 88, 84, 651, 97]","[1697548648163, 1697548649939, 1697548650036, 1697548650122, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651423, 1697548651513, 1697548651587, 1697548652443, 1697548652763, 1697548652851, 1697548652921, 1697548653327, 1697548653424, 1697548653512, 1697548653596, 1697548654247, 1697548654344]"
3985,3985,590,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693448,120,,,"[89, 2062]","[1697548691185, 1697548693247]"
3986,3986,479,30,[],200,llama-7b,128,1,11537.0,1.0,1,A100,1697548641384,1697548652921,120,140.0,36.0,"[22, 331, 42, 1261, 584, 1587, 85, 64, 954, 94, 91, 89, 68, 968, 97, 95, 93, 72, 93, 72, 891, 92, 88, 86, 731, 87, 85, 75, 73, 75, 992, 90, 75, 858, 320, 87, 70]","[1697548641406, 1697548641737, 1697548641779, 1697548643040, 1697548643624, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647721, 1697548647816, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650354, 1697548650429, 1697548651421, 1697548651511, 1697548651586, 1697548652444, 1697548652764, 1697548652851, 1697548652921]"
3987,3987,610,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671229,120,,,"[281, 2049, 471, 84, 63, 64, 80]","[1697548667659, 1697548669708, 1697548670179, 1697548670263, 1697548670326, 1697548670390, 1697548670470]"
3988,3988,25,48,[],200,llama-7b,128,1,1972.0,1.0,1,A100,1697548693459,1697548695431,120,12.0,1.0,"[24, 1948]","[1697548693483, 1697548695431]"
3989,3989,384,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695438,1697548697835,120,,,"[50, 1240]","[1697548695488, 1697548696728]"
3990,3990,40,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674837,120,,,"[297, 1536, 537, 80, 60]","[1697548671540, 1697548673076, 1697548673613, 1697548673693, 1697548673753]"
3991,3991,368,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677256,120,,,"[190, 1762]","[1697548675035, 1697548676797]"
3992,3992,726,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677271,1697548679649,120,,,"[16, 2186]","[1697548677287, 1697548679473]"
3993,3993,448,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548654348,1697548657502,120,,,"[29, 1803]","[1697548654377, 1697548656180]"
3994,3994,184,29,[],200,llama-7b,128,1,6621.0,1.0,1,A100,1697548647723,1697548654344,120,87.0,20.0,"[6, 2209, 98, 86, 85, 75, 74, 74, 993, 90, 74, 860, 319, 85, 70, 406, 97, 88, 84, 651, 97]","[1697548647729, 1697548649938, 1697548650036, 1697548650122, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651423, 1697548651513, 1697548651587, 1697548652447, 1697548652766, 1697548652851, 1697548652921, 1697548653327, 1697548653424, 1697548653512, 1697548653596, 1697548654247, 1697548654344]"
3995,3995,847,22,[],200,llama-7b,128,1,2360.0,1.0,1,A100,1697548630276,1697548632636,120,10.0,1.0,"[20, 2339]","[1697548630296, 1697548632635]"
3996,3996,807,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657548,1697548659865,120,,,"[210, 1818]","[1697548657758, 1697548659576]"
3997,3997,820,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548654347,1697548657502,120,,,"[25, 1807]","[1697548654372, 1697548656179]"
3998,3998,267,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548632639,1697548635904,120,,,"[16, 1776, 58]","[1697548632655, 1697548634431, 1697548634489]"
3999,3999,758,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685266,1697548688903,120,,,"[7, 2269]","[1697548685273, 1697548687542]"
4000,4000,156,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683205,120,,,"[202, 1830, 283, 91, 85, 82, 80]","[1697548679859, 1697548681689, 1697548681972, 1697548682063, 1697548682148, 1697548682230, 1697548682310]"
4001,4001,156,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688910,1697548691090,120,,,"[235, 1700]","[1697548689145, 1697548690845]"
4002,4002,514,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693449,120,,,"[157, 1995]","[1697548691253, 1697548693248]"
4003,4003,514,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683215,1697548686719,120,,,"[140, 1671, 236, 93, 88, 90, 69, 89, 84, 69]","[1697548683355, 1697548685026, 1697548685262, 1697548685355, 1697548685443, 1697548685533, 1697548685602, 1697548685691, 1697548685775, 1697548685844]"
4004,4004,0,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664206,120,,,"[211, 1700]","[1697548662078, 1697548663778]"
4005,4005,875,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686732,1697548691085,120,,,"[358, 2513]","[1697548687090, 1697548689603]"
4006,4006,250,34,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548657547,1697548659578,120,31.0,1.0,"[312, 1719]","[1697548657859, 1697548659578]"
4007,4007,604,35,[],200,llama-7b,128,1,3398.0,1.0,1,A100,1697548659583,1697548662981,120,161.0,4.0,"[58, 1060, 1195, 57, 1028]","[1697548659641, 1697548660701, 1697548661896, 1697548661953, 1697548662981]"
4008,4008,873,51,[],200,llama-7b,128,1,1973.0,1.0,1,A100,1697548693459,1697548695432,120,6.0,1.0,"[83, 1890]","[1697548693542, 1697548695432]"
4009,4009,389,43,[],200,llama-7b,128,1,2150.0,1.0,1,A100,1697548691097,1697548693247,120,8.0,1.0,"[259, 1891]","[1697548691356, 1697548693247]"
4010,4010,36,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548662986,1697548667371,120,,,"[15, 1720, 1745, 83, 66, 84]","[1697548663001, 1697548664721, 1697548666466, 1697548666549, 1697548666615, 1697548666699]"
4011,4011,747,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693252,1697548695715,120,,,"[32, 859]","[1697548693284, 1697548694143]"
4012,4012,298,52,[],200,llama-7b,128,1,1293.0,1.0,1,A100,1697548695436,1697548696729,120,17.0,1.0,"[22, 1270]","[1697548695458, 1697548696728]"
4013,4013,331,34,[],200,llama-7b,128,1,1819.0,1.0,1,A100,1697548664217,1697548666036,120,26.0,1.0,"[37, 1782]","[1697548664254, 1697548666036]"
4014,4014,177,45,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695725,1697548697684,120,14.0,1.0,"[255, 1703]","[1697548695980, 1697548697683]"
4015,4015,779,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599942,120,,,"[432, 1918]","[1697548597799, 1697548599717]"
4016,4016,204,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[210, 1723]","[1697548600162, 1697548601885]"
4017,4017,687,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666041,1697548667369,120,,,"[33, 1272]","[1697548666074, 1697548667346]"
4018,4018,366,37,[],200,llama-7b,128,1,3013.0,1.0,1,A100,1697548667378,1697548670391,120,85.0,6.0,"[85, 562, 35, 2118, 84, 65, 64]","[1697548667463, 1697548668025, 1697548668060, 1697548670178, 1697548670262, 1697548670327, 1697548670391]"
4019,4019,67,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619732,1697548622595,120,,,"[18, 1175]","[1697548619750, 1697548620925]"
4020,4020,558,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602129,1697548605264,120,,,"[312, 1412, 262, 77, 77]","[1697548602441, 1697548603853, 1697548604115, 1697548604192, 1697548604269]"
4021,4021,426,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624951,120,,,[69],[1697548622672]
4022,4022,118,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671227,120,,,"[99, 2229, 472, 84, 65, 64, 80]","[1697548667477, 1697548669706, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
4023,4023,727,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548670394,1697548674836,120,,,"[18, 1178, 2022, 80, 60]","[1697548670412, 1697548671590, 1697548673612, 1697548673692, 1697548673752]"
4024,4024,784,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624970,1697548627103,120,,,"[76, 1974]","[1697548625046, 1697548627020]"
4025,4025,917,11,[],200,llama-7b,128,1,2459.0,1.0,1,A100,1697548605279,1697548607738,120,123.0,2.0,"[31, 1919, 509]","[1697548605310, 1697548607229, 1697548607738]"
4026,4026,212,22,[],200,llama-7b,128,1,2241.0,1.0,1,A100,1697548627106,1697548629347,120,31.0,1.0,"[157, 2083]","[1697548627263, 1697548629346]"
4027,4027,114,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639646,120,,,"[56, 1541]","[1697548637758, 1697548639299]"
4028,4028,475,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674839,120,,,"[24, 1813, 538, 79, 61]","[1697548671262, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
4029,4029,723,8,[],200,llama-7b,128,1,2071.0,1.0,1,A100,1697548592855,1697548594926,120,14.0,1.0,"[20, 2051]","[1697548592875, 1697548594926]"
4030,4030,123,9,[],200,llama-7b,128,1,2406.0,1.0,1,A100,1697548594928,1697548597334,120,14.0,1.0,"[12, 2393]","[1697548594940, 1697548597333]"
4031,4031,543,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629358,1697548631889,120,,,"[87, 827]","[1697548629445, 1697548630272]"
4032,4032,478,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597337,1697548599939,120,,,"[47, 560, 36]","[1697548597384, 1697548597944, 1697548597980]"
4033,4033,606,16,[],200,llama-7b,128,1,2020.0,1.0,1,A100,1697548613011,1697548615031,120,9.0,1.0,"[174, 1845]","[1697548613185, 1697548615030]"
4034,4034,832,11,[],200,llama-7b,128,1,1933.0,1.0,1,A100,1697548599952,1697548601885,120,15.0,1.0,"[213, 1720]","[1697548600165, 1697548601885]"
4035,4035,34,17,[],200,llama-7b,128,1,838.0,1.0,1,A100,1697548615035,1697548615873,120,12.0,1.0,"[19, 818]","[1697548615054, 1697548615872]"
4036,4036,260,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601889,1697548605272,120,,,"[19, 767, 1438, 78, 76]","[1697548601908, 1697548602675, 1697548604113, 1697548604191, 1697548604267]"
4037,4037,367,18,[],200,llama-7b,128,1,3848.0,1.0,1,A100,1697548615875,1697548619723,120,92.0,6.0,"[6, 2074, 1441, 83, 83, 80, 80]","[1697548615881, 1697548617955, 1697548619396, 1697548619479, 1697548619562, 1697548619642, 1697548619722]"
4038,4038,620,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608963,120,,,"[99, 1849, 507, 79, 80, 77]","[1697548605382, 1697548607231, 1697548607738, 1697548607817, 1697548607897, 1697548607974]"
4039,4039,730,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619727,1697548622595,120,,,"[6, 1192]","[1697548619733, 1697548620925]"
4040,4040,158,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624951,120,,,"[83, 1990]","[1697548622687, 1697548624677]"
4041,4041,518,21,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624974,1697548627021,120,23.0,1.0,"[182, 1865]","[1697548625156, 1697548627021]"
4042,4042,872,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627024,1697548629598,120,,,"[41, 873]","[1697548627065, 1697548627938]"
4043,4043,27,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610824,120,,,[22],[1697548608995]
4044,4044,156,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677259,120,,,"[93, 1859]","[1697548674937, 1697548676796]"
4045,4045,267,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631889,120,,,"[249, 1760]","[1697548629857, 1697548631617]"
4046,4046,477,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639653,1697548641382,120,,,[236],[1697548639889]
4047,4047,386,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610847,1697548612997,120,,,"[21, 520, 50]","[1697548610868, 1697548611388, 1697548611438]"
4048,4048,839,28,[],200,llama-7b,128,1,3973.0,1.0,1,A100,1697548641388,1697548645361,120,58.0,5.0,"[391, 1793, 51, 1588, 86, 64]","[1697548641779, 1697548643572, 1697548643623, 1697548645211, 1697548645297, 1697548645361]"
4049,4049,744,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615233,120,,,"[60, 1965]","[1697548613066, 1697548615031]"
4050,4050,626,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633847,120,,,[277],[1697548632178]
4051,4051,897,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633849,120,,,[356],[1697548632252]
4052,4052,687,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622597,120,,,"[95, 1873]","[1697548620457, 1697548622330]"
4053,4053,833,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677257,120,,,"[205, 1746]","[1697548675050, 1697548676796]"
4054,4054,207,19,[],200,llama-7b,128,1,2073.0,1.0,1,A100,1697548622604,1697548624677,120,10.0,1.0,"[234, 1839]","[1697548622838, 1697548624677]"
4055,4055,331,25,[],200,llama-7b,128,1,578.0,1.0,1,A100,1697548633854,1697548634432,120,26.0,1.0,"[82, 496]","[1697548633936, 1697548634432]"
4056,4056,235,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677272,1697548679649,120,,,[200],[1697548677472]
4057,4057,566,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624681,1697548627100,120,,,"[7, 1086]","[1697548624688, 1697548625774]"
4058,4058,688,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548634436,1697548639635,120,,,"[38, 2034, 1223, 417]","[1697548634474, 1697548636508, 1697548637731, 1697548638148]"
4059,4059,512,40,[],200,llama-7b,128,1,2196.0,1.0,1,A100,1697548677278,1697548679474,120,11.0,1.0,"[165, 2031]","[1697548677443, 1697548679474]"
4060,4060,866,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679477,1697548683206,120,,,"[37, 843, 1615, 91, 84, 83, 81]","[1697548679514, 1697548680357, 1697548681972, 1697548682063, 1697548682147, 1697548682230, 1697548682311]"
4061,4061,269,29,[],200,llama-7b,128,1,1712.0,1.0,1,A100,1697548645364,1697548647076,120,11.0,1.0,"[7, 1705]","[1697548645371, 1697548647076]"
4062,4062,266,42,[],200,llama-7b,128,1,1811.0,1.0,1,A100,1697548683213,1697548685024,120,9.0,1.0,"[24, 1787]","[1697548683237, 1697548685024]"
4063,4063,598,30,[],200,llama-7b,128,1,5687.0,1.0,1,A100,1697548647078,1697548652765,120,345.0,12.0,"[29, 2832, 96, 86, 86, 75, 74, 74, 1000, 82, 74, 860, 319]","[1697548647107, 1697548649939, 1697548650035, 1697548650121, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651430, 1697548651512, 1697548651586, 1697548652446, 1697548652765]"
4064,4064,628,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548688902,120,,,[41],[1697548685071]
4065,4065,59,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635904,120,,,"[92, 1651]","[1697548633946, 1697548635597]"
4066,4066,778,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599952,1697548602107,120,,,"[320, 1611]","[1697548600272, 1697548601883]"
4067,4067,592,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683206,120,,,"[230, 1799, 286, 90, 86, 81, 80]","[1697548679888, 1697548681687, 1697548681973, 1697548682063, 1697548682149, 1697548682230, 1697548682310]"
4068,4068,60,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688910,1697548691087,120,,,[265],[1697548689175]
4069,4069,420,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635913,1697548637695,120,,,[272],[1697548636185]
4070,4070,750,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637701,1697548639637,120,,,"[150, 1449]","[1697548637851, 1697548639300]"
4071,4071,175,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641379,120,,,[25],[1697548639671]
4072,4072,924,21,[],200,llama-7b,128,1,2200.0,1.0,1,A100,1697548627137,1697548629337,120,9.0,1.0,"[213, 1987]","[1697548627350, 1697548629337]"
4073,4073,19,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548652769,1697548655272,120,,,"[13, 2284]","[1697548652782, 1697548655066]"
4074,4074,357,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629349,1697548631887,120,,,"[15, 908]","[1697548629364, 1697548630272]"
4075,4075,556,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[155],[1697548636071]
4076,4076,380,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655282,1697548657502,120,,,[259],[1697548655541]
4077,4077,711,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633844,120,,,[21],[1697548631916]
4078,4078,210,10,[],200,llama-7b,128,1,1987.0,1.0,1,A100,1697548602128,1697548604115,120,140.0,2.0,"[238, 1486, 262]","[1697548602366, 1697548603852, 1697548604114]"
4079,4079,110,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633853,1697548635902,120,,,"[52, 526, 59]","[1697548633905, 1697548634431, 1697548634490]"
4080,4080,917,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639639,120,,,"[260, 1336]","[1697548637963, 1697548639299]"
4081,4081,567,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548604117,1697548605272,120,,,"[11, 942]","[1697548604128, 1697548605070]"
4082,4082,378,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655282,1697548657536,120,,,[96],[1697548655378]
4083,4083,923,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605285,1697548608961,120,,,"[302, 1643, 508, 80, 79, 78]","[1697548605587, 1697548607230, 1697548607738, 1697548607818, 1697548607897, 1697548607975]"
4084,4084,735,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657545,1697548659869,120,,,"[123, 1911]","[1697548657668, 1697548659579]"
4085,4085,647,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649309,1697548655270,120,,,"[41, 2829, 267, 319, 88, 70, 403, 96, 89, 84, 650, 97, 84, 62, 80]","[1697548649350, 1697548652179, 1697548652446, 1697548652765, 1697548652853, 1697548652923, 1697548653326, 1697548653422, 1697548653511, 1697548653595, 1697548654245, 1697548654342, 1697548654426, 1697548654488, 1697548654568]"
4086,4086,348,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610824,120,,,[63],[1697548609036]
4087,4087,678,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610849,1697548612999,120,,,"[109, 1858]","[1697548610958, 1697548612816]"
4088,4088,110,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613013,1697548615233,120,,,"[342, 1677]","[1697548613355, 1697548615032]"
4089,4089,471,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617365,120,,,"[203, 1820]","[1697548615446, 1697548617266]"
4090,4090,449,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641377,120,,,[394],[1697548640049]
4091,4091,831,17,[],200,llama-7b,128,1,1907.0,1.0,1,A100,1697548617379,1697548619286,120,11.0,1.0,"[205, 1702]","[1697548617584, 1697548619286]"
4092,4092,297,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631626,1697548635896,120,,,"[62, 949, 1248, 57, 547]","[1697548631688, 1697548632637, 1697548633885, 1697548633942, 1697548634489]"
4093,4093,782,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642997,120,,,[200],[1697548641585]
4094,4094,212,30,[],200,llama-7b,128,1,1772.0,1.0,1,A100,1697548643006,1697548644778,120,31.0,1.0,"[129, 1642]","[1697548643135, 1697548644777]"
4095,4095,569,31,[],200,llama-7b,128,1,2295.0,1.0,1,A100,1697548644781,1697548647076,120,16.0,1.0,"[43, 2252]","[1697548644824, 1697548647076]"
4096,4096,421,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674839,120,,,"[111, 1727, 538, 79, 60]","[1697548671349, 1697548673076, 1697548673614, 1697548673693, 1697548673753]"
4097,4097,492,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688901,120,,,"[148, 1837]","[1697548686877, 1697548688714]"
4098,4098,91,18,[],200,llama-7b,128,1,1960.0,1.0,1,A100,1697548620369,1697548622329,120,23.0,1.0,"[291, 1668]","[1697548620660, 1697548622328]"
4099,4099,920,32,[],200,llama-7b,128,1,3130.0,1.0,1,A100,1697548647077,1697548650207,120,96.0,4.0,"[15, 2847, 96, 86, 86]","[1697548647092, 1697548649939, 1697548650035, 1697548650121, 1697548650207]"
4100,4100,267,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664204,120,,,"[96, 1814]","[1697548661963, 1697548663777]"
4101,4101,350,33,[],200,llama-7b,128,1,3019.0,1.0,1,A100,1697548650209,1697548653228,120,216.0,1.0,"[26, 2993]","[1697548650235, 1697548653228]"
4102,4102,780,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674846,1697548677259,120,,,"[234, 1716]","[1697548675080, 1697548676796]"
4103,4103,534,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624953,120,,,"[47, 1075]","[1697548622382, 1697548623457]"
4104,4104,795,34,[],200,llama-7b,128,1,1835.0,1.0,1,A100,1697548653232,1697548655067,120,12.0,1.0,"[16, 1819]","[1697548653248, 1697548655067]"
4105,4105,342,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599720,1697548602107,120,,,"[24, 796]","[1697548599744, 1697548600540]"
4106,4106,403,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624682,1697548627101,120,,,"[21, 1072]","[1697548624703, 1697548625775]"
4107,4107,851,45,[],200,llama-7b,128,1,1944.0,1.0,1,A100,1697548688910,1697548690854,120,23.0,1.0,"[266, 1678]","[1697548689176, 1697548690854]"
4108,4108,725,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641387,1697548643000,120,,,[297],[1697548641684]
4109,4109,735,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627105,1697548629598,120,,,"[64, 2168]","[1697548627169, 1697548629337]"
4110,4110,123,27,[],200,llama-7b,128,1,1768.0,1.0,1,A100,1697548643010,1697548644778,120,14.0,1.0,"[147, 1621]","[1697548643157, 1697548644778]"
4111,4111,209,38,[],200,llama-7b,128,1,2201.0,1.0,1,A100,1697548677274,1697548679475,120,20.0,1.0,"[294, 1906]","[1697548677568, 1697548679474]"
4112,4112,537,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679478,1697548683206,120,,,"[35, 844, 1614, 92, 84, 83, 81]","[1697548679513, 1697548680357, 1697548681971, 1697548682063, 1697548682147, 1697548682230, 1697548682311]"
4113,4113,226,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655070,1697548657503,120,,,"[20, 1090]","[1697548655090, 1697548656180]"
4114,4114,598,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693452,120,,,"[294, 1856]","[1697548691391, 1697548693247]"
4115,4115,480,28,[],200,llama-7b,128,1,2293.0,1.0,1,A100,1697548644783,1697548647076,120,26.0,1.0,"[53, 2239]","[1697548644836, 1697548647075]"
4116,4116,897,40,[],200,llama-7b,128,1,1812.0,1.0,1,A100,1697548683214,1697548685026,120,9.0,1.0,"[126, 1686]","[1697548683340, 1697548685026]"
4117,4117,590,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657544,1697548659867,120,,,"[19, 2013]","[1697548657563, 1697548659576]"
4118,4118,16,11,[],200,llama-7b,128,1,1933.0,1.0,1,A100,1697548599951,1697548601884,120,9.0,1.0,"[39, 1894]","[1697548599990, 1697548601884]"
4119,4119,325,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548686716,120,,,"[6, 1144]","[1697548685036, 1697548686180]"
4120,4120,18,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659874,1697548661855,120,,,[298],[1697548660172]
4121,4121,701,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602128,1697548605266,120,,,"[208, 1515, 263, 79, 75]","[1697548602336, 1697548603851, 1697548604114, 1697548604193, 1697548604268]"
4122,4122,349,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664206,120,,,"[270, 1641]","[1697548662137, 1697548663778]"
4123,4123,126,10,[],200,llama-7b,128,1,1946.0,1.0,1,A100,1697548605284,1697548607230,120,19.0,1.0,"[272, 1674]","[1697548605556, 1697548607230]"
4124,4124,699,39,[],200,llama-7b,128,1,1819.0,1.0,1,A100,1697548664217,1697548666036,120,39.0,1.0,"[38, 1781]","[1697548664255, 1697548666036]"
4125,4125,480,11,[],200,llama-7b,128,1,1533.0,1.0,1,A100,1697548607237,1697548608770,120,26.0,1.0,"[61, 1472]","[1697548607298, 1697548608770]"
4126,4126,810,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608773,1697548612996,120,,,"[14, 838, 1243, 569]","[1697548608787, 1697548609625, 1697548610868, 1697548611437]"
4127,4127,128,40,[],200,llama-7b,128,1,1300.0,1.0,1,A100,1697548666047,1697548667347,120,9.0,1.0,"[41, 1259]","[1697548666088, 1697548667347]"
4128,4128,340,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548601889,1697548605272,120,,,"[14, 772, 1438, 78, 76]","[1697548601903, 1697548602675, 1697548604113, 1697548604191, 1697548604267]"
4129,4129,242,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615231,120,,,"[25, 1999]","[1697548613031, 1697548615030]"
4130,4130,603,14,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548615242,1697548617265,120,9.0,1.0,"[234, 1789]","[1697548615476, 1697548617265]"
4131,4131,28,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695715,120,,,"[203, 1768]","[1697548693663, 1697548695431]"
4132,4132,700,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605283,1697548608964,120,,,"[169, 2287, 79, 79, 78]","[1697548605452, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
4133,4133,132,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610832,120,,,[87],[1697548609060]
4134,4134,682,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686732,1697548691085,120,,,"[353, 2518]","[1697548687085, 1697548689603]"
4135,4135,773,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602107,120,,,"[93, 1840]","[1697548600044, 1697548601884]"
4136,4136,314,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631890,120,,,"[46, 1969]","[1697548629648, 1697548631617]"
4137,4137,489,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548612999,120,,,[320],[1697548611172]
4138,4138,675,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633845,120,,,[185],[1697548632081]
4139,4139,34,15,[],200,llama-7b,128,1,683.0,1.0,1,A100,1697548617273,1697548617956,120,12.0,1.0,"[72, 611]","[1697548617345, 1697548617956]"
4140,4140,10,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613008,1697548615234,120,,,"[132, 1892]","[1697548613140, 1697548615032]"
4141,4141,200,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605265,120,,,"[123, 1603, 260, 80, 74]","[1697548602250, 1697548603853, 1697548604113, 1697548604193, 1697548604267]"
4142,4142,112,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691095,1697548693452,120,,,"[18, 2132]","[1697548691113, 1697548693245]"
4143,4143,362,16,[],200,llama-7b,128,1,2364.0,1.0,1,A100,1697548617958,1697548620322,120,14.0,1.0,"[6, 2358]","[1697548617964, 1697548620322]"
4144,4144,718,17,[],200,llama-7b,128,1,600.0,1.0,1,A100,1697548620325,1697548620925,120,13.0,1.0,"[9, 591]","[1697548620334, 1697548620925]"
4145,4145,143,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620928,1697548624950,120,,,"[20, 2508]","[1697548620948, 1697548623456]"
4146,4146,386,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697836,120,,,"[33, 1925]","[1697548695757, 1697548697682]"
4147,4147,561,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605285,1697548608963,120,,,"[359, 1587, 508, 79, 79, 78]","[1697548605644, 1697548607231, 1697548607739, 1697548607818, 1697548607897, 1697548607975]"
4148,4148,237,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.29 GiB is free. Process 1412106 has 32.10 GiB memory in use. Of the allocated memory 22.99 GiB is allocated by PyTorch, and 7.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548583927,1697548586198,120,,,"[243, 1786]","[1697548584170, 1697548585956]"
4149,4149,364,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615245,1697548617370,120,,,"[409, 1613]","[1697548615654, 1697548617267]"
4150,4150,572,4,[],200,llama-7b,128,1,1962.0,1.0,1,A100,1697548586210,1697548588172,120,16.0,1.0,"[265, 1697]","[1697548586475, 1697548588172]"
4151,4151,1,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.67 GiB is free. Process 1412106 has 31.71 GiB memory in use. Of the allocated memory 23.52 GiB is allocated by PyTorch, and 6.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548588176,1697548589559,120,,,"[67, 1137]","[1697548588243, 1697548589380]"
4152,4152,718,37,[],200,llama-7b,128,1,1298.0,1.0,1,A100,1697548666048,1697548667346,120,13.0,1.0,"[61, 1237]","[1697548666109, 1697548667346]"
4153,4153,304,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548656182,1697548659864,120,,,"[33, 2073]","[1697548656215, 1697548658288]"
4154,4154,361,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.22 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.84 GiB is allocated by PyTorch, and 7.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548589567,1697548592066,120,,,"[297, 1725]","[1697548589864, 1697548591589]"
4155,4155,441,44,[],200,llama-7b,128,1,1969.0,1.0,1,A100,1697548693463,1697548695432,120,6.0,1.0,"[215, 1754]","[1697548693678, 1697548695432]"
4156,4156,714,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,"[288, 1799]","[1697548592361, 1697548594160]"
4157,4157,658,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661852,120,,,[9],[1697548659880]
4158,4158,141,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594355,1697548597361,120,,,"[315, 1428, 248, 73, 58]","[1697548594670, 1697548596098, 1697548596346, 1697548596419, 1697548596477]"
4159,4159,147,38,[],200,llama-7b,128,1,676.0,1.0,1,A100,1697548667349,1697548668025,120,182.0,1.0,"[19, 656]","[1697548667368, 1697548668024]"
4160,4160,803,45,[],200,llama-7b,128,1,1290.0,1.0,1,A100,1697548695438,1697548696728,120,20.0,1.0,"[60, 1230]","[1697548695498, 1697548696728]"
4161,4161,919,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610841,120,,,[82],[1697548609055]
4162,4162,889,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674323,1697548677257,120,,,"[48, 1102]","[1697548674371, 1697548675473]"
4163,4163,55,41,[],200,llama-7b,128,1,1856.0,1.0,1,A100,1697548673617,1697548675473,120,12.0,1.0,"[15, 1841]","[1697548673632, 1697548675473]"
4164,4164,659,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617388,1697548620356,120,,,"[386, 1515, 108, 84, 83, 80, 81]","[1697548617774, 1697548619289, 1697548619397, 1697548619481, 1697548619564, 1697548619644, 1697548619725]"
4165,4165,63,35,[],200,llama-7b,128,1,1909.0,1.0,1,A100,1697548661868,1697548663777,120,39.0,1.0,"[110, 1799]","[1697548661978, 1697548663777]"
4166,4166,409,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548675476,1697548679647,120,,,"[20, 2745]","[1697548675496, 1697548678241]"
4167,4167,469,9,[],200,llama-7b,128,1,2346.0,1.0,1,A100,1697548597372,1697548599718,120,17.0,1.0,"[494, 1852]","[1697548597866, 1697548599718]"
4168,4168,738,43,[],200,llama-7b,128,1,2653.0,1.0,1,A100,1697548679657,1697548682310,120,79.0,6.0,"[119, 2196, 91, 85, 83, 79]","[1697548679776, 1697548681972, 1697548682063, 1697548682148, 1697548682231, 1697548682310]"
4169,4169,725,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617379,1697548620358,120,,,"[186, 1720, 111, 84, 82, 81, 80]","[1697548617565, 1697548619285, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
4170,4170,420,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663782,1697548667372,120,,,"[45, 895, 1744, 83, 67, 84]","[1697548663827, 1697548664722, 1697548666466, 1697548666549, 1697548666616, 1697548666700]"
4171,4171,830,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599722,1697548602107,120,,,"[72, 746]","[1697548599794, 1697548600540]"
4172,4172,834,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548652925,1697548657536,120,,,"[8, 3245]","[1697548652933, 1697548656178]"
4173,4173,172,44,[],200,llama-7b,128,1,1729.0,1.0,1,A100,1697548682315,1697548684044,120,19.0,1.0,"[16, 1712]","[1697548682331, 1697548684043]"
4174,4174,532,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548684046,1697548686708,120,,,"[7, 2126]","[1697548684053, 1697548686179]"
4175,4175,782,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667381,1697548671226,120,,,"[495, 1833, 470, 84, 64, 65, 80]","[1697548667876, 1697548669709, 1697548670179, 1697548670263, 1697548670327, 1697548670392, 1697548670472]"
4176,4176,263,32,[],200,llama-7b,128,1,2034.0,1.0,1,A100,1697548657546,1697548659580,120,15.0,1.0,"[202, 1831]","[1697548657748, 1697548659579]"
4177,4177,892,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688902,120,,,"[223, 1760]","[1697548686952, 1697548688712]"
4178,4178,212,38,[],200,llama-7b,128,1,1838.0,1.0,1,A100,1697548671238,1697548673076,120,31.0,1.0,"[298, 1540]","[1697548671536, 1697548673076]"
4179,4179,624,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659585,1697548664207,120,,,"[89, 1027, 1195, 58, 1027]","[1697548659674, 1697548660701, 1697548661896, 1697548661954, 1697548662981]"
4180,4180,537,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673080,1697548674842,120,,,"[47, 1191]","[1697548673127, 1697548674318]"
4181,4181,432,29,[],200,llama-7b,128,1,1833.0,1.0,1,A100,1697548654347,1697548656180,120,13.0,1.0,"[29, 1804]","[1697548654376, 1697548656180]"
4182,4182,740,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642999,120,,,[284],[1697548641669]
4183,4183,288,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691087,120,,,"[271, 1673]","[1697548689180, 1697548690853]"
4184,4184,763,30,[],200,llama-7b,128,1,2105.0,1.0,1,A100,1697548656183,1697548658288,120,20.0,1.0,"[24, 2081]","[1697548656207, 1697548658288]"
4185,4185,188,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548658291,1697548664203,120,,,"[13, 2396, 1196, 57, 1026]","[1697548658304, 1697548660700, 1697548661896, 1697548661953, 1697548662979]"
4186,4186,642,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693452,120,,,"[350, 1801]","[1697548691447, 1697548693248]"
4187,4187,264,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602128,1697548605266,120,,,"[147, 1579, 259, 80, 75]","[1697548602275, 1697548603854, 1697548604113, 1697548604193, 1697548604268]"
4188,4188,891,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679646,120,,,"[393, 3000]","[1697548675241, 1697548678241]"
4189,4189,72,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693461,1697548695715,120,,,"[197, 1776]","[1697548693658, 1697548695434]"
4190,4190,29,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667373,120,,,"[47, 1772, 431, 82, 67, 85]","[1697548664264, 1697548666036, 1697548666467, 1697548666549, 1697548666616, 1697548666701]"
4191,4191,321,41,[],200,llama-7b,128,1,2492.0,1.0,1,A100,1697548679656,1697548682148,120,182.0,4.0,"[12, 2018, 286, 91, 85]","[1697548679668, 1697548681686, 1697548681972, 1697548682063, 1697548682148]"
4192,4192,165,33,[],200,llama-7b,128,1,6297.0,1.0,1,A100,1697548643008,1697548649305,120,83.0,20.0,"[142, 1628, 433, 85, 65, 952, 95, 91, 89, 68, 970, 94, 97, 93, 72, 93, 70, 893, 92, 88, 86]","[1697548643150, 1697548644778, 1697548645211, 1697548645296, 1697548645361, 1697548646313, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647626, 1697548647720, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648145, 1697548649038, 1697548649130, 1697548649218, 1697548649304]"
4193,4193,301,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548609629,1697548612998,120,,,"[6, 1753, 50]","[1697548609635, 1697548611388, 1697548611438]"
4194,4194,622,12,[],200,llama-7b,128,1,1953.0,1.0,1,A100,1697548605280,1697548607233,120,20.0,1.0,"[195, 1758]","[1697548605475, 1697548607233]"
4195,4195,47,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607238,1697548608967,120,,,"[104, 1429]","[1697548607342, 1697548608771]"
4196,4196,387,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671225,120,,,"[465, 1865, 470, 85, 64, 64, 81]","[1697548667843, 1697548669708, 1697548670178, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
4197,4197,682,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682152,1697548686705,120,,,"[13, 1879, 1218, 91, 91, 89, 69, 87, 86, 69]","[1697548682165, 1697548684044, 1697548685262, 1697548685353, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
4198,4198,236,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661857,120,,,[183],[1697548660055]
4199,4199,656,16,[],200,llama-7b,128,1,2014.0,1.0,1,A100,1697548613019,1697548615033,120,26.0,1.0,"[345, 1668]","[1697548613364, 1697548615032]"
4200,4200,56,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615036,1697548617365,120,,,"[19, 817]","[1697548615055, 1697548615872]"
4201,4201,746,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674840,120,,,"[131, 1708, 537, 79, 60]","[1697548671369, 1697548673077, 1697548673614, 1697548673693, 1697548673753]"
4202,4202,38,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633846,120,,,[263],[1697548632158]
4203,4203,415,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617377,1697548620356,120,,,"[33, 1987, 84, 82, 80, 81]","[1697548617410, 1697548619397, 1697548619481, 1697548619563, 1697548619643, 1697548619724]"
4204,4204,393,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635904,120,,,"[41, 536, 59]","[1697548633895, 1697548634431, 1697548634490]"
4205,4205,91,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637695,120,,,[126],[1697548636041]
4206,4206,747,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548639638,120,,,"[365, 1813, 54]","[1697548636281, 1697548638094, 1697548638148]"
4207,4207,556,20,[],200,llama-7b,128,1,2240.0,1.0,1,A100,1697548627106,1697548629346,120,9.0,1.0,"[162, 2073]","[1697548627268, 1697548629341]"
4208,4208,535,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642998,120,,,[81],[1697548641467]
4209,4209,889,30,[],200,llama-7b,128,1,6295.0,1.0,1,A100,1697548643008,1697548649303,120,86.0,20.0,"[253, 1517, 434, 85, 64, 953, 94, 91, 89, 69, 967, 96, 96, 94, 71, 93, 72, 891, 92, 88, 86]","[1697548643261, 1697548644778, 1697548645212, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646657, 1697548647624, 1697548647720, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303]"
4210,4210,148,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639647,1697548641376,120,,,[107],[1697548639754]
4211,4211,71,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629351,1697548631888,120,,,"[48, 873]","[1697548629399, 1697548630272]"
4212,4212,24,41,[],200,llama-7b,128,1,2629.0,1.0,1,A100,1697548683215,1697548685844,120,79.0,9.0,"[162, 1885, 92, 90, 89, 69, 87, 86, 69]","[1697548683377, 1697548685262, 1697548685354, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
4213,4213,422,27,[],200,llama-7b,128,1,1598.0,1.0,1,A100,1697548637703,1697548639301,120,26.0,1.0,"[235, 1363]","[1697548637938, 1697548639301]"
4214,4214,431,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633844,120,,,[36],[1697548631931]
4215,4215,429,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639651,1697548641380,120,,,[218],[1697548639869]
4216,4216,824,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[64, 2010]","[1697548622667, 1697548624677]"
4217,4217,608,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693252,1697548695715,120,,,"[32, 859]","[1697548693284, 1697548694143]"
4218,4218,788,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642996,120,,,[204],[1697548641590]
4219,4219,122,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,"[53, 1906]","[1697548695777, 1697548697683]"
4220,4220,62,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686720,1697548688899,120,,,"[33, 1959]","[1697548686753, 1697548688712]"
4221,4221,609,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599939,120,,,"[306, 2044]","[1697548597673, 1697548599717]"
4222,4222,230,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,"[58, 1901]","[1697548695782, 1697548697683]"
4223,4223,12,9,[],200,llama-7b,128,1,1934.0,1.0,1,A100,1697548599951,1697548601885,120,11.0,1.0,"[105, 1828]","[1697548600056, 1697548601884]"
4224,4224,255,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627101,120,,,"[305, 1743]","[1697548625279, 1697548627022]"
4225,4225,371,10,[],200,llama-7b,128,1,783.0,1.0,1,A100,1697548601893,1697548602676,120,13.0,1.0,"[76, 707]","[1697548601969, 1697548602676]"
4226,4226,802,17,[],200,llama-7b,128,1,1964.0,1.0,1,A100,1697548610852,1697548612816,120,9.0,1.0,"[315, 1649]","[1697548611167, 1697548612816]"
4227,4227,817,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613006,1697548615232,120,,,"[50, 1975]","[1697548613056, 1697548615031]"
4228,4228,106,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635903,120,,,"[85, 493, 58]","[1697548633939, 1697548634432, 1697548634490]"
4229,4229,595,21,[],200,llama-7b,128,1,2011.0,1.0,1,A100,1697548629608,1697548631619,120,8.0,1.0,"[284, 1726]","[1697548629892, 1697548631618]"
4230,4230,501,19,[],200,llama-7b,128,1,2058.0,1.0,1,A100,1697548624962,1697548627020,120,19.0,1.0,"[25, 2033]","[1697548624987, 1697548627020]"
4231,4231,121,28,[],200,llama-7b,128,1,1770.0,1.0,1,A100,1697548643008,1697548644778,120,13.0,1.0,"[245, 1525]","[1697548643253, 1697548644778]"
4232,4232,633,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661853,120,,,[265],[1697548660137]
4233,4233,353,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617365,120,,,"[230, 1792]","[1697548615473, 1697548617265]"
4234,4234,204,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612820,1697548615235,120,,,"[32, 686]","[1697548612852, 1697548613538]"
4235,4235,155,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620369,1697548622598,120,,,"[93, 1868]","[1697548620462, 1697548622330]"
4236,4236,714,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620357,120,,,"[78, 1829, 112, 84, 82, 80, 81]","[1697548617456, 1697548619285, 1697548619397, 1697548619481, 1697548619563, 1697548619643, 1697548619724]"
4237,4237,107,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.91 GiB is allocated by PyTorch, and 7.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548592073,1697548594348,120,,,[31],[1697548592104]
4238,4238,145,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622594,120,,,"[221, 1748]","[1697548620584, 1697548622332]"
4239,4239,463,14,[],200,llama-7b,128,1,2025.0,1.0,1,A100,1697548613006,1697548615031,120,39.0,1.0,"[30, 1994]","[1697548613036, 1697548615030]"
4240,4240,505,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624958,120,,,[18],[1697548622621]
4241,4241,440,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548594356,1697548597361,120,,,"[280, 1460, 250, 73, 58]","[1697548594636, 1697548596096, 1697548596346, 1697548596419, 1697548596477]"
4242,4242,791,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617365,120,,,[35],[1697548615070]
4243,4243,859,19,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624974,1697548627021,120,23.0,1.0,"[99, 1948]","[1697548625073, 1697548627021]"
4244,4244,260,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627024,1697548629598,120,,,"[58, 857]","[1697548627082, 1697548627939]"
4245,4245,217,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620356,120,,,"[77, 1830, 112, 84, 82, 80, 81]","[1697548617455, 1697548619285, 1697548619397, 1697548619481, 1697548619563, 1697548619643, 1697548619724]"
4246,4246,168,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631891,120,,,"[335, 1676]","[1697548629943, 1697548631619]"
4247,4247,325,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548676800,1697548679648,120,,,"[21, 1421]","[1697548676821, 1697548678242]"
4248,4248,526,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633850,120,,,[165],[1697548632066]
4249,4249,688,42,[],200,llama-7b,128,1,2491.0,1.0,1,A100,1697548679658,1697548682149,120,345.0,4.0,"[280, 1749, 286, 90, 86]","[1697548679938, 1697548681687, 1697548681973, 1697548682063, 1697548682149]"
4250,4250,585,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642996,120,,,[195],[1697548641580]
4251,4251,880,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635903,120,,,"[311, 1432]","[1697548634166, 1697548635598]"
4252,4252,430,50,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695724,1697548697683,120,15.0,1.0,"[216, 1743]","[1697548695940, 1697548697683]"
4253,4253,658,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637695,120,,,[59],[1697548635974]
4254,4254,306,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637693,120,,,[234],[1697548636150]
4255,4255,16,28,[],200,llama-7b,128,1,2729.0,1.0,1,A100,1697548643008,1697548645737,120,9.0,1.0,"[289, 2440]","[1697548643297, 1697548645737]"
4256,4256,751,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637700,1697548639636,120,,,[80],[1697548637780]
4257,4257,579,17,[],200,llama-7b,128,1,1961.0,1.0,1,A100,1697548620368,1697548622329,120,19.0,1.0,"[311, 1650]","[1697548620679, 1697548622329]"
4258,4258,7,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624960,120,,,"[54, 1068]","[1697548622389, 1697548623457]"
4259,4259,182,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639651,1697548641381,120,,,[223],[1697548639874]
4260,4260,370,29,[],200,llama-7b,128,1,2699.0,1.0,1,A100,1697548645739,1697548648438,120,31.0,1.0,"[9, 2690]","[1697548645748, 1697548648438]"
4261,4261,543,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641389,1697548655265,120,,,"[392, 1791, 52, 1587, 86, 64, 954, 93, 91, 89, 69, 968, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86, 731, 87, 85, 75, 74, 74, 992, 91, 74, 858, 320, 88, 69, 404, 96, 89, 84, 651, 96, 85, 63, 79]","[1697548641781, 1697548643572, 1697548643624, 1697548645211, 1697548645297, 1697548645361, 1697548646315, 1697548646408, 1697548646499, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421, 1697548651512, 1697548651586, 1697548652444, 1697548652764, 1697548652852, 1697548652921, 1697548653325, 1697548653421, 1697548653510, 1697548653594, 1697548654245, 1697548654341, 1697548654426, 1697548654489, 1697548654568]"
4262,4262,369,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627101,120,,,"[265, 1781]","[1697548625239, 1697548627020]"
4263,4263,493,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610834,120,,,[307],[1697548609282]
4264,4264,847,15,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548610849,1697548612816,120,10.0,1.0,"[99, 1867]","[1697548610948, 1697548612815]"
4265,4265,87,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682152,1697548686706,120,,,"[9, 1883, 1218, 91, 91, 88, 70, 87, 86, 68]","[1697548682161, 1697548684044, 1697548685262, 1697548685353, 1697548685444, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
4266,4266,696,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629598,120,,,"[58, 2173]","[1697548627164, 1697548629337]"
4267,4267,280,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612821,1697548615234,120,,,"[25, 692]","[1697548612846, 1697548613538]"
4268,4268,729,30,[],200,llama-7b,128,1,2991.0,1.0,1,A100,1697548648440,1697548651431,120,874.0,2.0,"[14, 2977]","[1697548648454, 1697548651431]"
4269,4269,124,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631890,120,,,[264],[1697548629872]
4270,4270,638,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617367,120,,,"[242, 1779]","[1697548615486, 1697548617265]"
4271,4271,70,11,[],200,llama-7b,128,1,1947.0,1.0,1,A100,1697548605284,1697548607231,120,39.0,1.0,"[312, 1635]","[1697548605596, 1697548607231]"
4272,4272,154,31,[],200,llama-7b,128,1,2455.0,1.0,1,A100,1697548651438,1697548653893,120,13.0,1.0,"[8, 2446]","[1697548651446, 1697548653892]"
4273,4273,483,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653896,1697548657538,120,,,"[13, 2270]","[1697548653909, 1697548656179]"
4274,4274,429,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607237,1697548608966,120,,,"[36, 1498]","[1697548607273, 1697548608771]"
4275,4275,483,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631902,1697548633848,120,,,[138],[1697548632040]
4276,4276,838,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635903,120,,,"[310, 1433]","[1697548634165, 1697548635598]"
4277,4277,441,44,[],200,llama-7b,128,1,1989.0,1.0,1,A100,1697548686723,1697548688712,120,6.0,1.0,"[45, 1944]","[1697548686768, 1697548688712]"
4278,4278,270,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637694,120,,,[259],[1697548636175]
4279,4279,801,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688716,1697548691086,120,,,"[20, 866]","[1697548688736, 1697548689602]"
4280,4280,787,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610824,120,,,[272],[1697548609246]
4281,4281,716,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639645,120,,,[31],[1697548637733]
4282,4282,41,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617387,1697548620356,120,,,"[383, 1518, 109, 84, 83, 80, 81]","[1697548617770, 1697548619288, 1697548619397, 1697548619481, 1697548619564, 1697548619644, 1697548619725]"
4283,4283,215,14,[],200,llama-7b,128,1,1964.0,1.0,1,A100,1697548610852,1697548612816,120,12.0,1.0,"[305, 1659]","[1697548611157, 1697548612816]"
4284,4284,148,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639650,1697548641381,120,,,[229],[1697548639879]
4285,4285,320,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607743,1697548612995,120,,,"[24, 3101, 569]","[1697548607767, 1697548610868, 1697548611437]"
4286,4286,169,17,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548615243,1697548617266,120,10.0,1.0,"[135, 1888]","[1697548615378, 1697548617266]"
4287,4287,576,14,[],200,llama-7b,128,1,665.0,1.0,1,A100,1697548605073,1697548605738,120,14.0,1.0,"[17, 648]","[1697548605090, 1697548605738]"
4288,4288,502,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641387,1697548643003,120,,,[369],[1697548641756]
4289,4289,7,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605739,1697548608965,120,,,"[14, 3016]","[1697548605753, 1697548608769]"
4290,4290,335,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610834,120,,,[204],[1697548609178]
4291,4291,401,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622594,120,,,"[211, 1753]","[1697548620579, 1697548622332]"
4292,4292,693,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610850,1697548612999,120,,,"[128, 1839]","[1697548610978, 1697548612817]"
4293,4293,123,18,[],200,llama-7b,128,1,2019.0,1.0,1,A100,1697548613013,1697548615032,120,14.0,1.0,"[336, 1683]","[1697548613349, 1697548615032]"
4294,4294,861,28,[],200,llama-7b,128,1,1766.0,1.0,1,A100,1697548643012,1697548644778,120,10.0,1.0,"[251, 1515]","[1697548643263, 1697548644778]"
4295,4295,480,19,[],200,llama-7b,128,1,835.0,1.0,1,A100,1697548615039,1697548615874,120,26.0,1.0,"[97, 737]","[1697548615136, 1697548615873]"
4296,4296,755,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624951,120,,,"[73, 2000]","[1697548622677, 1697548624677]"
4297,4297,184,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624968,1697548627103,120,,,"[39, 2013]","[1697548625007, 1697548627020]"
4298,4298,286,29,[],200,llama-7b,128,1,4521.0,1.0,1,A100,1697548644783,1697548649304,120,161.0,12.0,"[56, 2787, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86]","[1697548644839, 1697548647626, 1697548647722, 1697548647817, 1697548647911, 1697548647982, 1697548648075, 1697548648147, 1697548649038, 1697548649130, 1697548649218, 1697548649304]"
4299,4299,539,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627139,1697548629599,120,,,"[338, 1862]","[1697548627477, 1697548629339]"
4300,4300,524,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617270,1697548620355,120,,,"[40, 646, 1440, 84, 82, 81, 80]","[1697548617310, 1697548617956, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
4301,4301,871,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629607,1697548631887,120,,,"[155, 1858]","[1697548629762, 1697548631620]"
4302,4302,320,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649310,1697548655271,120,,,"[44, 3092, 319, 88, 70, 403, 96, 89, 84, 650, 97, 84, 62, 81]","[1697548649354, 1697548652446, 1697548652765, 1697548652853, 1697548652923, 1697548653326, 1697548653422, 1697548653511, 1697548653595, 1697548654245, 1697548654342, 1697548654426, 1697548654488, 1697548654569]"
4303,4303,852,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620369,1697548622597,120,,,"[396, 1565]","[1697548620765, 1697548622330]"
4304,4304,838,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615877,1697548620354,120,,,"[19, 2060, 1440, 84, 82, 80, 81]","[1697548615896, 1697548617956, 1697548619396, 1697548619480, 1697548619562, 1697548619642, 1697548619723]"
4305,4305,285,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624952,120,,,"[78, 1995]","[1697548622682, 1697548624677]"
4306,4306,239,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622597,120,,,"[38, 1928]","[1697548620400, 1697548622328]"
4307,4307,647,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624962,1697548627103,120,,,"[30, 2028]","[1697548624992, 1697548627020]"
4308,4308,506,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642996,120,,,[199],[1697548641585]
4309,4309,681,13,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548613008,1697548615031,120,23.0,1.0,"[266, 1757]","[1697548613274, 1697548615031]"
4310,4310,868,30,[],200,llama-7b,128,1,6299.0,1.0,1,A100,1697548643005,1697548649304,120,85.0,20.0,"[55, 1716, 434, 86, 64, 953, 94, 91, 91, 66, 970, 97, 95, 93, 72, 93, 70, 893, 92, 88, 86]","[1697548643060, 1697548644776, 1697548645210, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646589, 1697548646655, 1697548647625, 1697548647722, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648145, 1697548649038, 1697548649130, 1697548649218, 1697548649304]"
4311,4311,617,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649309,1697548655270,120,,,"[20, 2849, 267, 320, 88, 69, 403, 97, 88, 85, 650, 96, 86, 63, 80]","[1697548649329, 1697548652178, 1697548652445, 1697548652765, 1697548652853, 1697548652922, 1697548653325, 1697548653422, 1697548653510, 1697548653595, 1697548654245, 1697548654341, 1697548654427, 1697548654490, 1697548654570]"
4312,4312,112,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615035,1697548617365,120,,,"[25, 812]","[1697548615060, 1697548615872]"
4313,4313,76,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629598,120,,,"[305, 1895]","[1697548627443, 1697548629338]"
4314,4314,469,15,[],200,llama-7b,128,1,1908.0,1.0,1,A100,1697548617378,1697548619286,120,17.0,1.0,"[99, 1809]","[1697548617477, 1697548619286]"
4315,4315,430,23,[],200,llama-7b,128,1,2017.0,1.0,1,A100,1697548629603,1697548631620,120,15.0,1.0,"[153, 1864]","[1697548629756, 1697548631620]"
4316,4316,877,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631624,1697548635895,120,,,"[46, 966, 1249, 57, 548]","[1697548631670, 1697548632636, 1697548633885, 1697548633942, 1697548634490]"
4317,4317,597,22,[],200,llama-7b,128,1,2074.0,1.0,1,A100,1697548622604,1697548624678,120,39.0,1.0,"[152, 1922]","[1697548622756, 1697548624678]"
4318,4318,22,23,[],200,llama-7b,128,1,1091.0,1.0,1,A100,1697548624683,1697548625774,120,16.0,1.0,"[59, 1032]","[1697548624742, 1697548625774]"
4319,4319,49,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657536,120,,,"[65, 2079]","[1697548655346, 1697548657425]"
4320,4320,300,31,[],200,llama-7b,128,1,2869.0,1.0,1,A100,1697548649310,1697548652179,120,9.0,1.0,"[26, 2842]","[1697548649336, 1697548652178]"
4321,4321,776,28,[],200,llama-7b,128,1,2110.0,1.0,1,A100,1697548639304,1697548641414,120,67.0,2.0,"[24, 854, 1232]","[1697548639328, 1697548640182, 1697548641414]"
4322,4322,795,23,[],200,llama-7b,128,1,577.0,1.0,1,A100,1697548633854,1697548634431,120,12.0,1.0,"[36, 541]","[1697548633890, 1697548634431]"
4323,4323,385,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548625777,1697548629597,120,,,"[19, 2142]","[1697548625796, 1697548627938]"
4324,4324,302,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637693,120,,,[224],[1697548636140]
4325,4325,742,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629603,1697548631887,120,,,"[139, 1877]","[1697548629742, 1697548631619]"
4326,4326,660,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639651,120,,,"[260, 1336]","[1697548637963, 1697548639299]"
4327,4327,144,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633843,120,,,[174],[1697548632070]
4328,4328,49,39,[],200,llama-7b,128,1,2138.0,1.0,1,A100,1697548683215,1697548685353,120,109.0,3.0,"[152, 1660, 235, 91]","[1697548683367, 1697548685027, 1697548685262, 1697548685353]"
4329,4329,501,27,[],200,llama-7b,128,1,578.0,1.0,1,A100,1697548633853,1697548634431,120,19.0,1.0,"[50, 528]","[1697548633903, 1697548634431]"
4330,4330,93,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639656,1697548641377,120,,,[379],[1697548640035]
4331,4331,201,29,[],200,llama-7b,128,1,7801.0,1.0,1,A100,1697548641417,1697548649218,120,67.0,20.0,"[367, 1788, 51, 1588, 86, 64, 954, 93, 91, 89, 69, 968, 96, 95, 94, 71, 94, 71, 892, 91, 89]","[1697548641784, 1697548643572, 1697548643623, 1697548645211, 1697548645297, 1697548645361, 1697548646315, 1697548646408, 1697548646499, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648075, 1697548648146, 1697548649038, 1697548649129, 1697548649218]"
4332,4332,858,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548634436,1697548635904,120,,,[43],[1697548634479]
4333,4333,380,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685356,1697548688903,120,,,[15],[1697548685371]
4334,4334,425,28,[],200,llama-7b,128,1,7653.0,1.0,1,A100,1697548641384,1697548649037,120,88.0,20.0,"[10, 343, 42, 1261, 584, 1587, 85, 64, 954, 94, 91, 89, 68, 968, 96, 96, 93, 72, 93, 72, 891]","[1697548641394, 1697548641737, 1697548641779, 1697548643040, 1697548643624, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647816, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649037]"
4335,4335,736,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688912,1697548691088,120,,,"[349, 1592]","[1697548689261, 1697548690853]"
4336,4336,289,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[341],[1697548636257]
4337,4337,410,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657544,1697548659868,120,,,"[92, 1942]","[1697548657636, 1697548659578]"
4338,4338,225,24,[],200,llama-7b,128,1,1165.0,1.0,1,A100,1697548634435,1697548635600,120,23.0,1.0,"[29, 1135]","[1697548634464, 1697548635599]"
4339,4339,642,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637704,1697548639646,120,,,"[351, 1246]","[1697548638055, 1697548639301]"
4340,4340,554,25,[],200,llama-7b,128,1,905.0,1.0,1,A100,1697548635604,1697548636509,120,26.0,1.0,"[58, 846]","[1697548635662, 1697548636508]"
4341,4341,162,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639654,1697548641374,120,,,[297],[1697548639951]
4342,4342,521,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548643000,120,,,[165],[1697548641550]
4343,4343,883,33,[],200,llama-7b,128,1,2729.0,1.0,1,A100,1697548643008,1697548645737,120,563.0,1.0,"[265, 2464]","[1697548643273, 1697548645737]"
4344,4344,313,34,[],200,llama-7b,128,1,2698.0,1.0,1,A100,1697548645740,1697548648438,120,20.0,1.0,"[23, 2675]","[1697548645763, 1697548648438]"
4345,4345,560,30,[],200,llama-7b,128,1,5268.0,1.0,1,A100,1697548649222,1697548654490,120,161.0,13.0,"[24, 3199, 319, 88, 70, 403, 96, 89, 85, 653, 97, 82, 63]","[1697548649246, 1697548652445, 1697548652764, 1697548652852, 1697548652922, 1697548653325, 1697548653421, 1697548653510, 1697548653595, 1697548654248, 1697548654345, 1697548654427, 1697548654490]"
4346,4346,637,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548648441,1697548655269,120,,,"[25, 2403, 562, 82, 74, 857, 320, 88, 69, 407, 93, 92, 83, 651, 97, 83, 63, 80]","[1697548648466, 1697548650869, 1697548651431, 1697548651513, 1697548651587, 1697548652444, 1697548652764, 1697548652852, 1697548652921, 1697548653328, 1697548653421, 1697548653513, 1697548653596, 1697548654247, 1697548654344, 1697548654427, 1697548654490, 1697548654570]"
4347,4347,908,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548636512,1697548639639,120,,,"[19, 1563, 54]","[1697548636531, 1697548638094, 1697548638148]"
4348,4348,769,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661853,120,,,[266],[1697548660138]
4349,4349,784,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649041,1697548655269,120,,,"[16, 1812, 561, 84, 74, 856, 320, 88, 69, 403, 97, 92, 84, 650, 97, 83, 63, 80]","[1697548649057, 1697548650869, 1697548651430, 1697548651514, 1697548651588, 1697548652444, 1697548652764, 1697548652852, 1697548652921, 1697548653324, 1697548653421, 1697548653513, 1697548653597, 1697548654247, 1697548654344, 1697548654427, 1697548654490, 1697548654570]"
4350,4350,164,34,[],200,llama-7b,128,1,1910.0,1.0,1,A100,1697548661867,1697548663777,120,15.0,1.0,"[96, 1814]","[1697548661963, 1697548663777]"
4351,4351,335,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641377,120,,,[343],[1697548639998]
4352,4352,921,31,[],200,llama-7b,128,1,1686.0,1.0,1,A100,1697548654494,1697548656180,120,31.0,1.0,"[6, 1680]","[1697548654500, 1697548656180]"
4353,4353,519,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663782,1697548667371,120,,,"[35, 905, 1744, 83, 66, 85]","[1697548663817, 1697548664722, 1697548666466, 1697548666549, 1697548666615, 1697548666700]"
4354,4354,878,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671227,120,,,"[169, 2160, 471, 85, 64, 64, 80]","[1697548667547, 1697548669707, 1697548670178, 1697548670263, 1697548670327, 1697548670391, 1697548670471]"
4355,4355,209,30,[],200,llama-7b,128,1,2145.0,1.0,1,A100,1697548655280,1697548657425,120,20.0,1.0,"[188, 1957]","[1697548655468, 1697548657425]"
4356,4356,567,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657429,1697548659866,120,,,"[46, 813]","[1697548657475, 1697548658288]"
4357,4357,922,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661853,120,,,[213],[1697548660085]
4358,4358,692,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642997,120,,,[42],[1697548641426]
4359,4359,307,37,[],200,llama-7b,128,1,3072.0,1.0,1,A100,1697548671247,1697548674319,120,26.0,1.0,"[415, 2657]","[1697548671662, 1697548674319]"
4360,4360,672,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674322,1697548677257,120,,,"[39, 1111]","[1697548674361, 1697548675472]"
4361,4361,326,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664211,120,,,"[71, 814, 228]","[1697548661938, 1697548662752, 1697548662980]"
4362,4362,684,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667370,120,,,"[262, 1556, 430, 85, 65, 84]","[1697548664480, 1697548666036, 1697548666466, 1697548666551, 1697548666616, 1697548666700]"
4363,4363,124,29,[],200,llama-7b,128,1,2203.0,1.0,1,A100,1697548643008,1697548645211,120,83.0,2.0,"[132, 2071]","[1697548643140, 1697548645211]"
4364,4364,73,39,[],200,llama-7b,128,1,2200.0,1.0,1,A100,1697548677273,1697548679473,120,9.0,1.0,"[200, 2000]","[1697548677473, 1697548679473]"
4365,4365,430,40,[],200,llama-7b,128,1,880.0,1.0,1,A100,1697548679477,1697548680357,120,15.0,1.0,"[27, 853]","[1697548679504, 1697548680357]"
4366,4366,62,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655282,1697548657537,120,,,"[103, 2041]","[1697548655385, 1697548657426]"
4367,4367,117,35,[],200,llama-7b,128,1,2801.0,1.0,1,A100,1697548667378,1697548670179,120,364.0,2.0,"[185, 2144, 472]","[1697548667563, 1697548669707, 1697548670179]"
4368,4368,455,30,[],200,llama-7b,128,1,8207.0,1.0,1,A100,1697548645216,1697548653423,120,91.0,20.0,"[22, 3200, 600, 92, 88, 86, 730, 87, 85, 76, 73, 74, 1001, 85, 72, 858, 320, 88, 69, 404, 96]","[1697548645238, 1697548648438, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650034, 1697548650121, 1697548650206, 1697548650282, 1697548650355, 1697548650429, 1697548651430, 1697548651515, 1697548651587, 1697548652445, 1697548652765, 1697548652853, 1697548652922, 1697548653326, 1697548653422]"
4369,4369,471,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548670182,1697548674836,120,,,"[20, 1388, 2022, 80, 60]","[1697548670202, 1697548671590, 1697548673612, 1697548673692, 1697548673752]"
4370,4370,422,37,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548657547,1697548659578,120,26.0,1.0,"[293, 1737]","[1697548657840, 1697548659577]"
4371,4371,782,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664207,120,,,"[63, 1056, 1195, 57, 1028]","[1697548659645, 1697548660701, 1697548661896, 1697548661953, 1697548662981]"
4372,4372,830,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674843,1697548677257,120,,,"[14, 1939]","[1697548674857, 1697548676796]"
4373,4373,230,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679646,120,,,"[91, 2113]","[1697548677360, 1697548679473]"
4374,4374,785,41,[],200,llama-7b,128,1,2548.0,1.0,1,A100,1697548680361,1697548682909,120,10.0,1.0,"[15, 2533]","[1697548680376, 1697548682909]"
4375,4375,215,39,[],200,llama-7b,128,1,1818.0,1.0,1,A100,1697548664218,1697548666036,120,12.0,1.0,"[257, 1561]","[1697548664475, 1697548666036]"
4376,4376,545,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666046,1697548667369,120,,,"[37, 1263]","[1697548666083, 1697548667346]"
4377,4377,210,42,[],200,llama-7b,128,1,2352.0,1.0,1,A100,1697548682911,1697548685263,120,140.0,2.0,"[6, 1127, 1219]","[1697548682917, 1697548684044, 1697548685263]"
4378,4378,587,39,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679657,1697548681687,120,13.0,1.0,"[83, 1947]","[1697548679740, 1697548681687]"
4379,4379,903,41,[],200,llama-7b,128,1,3094.0,1.0,1,A100,1697548667377,1697548670471,120,244.0,7.0,"[82, 566, 35, 2118, 84, 65, 64, 80]","[1697548667459, 1697548668025, 1697548668060, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
4380,4380,18,40,[],200,llama-7b,128,1,1211.0,1.0,1,A100,1697548681698,1697548682909,120,15.0,1.0,"[55, 1156]","[1697548681753, 1697548682909]"
4381,4381,375,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682910,1697548686707,120,,,"[6, 2347, 91, 90, 89, 69, 87, 87, 68]","[1697548682916, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685776, 1697548685844]"
4382,4382,324,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548670476,1697548674837,120,,,"[23, 1092, 2021, 80, 60]","[1697548670499, 1697548671591, 1697548673612, 1697548673692, 1697548673752]"
4383,4383,616,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627139,1697548629597,120,,,"[197, 2010]","[1697548627336, 1697548629346]"
4384,4384,537,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629600,1697548631889,120,,,"[18, 1998]","[1697548629618, 1697548631616]"
4385,4385,732,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602679,1697548605263,120,,,"[24, 2366]","[1697548602703, 1697548605069]"
4386,4386,355,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622596,120,,,"[299, 1659]","[1697548620669, 1697548622328]"
4387,4387,346,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619290,1697548620358,120,,,"[33, 1000]","[1697548619323, 1697548620323]"
4388,4388,697,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622595,120,,,"[284, 1674]","[1697548620654, 1697548622328]"
4389,4389,803,17,[],200,llama-7b,128,1,2073.0,1.0,1,A100,1697548622604,1697548624677,120,20.0,1.0,"[176, 1897]","[1697548622780, 1697548624677]"
4390,4390,554,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635917,1697548637694,120,,,[253],[1697548636170]
4391,4391,72,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674836,120,,,"[226, 1606, 538, 79, 61]","[1697548671469, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
4392,4392,912,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637701,1697548639644,120,,,"[12, 381, 55]","[1697548637713, 1697548638094, 1697548638149]"
4393,4393,507,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548668031,1697548671226,120,,,"[12, 1666, 470, 84, 65, 64, 81]","[1697548668043, 1697548669709, 1697548670179, 1697548670263, 1697548670328, 1697548670392, 1697548670473]"
4394,4394,683,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677255,120,,,[113],[1697548674957]
4395,4395,498,35,[],200,llama-7b,128,1,1911.0,1.0,1,A100,1697548661867,1697548663778,120,9.0,1.0,"[199, 1712]","[1697548662066, 1697548663778]"
4396,4396,112,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679650,120,,,"[66, 2138]","[1697548677335, 1697548679473]"
4397,4397,859,36,[],200,llama-7b,128,1,938.0,1.0,1,A100,1697548663784,1697548664722,120,23.0,1.0,"[58, 880]","[1697548663842, 1697548664722]"
4398,4398,446,45,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548679658,1697548681689,120,26.0,1.0,"[388, 1643]","[1697548680046, 1697548681689]"
4399,4399,290,37,[],200,llama-7b,128,1,2622.0,1.0,1,A100,1697548664724,1697548667346,120,14.0,1.0,"[7, 2614]","[1697548664731, 1697548667345]"
4400,4400,16,30,[],200,llama-7b,128,1,2867.0,1.0,1,A100,1697548649311,1697548652178,120,9.0,1.0,"[53, 2814]","[1697548649364, 1697548652178]"
4401,4401,373,31,[],200,llama-7b,128,1,1713.0,1.0,1,A100,1697548652180,1697548653893,120,15.0,1.0,"[15, 1698]","[1697548652195, 1697548653893]"
4402,4402,644,38,[],200,llama-7b,128,1,672.0,1.0,1,A100,1697548667353,1697548668025,120,19.0,1.0,"[91, 581]","[1697548667444, 1697548668025]"
4403,4403,807,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548683212,120,,,"[55, 1156]","[1697548681753, 1697548682909]"
4404,4404,738,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653896,1697548657501,120,,,[24],[1697548653920]
4405,4405,663,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667373,120,,,"[32, 2218, 82, 67, 85]","[1697548664249, 1697548666467, 1697548666549, 1697548666616, 1697548666701]"
4406,4406,235,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683218,1697548686708,120,,,"[344, 1701, 91, 91, 89, 69, 87, 86, 69]","[1697548683562, 1697548685263, 1697548685354, 1697548685445, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685845]"
4407,4407,175,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674843,1697548677258,120,,,[35],[1697548674878]
4408,4408,178,19,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548620371,1697548622330,120,11.0,1.0,"[379, 1580]","[1697548620750, 1697548622330]"
4409,4409,661,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612821,1697548615234,120,,,"[30, 687]","[1697548612851, 1697548613538]"
4410,4410,494,34,[],200,llama-7b,128,1,4936.0,1.0,1,A100,1697548649309,1697548654245,120,6.0,10.0,"[25, 2844, 267, 320, 88, 69, 403, 97, 88, 85, 650]","[1697548649334, 1697548652178, 1697548652445, 1697548652765, 1697548652853, 1697548652922, 1697548653325, 1697548653422, 1697548653510, 1697548653595, 1697548654245]"
4411,4411,626,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637695,120,,,[70],[1697548635986]
4412,4412,536,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677279,1697548679651,120,,,"[359, 1837]","[1697548677638, 1697548679475]"
4413,4413,55,25,[],200,llama-7b,128,1,1598.0,1.0,1,A100,1697548637703,1697548639301,120,12.0,1.0,"[163, 1435]","[1697548637866, 1697548639301]"
4414,4414,511,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622338,1697548624959,120,,,"[80, 1040]","[1697548622418, 1697548623458]"
4415,4415,867,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683207,120,,,"[331, 1699, 285, 91, 85, 82, 80]","[1697548679989, 1697548681688, 1697548681973, 1697548682064, 1697548682149, 1697548682231, 1697548682311]"
4416,4416,864,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627100,120,,,"[193, 1853]","[1697548625167, 1697548627020]"
4417,4417,298,42,[],200,llama-7b,128,1,1811.0,1.0,1,A100,1697548683213,1697548685024,120,17.0,1.0,"[24, 1787]","[1697548683237, 1697548685024]"
4418,4418,654,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548688902,120,,,"[51, 2461]","[1697548685081, 1697548687542]"
4419,4419,294,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627105,1697548629598,120,,,[55],[1697548627160]
4420,4420,648,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631889,120,,,"[254, 1755]","[1697548629862, 1697548631617]"
4421,4421,82,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691087,120,,,"[135, 1809]","[1697548689044, 1697548690853]"
4422,4422,436,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693451,120,,,"[279, 1871]","[1697548691376, 1697548693247]"
4423,4423,418,26,[],200,llama-7b,128,1,2474.0,1.0,1,A100,1697548639304,1697548641778,120,286.0,3.0,"[12, 866, 1232, 364]","[1697548639316, 1697548640182, 1697548641414, 1697548641778]"
4424,4424,883,46,[],200,llama-7b,128,1,1973.0,1.0,1,A100,1697548693460,1697548695433,120,563.0,1.0,"[107, 1866]","[1697548693567, 1697548695433]"
4425,4425,315,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695436,1697548697834,120,,,"[27, 1265]","[1697548695463, 1697548696728]"
4426,4426,751,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641782,1697548655266,120,,,"[9, 1832, 1589, 85, 64, 954, 93, 91, 89, 69, 968, 96, 95, 93, 72, 94, 71, 892, 91, 89, 85, 731, 87, 85, 75, 74, 74, 992, 91, 74, 858, 320, 88, 70, 403, 96, 89, 85, 650, 96, 85, 63, 79]","[1697548641791, 1697548643623, 1697548645212, 1697548645297, 1697548645361, 1697548646315, 1697548646408, 1697548646499, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647909, 1697548647981, 1697548648075, 1697548648146, 1697548649038, 1697548649129, 1697548649218, 1697548649303, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421, 1697548651512, 1697548651586, 1697548652444, 1697548652764, 1697548652852, 1697548652922, 1697548653325, 1697548653421, 1697548653510, 1697548653595, 1697548654245, 1697548654341, 1697548654426, 1697548654489, 1697548654568]"
4427,4427,81,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633846,120,,,[257],[1697548632153]
4428,4428,120,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641375,120,,,[94],[1697548639740]
4429,4429,90,16,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548615243,1697548617266,120,19.0,1.0,"[125, 1898]","[1697548615368, 1697548617266]"
4430,4430,446,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642997,120,,,[55],[1697548641440]
4431,4431,466,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635912,1697548637697,120,,,[150],[1697548636062]
4432,4432,411,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635895,120,,,[218],[1697548634073]
4433,4433,768,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[350],[1697548636266]
4434,4434,449,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617269,1697548620354,120,,,"[11, 676, 1440, 84, 82, 80, 81]","[1697548617280, 1697548617956, 1697548619396, 1697548619480, 1697548619562, 1697548619642, 1697548619723]"
4435,4435,197,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637704,1697548639654,120,,,"[284, 1312]","[1697548637988, 1697548639300]"
4436,4436,112,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686722,1697548688901,120,,,"[138, 1853]","[1697548686860, 1697548688713]"
4437,4437,561,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691086,120,,,"[81, 1856]","[1697548688990, 1697548690846]"
4438,4438,807,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620362,1697548622597,120,,,"[83, 1884]","[1697548620445, 1697548622329]"
4439,4439,629,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548652188,1697548655271,120,,,"[22, 2856]","[1697548652210, 1697548655066]"
4440,4440,55,33,[],200,llama-7b,128,1,2145.0,1.0,1,A100,1697548655281,1697548657426,120,12.0,1.0,"[276, 1869]","[1697548655557, 1697548657426]"
4441,4441,409,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657430,1697548659865,120,,,"[26, 832]","[1697548657456, 1697548658288]"
4442,4442,206,19,[],200,llama-7b,128,1,2072.0,1.0,1,A100,1697548622604,1697548624676,120,16.0,1.0,"[176, 1896]","[1697548622780, 1697548624676]"
4443,4443,918,45,[],200,llama-7b,128,1,2152.0,1.0,1,A100,1697548691097,1697548693249,120,23.0,1.0,"[176, 1976]","[1697548691273, 1697548693249]"
4444,4444,345,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693254,1697548695716,120,,,"[70, 820]","[1697548693324, 1697548694144]"
4445,4445,171,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661856,120,,,[387],[1697548660264]
4446,4446,767,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661853,120,,,[275],[1697548660147]
4447,4447,566,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624682,1697548627102,120,,,"[36, 1056]","[1697548624718, 1697548625774]"
4448,4448,198,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664208,120,,,"[316, 1594]","[1697548662184, 1697548663778]"
4449,4449,501,35,[],200,llama-7b,128,1,1908.0,1.0,1,A100,1697548661869,1697548663777,120,19.0,1.0,"[168, 1740]","[1697548662037, 1697548663777]"
4450,4450,213,32,[],200,llama-7b,128,1,3404.0,1.0,1,A100,1697548643005,1697548646409,120,123.0,6.0,"[49, 1722, 434, 86, 64, 955, 94]","[1697548643054, 1697548644776, 1697548645210, 1697548645296, 1697548645360, 1697548646315, 1697548646409]"
4451,4451,699,47,[],200,llama-7b,128,1,1960.0,1.0,1,A100,1697548695723,1697548697683,120,39.0,1.0,"[39, 1921]","[1697548695762, 1697548697683]"
4452,4452,533,37,[],200,llama-7b,128,1,2251.0,1.0,1,A100,1697548664217,1697548666468,120,216.0,2.0,"[156, 2095]","[1697548664373, 1697548666468]"
4453,4453,892,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666471,1697548667370,120,,,"[15, 861]","[1697548666486, 1697548667347]"
4454,4454,573,33,[],200,llama-7b,128,1,2627.0,1.0,1,A100,1697548646412,1697548649039,120,874.0,2.0,"[15, 2612]","[1697548646427, 1697548649039]"
4455,4455,920,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[138, 2094]","[1697548627244, 1697548629338]"
4456,4456,859,36,[],200,llama-7b,128,1,940.0,1.0,1,A100,1697548663782,1697548664722,120,23.0,1.0,"[20, 920]","[1697548663802, 1697548664722]"
4457,4457,284,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664725,1697548667372,120,,,[6],[1697548664731]
4458,4458,320,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671230,120,,,"[311, 2490, 85, 62, 64, 81]","[1697548667689, 1697548670179, 1697548670264, 1697548670326, 1697548670390, 1697548670471]"
4459,4459,640,38,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667378,1697548669708,120,15.0,1.0,"[276, 2054]","[1697548667654, 1697548669708]"
4460,4460,351,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629603,1697548631887,120,,,"[75, 1941]","[1697548629678, 1697548631619]"
4461,4461,68,39,[],200,llama-7b,128,1,1490.0,1.0,1,A100,1697548669714,1697548671204,120,12.0,1.0,"[100, 1390]","[1697548669814, 1697548671204]"
4462,4462,399,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671206,1697548674838,120,,,"[26, 1842, 539, 79, 61]","[1697548671232, 1697548673074, 1697548673613, 1697548673692, 1697548673753]"
4463,4463,763,41,[],200,llama-7b,128,1,3394.0,1.0,1,A100,1697548674848,1697548678242,120,20.0,1.0,"[341, 3053]","[1697548675189, 1697548678242]"
4464,4464,732,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688901,120,,,"[141, 1844]","[1697548686870, 1697548688714]"
4465,4465,192,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548678245,1697548683205,120,,,"[26, 2085, 1615, 92, 85, 81, 81]","[1697548678271, 1697548680356, 1697548681971, 1697548682063, 1697548682148, 1697548682229, 1697548682310]"
4466,4466,157,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548605279,1697548608962,120,,,"[28, 1922, 508, 80, 79, 78]","[1697548605307, 1697548607229, 1697548607737, 1697548607817, 1697548607896, 1697548607974]"
4467,4467,925,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693450,120,,,"[249, 1900]","[1697548691346, 1697548693246]"
4468,4468,710,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631903,1697548633849,120,,,[346],[1697548632249]
4469,4469,896,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633846,120,,,[71],[1697548631972]
4470,4470,355,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695715,120,,,"[208, 1763]","[1697548693668, 1697548695431]"
4471,4471,551,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683215,1697548686709,120,,,"[122, 1689, 236, 93, 88, 89, 72, 86, 85, 68]","[1697548683337, 1697548685026, 1697548685262, 1697548685355, 1697548685443, 1697548685532, 1697548685604, 1697548685690, 1697548685775, 1697548685843]"
4472,4472,127,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622606,1697548624953,120,,,"[231, 1840]","[1697548622837, 1697548624677]"
4473,4473,716,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,[116],[1697548695840]
4474,4474,412,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635903,120,,,"[87, 491, 58]","[1697548633941, 1697548634432, 1697548634490]"
4475,4475,905,44,[],200,llama-7b,128,1,2871.0,1.0,1,A100,1697548686731,1697548689602,120,11.0,1.0,"[334, 2537]","[1697548687065, 1697548689602]"
4476,4476,488,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610833,120,,,[92],[1697548609065]
4477,4477,417,45,[],200,llama-7b,128,1,2244.0,1.0,1,A100,1697548689608,1697548691852,120,17.0,1.0,"[22, 2222]","[1697548689630, 1697548691852]"
4478,4478,485,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624969,1697548627100,120,,,"[173, 1879]","[1697548625142, 1697548627021]"
4479,4479,776,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691856,1697548695713,120,,,"[11, 2276]","[1697548691867, 1697548694143]"
4480,4480,846,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627137,1697548629598,120,,,"[223, 1977]","[1697548627360, 1697548629337]"
4481,4481,842,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610848,1697548612998,120,,,"[90, 1877]","[1697548610938, 1697548612815]"
4482,4482,250,23,[],200,llama-7b,128,1,2009.0,1.0,1,A100,1697548629608,1697548631617,120,31.0,1.0,"[234, 1775]","[1697548629842, 1697548631617]"
4483,4483,263,29,[],200,llama-7b,128,1,2956.0,1.0,1,A100,1697548649221,1697548652177,120,15.0,1.0,"[19, 2937]","[1697548649240, 1697548652177]"
4484,4484,708,26,[],200,llama-7b,128,1,1744.0,1.0,1,A100,1697548633853,1697548635597,120,140.0,1.0,"[106, 1638]","[1697548633959, 1697548635597]"
4485,4485,272,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613004,1697548615231,120,,,"[16, 2010]","[1697548613020, 1697548615030]"
4486,4486,608,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631623,1697548635895,120,,,"[45, 968, 1249, 57, 548]","[1697548631668, 1697548632636, 1697548633885, 1697548633942, 1697548634490]"
4487,4487,633,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615242,1697548617367,120,,,"[26, 1997]","[1697548615268, 1697548617265]"
4488,4488,139,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635602,1697548639636,120,,,"[22, 885, 1222, 417]","[1697548635624, 1697548636509, 1697548637731, 1697548638148]"
4489,4489,66,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617380,1697548620359,120,,,"[260, 1646, 110, 84, 82, 81, 81]","[1697548617640, 1697548619286, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619724]"
4490,4490,622,30,[],200,llama-7b,128,1,2886.0,1.0,1,A100,1697548652181,1697548655067,120,20.0,1.0,"[19, 2866]","[1697548652200, 1697548655066]"
4491,4491,512,18,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548620370,1697548622329,120,11.0,1.0,"[305, 1654]","[1697548620675, 1697548622329]"
4492,4492,865,19,[],200,llama-7b,128,1,1123.0,1.0,1,A100,1697548622335,1697548623458,120,9.0,1.0,"[62, 1060]","[1697548622397, 1697548623457]"
4493,4493,290,20,[],200,llama-7b,128,1,2313.0,1.0,1,A100,1697548623461,1697548625774,120,14.0,1.0,"[25, 2288]","[1697548623486, 1697548625774]"
4494,4494,209,47,[],200,llama-7b,128,1,1957.0,1.0,1,A100,1697548695727,1697548697684,120,20.0,1.0,"[261, 1696]","[1697548695988, 1697548697684]"
4495,4495,649,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548625777,1697548629596,120,,,"[29, 2132]","[1697548625806, 1697548627938]"
4496,4496,22,31,[],200,llama-7b,128,1,1110.0,1.0,1,A100,1697548655070,1697548656180,120,16.0,1.0,"[18, 1092]","[1697548655088, 1697548656180]"
4497,4497,79,22,[],200,llama-7b,128,1,2013.0,1.0,1,A100,1697548629608,1697548631621,120,12.0,1.0,"[167, 1846]","[1697548629775, 1697548631621]"
4498,4498,414,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631625,1697548635895,120,,,"[58, 954, 1248, 57, 548]","[1697548631683, 1697548632637, 1697548633885, 1697548633942, 1697548634490]"
4499,4499,379,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548656182,1697548659864,120,,,[18],[1697548656200]
4500,4500,774,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[360],[1697548636276]
4501,4501,202,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637704,1697548655271,120,,,"[351, 2127, 1231, 365, 1262, 583, 1587, 85, 64, 954, 94, 91, 89, 68, 969, 96, 95, 94, 71, 93, 72, 891, 93, 88, 86, 731, 86, 85, 76, 74, 74, 991, 91, 74, 858, 320, 88, 70, 403, 96, 90, 84, 651, 96, 84, 63, 80]","[1697548638055, 1697548640182, 1697548641413, 1697548641778, 1697548643040, 1697548643623, 1697548645210, 1697548645295, 1697548645359, 1697548646313, 1697548646407, 1697548646498, 1697548646587, 1697548646655, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647980, 1697548648073, 1697548648145, 1697548649036, 1697548649129, 1697548649217, 1697548649303, 1697548650034, 1697548650120, 1697548650205, 1697548650281, 1697548650355, 1697548650429, 1697548651420, 1697548651511, 1697548651585, 1697548652443, 1697548652763, 1697548652851, 1697548652921, 1697548653324, 1697548653420, 1697548653510, 1697548653594, 1697548654245, 1697548654341, 1697548654425, 1697548654488, 1697548654568]"
4502,4502,587,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641379,120,,,[26],[1697548639672]
4503,4503,19,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643001,120,,,[189],[1697548641575]
4504,4504,739,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661856,120,,,[166],[1697548660038]
4505,4505,337,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608973,1697548610824,120,,,[41],[1697548609014]
4506,4506,378,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548643012,1697548655267,120,,,"[244, 1956, 85, 64, 953, 94, 91, 89, 68, 968, 96, 96, 94, 71, 93, 72, 891, 92, 88, 86, 732, 86, 85, 75, 74, 75, 991, 91, 75, 858, 319, 88, 70, 403, 97, 89, 84, 650, 97, 84, 63, 80]","[1697548643256, 1697548645212, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650035, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650430, 1697548651421, 1697548651512, 1697548651587, 1697548652445, 1697548652764, 1697548652852, 1697548652922, 1697548653325, 1697548653422, 1697548653511, 1697548653595, 1697548654245, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
4507,4507,695,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610848,1697548612998,120,,,"[33, 1933]","[1697548610881, 1697548612814]"
4508,4508,129,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613020,1697548615234,120,,,"[355, 1657]","[1697548613375, 1697548615032]"
4509,4509,490,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617365,120,,,"[210, 1813]","[1697548615453, 1697548617266]"
4510,4510,175,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622594,120,,,[213],[1697548620576]
4511,4511,528,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[24, 2049]","[1697548622627, 1697548624676]"
4512,4512,6,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617378,1697548620357,120,,,"[83, 1936, 84, 83, 79, 81]","[1697548617461, 1697548619397, 1697548619481, 1697548619564, 1697548619643, 1697548619724]"
4513,4513,891,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627102,120,,,"[320, 1728]","[1697548625294, 1697548627022]"
4514,4514,863,40,[],200,llama-7b,128,1,1812.0,1.0,1,A100,1697548683213,1697548685025,120,10.0,1.0,"[44, 1768]","[1697548683257, 1697548685025]"
4515,4515,320,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627105,1697548629598,120,,,[59],[1697548627164]
4516,4516,381,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548686716,120,,,"[21, 1128]","[1697548685051, 1697548686179]"
4517,4517,681,21,[],200,llama-7b,128,1,2009.0,1.0,1,A100,1697548629608,1697548631617,120,23.0,1.0,"[244, 1765]","[1697548629852, 1697548631617]"
4518,4518,82,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631624,1697548635895,120,,,"[36, 976, 1249, 57, 548]","[1697548631660, 1697548632636, 1697548633885, 1697548633942, 1697548634490]"
4519,4519,437,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637695,120,,,[279],[1697548636195]
4520,4520,742,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686732,1697548691085,120,,,"[403, 2468]","[1697548687135, 1697548689603]"
4521,4521,791,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639644,120,,,[36],[1697548637738]
4522,4522,221,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639649,1697548641378,120,,,[130],[1697548639779]
4523,4523,89,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667379,1697548671231,120,,,"[387, 1940, 472, 84, 65, 64, 80]","[1697548667766, 1697548669706, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
4524,4524,582,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642998,120,,,[91],[1697548641477]
4525,4525,912,27,[],200,llama-7b,128,1,6297.0,1.0,1,A100,1697548643008,1697548649305,120,92.0,20.0,"[157, 1613, 433, 85, 65, 953, 94, 91, 89, 68, 968, 96, 97, 93, 70, 95, 70, 894, 91, 88, 87]","[1697548643165, 1697548644778, 1697548645211, 1697548645296, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647817, 1697548647910, 1697548647980, 1697548648075, 1697548648145, 1697548649039, 1697548649130, 1697548649218, 1697548649305]"
4526,4526,366,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622594,120,,,"[203, 1761]","[1697548620571, 1697548622332]"
4527,4527,716,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624959,120,,,[58],[1697548622662]
4528,4528,144,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624971,1697548627102,120,,,"[322, 1729]","[1697548625293, 1697548627022]"
4529,4529,120,27,[],200,llama-7b,128,1,1833.0,1.0,1,A100,1697548641740,1697548643573,120,17.0,1.0,"[49, 1784]","[1697548641789, 1697548643573]"
4530,4530,487,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667350,1697548671225,120,,,"[26, 684, 2117, 85, 64, 64, 80]","[1697548667376, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670390, 1697548670470]"
4531,4531,503,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627138,1697548629599,120,,,"[237, 1962]","[1697548627375, 1697548629337]"
4532,4532,470,28,[],200,llama-7b,128,1,4050.0,1.0,1,A100,1697548643575,1697548647625,120,39.0,2.0,"[11, 3489, 550]","[1697548643586, 1697548647075, 1697548647625]"
4533,4533,916,29,[],200,llama-7b,128,1,2310.0,1.0,1,A100,1697548647629,1697548649939,120,8.0,1.0,"[15, 2294]","[1697548647644, 1697548649938]"
4534,4534,345,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649941,1697548655265,120,,,"[20, 2218, 267, 320, 88, 69, 404, 96, 88, 85, 650, 96, 84, 63, 80]","[1697548649961, 1697548652179, 1697548652446, 1697548652766, 1697548652854, 1697548652923, 1697548653327, 1697548653423, 1697548653511, 1697548653596, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
4535,4535,839,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631885,120,,,"[344, 1667]","[1697548629952, 1697548631619]"
4536,4536,860,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627023,1697548629597,120,,,"[22, 893]","[1697548627045, 1697548627938]"
4537,4537,565,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617368,120,,,"[313, 1708]","[1697548615557, 1697548617265]"
4538,4538,382,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631890,120,,,"[329, 1682]","[1697548629937, 1697548631619]"
4539,4539,923,20,[],200,llama-7b,128,1,2345.0,1.0,1,A100,1697548617379,1697548619724,120,140.0,6.0,"[176, 1730, 111, 84, 82, 81, 80]","[1697548617555, 1697548619285, 1697548619396, 1697548619480, 1697548619562, 1697548619643, 1697548619723]"
4540,4540,506,20,[],200,llama-7b,128,1,2072.0,1.0,1,A100,1697548622608,1697548624680,120,16.0,1.0,"[337, 1735]","[1697548622945, 1697548624680]"
4541,4541,846,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659864,120,,,"[190, 1843]","[1697548657736, 1697548659579]"
4542,4542,863,21,[],200,llama-7b,128,1,1092.0,1.0,1,A100,1697548624682,1697548625774,120,10.0,1.0,"[26, 1066]","[1697548624708, 1697548625774]"
4543,4543,80,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661856,120,,,[370],[1697548660247]
4544,4544,264,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548625777,1697548629596,120,,,"[19, 2142]","[1697548625796, 1697548627938]"
4545,4545,419,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693452,120,,,"[22, 2128]","[1697548691118, 1697548693246]"
4546,4546,440,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664211,120,,,"[41, 844, 228]","[1697548661908, 1697548662752, 1697548662980]"
4547,4547,864,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693463,1697548695716,120,,,"[288, 1681]","[1697548693751, 1697548695432]"
4548,4548,348,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619727,1697548622595,120,,,"[11, 1187]","[1697548619738, 1697548620925]"
4549,4549,286,47,[],200,llama-7b,128,1,2662.0,1.0,1,A100,1697548695726,1697548698388,120,161.0,12.0,"[356, 2085, 22, 23, 22, 22, 22, 22, 22, 22, 22, 22]","[1697548696082, 1697548698167, 1697548698189, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388]"
4550,4550,797,35,[],200,llama-7b,128,1,1821.0,1.0,1,A100,1697548664216,1697548666037,120,26.0,1.0,"[73, 1748]","[1697548664289, 1697548666037]"
4551,4551,300,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633844,120,,,[21],[1697548631916]
4552,4552,739,33,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548657547,1697548659578,120,216.0,1.0,"[313, 1718]","[1697548657860, 1697548659578]"
4553,4553,850,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548654250,1697548657502,120,,,"[7, 1922]","[1697548654257, 1697548656179]"
4554,4554,662,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633853,1697548635893,120,,,"[116, 1629]","[1697548633969, 1697548635598]"
4555,4555,629,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629605,1697548631887,120,,,"[137, 1877]","[1697548629742, 1697548631619]"
4556,4556,171,34,[],200,llama-7b,128,1,1118.0,1.0,1,A100,1697548659583,1697548660701,120,6.0,1.0,"[66, 1052]","[1697548659649, 1697548660701]"
4557,4557,57,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633845,120,,,[242],[1697548632138]
4558,4558,501,35,[],200,llama-7b,128,1,2048.0,1.0,1,A100,1697548660704,1697548662752,120,19.0,1.0,"[6, 2042]","[1697548660710, 1697548662752]"
4559,4559,199,36,[],200,llama-7b,128,1,1301.0,1.0,1,A100,1697548666047,1697548667348,120,13.0,1.0,"[52, 1248]","[1697548666099, 1697548667347]"
4560,4560,556,37,[],200,llama-7b,128,1,674.0,1.0,1,A100,1697548667351,1697548668025,120,9.0,1.0,"[35, 639]","[1697548667386, 1697548668025]"
4561,4561,87,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635907,1697548637692,120,,,[20],[1697548635927]
4562,4562,855,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548662755,1697548664209,120,,,"[19, 1005]","[1697548662774, 1697548663779]"
4563,4563,447,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637701,1697548639636,120,,,[138],[1697548637839]
4564,4564,284,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667370,120,,,"[278, 1970, 85, 65, 84]","[1697548664496, 1697548666466, 1697548666551, 1697548666616, 1697548666700]"
4565,4565,282,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657547,1697548659865,120,,,"[318, 1713]","[1697548657865, 1697548659578]"
4566,4566,775,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639651,1697548641380,120,,,[208],[1697548639859]
4567,4567,913,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548668032,1697548671226,120,,,"[11, 1666, 470, 84, 65, 64, 81]","[1697548668043, 1697548669709, 1697548670179, 1697548670263, 1697548670328, 1697548670392, 1697548670473]"
4568,4568,204,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642997,120,,,[62],[1697548641447]
4569,4569,563,30,[],200,llama-7b,128,1,6124.0,1.0,1,A100,1697548643006,1697548649130,120,874.0,18.0,"[131, 1640, 434, 85, 64, 953, 94, 91, 90, 68, 970, 96, 95, 93, 72, 93, 70, 893, 92]","[1697548643137, 1697548644777, 1697548645211, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646588, 1697548646656, 1697548647626, 1697548647722, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648145, 1697548649038, 1697548649130]"
4570,4570,827,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639639,120,,,"[255, 1341]","[1697548637958, 1697548639299]"
4571,4571,343,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671237,1697548674839,120,,,"[29, 1809, 538, 79, 61]","[1697548671266, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
4572,4572,256,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641377,120,,,[389],[1697548640044]
4573,4573,640,38,[],200,llama-7b,128,1,647.0,1.0,1,A100,1697548667378,1697548668025,120,15.0,1.0,"[89, 558]","[1697548667467, 1697548668025]"
4574,4574,587,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548643000,120,,,[175],[1697548641560]
4575,4575,701,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679645,120,,,"[331, 3063]","[1697548675179, 1697548678242]"
4576,4576,641,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661854,120,,,[82],[1697548659953]
4577,4577,101,41,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548679657,1697548681690,120,13.0,1.0,"[221, 1812]","[1697548679878, 1697548681690]"
4578,4578,304,9,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.67 GiB. GPU 0 has a total capacty of 39.39 GiB of which 974.06 MiB is free. Process 1412106 has 38.44 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 12.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548596103,1697548597358,120,,,"[71, 1160]","[1697548596174, 1697548597334]"
4579,4579,15,29,[],200,llama-7b,128,1,6298.0,1.0,1,A100,1697548643008,1697548649306,120,100.0,20.0,"[240, 1529, 435, 85, 64, 953, 94, 91, 89, 68, 968, 96, 96, 94, 71, 93, 72, 890, 93, 88, 89]","[1697548643248, 1697548644777, 1697548645212, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649036, 1697548649129, 1697548649217, 1697548649306]"
4580,4580,44,38,[],200,llama-7b,128,1,1911.0,1.0,1,A100,1697548661868,1697548663779,120,12.0,1.0,"[287, 1624]","[1697548662155, 1697548663779]"
4581,4581,460,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548683211,120,,,"[29, 1182]","[1697548681727, 1697548682909]"
4582,4582,821,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686707,120,,,"[331, 1479, 237, 91, 91, 89, 69, 87, 86, 69]","[1697548683547, 1697548685026, 1697548685263, 1697548685354, 1697548685445, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685845]"
4583,4583,163,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655282,1697548657537,120,,,"[161, 1984]","[1697548655443, 1697548657427]"
4584,4584,72,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548668032,1697548671226,120,,,"[21, 1657, 469, 84, 65, 64, 81]","[1697548668053, 1697548669710, 1697548670179, 1697548670263, 1697548670328, 1697548670392, 1697548670473]"
4585,4585,398,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663783,1697548667372,120,,,"[39, 900, 1744, 83, 67, 84]","[1697548663822, 1697548664722, 1697548666466, 1697548666549, 1697548666616, 1697548666700]"
4586,4586,757,40,[],200,llama-7b,128,1,2331.0,1.0,1,A100,1697548667378,1697548669709,120,20.0,1.0,"[296, 2035]","[1697548667674, 1697548669709]"
4587,4587,186,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669714,1697548671228,120,,,"[75, 1414]","[1697548669789, 1697548671203]"
4588,4588,543,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671237,1697548674839,120,,,"[40, 1798, 539, 79, 60]","[1697548671277, 1697548673075, 1697548673614, 1697548673693, 1697548673753]"
4589,4589,5,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649042,1697548655269,120,,,"[25, 1802, 562, 83, 74, 856, 320, 88, 70, 402, 97, 89, 84, 654, 97, 82, 63, 80]","[1697548649067, 1697548650869, 1697548651431, 1697548651514, 1697548651588, 1697548652444, 1697548652764, 1697548652852, 1697548652922, 1697548653324, 1697548653421, 1697548653510, 1697548653594, 1697548654248, 1697548654345, 1697548654427, 1697548654490, 1697548654570]"
4590,4590,874,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674846,1697548677259,120,,,"[233, 1717]","[1697548675079, 1697548676796]"
4591,4591,302,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677273,1697548679646,120,,,"[102, 2099]","[1697548677375, 1697548679474]"
4592,4592,524,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659865,120,,,"[226, 1805]","[1697548657772, 1697548659577]"
4593,4593,887,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659878,1697548664208,120,,,"[314, 2560, 229]","[1697548660192, 1697548662752, 1697548662981]"
4594,4594,656,45,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679658,1697548681688,120,26.0,1.0,"[382, 1648]","[1697548680040, 1697548681688]"
4595,4595,90,46,[],200,llama-7b,128,1,1211.0,1.0,1,A100,1697548681698,1697548682909,120,19.0,1.0,"[70, 1141]","[1697548681768, 1697548682909]"
4596,4596,447,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682913,1697548686707,120,,,"[23, 2327, 91, 90, 89, 70, 87, 86, 68]","[1697548682936, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
4597,4597,894,48,[],200,llama-7b,128,1,1984.0,1.0,1,A100,1697548686729,1697548688713,120,14.0,1.0,"[53, 1931]","[1697548686782, 1697548688713]"
4598,4598,322,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688716,1697548691086,120,,,"[14, 872]","[1697548688730, 1697548689602]"
4599,4599,316,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667370,120,,,"[252, 1566, 430, 85, 65, 84]","[1697548664470, 1697548666036, 1697548666466, 1697548666551, 1697548666616, 1697548666700]"
4600,4600,671,40,[],200,llama-7b,128,1,1832.0,1.0,1,A100,1697548671243,1697548673075,120,12.0,1.0,"[206, 1626]","[1697548671449, 1697548673075]"
4601,4601,670,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671228,120,,,"[271, 2059, 471, 84, 65, 64, 80]","[1697548667649, 1697548669708, 1697548670179, 1697548670263, 1697548670328, 1697548670392, 1697548670472]"
4602,4602,101,41,[],200,llama-7b,128,1,1238.0,1.0,1,A100,1697548673080,1697548674318,120,13.0,1.0,"[42, 1196]","[1697548673122, 1697548674318]"
4603,4603,45,21,[],200,llama-7b,128,1,2013.0,1.0,1,A100,1697548629608,1697548631621,120,19.0,1.0,"[153, 1860]","[1697548629761, 1697548631621]"
4604,4604,546,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674321,1697548677256,120,,,"[16, 1136]","[1697548674337, 1697548675473]"
4605,4605,680,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693451,120,,,"[289, 1861]","[1697548691386, 1697548693247]"
4606,4606,378,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631627,1697548635896,120,,,"[76, 2182, 57, 547]","[1697548631703, 1697548633885, 1697548633942, 1697548634489]"
4607,4607,111,51,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695718,120,,,[98],[1697548693557]
4608,4608,732,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637694,120,,,[53],[1697548635969]
4609,4609,907,43,[],200,llama-7b,128,1,2204.0,1.0,1,A100,1697548677269,1697548679473,120,10.0,1.0,"[41, 2163]","[1697548677310, 1697548679473]"
4610,4610,341,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679477,1697548683205,120,,,"[21, 859, 1614, 92, 84, 83, 80]","[1697548679498, 1697548680357, 1697548681971, 1697548682063, 1697548682147, 1697548682230, 1697548682310]"
4611,4611,160,24,[],200,llama-7b,128,1,1600.0,1.0,1,A100,1697548637700,1697548639300,120,13.0,1.0,"[159, 1441]","[1697548637859, 1697548639300]"
4612,4612,464,52,[],200,llama-7b,128,1,2412.0,1.0,1,A100,1697548695727,1697548698139,120,12.0,1.0,"[341, 2071]","[1697548696068, 1697548698139]"
4613,4613,516,25,[],200,llama-7b,128,1,5906.0,1.0,1,A100,1697548639304,1697548645210,120,140.0,6.0,"[27, 852, 1231, 364, 1261, 584, 1587]","[1697548639331, 1697548640183, 1697548641414, 1697548641778, 1697548643039, 1697548643623, 1697548645210]"
4614,4614,699,45,[],200,llama-7b,128,1,1812.0,1.0,1,A100,1697548683215,1697548685027,120,39.0,1.0,"[132, 1679]","[1697548683347, 1697548685026]"
4615,4615,878,26,[],200,llama-7b,128,1,6302.0,1.0,1,A100,1697548645213,1697548651515,120,83.0,20.0,"[15, 1847, 551, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86, 732, 86, 83, 77, 74, 73, 995, 91]","[1697548645228, 1697548647075, 1697548647626, 1697548647722, 1697548647817, 1697548647911, 1697548647982, 1697548648075, 1697548648147, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650036, 1697548650122, 1697548650205, 1697548650282, 1697548650356, 1697548650429, 1697548651424, 1697548651515]"
4616,4616,374,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649312,1697548655271,120,,,"[43, 2823, 268, 319, 88, 70, 403, 96, 89, 84, 650, 97, 84, 63, 80]","[1697548649355, 1697548652178, 1697548652446, 1697548652765, 1697548652853, 1697548652923, 1697548653326, 1697548653422, 1697548653511, 1697548653595, 1697548654245, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
4617,4617,123,46,[],200,llama-7b,128,1,2512.0,1.0,1,A100,1697548685031,1697548687543,120,14.0,1.0,"[65, 2447]","[1697548685096, 1697548687543]"
4618,4618,516,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671246,1697548674839,120,,,"[407, 2665]","[1697548671653, 1697548674318]"
4619,4619,876,41,[],200,llama-7b,128,1,1953.0,1.0,1,A100,1697548674845,1697548676798,120,11.0,1.0,"[194, 1758]","[1697548675039, 1697548676797]"
4620,4620,114,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635895,120,,,"[213, 1532]","[1697548634068, 1697548635600]"
4621,4621,304,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548676802,1697548679649,120,,,"[44, 1396]","[1697548676846, 1697548678242]"
4622,4622,469,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637695,120,,,[284],[1697548636200]
4623,4623,450,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548687546,1697548691085,120,,,"[15, 2043]","[1697548687561, 1697548689604]"
4624,4624,663,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683205,120,,,"[227, 2089, 90, 86, 81, 80]","[1697548679884, 1697548681973, 1697548682063, 1697548682149, 1697548682230, 1697548682310]"
4625,4625,808,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691095,1697548693454,120,,,"[81, 2071]","[1697548691176, 1697548693247]"
4626,4626,92,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683215,1697548686705,120,,,"[155, 1657, 235, 91, 90, 90, 69, 87, 86, 69]","[1697548683370, 1697548685027, 1697548685262, 1697548685353, 1697548685443, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
4627,4627,827,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639638,120,,,"[241, 1357]","[1697548637944, 1697548639301]"
4628,4628,234,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624682,1697548627101,120,,,"[11, 1081]","[1697548624693, 1697548625774]"
4629,4629,238,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695718,120,,,"[332, 1638]","[1697548693796, 1697548695434]"
4630,4630,252,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641383,120,,,[244],[1697548639899]
4631,4631,611,28,[],200,llama-7b,128,1,2185.0,1.0,1,A100,1697548641388,1697548643573,120,14.0,1.0,"[388, 1796]","[1697548641776, 1697548643572]"
4632,4632,550,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635904,120,,,"[19, 559, 58]","[1697548633873, 1697548634432, 1697548634490]"
4633,4633,599,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,"[115, 1844]","[1697548695839, 1697548697683]"
4634,4634,420,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686721,1697548688899,120,,,"[248, 1744]","[1697548686969, 1697548688713]"
4635,4635,724,18,[],200,llama-7b,128,1,1901.0,1.0,1,A100,1697548617387,1697548619288,120,11.0,1.0,"[362, 1539]","[1697548617749, 1697548619288]"
4636,4636,883,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[336],[1697548636252]
4637,4637,780,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[38, 1899]","[1697548688945, 1697548690844]"
4638,4638,152,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619291,1697548620358,120,,,"[62, 970]","[1697548619353, 1697548620323]"
4639,4639,279,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548651517,1697548655269,120,,,"[15, 2361, 355, 95, 84, 63, 80]","[1697548651532, 1697548653893, 1697548654248, 1697548654343, 1697548654427, 1697548654490, 1697548654570]"
4640,4640,208,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693450,120,,,"[240, 1910]","[1697548691336, 1697548693246]"
4641,4641,321,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677272,1697548679647,120,,,"[113, 2089]","[1697548677385, 1697548679474]"
4642,4642,510,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620365,1697548622595,120,,,[276],[1697548620641]
4643,4643,435,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677258,120,,,"[33, 1919]","[1697548674877, 1697548676796]"
4644,4644,266,32,[],200,llama-7b,128,1,2869.0,1.0,1,A100,1697548649310,1697548652179,120,9.0,1.0,"[35, 2834]","[1697548649345, 1697548652179]"
4645,4645,675,40,[],200,llama-7b,128,1,2574.0,1.0,1,A100,1697548679657,1697548682231,120,563.0,5.0,"[99, 1932, 284, 91, 85, 83]","[1697548679756, 1697548681688, 1697548681972, 1697548682063, 1697548682148, 1697548682231]"
4646,4646,623,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548652180,1697548655270,120,,,[14],[1697548652194]
4647,4647,840,21,[],200,llama-7b,128,1,2074.0,1.0,1,A100,1697548622603,1697548624677,120,17.0,1.0,"[74, 2000]","[1697548622677, 1697548624677]"
4648,4648,313,26,[],200,llama-7b,128,1,1596.0,1.0,1,A100,1697548637705,1697548639301,120,20.0,1.0,"[355, 1241]","[1697548638060, 1697548639301]"
4649,4649,268,22,[],200,llama-7b,128,1,1090.0,1.0,1,A100,1697548624684,1697548625774,120,19.0,1.0,"[60, 1030]","[1697548624744, 1697548625774]"
4650,4650,626,23,[],200,llama-7b,128,1,2162.0,1.0,1,A100,1697548625776,1697548627938,120,10.0,1.0,"[15, 2147]","[1697548625791, 1697548627938]"
4651,4651,792,42,[],200,llama-7b,128,1,2200.0,1.0,1,A100,1697548677274,1697548679474,120,11.0,1.0,"[282, 1918]","[1697548677556, 1697548679474]"
4652,4652,56,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627941,1697548631885,120,,,"[16, 2314]","[1697548627957, 1697548630271]"
4653,4653,76,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682234,1697548686706,120,,,"[18, 1791, 1219, 91, 91, 89, 69, 87, 86, 69]","[1697548682252, 1697548684043, 1697548685262, 1697548685353, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
4654,4654,223,43,[],200,llama-7b,128,1,880.0,1.0,1,A100,1697548679478,1697548680358,120,16.0,1.0,"[40, 839]","[1697548679518, 1697548680357]"
4655,4655,671,27,[],200,llama-7b,128,1,879.0,1.0,1,A100,1697548639304,1697548640183,120,12.0,1.0,"[48, 831]","[1697548639352, 1697548640183]"
4656,4656,145,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657536,120,,,"[286, 1859]","[1697548655567, 1697548657426]"
4657,4657,92,28,[],200,llama-7b,128,1,8851.0,1.0,1,A100,1697548640186,1697548649037,120,85.0,20.0,"[14, 1537, 42, 1260, 585, 1587, 85, 64, 954, 94, 91, 89, 68, 968, 96, 95, 94, 72, 93, 72, 891]","[1697548640200, 1697548641737, 1697548641779, 1697548643039, 1697548643624, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649037]"
4658,4658,414,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631900,1697548633845,120,,,[243],[1697548632143]
4659,4659,498,35,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548657547,1697548659577,120,9.0,1.0,"[239, 1791]","[1697548657786, 1697548659577]"
4660,4660,746,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633853,1697548635904,120,,,"[98, 1646]","[1697548633951, 1697548635597]"
4661,4661,430,42,[],200,llama-7b,128,1,1986.0,1.0,1,A100,1697548686729,1697548688715,120,15.0,1.0,"[321, 1664]","[1697548687050, 1697548688714]"
4662,4662,173,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637696,120,,,[131],[1697548636046]
4663,4663,573,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548680360,1697548683209,120,,,[6],[1697548680366]
4664,4664,856,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664205,120,,,"[44, 1074, 1196, 57, 1027]","[1697548659626, 1697548660700, 1697548661896, 1697548661953, 1697548662980]"
4665,4665,531,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639646,120,,,"[63, 1534]","[1697548637765, 1697548639299]"
4666,4666,889,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639651,1697548641382,120,,,[233],[1697548639884]
4667,4667,318,30,[],200,llama-7b,128,1,4927.0,1.0,1,A100,1697548641388,1697548646315,120,6.0,6.0,"[372, 1812, 52, 1587, 85, 64, 954]","[1697548641760, 1697548643572, 1697548643624, 1697548645211, 1697548645296, 1697548645360, 1697548646314]"
4668,4668,323,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610851,1697548612996,120,,,"[211, 1756]","[1697548611062, 1697548612818]"
4669,4669,451,29,[],200,llama-7b,128,1,3137.0,1.0,1,A100,1697548649041,1697548652178,120,286.0,1.0,"[25, 3111]","[1697548649066, 1697548652177]"
4670,4670,677,13,[],200,llama-7b,128,1,2028.0,1.0,1,A100,1697548613006,1697548615034,120,9.0,1.0,"[164, 1864]","[1697548613170, 1697548615034]"
4671,4671,790,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688717,1697548691084,120,,,"[34, 851]","[1697548688751, 1697548689602]"
4672,4672,779,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548652180,1697548655270,120,,,"[7, 2879]","[1697548652187, 1697548655066]"
4673,4673,339,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639652,1697548641383,120,,,[287],[1697548639939]
4674,4674,901,45,[],200,llama-7b,128,1,1810.0,1.0,1,A100,1697548683216,1697548685026,120,17.0,1.0,"[326, 1484]","[1697548683542, 1697548685026]"
4675,4675,890,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624962,1697548627103,120,,,"[35, 2023]","[1697548624997, 1697548627020]"
4676,4676,323,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627105,1697548629598,120,,,"[40, 2192]","[1697548627145, 1697548629337]"
4677,4677,332,46,[],200,llama-7b,128,1,2510.0,1.0,1,A100,1697548685033,1697548687543,120,39.0,1.0,"[73, 2437]","[1697548685106, 1697548687543]"
4678,4678,686,22,[],200,llama-7b,128,1,2010.0,1.0,1,A100,1697548629608,1697548631618,120,31.0,1.0,"[279, 1731]","[1697548629887, 1697548631618]"
4679,4679,222,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691095,1697548693452,120,,,"[12, 2139]","[1697548691107, 1697548693246]"
4680,4680,693,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548687549,1697548691085,120,,,"[36, 2017]","[1697548687585, 1697548689602]"
4681,4681,168,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657544,1697548659867,120,,,"[44, 1990]","[1697548657588, 1697548659578]"
4682,4682,693,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642998,120,,,[268],[1697548641654]
4683,4683,126,48,[],200,llama-7b,128,1,2151.0,1.0,1,A100,1697548691096,1697548693247,120,19.0,1.0,"[74, 2077]","[1697548691170, 1697548693247]"
4684,4684,582,45,[],200,llama-7b,128,1,1969.0,1.0,1,A100,1697548693465,1697548695434,120,19.0,1.0,"[390, 1579]","[1697548693855, 1697548695434]"
4685,4685,106,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615036,1697548617366,120,,,"[54, 782]","[1697548615090, 1697548615872]"
4686,4686,173,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693452,120,,,"[17, 2133]","[1697548691113, 1697548693246]"
4687,4687,100,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695438,1697548697835,120,,,"[55, 1235]","[1697548695493, 1697548696728]"
4688,4688,123,29,[],200,llama-7b,128,1,2725.0,1.0,1,A100,1697548643012,1697548645737,120,14.0,1.0,"[264, 2461]","[1697548643276, 1697548645737]"
4689,4689,527,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693463,1697548695716,120,,,[278],[1697548693741]
4690,4690,496,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661856,120,,,[182],[1697548660054]
4691,4691,847,35,[],200,llama-7b,128,1,886.0,1.0,1,A100,1697548661867,1697548662753,120,10.0,1.0,"[86, 799]","[1697548661953, 1697548662752]"
4692,4692,455,30,[],200,llama-7b,128,1,7683.0,1.0,1,A100,1697548645741,1697548653424,120,91.0,20.0,"[32, 2665, 600, 92, 88, 87, 729, 87, 85, 75, 74, 75, 991, 91, 74, 860, 319, 88, 69, 404, 97]","[1697548645773, 1697548648438, 1697548649038, 1697548649130, 1697548649218, 1697548649305, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650430, 1697548651421, 1697548651512, 1697548651586, 1697548652446, 1697548652765, 1697548652853, 1697548652922, 1697548653326, 1697548653423]"
4693,4693,881,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,"[121, 1838]","[1697548695845, 1697548697683]"
4694,4694,85,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631624,1697548635894,120,,,"[16, 996, 1249, 57, 547]","[1697548631640, 1697548632636, 1697548633885, 1697548633942, 1697548634489]"
4695,4695,444,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671242,1697548674840,120,,,"[102, 1732, 538, 79, 60]","[1697548671344, 1697548673076, 1697548673614, 1697548673693, 1697548673753]"
4696,4696,277,36,[],200,llama-7b,128,1,1023.0,1.0,1,A100,1697548662757,1697548663780,120,18.0,1.0,"[21, 1001]","[1697548662778, 1697548663779]"
4697,4697,572,49,[],200,llama-7b,128,1,892.0,1.0,1,A100,1697548693252,1697548694144,120,16.0,1.0,"[52, 840]","[1697548693304, 1697548694144]"
4698,4698,816,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653426,1697548657538,120,,,"[10, 2743]","[1697548653436, 1697548656179]"
4699,4699,926,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548694154,1697548697833,120,,,"[15, 2559]","[1697548694169, 1697548696728]"
4700,4700,246,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657549,1697548659866,120,,,"[395, 1635]","[1697548657944, 1697548659579]"
4701,4701,801,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679646,120,,,"[387, 3006]","[1697548675235, 1697548678241]"
4702,4702,276,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690863,1697548693452,120,,,[49],[1697548690912]
4703,4703,55,16,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548615244,1697548617267,120,12.0,1.0,"[405, 1618]","[1697548615649, 1697548617267]"
4704,4704,635,37,[],200,llama-7b,128,1,939.0,1.0,1,A100,1697548663784,1697548664723,120,23.0,1.0,"[68, 870]","[1697548663852, 1697548664722]"
4705,4705,725,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695714,120,,,"[188, 1785]","[1697548693648, 1697548695433]"
4706,4706,604,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661854,120,,,[84],[1697548659956]
4707,4707,155,48,[],200,llama-7b,128,1,2819.0,1.0,1,A100,1697548695726,1697548698545,120,90.0,20.0,"[311, 2102, 28, 22, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 19, 19, 19, 18, 19, 20]","[1697548696037, 1697548698139, 1697548698167, 1697548698189, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388, 1697548698410, 1697548698431, 1697548698450, 1697548698469, 1697548698488, 1697548698506, 1697548698525, 1697548698545]"
4708,4708,25,34,[],200,llama-7b,128,1,1911.0,1.0,1,A100,1697548661867,1697548663778,120,12.0,1.0,"[171, 1739]","[1697548662038, 1697548663777]"
4709,4709,503,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617272,1697548620355,120,,,"[73, 611, 1440, 84, 83, 80, 81]","[1697548617345, 1697548617956, 1697548619396, 1697548619480, 1697548619563, 1697548619643, 1697548619724]"
4710,4710,355,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663781,1697548667371,120,,,"[16, 925, 1744, 83, 66, 84]","[1697548663797, 1697548664722, 1697548666466, 1697548666549, 1697548666615, 1697548666699]"
4711,4711,66,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664725,1697548667372,120,,,"[16, 2604]","[1697548664741, 1697548667345]"
4712,4712,863,18,[],200,llama-7b,128,1,1967.0,1.0,1,A100,1697548620362,1697548622329,120,10.0,1.0,"[85, 1882]","[1697548620447, 1697548622329]"
4713,4713,291,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624951,120,,,[11],[1697548622346]
4714,4714,649,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624969,1697548627103,120,,,"[80, 1971]","[1697548625049, 1697548627020]"
4715,4715,518,39,[],200,llama-7b,128,1,2328.0,1.0,1,A100,1697548667379,1697548669707,120,23.0,1.0,"[407, 1921]","[1697548667786, 1697548669707]"
4716,4716,874,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671228,120,,,"[56, 1434]","[1697548669769, 1697548671203]"
4717,4717,79,21,[],200,llama-7b,128,1,2200.0,1.0,1,A100,1697548627138,1697548629338,120,12.0,1.0,"[314, 1886]","[1697548627452, 1697548629338]"
4718,4718,408,22,[],200,llama-7b,128,1,922.0,1.0,1,A100,1697548629350,1697548630272,120,16.0,1.0,"[29, 893]","[1697548629379, 1697548630272]"
4719,4719,769,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548630275,1697548635893,120,,,"[11, 2349, 1250, 57, 546]","[1697548630286, 1697548632635, 1697548633885, 1697548633942, 1697548634488]"
4720,4720,305,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674837,120,,,"[250, 1583, 537, 80, 60]","[1697548671493, 1697548673076, 1697548673613, 1697548673693, 1697548673753]"
4721,4721,195,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635907,1697548637692,120,,,[17],[1697548635924]
4722,4722,655,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677258,120,,,"[44, 1908]","[1697548674888, 1697548676796]"
4723,4723,553,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639637,120,,,"[173, 1425]","[1697548637876, 1697548639301]"
4724,4724,84,43,[],200,llama-7b,128,1,2196.0,1.0,1,A100,1697548677278,1697548679474,120,26.0,1.0,"[280, 1916]","[1697548677558, 1697548679474]"
4725,4725,414,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679477,1697548683205,120,,,"[26, 854, 1615, 90, 85, 83, 80]","[1697548679503, 1697548680357, 1697548681972, 1697548682062, 1697548682147, 1697548682230, 1697548682310]"
4726,4726,775,45,[],200,llama-7b,128,1,1813.0,1.0,1,A100,1697548683213,1697548685026,120,17.0,1.0,"[64, 1748]","[1697548683277, 1697548685025]"
4727,4727,209,46,[],200,llama-7b,128,1,1149.0,1.0,1,A100,1697548685030,1697548686179,120,20.0,1.0,"[16, 1133]","[1697548685046, 1697548686179]"
4728,4728,907,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639645,1697548641379,120,,,[14],[1697548639659]
4729,4729,567,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686182,1697548688898,120,,,"[12, 1350]","[1697548686194, 1697548687544]"
4730,4730,311,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642998,120,,,[88],[1697548641472]
4731,4731,920,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[17, 1920]","[1697548688924, 1697548690844]"
4732,4732,318,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691095,1697548693453,120,,,"[48, 2103]","[1697548691143, 1697548693246]"
4733,4733,675,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695716,120,,,"[302, 1667]","[1697548693766, 1697548695433]"
4734,4734,58,34,[],200,llama-7b,128,1,1910.0,1.0,1,A100,1697548661867,1697548663777,120,15.0,1.0,"[106, 1804]","[1697548661973, 1697548663777]"
4735,4735,88,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639654,120,,,"[26, 366, 55]","[1697548637728, 1697548638094, 1697548638149]"
4736,4736,417,35,[],200,llama-7b,128,1,941.0,1.0,1,A100,1697548663782,1697548664723,120,17.0,1.0,"[40, 900]","[1697548663822, 1697548664722]"
4737,4737,627,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664219,1697548667371,120,,,"[337, 1482, 429, 84, 65, 85]","[1697548664556, 1697548666038, 1697548666467, 1697548666551, 1697548666616, 1697548666701]"
4738,4738,414,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639661,1697548641378,120,,,[407],[1697548640068]
4739,4739,772,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664725,1697548667372,120,,,"[21, 2599]","[1697548664746, 1697548667345]"
4740,4740,773,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642999,120,,,[150],[1697548641535]
4741,4741,198,28,[],200,llama-7b,128,1,7194.0,1.0,1,A100,1697548643013,1697548650207,120,96.0,20.0,"[279, 2445, 578, 93, 92, 88, 69, 968, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86, 732, 86, 85]","[1697548643292, 1697548645737, 1697548646315, 1697548646408, 1697548646500, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650035, 1697548650121, 1697548650206]"
4742,4742,450,29,[],200,llama-7b,128,1,6734.0,1.0,1,A100,1697548644781,1697548651515,120,91.0,20.0,"[33, 2262, 550, 95, 96, 94, 71, 93, 71, 891, 93, 88, 86, 732, 86, 86, 74, 74, 74, 1000, 84]","[1697548644814, 1697548647076, 1697548647626, 1697548647721, 1697548647817, 1697548647911, 1697548647982, 1697548648075, 1697548648146, 1697548649037, 1697548649130, 1697548649218, 1697548649304, 1697548650036, 1697548650122, 1697548650208, 1697548650282, 1697548650356, 1697548650430, 1697548651430, 1697548651514]"
4743,4743,204,37,[],200,llama-7b,128,1,3094.0,1.0,1,A100,1697548667378,1697548670472,120,67.0,6.0,"[423, 1906, 471, 85, 64, 64, 81]","[1697548667801, 1697548669707, 1697548670178, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
4744,4744,740,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631902,1697548633849,120,,,[337],[1697548632239]
4745,4745,171,37,[],200,llama-7b,128,1,1949.0,1.0,1,A100,1697548674848,1697548676797,120,6.0,1.0,"[291, 1658]","[1697548675139, 1697548676797]"
4746,4746,170,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635894,120,,,"[183, 1561]","[1697548634038, 1697548635599]"
4747,4747,521,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[351],[1697548636267]
4748,4748,556,29,[],200,llama-7b,128,1,3017.0,1.0,1,A100,1697548650210,1697548653227,120,9.0,1.0,"[30, 2987]","[1697548650240, 1697548653227]"
4749,4749,880,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637704,1697548639645,120,,,"[356, 1241]","[1697548638060, 1697548639301]"
4750,4750,525,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548676800,1697548679648,120,,,"[36, 1406]","[1697548676836, 1697548678242]"
4751,4751,919,30,[],200,llama-7b,128,1,1838.0,1.0,1,A100,1697548653229,1697548655067,120,14.0,1.0,"[5, 1833]","[1697548653234, 1697548655067]"
4752,4752,321,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655069,1697548657502,120,,,"[9, 1102]","[1697548655078, 1697548656180]"
4753,4753,42,39,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679657,1697548681687,120,10.0,1.0,"[226, 1804]","[1697548679883, 1697548681687]"
4754,4754,702,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[44, 2030]","[1697548622647, 1697548624677]"
4755,4755,679,32,[],200,llama-7b,128,1,2034.0,1.0,1,A100,1697548657544,1697548659578,120,15.0,1.0,"[39, 1994]","[1697548657583, 1697548659577]"
4756,4756,280,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639650,1697548641379,120,,,[197],[1697548639847]
4757,4757,105,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659584,1697548664207,120,,,"[73, 1044, 1195, 57, 1028]","[1697548659657, 1697548660701, 1697548661896, 1697548661953, 1697548662981]"
4758,4758,641,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641388,1697548655265,120,,,"[378, 1806, 51, 1588, 85, 65, 954, 93, 91, 89, 69, 968, 96, 95, 93, 72, 93, 72, 891, 92, 88, 86, 731, 87, 85, 75, 74, 74, 992, 91, 74, 858, 320, 88, 69, 404, 96, 89, 84, 651, 96, 85, 62, 80]","[1697548641766, 1697548643572, 1697548643623, 1697548645211, 1697548645296, 1697548645361, 1697548646315, 1697548646408, 1697548646499, 1697548646588, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421, 1697548651512, 1697548651586, 1697548652444, 1697548652764, 1697548652852, 1697548652921, 1697548653325, 1697548653421, 1697548653510, 1697548653594, 1697548654245, 1697548654341, 1697548654426, 1697548654488, 1697548654568]"
4759,4759,104,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627101,120,,,"[275, 1772]","[1697548625249, 1697548627021]"
4760,4760,417,25,[],200,llama-7b,128,1,1746.0,1.0,1,A100,1697548633855,1697548635601,120,17.0,1.0,"[281, 1465]","[1697548634136, 1697548635601]"
4761,4761,403,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681697,1697548683209,120,,,"[11, 1201]","[1697548681708, 1697548682909]"
4762,4762,771,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635603,1697548639636,120,,,"[39, 867, 1223, 416]","[1697548635642, 1697548636509, 1697548637732, 1697548638148]"
4763,4763,764,41,[],200,llama-7b,128,1,1810.0,1.0,1,A100,1697548683216,1697548685026,120,39.0,1.0,"[321, 1489]","[1697548683537, 1697548685026]"
4764,4764,194,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548688902,120,,,"[46, 2466]","[1697548685076, 1697548687542]"
4765,4765,549,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691090,120,,,"[227, 1709]","[1697548689136, 1697548690845]"
4766,4766,465,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[229, 2012]","[1697548627335, 1697548629347]"
4767,4767,284,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641377,120,,,[384],[1697548640039]
4768,4768,879,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693450,120,,,"[187, 1966]","[1697548691283, 1697548693249]"
4769,4769,304,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695719,120,,,"[109, 1864]","[1697548693569, 1697548695433]"
4770,4770,823,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629607,1697548631887,120,,,"[161, 1853]","[1697548629768, 1697548631621]"
4771,4771,642,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643001,120,,,[313],[1697548641699]
4772,4772,766,32,[],200,llama-7b,128,1,2144.0,1.0,1,A100,1697548655282,1697548657426,120,11.0,1.0,"[68, 2075]","[1697548655350, 1697548657425]"
4773,4773,257,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633844,120,,,[175],[1697548632076]
4774,4774,199,33,[],200,llama-7b,128,1,856.0,1.0,1,A100,1697548657432,1697548658288,120,13.0,1.0,"[63, 793]","[1697548657495, 1697548658288]"
4775,4775,553,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548658291,1697548664203,120,,,"[29, 2380, 1196, 57, 1026]","[1697548658320, 1697548660700, 1697548661896, 1697548661953, 1697548662979]"
4776,4776,323,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548656182,1697548659864,120,,,"[35, 2071]","[1697548656217, 1697548658288]"
4777,4777,179,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655282,1697548657537,120,,,"[156, 1989]","[1697548655438, 1697548657427]"
4778,4778,74,29,[],200,llama-7b,128,1,6296.0,1.0,1,A100,1697548643008,1697548649304,120,88.0,20.0,"[61, 1707, 434, 86, 64, 953, 94, 91, 91, 66, 970, 97, 95, 93, 72, 93, 70, 893, 92, 88, 86]","[1697548643069, 1697548644776, 1697548645210, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646589, 1697548646655, 1697548647625, 1697548647722, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648145, 1697548649038, 1697548649130, 1697548649218, 1697548649304]"
4779,4779,539,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657549,1697548659866,120,,,"[340, 1690]","[1697548657889, 1697548659579]"
4780,4780,437,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649309,1697548655270,120,,,"[13, 2856, 267, 320, 88, 69, 403, 97, 88, 85, 650, 100, 82, 63, 80]","[1697548649322, 1697548652178, 1697548652445, 1697548652765, 1697548652853, 1697548652922, 1697548653325, 1697548653422, 1697548653510, 1697548653595, 1697548654245, 1697548654345, 1697548654427, 1697548654490, 1697548654570]"
4781,4781,912,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664216,1697548667369,120,,,"[137, 1684, 430, 83, 67, 85]","[1697548664353, 1697548666037, 1697548666467, 1697548666550, 1697548666617, 1697548666702]"
4782,4782,889,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661854,120,,,[80],[1697548659951]
4783,4783,317,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664208,120,,,"[315, 1596]","[1697548662182, 1697548663778]"
4784,4784,647,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667371,120,,,"[340, 1480, 429, 84, 66, 84]","[1697548664558, 1697548666038, 1697548666467, 1697548666551, 1697548666617, 1697548666701]"
4785,4785,765,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657536,120,,,[64],[1697548655345]
4786,4786,83,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671226,120,,,"[70, 577, 35, 2117, 85, 64, 64, 80]","[1697548667448, 1697548668025, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670390, 1697548670470]"
4787,4787,337,35,[],200,llama-7b,128,1,2146.0,1.0,1,A100,1697548655281,1697548657427,120,12.0,1.0,"[167, 1979]","[1697548655448, 1697548657427]"
4788,4788,696,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657430,1697548659866,120,,,"[60, 798]","[1697548657490, 1697548658288]"
4789,4789,664,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599939,120,,,"[311, 2039]","[1697548597678, 1697548599717]"
4790,4790,120,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661853,120,,,[71],[1697548659942]
4791,4791,921,31,[],200,llama-7b,128,1,1735.0,1.0,1,A100,1697548649135,1697548650870,120,31.0,1.0,"[14, 1720]","[1697548649149, 1697548650869]"
4792,4792,475,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661866,1697548664210,120,,,"[37, 849, 228]","[1697548661903, 1697548662752, 1697548662980]"
4793,4793,346,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548650872,1697548655268,120,,,"[15, 3006, 354, 96, 84, 63, 80]","[1697548650887, 1697548653893, 1697548654247, 1697548654343, 1697548654427, 1697548654490, 1697548654570]"
4794,4794,442,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674840,120,,,"[151, 1683, 537, 78, 61]","[1697548671394, 1697548673077, 1697548673614, 1697548673692, 1697548673753]"
4795,4795,773,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635912,1697548637696,120,,,[334],[1697548636246]
4796,4796,23,22,[],200,llama-7b,128,1,1014.0,1.0,1,A100,1697548631624,1697548632638,120,26.0,1.0,"[69, 944]","[1697548631693, 1697548632637]"
4797,4797,795,33,[],200,llama-7b,128,1,2144.0,1.0,1,A100,1697548655282,1697548657426,120,12.0,1.0,"[76, 2068]","[1697548655358, 1697548657426]"
4798,4798,834,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667371,120,,,"[281, 1538, 429, 85, 65, 85]","[1697548664499, 1697548666037, 1697548666466, 1697548666551, 1697548666616, 1697548666701]"
4799,4799,233,40,[],200,llama-7b,128,1,2328.0,1.0,1,A100,1697548667379,1697548669707,120,6.0,1.0,"[388, 1940]","[1697548667767, 1697548669707]"
4800,4800,383,23,[],200,llama-7b,128,1,1788.0,1.0,1,A100,1697548632644,1697548634432,120,15.0,1.0,"[30, 1758]","[1697548632674, 1697548634432]"
4801,4801,599,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669714,1697548671228,120,,,"[65, 1425]","[1697548669779, 1697548671204]"
4802,4802,207,26,[],200,llama-7b,128,1,1598.0,1.0,1,A100,1697548637703,1697548639301,120,10.0,1.0,"[347, 1251]","[1697548638050, 1697548639301]"
4803,4803,29,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671246,1697548674839,120,,,"[406, 2666]","[1697548671652, 1697548674318]"
4804,4804,564,27,[],200,llama-7b,128,1,8840.0,1.0,1,A100,1697548639306,1697548648146,120,84.0,20.0,"[60, 817, 1231, 365, 1260, 584, 1588, 85, 64, 954, 93, 91, 89, 68, 969, 96, 95, 94, 72, 93, 72]","[1697548639366, 1697548640183, 1697548641414, 1697548641779, 1697548643039, 1697548643623, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646407, 1697548646498, 1697548646587, 1697548646655, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647981, 1697548648074, 1697548648146]"
4805,4805,387,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677256,120,,,"[137, 1815]","[1697548674982, 1697548676797]"
4806,4806,717,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679650,120,,,"[71, 2133]","[1697548677340, 1697548679473]"
4807,4807,138,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683206,120,,,"[286, 1744, 286, 90, 86, 81, 81]","[1697548679943, 1697548681687, 1697548681973, 1697548682063, 1697548682149, 1697548682230, 1697548682311]"
4808,4808,711,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548634434,1697548635904,120,,,"[25, 1140]","[1697548634459, 1697548635599]"
4809,4809,495,46,[],200,llama-7b,128,1,1812.0,1.0,1,A100,1697548683212,1697548685024,120,13.0,1.0,"[15, 1797]","[1697548683227, 1697548685024]"
4810,4810,855,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548688901,120,,,"[36, 2475]","[1697548685066, 1697548687541]"
4811,4811,290,48,[],200,llama-7b,128,1,1944.0,1.0,1,A100,1697548688910,1697548690854,120,14.0,1.0,"[250, 1686]","[1697548689160, 1697548690846]"
4812,4812,735,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690860,1697548693452,120,,,"[47, 946]","[1697548690907, 1697548691853]"
4813,4813,139,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637692,120,,,[170],[1697548636086]
4814,4814,165,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695719,120,,,"[376, 1594]","[1697548693840, 1697548695434]"
4815,4815,498,26,[],200,llama-7b,128,1,1600.0,1.0,1,A100,1697548637700,1697548639300,120,9.0,1.0,"[85, 1515]","[1697548637785, 1697548639300]"
4816,4816,224,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657430,1697548659865,120,,,"[15, 843]","[1697548657445, 1697548658288]"
4817,4817,519,51,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695725,1697548697834,120,,,"[248, 1711]","[1697548695973, 1697548697684]"
4818,4818,800,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679645,120,,,[312],[1697548675160]
4819,4819,225,36,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548679657,1697548681688,120,23.0,1.0,"[104, 1927]","[1697548679761, 1697548681688]"
4820,4820,585,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661856,120,,,[177],[1697548660049]
4821,4821,668,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548683210,120,,,"[25, 1186]","[1697548681723, 1697548682909]"
4822,4822,894,28,[],200,llama-7b,128,1,1790.0,1.0,1,A100,1697548648150,1697548649940,120,14.0,1.0,"[24, 1765]","[1697548648174, 1697548649939]"
4823,4823,316,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649942,1697548655265,120,,,"[29, 2208, 267, 320, 88, 69, 404, 96, 88, 85, 650, 96, 84, 63, 80]","[1697548649971, 1697548652179, 1697548652446, 1697548652766, 1697548652854, 1697548652923, 1697548653327, 1697548653423, 1697548653511, 1697548653596, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
4824,4824,841,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674839,120,,,"[116, 2260, 79, 60]","[1697548671354, 1697548673614, 1697548673693, 1697548673753]"
4825,4825,97,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686706,120,,,"[242, 1567, 238, 91, 90, 89, 70, 86, 87, 68]","[1697548683458, 1697548685025, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685689, 1697548685776, 1697548685844]"
4826,4826,673,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655282,1697548657537,120,,,"[153, 1991]","[1697548655435, 1697548657426]"
4827,4827,646,31,[],200,llama-7b,128,1,2119.0,1.0,1,A100,1697548646319,1697548648438,120,14.0,1.0,"[12, 2107]","[1697548646331, 1697548648438]"
4828,4828,215,31,[],200,llama-7b,128,1,2143.0,1.0,1,A100,1697548655282,1697548657425,120,12.0,1.0,"[196, 1947]","[1697548655478, 1697548657425]"
4829,4829,270,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677258,120,,,"[224, 1727]","[1697548675069, 1697548676796]"
4830,4830,69,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548668033,1697548671226,120,,,"[17, 1660, 469, 84, 65, 64, 81]","[1697548668050, 1697548669710, 1697548670179, 1697548670263, 1697548670328, 1697548670392, 1697548670473]"
4831,4831,574,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657429,1697548659864,120,,,"[12, 847]","[1697548657441, 1697548658288]"
4832,4832,77,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548648441,1697548655268,120,,,"[16, 2412, 562, 82, 74, 857, 320, 88, 69, 406, 98, 87, 84, 651, 97, 83, 62, 81]","[1697548648457, 1697548650869, 1697548651431, 1697548651513, 1697548651587, 1697548652444, 1697548652764, 1697548652852, 1697548652921, 1697548653327, 1697548653425, 1697548653512, 1697548653596, 1697548654247, 1697548654344, 1697548654427, 1697548654489, 1697548654570]"
4833,4833,629,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677280,1697548683204,120,,,"[368, 2707, 1616, 92, 84, 82, 80]","[1697548677648, 1697548680355, 1697548681971, 1697548682063, 1697548682147, 1697548682229, 1697548682309]"
4834,4834,457,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688902,120,,,[218],[1697548686947]
4835,4835,820,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688911,1697548691087,120,,,"[335, 1608]","[1697548689246, 1697548690854]"
4836,4836,438,33,[],200,llama-7b,128,1,2144.0,1.0,1,A100,1697548655283,1697548657427,120,9.0,1.0,"[288, 1855]","[1697548655571, 1697548657426]"
4837,4837,792,34,[],200,llama-7b,128,1,858.0,1.0,1,A100,1697548657430,1697548658288,120,11.0,1.0,"[50, 808]","[1697548657480, 1697548658288]"
4838,4838,54,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686723,120,,,"[141, 1670, 235, 91, 90, 90, 69, 89, 84, 69]","[1697548683357, 1697548685027, 1697548685262, 1697548685353, 1697548685443, 1697548685533, 1697548685602, 1697548685691, 1697548685775, 1697548685844]"
4839,4839,309,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548658291,1697548664203,120,,,"[24, 2385, 1196, 57, 1026]","[1697548658315, 1697548660700, 1697548661896, 1697548661953, 1697548662979]"
4840,4840,398,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674840,120,,,"[141, 1697, 538, 78, 61]","[1697548671379, 1697548673076, 1697548673614, 1697548673692, 1697548673753]"
4841,4841,713,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671225,120,,,"[475, 2326, 84, 64, 64, 81]","[1697548667853, 1697548670179, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
4842,4842,408,44,[],200,llama-7b,128,1,2870.0,1.0,1,A100,1697548686732,1697548689602,120,16.0,1.0,"[338, 2532]","[1697548687070, 1697548689602]"
4843,4843,663,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664219,1697548667372,120,,,"[352, 1896, 85, 65, 85]","[1697548664571, 1697548666467, 1697548666552, 1697548666617, 1697548666702]"
4844,4844,760,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679646,120,,,"[399, 2995]","[1697548675247, 1697548678242]"
4845,4845,739,45,[],200,llama-7b,128,1,2244.0,1.0,1,A100,1697548689609,1697548691853,120,216.0,1.0,"[41, 2203]","[1697548689650, 1697548691853]"
4846,4846,97,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667381,1697548671225,120,,,"[492, 1836, 470, 84, 64, 65, 80]","[1697548667873, 1697548669709, 1697548670179, 1697548670263, 1697548670327, 1697548670392, 1697548670472]"
4847,4847,170,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691857,1697548695714,120,,,"[25, 2261]","[1697548691882, 1697548694143]"
4848,4848,704,31,[],200,llama-7b,128,1,2142.0,1.0,1,A100,1697548655283,1697548657425,120,14.0,1.0,"[176, 1966]","[1697548655459, 1697548657425]"
4849,4849,140,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657429,1697548659865,120,,,"[21, 838]","[1697548657450, 1697548658288]"
4850,4850,469,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661854,120,,,[295],[1697548660172]
4851,4851,192,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683207,120,,,"[31, 1998, 286, 91, 85, 82, 81]","[1697548679688, 1697548681686, 1697548681972, 1697548682063, 1697548682148, 1697548682230, 1697548682311]"
4852,4852,531,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697836,120,,,"[22, 1936]","[1697548695746, 1697548697682]"
4853,4853,828,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664207,120,,,[299],[1697548662167]
4854,4854,253,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667373,120,,,"[17, 1801, 432, 82, 67, 84]","[1697548664234, 1697548666035, 1697548666467, 1697548666549, 1697548666616, 1697548666700]"
4855,4855,269,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633844,120,,,[31],[1697548631926]
4856,4856,202,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679656,1697548683207,120,,,"[18, 2012, 286, 91, 85, 82, 81]","[1697548679674, 1697548681686, 1697548681972, 1697548682063, 1697548682148, 1697548682230, 1697548682311]"
4857,4857,145,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671237,1697548674839,120,,,"[30, 1808, 538, 79, 61]","[1697548671267, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
4858,4858,606,36,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667379,1697548669709,120,9.0,1.0,"[285, 2045]","[1697548667664, 1697548669709]"
4859,4859,627,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635894,120,,,"[188, 1556]","[1697548634043, 1697548635599]"
4860,4860,614,21,[],200,llama-7b,128,1,2011.0,1.0,1,A100,1697548629608,1697548631619,120,15.0,1.0,"[340, 1671]","[1697548629948, 1697548631619]"
4861,4861,567,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683213,1697548686709,120,,,"[58, 1754, 239, 90, 91, 89, 69, 87, 86, 68]","[1697548683271, 1697548685025, 1697548685264, 1697548685354, 1697548685445, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
4862,4862,783,38,[],200,llama-7b,128,1,890.0,1.0,1,A100,1697548661862,1697548662752,120,286.0,1.0,"[16, 874]","[1697548661878, 1697548662752]"
4863,4863,677,32,[],200,llama-7b,128,1,1954.0,1.0,1,A100,1697548647985,1697548649939,120,9.0,1.0,"[16, 1938]","[1697548648001, 1697548649939]"
4864,4864,927,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657536,120,,,"[92, 2053]","[1697548655373, 1697548657426]"
4865,4865,47,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631624,1697548635894,120,,,"[26, 987, 1248, 57, 547]","[1697548631650, 1697548632637, 1697548633885, 1697548633942, 1697548634489]"
4866,4866,352,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659865,120,,,"[231, 1800]","[1697548657777, 1697548659577]"
4867,4867,37,37,[],200,llama-7b,128,1,1489.0,1.0,1,A100,1697548669715,1697548671204,120,20.0,1.0,"[98, 1391]","[1697548669813, 1697548671204]"
4868,4868,710,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661853,120,,,[34],[1697548659905]
4869,4869,369,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671207,1697548674838,120,,,"[30, 1837, 539, 79, 61]","[1697548671237, 1697548673074, 1697548673613, 1697548673692, 1697548673753]"
4870,4870,135,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664205,120,,,"[191, 1719]","[1697548662058, 1697548663777]"
4871,4871,106,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649942,1697548655265,120,,,"[24, 2213, 267, 320, 88, 69, 404, 96, 88, 85, 650, 96, 84, 63, 80]","[1697548649966, 1697548652179, 1697548652446, 1697548652766, 1697548652854, 1697548652923, 1697548653327, 1697548653423, 1697548653511, 1697548653596, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
4872,4872,732,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677258,120,,,"[98, 1854]","[1697548674942, 1697548676796]"
4873,4873,495,34,[],200,llama-7b,128,1,1820.0,1.0,1,A100,1697548664217,1697548666037,120,13.0,1.0,"[58, 1761]","[1697548664275, 1697548666036]"
4874,4874,463,34,[],200,llama-7b,128,1,2144.0,1.0,1,A100,1697548655281,1697548657425,120,39.0,1.0,"[25, 2119]","[1697548655306, 1697548657425]"
4875,4875,794,35,[],200,llama-7b,128,1,859.0,1.0,1,A100,1697548657429,1697548658288,120,11.0,1.0,"[36, 823]","[1697548657465, 1697548658288]"
4876,4876,219,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548658293,1697548664204,120,,,"[32, 2375, 1196, 57, 1026]","[1697548658325, 1697548660700, 1697548661896, 1697548661953, 1697548662979]"
4877,4877,406,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635912,1697548637695,120,,,[64],[1697548635976]
4878,4878,161,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679647,120,,,[169],[1697548677438]
4879,4879,515,41,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679657,1697548681687,120,11.0,1.0,"[27, 2003]","[1697548679684, 1697548681687]"
4880,4880,216,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617366,120,,,"[220, 1804]","[1697548615463, 1697548617267]"
4881,4881,834,29,[],200,llama-7b,128,1,7265.0,1.0,1,A100,1697548647078,1697548654343,120,85.0,20.0,"[24, 2837, 96, 86, 86, 75, 73, 75, 991, 91, 74, 860, 319, 89, 69, 404, 97, 88, 84, 650, 97]","[1697548647102, 1697548649939, 1697548650035, 1697548650121, 1697548650207, 1697548650282, 1697548650355, 1697548650430, 1697548651421, 1697548651512, 1697548651586, 1697548652446, 1697548652765, 1697548652854, 1697548652923, 1697548653327, 1697548653424, 1697548653512, 1697548653596, 1697548654246, 1697548654343]"
4882,4882,541,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548654347,1697548657502,120,,,"[35, 1798]","[1697548654382, 1697548656180]"
4883,4883,925,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688900,120,,,"[118, 1866]","[1697548686847, 1697548688713]"
4884,4884,874,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548683211,120,,,"[50, 1161]","[1697548681748, 1697548682909]"
4885,4885,647,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620371,1697548622597,120,,,"[370, 1589]","[1697548620741, 1697548622330]"
4886,4886,30,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671229,120,,,"[291, 2040, 470, 84, 63, 64, 80]","[1697548667669, 1697548669709, 1697548670179, 1697548670263, 1697548670326, 1697548670390, 1697548670470]"
4887,4887,278,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661851,120,,,[14],[1697548659885]
4888,4888,77,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624958,120,,,"[253, 1821]","[1697548622857, 1697548624678]"
4889,4889,635,35,[],200,llama-7b,128,1,1910.0,1.0,1,A100,1697548661867,1697548663777,120,23.0,1.0,"[91, 1819]","[1697548661958, 1697548663777]"
4890,4890,32,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663784,1697548667372,120,,,"[63, 875, 1745, 82, 67, 84]","[1697548663847, 1697548664722, 1697548666467, 1697548666549, 1697548666616, 1697548666700]"
4891,4891,389,34,[],200,llama-7b,128,1,1838.0,1.0,1,A100,1697548671238,1697548673076,120,8.0,1.0,"[250, 1588]","[1697548671488, 1697548673076]"
4892,4892,386,37,[],200,llama-7b,128,1,3093.0,1.0,1,A100,1697548667378,1697548670471,120,140.0,6.0,"[384, 1944, 472, 84, 65, 64, 80]","[1697548667762, 1697548669706, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
4893,4893,355,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691089,120,,,"[164, 1771]","[1697548689073, 1697548690844]"
4894,4894,901,31,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548657544,1697548659577,120,17.0,1.0,"[14, 2018]","[1697548657558, 1697548659576]"
4895,4895,751,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673083,1697548677261,120,,,[91],[1697548673174]
4896,4896,435,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624962,1697548627102,120,,,"[13, 2045]","[1697548624975, 1697548627020]"
4897,4897,705,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693453,120,,,[32],[1697548691128]
4898,4898,322,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664204,120,,,"[32, 1087, 1195, 57, 1026]","[1697548659614, 1697548660701, 1697548661896, 1697548661953, 1697548662979]"
4899,4899,179,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677278,1697548679649,120,,,"[283, 1913]","[1697548677561, 1697548679474]"
4900,4900,746,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548670475,1697548674837,120,,,"[9, 1107, 2021, 80, 60]","[1697548670484, 1697548671591, 1697548673612, 1697548673692, 1697548673752]"
4901,4901,505,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679659,1697548683209,120,,,[396],[1697548680055]
4902,4902,175,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674843,1697548677258,120,,,[24],[1697548674867]
4903,4903,879,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[73, 2158]","[1697548627179, 1697548629337]"
4904,4904,539,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677270,1697548679648,120,,,"[188, 2017]","[1697548677458, 1697548679475]"
4905,4905,57,41,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679658,1697548681688,120,13.0,1.0,"[316, 1714]","[1697548679974, 1697548681688]"
4906,4906,270,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686707,120,,,"[336, 1474, 237, 91, 91, 89, 69, 87, 86, 69]","[1697548683552, 1697548685026, 1697548685263, 1697548685354, 1697548685445, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685845]"
4907,4907,650,33,[],200,llama-7b,128,1,1820.0,1.0,1,A100,1697548664219,1697548666039,120,13.0,1.0,"[357, 1463]","[1697548664576, 1697548666039]"
4908,4908,80,34,[],200,llama-7b,128,1,1303.0,1.0,1,A100,1697548666043,1697548667346,120,13.0,1.0,"[16, 1287]","[1697548666059, 1697548667346]"
4909,4909,442,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667351,1697548671226,120,,,"[38, 636, 35, 2117, 85, 64, 64, 80]","[1697548667389, 1697548668025, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670390, 1697548670470]"
4910,4910,415,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681697,1697548683210,120,,,[15],[1697548681712]
4911,4911,764,43,[],200,llama-7b,128,1,1810.0,1.0,1,A100,1697548683216,1697548685026,120,39.0,1.0,"[332, 1478]","[1697548683548, 1697548685026]"
4912,4912,194,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548688901,120,,,"[31, 2480]","[1697548685061, 1697548687541]"
4913,4913,859,38,[],200,llama-7b,128,1,1810.0,1.0,1,A100,1697548683216,1697548685026,120,23.0,1.0,"[277, 1533]","[1697548683493, 1697548685026]"
4914,4914,288,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685031,1697548688903,120,,,"[55, 2456]","[1697548685086, 1697548687542]"
4915,4915,310,26,[],200,llama-7b,128,1,2015.0,1.0,1,A100,1697548629602,1697548631617,120,26.0,1.0,"[41, 1974]","[1697548629643, 1697548631617]"
4916,4916,552,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[7, 1930]","[1697548688914, 1697548690844]"
4917,4917,804,29,[],200,llama-7b,128,1,2725.0,1.0,1,A100,1697548643012,1697548645737,120,20.0,1.0,"[268, 2457]","[1697548643280, 1697548645737]"
4918,4918,805,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671244,1697548674837,120,,,"[302, 1531, 537, 79, 60]","[1697548671546, 1697548673077, 1697548673614, 1697548673693, 1697548673753]"
4919,4919,205,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677258,120,,,"[98, 1854]","[1697548674942, 1697548676796]"
4920,4920,229,30,[],200,llama-7b,128,1,2697.0,1.0,1,A100,1697548645741,1697548648438,120,15.0,1.0,"[19, 2678]","[1697548645760, 1697548648438]"
4921,4921,558,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677270,1697548679648,120,,,"[173, 2031]","[1697548677443, 1697548679474]"
4922,4922,809,30,[],200,llama-7b,128,1,2376.0,1.0,1,A100,1697548651517,1697548653893,120,16.0,1.0,"[34, 2342]","[1697548651551, 1697548653893]"
4923,4923,918,39,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679659,1697548681689,120,23.0,1.0,"[411, 1619]","[1697548680070, 1697548681689]"
4924,4924,884,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693453,120,,,"[27, 2123]","[1697548691123, 1697548693246]"
4925,4925,238,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653896,1697548657501,120,,,"[19, 2265]","[1697548653915, 1697548656180]"
4926,4926,671,27,[],200,llama-7b,128,1,1011.0,1.0,1,A100,1697548631625,1697548632636,120,12.0,1.0,"[50, 961]","[1697548631675, 1697548632636]"
4927,4927,589,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548648441,1697548655268,120,,,"[6, 2422, 562, 82, 74, 857, 319, 88, 70, 406, 98, 87, 84, 651, 97, 83, 62, 80]","[1697548648447, 1697548650869, 1697548651431, 1697548651513, 1697548651587, 1697548652444, 1697548652763, 1697548652851, 1697548652921, 1697548653327, 1697548653425, 1697548653512, 1697548653596, 1697548654247, 1697548654344, 1697548654427, 1697548654489, 1697548654569]"
4928,4928,557,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639661,1697548641378,120,,,[403],[1697548640064]
4929,4929,597,32,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548657544,1697548659577,120,39.0,1.0,"[34, 1999]","[1697548657578, 1697548659577]"
4930,4930,318,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693463,1697548695716,120,,,"[278, 1691]","[1697548693741, 1697548695432]"
4931,4931,95,28,[],200,llama-7b,128,1,1792.0,1.0,1,A100,1697548632639,1697548634431,120,12.0,1.0,"[21, 1771]","[1697548632660, 1697548634431]"
4932,4932,21,32,[],200,llama-7b,128,1,2143.0,1.0,1,A100,1697548655283,1697548657426,120,15.0,1.0,"[273, 1870]","[1697548655556, 1697548657426]"
4933,4933,469,33,[],200,llama-7b,128,1,858.0,1.0,1,A100,1697548657430,1697548658288,120,17.0,1.0,"[35, 823]","[1697548657465, 1697548658288]"
4934,4934,914,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642999,120,,,[155],[1697548641540]
4935,4935,828,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548658291,1697548664203,120,,,"[19, 3586, 57, 1026]","[1697548658310, 1697548661896, 1697548661953, 1697548662979]"
4936,4936,311,30,[],200,llama-7b,128,1,6297.0,1.0,1,A100,1697548643008,1697548649305,120,93.0,20.0,"[154, 1616, 433, 85, 65, 953, 94, 91, 89, 68, 968, 96, 97, 93, 70, 95, 70, 894, 91, 88, 87]","[1697548643162, 1697548644778, 1697548645211, 1697548645296, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647817, 1697548647910, 1697548647980, 1697548648075, 1697548648145, 1697548649039, 1697548649130, 1697548649218, 1697548649305]"
4937,4937,257,35,[],200,llama-7b,128,1,1820.0,1.0,1,A100,1697548664217,1697548666037,120,14.0,1.0,"[126, 1694]","[1697548664343, 1697548666037]"
4938,4938,612,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666047,1697548667369,120,,,"[57, 1244]","[1697548666104, 1697548667348]"
4939,4939,37,37,[],200,llama-7b,128,1,2332.0,1.0,1,A100,1697548667378,1697548669710,120,20.0,1.0,"[306, 2025]","[1697548667684, 1697548669709]"
4940,4940,365,38,[],200,llama-7b,128,1,1489.0,1.0,1,A100,1697548669714,1697548671203,120,23.0,1.0,"[79, 1410]","[1697548669793, 1697548671203]"
4941,4941,426,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548634434,1697548635904,120,,,[11],[1697548634445]
4942,4942,729,39,[],200,llama-7b,128,1,2407.0,1.0,1,A100,1697548671206,1697548673613,120,874.0,2.0,"[11, 2396]","[1697548671217, 1697548673613]"
4943,4943,161,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673616,1697548677264,120,,,[7],[1697548673623]
4944,4944,672,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649310,1697548655270,120,,,"[39, 2830, 267, 319, 88, 70, 403, 96, 89, 84, 650, 96, 85, 62, 80]","[1697548649349, 1697548652179, 1697548652446, 1697548652765, 1697548652853, 1697548652923, 1697548653326, 1697548653422, 1697548653511, 1697548653595, 1697548654245, 1697548654341, 1697548654426, 1697548654488, 1697548654568]"
4945,4945,780,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635915,1697548637695,120,,,[64],[1697548635979]
4946,4946,164,42,[],200,llama-7b,128,1,2149.0,1.0,1,A100,1697548691099,1697548693248,120,15.0,1.0,"[293, 1856]","[1697548691392, 1697548693248]"
4947,4947,519,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677278,1697548679649,120,,,"[285, 1911]","[1697548677563, 1697548679474]"
4948,4948,683,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661853,120,,,[36],[1697548659907]
4949,4949,213,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639647,120,,,"[21, 371, 55]","[1697548637723, 1697548638094, 1697548638149]"
4950,4950,849,42,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548679657,1697548681690,120,10.0,1.0,"[207, 1826]","[1697548679864, 1697548681690]"
4951,4951,108,34,[],200,llama-7b,128,1,1115.0,1.0,1,A100,1697548661866,1697548662981,120,182.0,2.0,"[32, 854, 228]","[1697548661898, 1697548662752, 1697548662980]"
4952,4952,420,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[42, 1895]","[1697548688949, 1697548690844]"
4953,4953,752,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693452,120,,,"[27, 2123]","[1697548691123, 1697548693246]"
4954,4954,186,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548697835,120,,,"[394, 2869]","[1697548693858, 1697548696727]"
4955,4955,462,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635917,1697548637695,120,,,[268],[1697548636185]
4956,4956,824,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637701,1697548639648,120,,,"[12, 381, 55]","[1697548637713, 1697548638094, 1697548638149]"
4957,4957,252,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641377,120,,,[385],[1697548640040]
4958,4958,571,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641376,120,,,[332],[1697548639987]
4959,4959,101,32,[],200,llama-7b,128,1,2145.0,1.0,1,A100,1697548655283,1697548657428,120,13.0,1.0,"[172, 1972]","[1697548655455, 1697548657427]"
4960,4960,578,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642997,120,,,[50],[1697548641435]
4961,4961,4,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642996,120,,,[51],[1697548641435]
4962,4962,341,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671228,120,,,"[194, 2135, 472, 84, 64, 65, 80]","[1697548667572, 1697548669707, 1697548670179, 1697548670263, 1697548670327, 1697548670392, 1697548670472]"
4963,4963,334,34,[],200,llama-7b,128,1,568.0,1.0,1,A100,1697548643004,1697548643572,120,15.0,1.0,"[35, 533]","[1697548643039, 1697548643572]"
4964,4964,803,16,[],200,llama-7b,128,1,1960.0,1.0,1,A100,1697548620370,1697548622330,120,20.0,1.0,"[329, 1630]","[1697548620699, 1697548622329]"
4965,4965,325,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657538,120,,,"[182, 1962]","[1697548655463, 1697548657425]"
4966,4966,679,34,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548657549,1697548659579,120,15.0,1.0,"[346, 1684]","[1697548657895, 1697548659579]"
4967,4967,202,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624958,120,,,"[58, 1064]","[1697548622393, 1697548623457]"
4968,4968,38,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[156],[1697548636072]
4969,4969,113,35,[],200,llama-7b,128,1,1117.0,1.0,1,A100,1697548659585,1697548660702,120,13.0,1.0,"[86, 1031]","[1697548659671, 1697548660702]"
4970,4970,562,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627101,120,,,"[304, 1744]","[1697548625278, 1697548627022]"
4971,4971,389,26,[],200,llama-7b,128,1,1595.0,1.0,1,A100,1697548637704,1697548639299,120,8.0,1.0,"[274, 1321]","[1697548637978, 1697548639299]"
4972,4972,470,36,[],200,llama-7b,128,1,2275.0,1.0,1,A100,1697548660705,1697548662980,120,39.0,2.0,"[20, 2027, 228]","[1697548660725, 1697548662752, 1697548662980]"
4973,4973,852,27,[],200,llama-7b,128,1,8842.0,1.0,1,A100,1697548639303,1697548648145,120,100.0,20.0,"[33, 847, 1231, 365, 1261, 583, 1587, 86, 64, 953, 94, 91, 89, 68, 969, 96, 95, 94, 71, 94, 71]","[1697548639336, 1697548640183, 1697548641414, 1697548641779, 1697548643040, 1697548643623, 1697548645210, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646587, 1697548646655, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647980, 1697548648074, 1697548648145]"
4974,4974,832,37,[],200,llama-7b,128,1,1737.0,1.0,1,A100,1697548662985,1697548664722,120,15.0,1.0,"[6, 1731]","[1697548662991, 1697548664722]"
4975,4975,349,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664725,1697548667372,120,,,"[10, 2610]","[1697548664735, 1697548667345]"
4976,4976,911,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627106,1697548629597,120,,,"[83, 2149]","[1697548627189, 1697548629338]"
4977,4977,340,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631890,120,,,"[31, 1984]","[1697548629633, 1697548631617]"
4978,4978,703,39,[],200,llama-7b,128,1,2328.0,1.0,1,A100,1697548667379,1697548669707,120,12.0,1.0,"[378, 1949]","[1697548667757, 1697548669706]"
4979,4979,746,27,[],200,llama-7b,128,1,8678.0,1.0,1,A100,1697548639303,1697548647981,120,345.0,18.0,"[18, 861, 1231, 365, 1262, 583, 1587, 85, 64, 954, 94, 91, 89, 68, 969, 96, 95, 94, 71]","[1697548639321, 1697548640182, 1697548641413, 1697548641778, 1697548643040, 1697548643623, 1697548645210, 1697548645295, 1697548645359, 1697548646313, 1697548646407, 1697548646498, 1697548646587, 1697548646655, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647980]"
4980,4980,133,40,[],200,llama-7b,128,1,1490.0,1.0,1,A100,1697548669713,1697548671203,120,15.0,1.0,"[46, 1444]","[1697548669759, 1697548671203]"
4981,4981,699,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631900,1697548633848,120,,,[293],[1697548632193]
4982,4982,102,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635903,120,,,"[315, 1428]","[1697548634170, 1697548635598]"
4983,4983,464,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637694,120,,,[259],[1697548636175]
4984,4984,823,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637701,1697548639649,120,,,"[17, 376, 55]","[1697548637718, 1697548638094, 1697548638149]"
4985,4985,248,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639654,1697548641375,120,,,[313],[1697548639967]
4986,4986,669,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674836,120,,,"[226, 1606, 538, 79, 61]","[1697548671469, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
4987,4987,607,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641387,1697548643002,120,,,[353],[1697548641740]
4988,4988,488,41,[],200,llama-7b,128,1,1868.0,1.0,1,A100,1697548671207,1697548673075,120,6.0,1.0,"[37, 1831]","[1697548671244, 1697548673075]"
4989,4989,695,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655283,1697548657502,120,,,"[254, 1888]","[1697548655537, 1697548657425]"
4990,4990,120,29,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548657544,1697548659577,120,17.0,1.0,"[29, 2004]","[1697548657573, 1697548659577]"
4991,4991,845,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673080,1697548674841,120,,,"[37, 1202]","[1697548673117, 1697548674319]"
4992,4992,465,15,[],200,llama-7b,128,1,2101.0,1.0,1,A100,1697548617379,1697548619480,120,364.0,3.0,"[210, 1697, 110, 84]","[1697548617589, 1697548619286, 1697548619396, 1697548619480]"
4993,4993,3,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661854,120,,,[75],[1697548659946]
4994,4994,249,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548677260,120,,,"[311, 1638]","[1697548675159, 1697548676797]"
4995,4995,100,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674843,1697548677258,120,,,"[40, 1913]","[1697548674883, 1697548676796]"
4996,4996,610,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677274,1697548679648,120,,,"[189, 2012]","[1697548677463, 1697548679475]"
4997,4997,822,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619483,1697548622595,120,,,"[13, 1429]","[1697548619496, 1697548620925]"
4998,4998,670,28,[],200,llama-7b,128,1,6034.0,1.0,1,A100,1697548643004,1697548649038,120,67.0,18.0,"[26, 542, 52, 1588, 83, 65, 955, 94, 90, 90, 68, 968, 96, 95, 94, 72, 93, 71, 892]","[1697548643030, 1697548643572, 1697548643624, 1697548645212, 1697548645295, 1697548645360, 1697548646315, 1697548646409, 1697548646499, 1697548646589, 1697548646657, 1697548647625, 1697548647721, 1697548647816, 1697548647910, 1697548647982, 1697548648075, 1697548648146, 1697548649038]"
4999,4999,340,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624951,120,,,"[78, 1995]","[1697548622682, 1697548624677]"
5000,5000,357,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661869,1697548664206,120,,,"[199, 1710]","[1697548662068, 1697548663778]"
5001,5001,557,26,[],200,llama-7b,128,1,2144.0,1.0,1,A100,1697548655282,1697548657426,120,31.0,1.0,"[81, 2063]","[1697548655363, 1697548657426]"
5002,5002,107,51,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,[111],[1697548695835]
5003,5003,698,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624969,1697548627100,120,,,"[172, 1880]","[1697548625141, 1697548627021]"
5004,5004,344,28,[],200,llama-7b,128,1,2867.0,1.0,1,A100,1697548649312,1697548652179,120,13.0,1.0,"[57, 2810]","[1697548649369, 1697548652179]"
5005,5005,123,19,[],200,llama-7b,128,1,2233.0,1.0,1,A100,1697548627106,1697548629339,120,14.0,1.0,"[148, 2085]","[1697548627254, 1697548629339]"
5006,5006,100,29,[],200,llama-7b,128,1,5304.0,1.0,1,A100,1697548649041,1697548654345,120,732.0,14.0,"[19, 1809, 562, 83, 74, 856, 320, 88, 69, 403, 97, 92, 81, 653, 98]","[1697548649060, 1697548650869, 1697548651431, 1697548651514, 1697548651588, 1697548652444, 1697548652764, 1697548652852, 1697548652921, 1697548653324, 1697548653421, 1697548653513, 1697548653594, 1697548654247, 1697548654345]"
5007,5007,486,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629351,1697548631887,120,,,"[38, 883]","[1697548629389, 1697548630272]"
5008,5008,911,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657429,1697548659866,120,,,"[56, 803]","[1697548657485, 1697548658288]"
5009,5009,701,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548652182,1697548655270,120,,,"[23, 2861]","[1697548652205, 1697548655066]"
5010,5010,311,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661854,120,,,[95],[1697548659966]
5011,5011,551,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683214,1697548686709,120,,,"[67, 1745, 238, 90, 91, 89, 69, 87, 86, 68]","[1697548683281, 1697548685026, 1697548685264, 1697548685354, 1697548685445, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
5012,5012,842,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633849,120,,,[155],[1697548632050]
5013,5013,848,42,[],200,llama-7b,128,1,1839.0,1.0,1,A100,1697548671238,1697548673077,120,47.0,1.0,"[323, 1516]","[1697548671561, 1697548673077]"
5014,5014,684,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667368,120,,,"[136, 1684, 430, 83, 67, 84]","[1697548664353, 1697548666037, 1697548666467, 1697548666550, 1697548666617, 1697548666701]"
5015,5015,246,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633855,1697548635903,120,,,"[305, 1438]","[1697548634160, 1697548635598]"
5016,5016,129,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655295,1697548657536,120,,,"[345, 1787]","[1697548655640, 1697548657427]"
5017,5017,601,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637697,120,,,[151],[1697548636067]
5018,5018,483,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657545,1697548659864,120,,,"[138, 1896]","[1697548657683, 1697548659579]"
5019,5019,30,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639645,120,,,"[342, 1255]","[1697548638045, 1697548639300]"
5020,5020,112,36,[],200,llama-7b,128,1,2802.0,1.0,1,A100,1697548667377,1697548670179,120,16.0,2.0,"[460, 1870, 471]","[1697548667837, 1697548669707, 1697548670178]"
5021,5021,471,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548670181,1697548671229,120,,,"[11, 1012]","[1697548670192, 1697548671204]"
5022,5022,253,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673083,1697548677261,120,,,"[94, 2295]","[1697548673177, 1697548675472]"
5023,5023,52,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635907,1697548637692,120,,,[22],[1697548635929]
5024,5024,213,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548662753,1697548664208,120,,,"[15, 1011]","[1697548662768, 1697548663779]"
5025,5025,610,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677278,1697548679649,120,,,"[273, 1922]","[1697548677551, 1697548679473]"
5026,5026,408,24,[],200,llama-7b,128,1,1599.0,1.0,1,A100,1697548637700,1697548639299,120,16.0,1.0,"[73, 1526]","[1697548637773, 1697548639299]"
5027,5027,384,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639651,1697548641382,120,,,[238],[1697548639889]
5028,5028,736,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639308,1697548655272,120,,,"[59, 816, 1231, 364, 1262, 583, 1588, 85, 64, 954, 94, 90, 90, 68, 968, 96, 95, 94, 72, 93, 72, 891, 92, 88, 86, 731, 86, 85, 76, 74, 74, 991, 91, 74, 858, 320, 88, 70, 403, 97, 89, 84, 651, 96, 84, 63, 80]","[1697548639367, 1697548640183, 1697548641414, 1697548641778, 1697548643040, 1697548643623, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646498, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647981, 1697548648074, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649303, 1697548650034, 1697548650120, 1697548650205, 1697548650281, 1697548650355, 1697548650429, 1697548651420, 1697548651511, 1697548651585, 1697548652443, 1697548652763, 1697548652851, 1697548652921, 1697548653324, 1697548653421, 1697548653510, 1697548653594, 1697548654245, 1697548654341, 1697548654425, 1697548654488, 1697548654568]"
5029,5029,744,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642997,120,,,[249],[1697548641635]
5030,5030,836,38,[],200,llama-7b,128,1,1838.0,1.0,1,A100,1697548671238,1697548673076,120,11.0,1.0,"[38, 1799]","[1697548671276, 1697548673075]"
5031,5031,147,27,[],200,llama-7b,128,1,1771.0,1.0,1,A100,1697548643006,1697548644777,120,182.0,1.0,"[64, 1707]","[1697548643070, 1697548644777]"
5032,5032,35,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683207,120,,,"[312, 1718, 286, 91, 85, 82, 80]","[1697548679969, 1697548681687, 1697548681973, 1697548682064, 1697548682149, 1697548682231, 1697548682311]"
5033,5033,506,28,[],200,llama-7b,128,1,2296.0,1.0,1,A100,1697548644780,1697548647076,120,16.0,1.0,"[24, 2271]","[1697548644804, 1697548647075]"
5034,5034,352,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673083,1697548674844,120,,,"[96, 1140]","[1697548673179, 1697548674319]"
5035,5035,798,7,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.44 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 7.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548597367,1697548599945,120,,,[272],[1697548597639]
5036,5036,711,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674850,1697548679646,120,,,"[406, 2986]","[1697548675256, 1697548678242]"
5037,5037,867,29,[],200,llama-7b,128,1,7265.0,1.0,1,A100,1697548647079,1697548654344,120,91.0,20.0,"[33, 2827, 96, 86, 86, 75, 74, 74, 993, 89, 74, 860, 319, 89, 69, 404, 97, 88, 84, 650, 97]","[1697548647112, 1697548649939, 1697548650035, 1697548650121, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651423, 1697548651512, 1697548651586, 1697548652446, 1697548652765, 1697548652854, 1697548652923, 1697548653327, 1697548653424, 1697548653512, 1697548653596, 1697548654246, 1697548654343]"
5038,5038,229,8,[],200,llama-7b,128,1,1935.0,1.0,1,A100,1697548599953,1697548601888,120,15.0,1.0,"[487, 1448]","[1697548600440, 1697548601888]"
5039,5039,136,41,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548679656,1697548681687,120,31.0,1.0,"[17, 2013]","[1697548679673, 1697548681686]"
5040,5040,582,9,[],200,llama-7b,128,1,783.0,1.0,1,A100,1697548601893,1697548602676,120,19.0,1.0,"[76, 707]","[1697548601969, 1697548602676]"
5041,5041,490,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548683212,120,,,"[65, 1146]","[1697548681763, 1697548682909]"
5042,5042,827,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666047,1697548667369,120,,,"[46, 1254]","[1697548666093, 1697548667347]"
5043,5043,849,43,[],200,llama-7b,128,1,1808.0,1.0,1,A100,1697548683219,1697548685027,120,10.0,1.0,"[354, 1454]","[1697548683573, 1697548685027]"
5044,5044,259,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671227,120,,,"[110, 2218, 472, 84, 65, 64, 80]","[1697548667488, 1697548669706, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
5045,5045,253,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685031,1697548688903,120,,,"[65, 2446]","[1697548685096, 1697548687542]"
5046,5046,391,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686705,120,,,"[226, 1821, 91, 90, 89, 69, 87, 87, 68]","[1697548683442, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685776, 1697548685844]"
5047,5047,615,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688910,1697548691087,120,,,"[261, 1683]","[1697548689171, 1697548690854]"
5048,5048,44,46,[],200,llama-7b,128,1,2152.0,1.0,1,A100,1697548691096,1697548693248,120,12.0,1.0,"[85, 2066]","[1697548691181, 1697548693247]"
5049,5049,7,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602679,1697548605263,120,,,"[19, 2371]","[1697548602698, 1697548605069]"
5050,5050,397,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693252,1697548695716,120,,,"[47, 845]","[1697548693299, 1697548694144]"
5051,5051,748,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686720,1697548688902,120,,,[233],[1697548686953]
5052,5052,757,48,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695724,1697548697683,120,20.0,1.0,"[146, 1812]","[1697548695870, 1697548697682]"
5053,5053,618,37,[],200,llama-7b,128,1,1833.0,1.0,1,A100,1697548671243,1697548673076,120,9.0,1.0,"[298, 1535]","[1697548671541, 1697548673076]"
5054,5054,152,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691088,120,,,"[140, 1803]","[1697548689049, 1697548690852]"
5055,5055,47,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673080,1697548674843,120,,,"[65, 1173]","[1697548673145, 1697548674318]"
5056,5056,453,11,[],200,llama-7b,128,1,1953.0,1.0,1,A100,1697548605280,1697548607233,120,26.0,1.0,"[399, 1554]","[1697548605679, 1697548607233]"
5057,5057,397,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674849,1697548679646,120,,,"[402, 2991]","[1697548675251, 1697548678242]"
5058,5058,814,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607239,1697548608968,120,,,"[125, 1407]","[1697548607364, 1697548608771]"
5059,5059,597,30,[],200,llama-7b,128,1,1909.0,1.0,1,A100,1697548661870,1697548663779,120,39.0,1.0,"[272, 1637]","[1697548662142, 1697548663779]"
5060,5060,106,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695717,120,,,"[312, 1657]","[1697548693776, 1697548695433]"
5061,5061,727,40,[],200,llama-7b,128,1,2574.0,1.0,1,A100,1697548679657,1697548682231,120,58.0,5.0,"[12, 2017, 286, 91, 85, 82]","[1697548679669, 1697548681686, 1697548681972, 1697548682063, 1697548682148, 1697548682230]"
5062,5062,464,44,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695724,1697548697683,120,12.0,1.0,"[47, 1912]","[1697548695771, 1697548697683]"
5063,5063,511,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691095,1697548693448,120,,,"[85, 2067]","[1697548691180, 1697548693247]"
5064,5064,247,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608975,1697548610832,120,,,[384],[1697548609359]
5065,5065,872,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695717,120,,,"[20, 1952]","[1697548693479, 1697548695431]"
5066,5066,606,14,[],200,llama-7b,128,1,1968.0,1.0,1,A100,1697548610847,1697548612815,120,9.0,1.0,"[29, 1939]","[1697548610876, 1697548612815]"
5067,5067,297,51,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695725,1697548697834,120,,,"[250, 1709]","[1697548695975, 1697548697684]"
5068,5068,156,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682233,1697548686706,120,,,"[11, 1799, 1219, 91, 91, 89, 69, 87, 86, 69]","[1697548682244, 1697548684043, 1697548685262, 1697548685353, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
5069,5069,2,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663783,1697548667372,120,,,"[35, 904, 1744, 83, 67, 84]","[1697548663818, 1697548664722, 1697548666466, 1697548666549, 1697548666616, 1697548666700]"
5070,5070,359,32,[],200,llama-7b,128,1,2328.0,1.0,1,A100,1697548667381,1697548669709,120,10.0,1.0,"[500, 1828]","[1697548667881, 1697548669709]"
5071,5071,519,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688901,120,,,"[133, 1852]","[1697548686862, 1697548688714]"
5072,5072,713,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669714,1697548671228,120,,,[90],[1697548669804]
5073,5073,31,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548612820,1697548615234,120,,,"[21, 697]","[1697548612841, 1697548613538]"
5074,5074,361,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615243,1697548617364,120,,,"[198, 1825]","[1697548615441, 1697548617266]"
5075,5075,534,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548670476,1697548674837,120,,,"[16, 1099, 2021, 80, 60]","[1697548670492, 1697548671591, 1697548673612, 1697548673692, 1697548673752]"
5076,5076,651,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691089,120,,,"[169, 1766]","[1697548689078, 1697548690844]"
5077,5077,715,17,[],200,llama-7b,128,1,1907.0,1.0,1,A100,1697548617378,1697548619285,120,20.0,1.0,"[27, 1880]","[1697548617405, 1697548619285]"
5078,5078,892,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677255,120,,,"[123, 1830]","[1697548674967, 1697548676797]"
5079,5079,820,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619290,1697548620357,120,,,"[23, 1010]","[1697548619313, 1697548620323]"
5080,5080,83,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693449,120,,,"[157, 1995]","[1697548691253, 1697548693248]"
5081,5081,140,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671247,1697548674840,120,,,"[410, 2661]","[1697548671657, 1697548674318]"
5082,5082,880,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691087,120,,,"[134, 1811]","[1697548689043, 1697548690854]"
5083,5083,413,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695718,120,,,"[38, 1934]","[1697548693497, 1697548695431]"
5084,5084,396,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693450,120,,,"[244, 1905]","[1697548691341, 1697548693246]"
5085,5085,497,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548677260,120,,,"[301, 1648]","[1697548675149, 1697548676797]"
5086,5086,771,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695725,1697548697834,120,,,"[233, 1726]","[1697548695958, 1697548697684]"
5087,5087,829,36,[],200,llama-7b,128,1,2196.0,1.0,1,A100,1697548677279,1697548679475,120,20.0,1.0,"[309, 1887]","[1697548677588, 1697548679475]"
5088,5088,320,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677268,1697548679646,120,,,[87],[1697548677355]
5089,5089,147,18,[],200,llama-7b,128,1,1034.0,1.0,1,A100,1697548619288,1697548620322,120,182.0,1.0,"[12, 1022]","[1697548619300, 1697548620322]"
5090,5090,506,19,[],200,llama-7b,128,1,2001.0,1.0,1,A100,1697548620327,1697548622328,120,16.0,1.0,"[29, 1972]","[1697548620356, 1697548622328]"
5091,5091,838,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622335,1697548624953,120,,,"[44, 1077]","[1697548622379, 1697548623456]"
5092,5092,260,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679480,1697548683206,120,,,"[48, 829, 1615, 91, 85, 82, 81]","[1697548679528, 1697548680357, 1697548681972, 1697548682063, 1697548682148, 1697548682230, 1697548682311]"
5093,5093,263,21,[],200,llama-7b,128,1,2052.0,1.0,1,A100,1697548624970,1697548627022,120,15.0,1.0,"[303, 1749]","[1697548625273, 1697548627022]"
5094,5094,623,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627024,1697548629598,120,,,[48],[1697548627072]
5095,5095,755,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695717,120,,,"[322, 1647]","[1697548693786, 1697548695433]"
5096,5096,459,34,[],200,llama-7b,128,1,2484.0,1.0,1,A100,1697548664217,1697548666701,120,58.0,5.0,"[62, 1757, 431, 82, 67, 85]","[1697548664279, 1697548666036, 1697548666467, 1697548666549, 1697548666616, 1697548666701]"
5097,5097,51,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629608,1697548631890,120,,,"[274, 1736]","[1697548629882, 1697548631618]"
5098,5098,409,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633846,120,,,[65],[1697548631961]
5099,5099,679,41,[],200,llama-7b,128,1,2032.0,1.0,1,A100,1697548679657,1697548681689,120,15.0,1.0,"[114, 1918]","[1697548679771, 1697548681689]"
5100,5100,176,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,[125],[1697548695849]
5101,5101,926,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664204,120,,,"[24, 1095, 1194, 58, 1026]","[1697548659606, 1697548660701, 1697548661895, 1697548661953, 1697548662979]"
5102,5102,342,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548683211,120,,,"[40, 1171]","[1697548681738, 1697548682909]"
5103,5103,107,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681697,1697548683210,120,,,[21],[1697548681718]
5104,5104,518,43,[],200,llama-7b,128,1,891.0,1.0,1,A100,1697548693254,1697548694145,120,23.0,1.0,"[65, 825]","[1697548693319, 1697548694144]"
5105,5105,878,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548694153,1697548697833,120,,,"[14, 2561]","[1697548694167, 1697548696728]"
5106,5106,739,25,[],200,llama-7b,128,1,1745.0,1.0,1,A100,1697548633855,1697548635600,120,216.0,1.0,"[203, 1542]","[1697548634058, 1697548635600]"
5107,5107,168,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635611,1697548639637,120,,,"[73, 825, 1222, 417]","[1697548635684, 1697548636509, 1697548637731, 1697548638148]"
5108,5108,621,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686706,120,,,"[237, 1571, 239, 91, 90, 89, 70, 86, 87, 68]","[1697548683453, 1697548685024, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685689, 1697548685776, 1697548685844]"
5109,5109,248,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686720,1697548688899,120,,,"[34, 1958]","[1697548686754, 1697548688712]"
5110,5110,692,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[43, 1894]","[1697548688950, 1697548690844]"
5111,5111,117,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693453,120,,,"[46, 2104]","[1697548691142, 1697548693246]"
5112,5112,461,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657431,1697548659867,120,,,[69],[1697548657500]
5113,5113,480,47,[],200,llama-7b,128,1,1970.0,1.0,1,A100,1697548693463,1697548695433,120,26.0,1.0,"[303, 1667]","[1697548693766, 1697548695433]"
5114,5114,250,43,[],200,llama-7b,128,1,1943.0,1.0,1,A100,1697548688911,1697548690854,120,31.0,1.0,"[344, 1599]","[1697548689255, 1697548690854]"
5115,5115,688,35,[],200,llama-7b,128,1,4242.0,1.0,1,A100,1697548643575,1697548647817,120,345.0,4.0,"[6, 3494, 550, 96, 96]","[1697548643581, 1697548647075, 1697548647625, 1697548647721, 1697548647817]"
5116,5116,837,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695437,1697548697835,120,,,"[46, 1245]","[1697548695483, 1697548696728]"
5117,5117,562,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693463,1697548695716,120,,,"[220, 1749]","[1697548693683, 1697548695432]"
5118,5118,925,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,"[126, 1833]","[1697548695850, 1697548697683]"
5119,5119,287,37,[],200,llama-7b,128,1,1818.0,1.0,1,A100,1697548664217,1697548666035,120,10.0,1.0,"[27, 1791]","[1697548664244, 1697548666035]"
5120,5120,644,38,[],200,llama-7b,128,1,1300.0,1.0,1,A100,1697548666047,1697548667347,120,19.0,1.0,"[101, 1199]","[1697548666148, 1697548667347]"
5121,5121,164,34,[],200,llama-7b,128,1,1911.0,1.0,1,A100,1697548661867,1697548663778,120,15.0,1.0,"[175, 1736]","[1697548662042, 1697548663778]"
5122,5122,607,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690857,1697548693451,120,,,"[23, 972]","[1697548690880, 1697548691852]"
5123,5123,818,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661853,120,,,[208],[1697548660080]
5124,5124,112,36,[],200,llama-7b,128,1,2216.0,1.0,1,A100,1697548647820,1697548650036,120,16.0,2.0,"[16, 2102, 98]","[1697548647836, 1697548649938, 1697548650036]"
5125,5125,524,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663783,1697548667372,120,,,"[49, 890, 1745, 82, 67, 84]","[1697548663832, 1697548664722, 1697548666467, 1697548666549, 1697548666616, 1697548666700]"
5126,5126,471,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548650039,1697548655266,120,,,"[6, 3182, 100, 96, 89, 84, 650, 96, 84, 63, 80]","[1697548650045, 1697548653227, 1697548653327, 1697548653423, 1697548653512, 1697548653596, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
5127,5127,119,27,[],200,llama-7b,128,1,1766.0,1.0,1,A100,1697548643012,1697548644778,120,31.0,1.0,"[246, 1520]","[1697548643258, 1697548644778]"
5128,5128,480,28,[],200,llama-7b,128,1,2295.0,1.0,1,A100,1697548644781,1697548647076,120,26.0,1.0,"[46, 2249]","[1697548644827, 1697548647076]"
5129,5129,839,29,[],200,llama-7b,128,1,3203.0,1.0,1,A100,1697548647079,1697548650282,120,58.0,5.0,"[33, 2827, 96, 87, 85, 75]","[1697548647112, 1697548649939, 1697548650035, 1697548650122, 1697548650207, 1697548650282]"
5130,5130,272,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548650284,1697548655268,120,,,"[14, 3595, 354, 96, 84, 63, 80]","[1697548650298, 1697548653893, 1697548654247, 1697548654343, 1697548654427, 1697548654490, 1697548654570]"
5131,5131,104,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659865,120,,,"[195, 1838]","[1697548657741, 1697548659579]"
5132,5132,465,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661855,120,,,[97],[1697548659968]
5133,5133,37,45,[],200,llama-7b,128,1,1970.0,1.0,1,A100,1697548693464,1697548695434,120,20.0,1.0,"[384, 1586]","[1697548693848, 1697548695434]"
5134,5134,39,45,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548679657,1697548681688,120,8.0,1.0,"[103, 1928]","[1697548679760, 1697548681688]"
5135,5135,269,22,[],200,llama-7b,128,1,1012.0,1.0,1,A100,1697548631624,1697548632636,120,11.0,1.0,"[34, 978]","[1697548631658, 1697548632636]"
5136,5136,797,33,[],200,llama-7b,128,1,885.0,1.0,1,A100,1697548661867,1697548662752,120,26.0,1.0,"[46, 839]","[1697548661913, 1697548662752]"
5137,5137,445,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635910,1697548637693,120,,,[34],[1697548635944]
5138,5138,395,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681697,1697548683211,120,,,"[41, 1171]","[1697548681738, 1697548682909]"
5139,5139,227,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548662753,1697548664208,120,,,"[16, 1010]","[1697548662769, 1697548663779]"
5140,5140,581,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667370,120,,,"[320, 1499, 429, 85, 65, 85]","[1697548664538, 1697548666037, 1697548666466, 1697548666551, 1697548666616, 1697548666701]"
5141,5141,799,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639639,120,,,"[241, 1355]","[1697548637944, 1697548639299]"
5142,5142,833,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655282,1697548657502,120,,,"[202, 1941]","[1697548655484, 1697548657425]"
5143,5143,236,39,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548657544,1697548659577,120,8.0,1.0,"[33, 2000]","[1697548657577, 1697548659577]"
5144,5144,595,40,[],200,llama-7b,128,1,1118.0,1.0,1,A100,1697548659583,1697548660701,120,8.0,1.0,"[69, 1049]","[1697548659652, 1697548660701]"
5145,5145,24,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548660704,1697548664209,120,,,"[16, 2260]","[1697548660720, 1697548662980]"
5146,5146,454,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674839,120,,,"[357, 2723]","[1697548671595, 1697548674318]"
5147,5147,815,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677257,120,,,"[209, 1742]","[1697548675054, 1697548676796]"
5148,5148,217,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677270,1697548679649,120,,,"[197, 2006]","[1697548677467, 1697548679473]"
5149,5149,571,41,[],200,llama-7b,128,1,2315.0,1.0,1,A100,1697548679657,1697548681972,120,67.0,2.0,"[182, 1850, 283]","[1697548679839, 1697548681689, 1697548681972]"
5150,5150,595,48,[],200,llama-7b,128,1,1988.0,1.0,1,A100,1697548686724,1697548688712,120,8.0,1.0,"[25, 1963]","[1697548686749, 1697548688712]"
5151,5151,16,49,[],200,llama-7b,128,1,885.0,1.0,1,A100,1697548688717,1697548689602,120,9.0,1.0,"[36, 849]","[1697548688753, 1697548689602]"
5152,5152,925,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681974,1697548686705,120,,,"[5, 2064, 1219, 91, 90, 89, 70, 87, 86, 68]","[1697548681979, 1697548684043, 1697548685262, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
5153,5153,224,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639651,1697548641380,120,,,[208],[1697548639859]
5154,5154,461,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548689608,1697548693449,120,,,[32],[1697548689640]
5155,5155,811,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661852,120,,,[21],[1697548659892]
5156,5156,541,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667368,120,,,"[140, 1680, 431, 82, 67, 85]","[1697548664357, 1697548666037, 1697548666468, 1697548666550, 1697548666617, 1697548666702]"
5157,5157,901,33,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667377,1697548669707,120,17.0,1.0,"[160, 2169]","[1697548667537, 1697548669706]"
5158,5158,820,51,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695717,120,,,"[25, 1947]","[1697548693484, 1697548695431]"
5159,5159,256,52,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695725,1697548697834,120,,,"[235, 1724]","[1697548695960, 1697548697684]"
5160,5160,583,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642999,120,,,[159],[1697548641545]
5161,5161,355,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686721,1697548688899,120,,,"[27, 1964]","[1697548686748, 1697548688712]"
5162,5162,915,28,[],200,llama-7b,128,1,1767.0,1.0,1,A100,1697548643012,1697548644779,120,182.0,1.0,"[148, 1618]","[1697548643160, 1697548644778]"
5163,5163,347,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548644782,1697548655267,120,,,"[52, 2792, 95, 96, 94, 71, 93, 72, 891, 92, 88, 86, 732, 86, 85, 75, 74, 74, 1000, 84, 73, 858, 319, 89, 69, 404, 96, 89, 84, 651, 96, 84, 63, 80]","[1697548644834, 1697548647626, 1697548647721, 1697548647817, 1697548647911, 1697548647982, 1697548648075, 1697548648147, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650036, 1697548650122, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651430, 1697548651514, 1697548651587, 1697548652445, 1697548652764, 1697548652853, 1697548652922, 1697548653326, 1697548653422, 1697548653511, 1697548653595, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
5164,5164,713,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691086,120,,,[71],[1697548688980]
5165,5165,468,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666704,1697548671225,120,,,"[17, 752, 587, 2117, 85, 64, 64, 80]","[1697548666721, 1697548667473, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670390, 1697548670470]"
5166,5166,241,33,[],200,llama-7b,128,1,1912.0,1.0,1,A100,1697548661867,1697548663779,120,19.0,1.0,"[283, 1629]","[1697548662150, 1697548663779]"
5167,5167,579,17,[],200,llama-7b,128,1,1902.0,1.0,1,A100,1697548617387,1697548619289,120,19.0,1.0,"[368, 1533]","[1697548617755, 1697548619288]"
5168,5168,606,34,[],200,llama-7b,128,1,938.0,1.0,1,A100,1697548663784,1697548664722,120,9.0,1.0,"[58, 880]","[1697548663842, 1697548664722]"
5169,5169,580,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667368,120,,,"[131, 1689, 430, 83, 67, 84]","[1697548664348, 1697548666037, 1697548666467, 1697548666550, 1697548666617, 1697548666701]"
5170,5170,11,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619291,1697548620358,120,,,"[52, 979]","[1697548619343, 1697548620322]"
5171,5171,368,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620370,1697548622595,120,,,"[279, 1679]","[1697548620649, 1697548622328]"
5172,5172,10,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667377,1697548671228,120,,,"[180, 2150, 471, 85, 64, 64, 80]","[1697548667557, 1697548669707, 1697548670178, 1697548670263, 1697548670327, 1697548670391, 1697548670471]"
5173,5173,780,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620363,1697548622595,120,,,"[273, 1692]","[1697548620636, 1697548622328]"
5174,5174,36,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664725,1697548667372,120,,,"[23, 2597]","[1697548664748, 1697548667345]"
5175,5175,371,39,[],200,llama-7b,128,1,1832.0,1.0,1,A100,1697548671243,1697548673075,120,13.0,1.0,"[221, 1611]","[1697548671464, 1697548673075]"
5176,5176,814,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673080,1697548674840,120,,,"[21, 1218]","[1697548673101, 1697548674319]"
5177,5177,119,45,[],200,llama-7b,128,1,2152.0,1.0,1,A100,1697548691097,1697548693249,120,31.0,1.0,"[186, 1966]","[1697548691283, 1697548693249]"
5178,5178,208,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622603,1697548624959,120,,,"[39, 2035]","[1697548622642, 1697548624677]"
5179,5179,242,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679645,120,,,"[321, 3072]","[1697548675169, 1697548678241]"
5180,5180,811,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622604,1697548624953,120,,,"[239, 1834]","[1697548622843, 1697548624677]"
5181,5181,478,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693253,1697548695716,120,,,"[56, 835]","[1697548693309, 1697548694144]"
5182,5182,597,42,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548679657,1697548681690,120,39.0,1.0,"[197, 1835]","[1697548679854, 1697548681689]"
5183,5183,539,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627100,120,,,"[188, 1859]","[1697548625162, 1697548627021]"
5184,5184,836,47,[],200,llama-7b,128,1,2412.0,1.0,1,A100,1697548695727,1697548698139,120,11.0,1.0,"[370, 2042]","[1697548696097, 1697548698139]"
5185,5185,893,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627137,1697548629598,120,,,"[223, 1977]","[1697548627360, 1697548629337]"
5186,5186,236,21,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624976,1697548627023,120,8.0,1.0,"[372, 1675]","[1697548625348, 1697548627023]"
5187,5187,318,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631890,120,,,"[41, 1974]","[1697548629643, 1697548631617]"
5188,5188,221,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620368,1697548622597,120,,,"[383, 1579]","[1697548620751, 1697548622330]"
5189,5189,594,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627027,1697548629598,120,,,"[108, 2202]","[1697548627135, 1697548629337]"
5190,5190,819,35,[],200,llama-7b,128,1,768.0,1.0,1,A100,1697548666705,1697548667473,120,13.0,1.0,"[21, 747]","[1697548666726, 1697548667473]"
5191,5191,27,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681701,1697548686705,120,,,"[82, 2260, 1219, 91, 90, 89, 70, 87, 86, 68]","[1697548681783, 1697548684043, 1697548685262, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
5192,5192,579,18,[],200,llama-7b,128,1,2074.0,1.0,1,A100,1697548622604,1697548624678,120,19.0,1.0,"[147, 1927]","[1697548622751, 1697548624678]"
5193,5193,335,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667476,1697548671226,120,,,"[417, 1816, 470, 84, 65, 64, 80]","[1697548667893, 1697548669709, 1697548670179, 1697548670263, 1697548670328, 1697548670392, 1697548670472]"
5194,5194,12,19,[],200,llama-7b,128,1,1092.0,1.0,1,A100,1697548624682,1697548625774,120,11.0,1.0,"[51, 1041]","[1697548624733, 1697548625774]"
5195,5195,676,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631896,1697548633845,120,,,[252],[1697548632148]
5196,5196,24,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629602,1697548631885,120,,,[56],[1697548629658]
5197,5197,373,20,[],200,llama-7b,128,1,2161.0,1.0,1,A100,1697548625777,1697548627938,120,15.0,1.0,"[24, 2137]","[1697548625801, 1697548627938]"
5198,5198,904,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657536,120,,,"[79, 2066]","[1697548655360, 1697548657426]"
5199,5199,552,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683218,1697548686708,120,,,"[345, 1464, 236, 91, 91, 89, 69, 87, 86, 69]","[1697548683563, 1697548685027, 1697548685263, 1697548685354, 1697548685445, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685845]"
5200,5200,611,27,[],200,llama-7b,128,1,578.0,1.0,1,A100,1697548633854,1697548634432,120,14.0,1.0,"[56, 522]","[1697548633910, 1697548634432]"
5201,5201,389,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633845,120,,,[45],[1697548631940]
5202,5202,719,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635903,120,,,"[11, 625]","[1697548633865, 1697548634490]"
5203,5203,305,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657545,1697548659869,120,,,"[116, 1917]","[1697548657661, 1697548659578]"
5204,5204,147,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637694,120,,,[244],[1697548636160]
5205,5205,11,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548634435,1697548639635,120,,,"[29, 2044, 1223, 417]","[1697548634464, 1697548636508, 1697548637731, 1697548638148]"
5206,5206,658,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659878,1697548661857,120,,,[391],[1697548660269]
5207,5207,365,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639646,1697548641374,120,,,[40],[1697548639686]
5208,5208,498,27,[],200,llama-7b,128,1,1598.0,1.0,1,A100,1697548637701,1697548639299,120,9.0,1.0,"[77, 1521]","[1697548637778, 1697548639299]"
5209,5209,702,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627941,1697548631886,120,,,"[18, 2312]","[1697548627959, 1697548630271]"
5210,5210,726,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548643001,120,,,[304],[1697548641689]
5211,5211,857,28,[],200,llama-7b,128,1,880.0,1.0,1,A100,1697548639303,1697548640183,120,18.0,1.0,"[20, 859]","[1697548639323, 1697548640182]"
5212,5212,286,29,[],200,llama-7b,128,1,6469.0,1.0,1,A100,1697548640187,1697548646656,120,161.0,12.0,"[19, 1573, 1260, 585, 1587, 85, 64, 954, 94, 91, 89, 68]","[1697548640206, 1697548641779, 1697548643039, 1697548643624, 1697548645211, 1697548645296, 1697548645360, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656]"
5213,5213,912,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686723,1697548688899,120,,,"[256, 1734]","[1697548686979, 1697548688713]"
5214,5214,132,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633844,120,,,[31],[1697548631926]
5215,5215,483,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633854,1697548635894,120,,,"[120, 1624]","[1697548633974, 1697548635598]"
5216,5216,341,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691086,120,,,"[48, 1890]","[1697548688955, 1697548690845]"
5217,5217,840,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635911,1697548637693,120,,,[46],[1697548635957]
5218,5218,271,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637700,1697548639636,120,,,"[90, 1510]","[1697548637790, 1697548639300]"
5219,5219,719,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639648,1697548641376,120,,,[100],[1697548639748]
5220,5220,731,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655282,1697548657537,120,,,"[176, 1967]","[1697548655458, 1697548657425]"
5221,5221,152,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642997,120,,,[255],[1697548641640]
5222,5222,160,32,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548657549,1697548659579,120,13.0,1.0,"[388, 1642]","[1697548657937, 1697548659579]"
5223,5223,700,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693452,120,,,[286],[1697548691382]
5224,5224,510,28,[],200,llama-7b,128,1,2205.0,1.0,1,A100,1697548643006,1697548645211,120,79.0,2.0,"[83, 2122]","[1697548643089, 1697548645211]"
5225,5225,67,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671242,1697548674836,120,,,"[222, 1611, 538, 79, 61]","[1697548671464, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
5226,5226,587,19,[],200,llama-7b,128,1,2200.0,1.0,1,A100,1697548627138,1697548629338,120,13.0,1.0,"[315, 1885]","[1697548627453, 1697548629338]"
5227,5227,128,47,[],200,llama-7b,128,1,1969.0,1.0,1,A100,1697548693463,1697548695432,120,9.0,1.0,"[293, 1676]","[1697548693756, 1697548695432]"
5228,5228,637,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655282,1697548657536,120,,,"[86, 2058]","[1697548655368, 1697548657426]"
5229,5229,864,29,[],200,llama-7b,128,1,6301.0,1.0,1,A100,1697548645214,1697548651515,120,83.0,20.0,"[19, 1842, 551, 96, 95, 94, 71, 93, 72, 891, 92, 88, 86, 732, 85, 85, 76, 74, 73, 1002, 84]","[1697548645233, 1697548647075, 1697548647626, 1697548647722, 1697548647817, 1697548647911, 1697548647982, 1697548648075, 1697548648147, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650036, 1697548650121, 1697548650206, 1697548650282, 1697548650356, 1697548650429, 1697548651431, 1697548651515]"
5230,5230,424,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674843,1697548677258,120,,,"[19, 1934]","[1697548674862, 1697548676796]"
5231,5231,456,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695437,1697548697835,120,,,"[50, 1241]","[1697548695487, 1697548696728]"
5232,5232,490,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659583,1697548664206,120,,,"[51, 1066, 1196, 57, 1027]","[1697548659634, 1697548660700, 1697548661896, 1697548661953, 1697548662980]"
5233,5233,786,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677270,1697548679648,120,,,"[178, 2026]","[1697548677448, 1697548679474]"
5234,5234,65,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659864,120,,,"[205, 1828]","[1697548657751, 1697548659579]"
5235,5235,848,34,[],200,llama-7b,128,1,1821.0,1.0,1,A100,1697548664218,1697548666039,120,47.0,1.0,"[355, 1466]","[1697548664573, 1697548666039]"
5236,5236,12,20,[],200,llama-7b,128,1,923.0,1.0,1,A100,1697548629349,1697548630272,120,11.0,1.0,"[20, 903]","[1697548629369, 1697548630272]"
5237,5237,424,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661852,120,,,[26],[1697548659897]
5238,5238,216,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683208,120,,,"[97, 1933, 284, 91, 85, 83, 81]","[1697548679755, 1697548681688, 1697548681972, 1697548682063, 1697548682148, 1697548682231, 1697548682312]"
5239,5239,371,21,[],200,llama-7b,128,1,2361.0,1.0,1,A100,1697548630274,1697548632635,120,13.0,1.0,"[13, 2348]","[1697548630287, 1697548632635]"
5240,5240,781,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664204,120,,,"[91, 1819]","[1697548661958, 1697548663777]"
5241,5241,273,35,[],200,llama-7b,128,1,1299.0,1.0,1,A100,1697548666048,1697548667347,120,19.0,1.0,"[92, 1207]","[1697548666140, 1697548667347]"
5242,5242,703,22,[],200,llama-7b,128,1,1791.0,1.0,1,A100,1697548632640,1697548634431,120,12.0,1.0,"[25, 1766]","[1697548632665, 1697548634431]"
5243,5243,636,36,[],200,llama-7b,128,1,672.0,1.0,1,A100,1697548667353,1697548668025,120,31.0,1.0,"[47, 625]","[1697548667400, 1697548668025]"
5244,5244,136,23,[],200,llama-7b,128,1,1165.0,1.0,1,A100,1697548634434,1697548635599,120,31.0,1.0,"[20, 1145]","[1697548634454, 1697548635599]"
5245,5245,65,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548668032,1697548671226,120,,,"[16, 1661, 470, 84, 65, 64, 81]","[1697548668048, 1697548669709, 1697548670179, 1697548670263, 1697548670328, 1697548670392, 1697548670473]"
5246,5246,297,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667373,120,,,"[22, 1796, 432, 82, 67, 85]","[1697548664239, 1697548666035, 1697548666467, 1697548666549, 1697548666616, 1697548666701]"
5247,5247,495,24,[],200,llama-7b,128,1,904.0,1.0,1,A100,1697548635605,1697548636509,120,13.0,1.0,"[74, 830]","[1697548635679, 1697548636509]"
5248,5248,853,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548636512,1697548639639,120,,,"[19, 1563, 55]","[1697548636531, 1697548638094, 1697548638149]"
5249,5249,656,33,[],200,llama-7b,128,1,2328.0,1.0,1,A100,1697548667381,1697548669709,120,26.0,1.0,"[497, 1831]","[1697548667878, 1697548669709]"
5250,5250,85,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669714,1697548671228,120,,,"[84, 1405]","[1697548669798, 1697548671203]"
5251,5251,250,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641376,120,,,[322],[1697548639977]
5252,5252,548,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686706,120,,,"[232, 1576, 239, 91, 90, 89, 69, 88, 86, 68]","[1697548683448, 1697548685024, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685602, 1697548685690, 1697548685776, 1697548685844]"
5253,5253,512,38,[],200,llama-7b,128,1,1834.0,1.0,1,A100,1697548671243,1697548673077,120,11.0,1.0,"[302, 1532]","[1697548671545, 1697548673077]"
5254,5254,605,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642997,120,,,[73],[1697548641457]
5255,5255,12,36,[],200,llama-7b,128,1,1910.0,1.0,1,A100,1697548661868,1697548663778,120,11.0,1.0,"[203, 1707]","[1697548662071, 1697548663778]"
5256,5256,46,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667349,1697548671225,120,,,"[14, 697, 2117, 85, 64, 64, 80]","[1697548667363, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670390, 1697548670470]"
5257,5257,341,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663782,1697548667371,120,,,"[25, 915, 1744, 83, 66, 85]","[1697548663807, 1697548664722, 1697548666466, 1697548666549, 1697548666615, 1697548666700]"
5258,5258,868,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673080,1697548674841,120,,,"[32, 1207]","[1697548673112, 1697548674319]"
5259,5259,42,36,[],200,llama-7b,128,1,2329.0,1.0,1,A100,1697548667379,1697548669708,120,10.0,1.0,"[473, 1856]","[1697548667852, 1697548669708]"
5260,5260,32,28,[],200,llama-7b,128,1,3649.0,1.0,1,A100,1697548643008,1697548646657,120,140.0,6.0,"[277, 2452, 578, 93, 92, 88, 69]","[1697548643285, 1697548645737, 1697548646315, 1697548646408, 1697548646500, 1697548646588, 1697548646657]"
5261,5261,439,35,[],200,llama-7b,128,1,2516.0,1.0,1,A100,1697548671238,1697548673754,120,13.0,4.0,"[206, 1630, 538, 80, 61]","[1697548671444, 1697548673074, 1697548673612, 1697548673692, 1697548673753]"
5262,5262,403,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671227,120,,,"[41, 1449]","[1697548669754, 1697548671203]"
5263,5263,297,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674846,1697548677259,120,,,"[239, 1711]","[1697548675085, 1697548676796]"
5264,5264,626,31,[],200,llama-7b,128,1,2147.0,1.0,1,A100,1697548655280,1697548657427,120,10.0,1.0,"[173, 1974]","[1697548655453, 1697548657427]"
5265,5265,802,36,[],200,llama-7b,128,1,1716.0,1.0,1,A100,1697548673757,1697548675473,120,9.0,1.0,"[6, 1710]","[1697548673763, 1697548675473]"
5266,5266,220,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693452,120,,,"[340, 1811]","[1697548691437, 1697548693248]"
5267,5267,759,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674841,120,,,"[216, 1616, 538, 79, 61]","[1697548671459, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
5268,5268,700,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671227,120,,,"[160, 2640, 85, 64, 64, 80]","[1697548667538, 1697548670178, 1697548670263, 1697548670327, 1697548670391, 1697548670471]"
5269,5269,403,40,[],200,llama-7b,128,1,2371.0,1.0,1,A100,1697548671243,1697548673614,120,874.0,2.0,"[255, 1578, 537]","[1697548671498, 1697548673076, 1697548673613]"
5270,5270,760,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673617,1697548677256,120,,,"[24, 1832]","[1697548673641, 1697548675473]"
5271,5271,27,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657432,1697548659867,120,,,"[68, 788]","[1697548657500, 1697548658288]"
5272,5272,191,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677268,1697548679649,120,,,"[16, 2189]","[1697548677284, 1697548679473]"
5273,5273,380,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667370,120,,,"[243, 2005, 84, 65, 85]","[1697548664461, 1697548666466, 1697548666550, 1697548666615, 1697548666700]"
5274,5274,385,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661857,120,,,[382],[1697548660259]
5275,5275,743,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664206,120,,,"[279, 1632]","[1697548662147, 1697548663779]"
5276,5276,685,15,[],200,llama-7b,128,1,2017.0,1.0,1,A100,1697548617379,1697548619396,120,364.0,2.0,"[183, 1723, 111]","[1697548617562, 1697548619285, 1697548619396]"
5277,5277,172,35,[],200,llama-7b,128,1,1819.0,1.0,1,A100,1697548664218,1697548666037,120,19.0,1.0,"[318, 1501]","[1697548664536, 1697548666037]"
5278,5278,851,24,[],200,llama-7b,128,1,1597.0,1.0,1,A100,1697548637703,1697548639300,120,23.0,1.0,"[285, 1312]","[1697548637988, 1697548639300]"
5279,5279,285,25,[],200,llama-7b,128,1,10903.0,1.0,1,A100,1697548639303,1697548650206,120,100.0,27.0,"[44, 836, 1231, 365, 1261, 583, 1588, 85, 64, 953, 94, 91, 89, 68, 969, 96, 95, 94, 72, 92, 72, 891, 93, 88, 86, 731, 86, 86]","[1697548639347, 1697548640183, 1697548641414, 1697548641779, 1697548643040, 1697548643623, 1697548645211, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646587, 1697548646655, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647981, 1697548648073, 1697548648145, 1697548649036, 1697548649129, 1697548649217, 1697548649303, 1697548650034, 1697548650120, 1697548650206]"
5280,5280,116,16,[],200,llama-7b,128,1,1526.0,1.0,1,A100,1697548619399,1697548620925,120,23.0,1.0,"[19, 1507]","[1697548619418, 1697548620925]"
5281,5281,734,43,[],200,llama-7b,128,1,3094.0,1.0,1,A100,1697548667378,1697548670472,120,100.0,6.0,"[175, 2154, 471, 85, 64, 64, 81]","[1697548667553, 1697548669707, 1697548670178, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
5282,5282,473,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620929,1697548624951,120,,,"[24, 2503]","[1697548620953, 1697548623456]"
5283,5283,249,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548670476,1697548674836,120,,,"[6, 1109, 2021, 80, 60]","[1697548670482, 1697548671591, 1697548673612, 1697548673692, 1697548673752]"
5284,5284,501,36,[],200,llama-7b,128,1,1300.0,1.0,1,A100,1697548666047,1697548667347,120,19.0,1.0,"[42, 1258]","[1697548666089, 1697548667347]"
5285,5285,612,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677258,120,,,"[93, 1859]","[1697548674937, 1697548676796]"
5286,5286,572,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610842,120,,,[283],[1697548609257]
5287,5287,542,40,[],200,llama-7b,128,1,1820.0,1.0,1,A100,1697548664217,1697548666037,120,11.0,1.0,"[160, 1660]","[1697548664377, 1697548666037]"
5288,5288,465,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613010,1697548615236,120,,,"[170, 1854]","[1697548613180, 1697548615034]"
5289,5289,44,46,[],200,llama-7b,128,1,2195.0,1.0,1,A100,1697548677279,1697548679474,120,12.0,1.0,"[287, 1908]","[1697548677566, 1697548679474]"
5290,5290,864,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667352,1697548671225,120,,,"[87, 586, 35, 2117, 85, 64, 64, 80]","[1697548667439, 1697548668025, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670390, 1697548670470]"
5291,5291,926,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610852,1697548612998,120,,,"[243, 1720]","[1697548611095, 1697548612815]"
5292,5292,820,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617369,120,,,"[328, 1694]","[1697548615572, 1697548617266]"
5293,5293,402,47,[],200,llama-7b,128,1,2834.0,1.0,1,A100,1697548679477,1697548682311,120,457.0,6.0,"[31, 849, 1614, 92, 85, 82, 81]","[1697548679508, 1697548680357, 1697548681971, 1697548682063, 1697548682148, 1697548682230, 1697548682311]"
5294,5294,903,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666041,1697548667368,120,,,"[27, 1278]","[1697548666068, 1697548667346]"
5295,5295,221,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617387,1697548620355,120,,,"[363, 1538, 109, 84, 82, 81, 81]","[1697548617750, 1697548619288, 1697548619397, 1697548619481, 1697548619563, 1697548619644, 1697548619725]"
5296,5296,732,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682315,1697548686706,120,,,"[11, 1717, 1220, 90, 91, 89, 69, 87, 86, 69]","[1697548682326, 1697548684043, 1697548685263, 1697548685353, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
5297,5297,289,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674840,120,,,"[121, 1717, 538, 79, 60]","[1697548671359, 1697548673076, 1697548673614, 1697548673693, 1697548673753]"
5298,5298,330,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667377,1697548671228,120,,,"[205, 2125, 472, 84, 65, 64, 80]","[1697548667582, 1697548669707, 1697548670179, 1697548670263, 1697548670328, 1697548670392, 1697548670472]"
5299,5299,582,18,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548620369,1697548622332,120,19.0,1.0,"[205, 1758]","[1697548620574, 1697548622332]"
5300,5300,268,30,[],200,llama-7b,128,1,1833.0,1.0,1,A100,1697548654347,1697548656180,120,19.0,1.0,"[24, 1809]","[1697548654371, 1697548656180]"
5301,5301,11,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622337,1697548624958,120,,,"[76, 1045]","[1697548622413, 1697548623458]"
5302,5302,388,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688900,120,,,"[273, 1712]","[1697548687002, 1697548688714]"
5303,5303,646,39,[],200,llama-7b,128,1,3393.0,1.0,1,A100,1697548674848,1697548678241,120,14.0,1.0,"[409, 2984]","[1697548675257, 1697548678241]"
5304,5304,568,43,[],200,llama-7b,128,1,2277.0,1.0,1,A100,1697548685265,1697548687542,120,11.0,1.0,"[8, 2269]","[1697548685273, 1697548687542]"
5305,5305,720,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691086,120,,,"[58, 1880]","[1697548688965, 1697548690845]"
5306,5306,373,20,[],200,llama-7b,128,1,2047.0,1.0,1,A100,1697548624974,1697548627021,120,15.0,1.0,"[266, 1781]","[1697548625240, 1697548627021]"
5307,5307,150,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693449,120,,,[172],[1697548691268]
5308,5308,530,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639654,1697548641375,120,,,[309],[1697548639963]
5309,5309,84,35,[],200,llama-7b,128,1,1905.0,1.0,1,A100,1697548661873,1697548663778,120,26.0,1.0,"[317, 1588]","[1697548662190, 1697548663778]"
5310,5310,703,41,[],200,llama-7b,128,1,1809.0,1.0,1,A100,1697548683216,1697548685025,120,12.0,1.0,"[46, 1763]","[1697548683262, 1697548685025]"
5311,5311,885,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641384,1697548642996,120,,,[37],[1697548641421]
5312,5312,153,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686720,1697548688900,120,,,"[277, 1717]","[1697548686997, 1697548688714]"
5313,5313,504,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695718,120,,,"[88, 1885]","[1697548693547, 1697548695432]"
5314,5314,815,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653427,1697548657538,120,,,"[13, 2739]","[1697548653440, 1697548656179]"
5315,5315,3,27,[],200,llama-7b,128,1,6299.0,1.0,1,A100,1697548643005,1697548649304,120,89.0,20.0,"[69, 1703, 433, 86, 64, 953, 94, 91, 91, 67, 969, 97, 95, 93, 72, 93, 70, 893, 92, 88, 86]","[1697548643074, 1697548644777, 1697548645210, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646589, 1697548646656, 1697548647625, 1697548647722, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648145, 1697548649038, 1697548649130, 1697548649218, 1697548649304]"
5316,5316,858,48,[],200,llama-7b,128,1,2660.0,1.0,1,A100,1697548695729,1697548698389,120,182.0,12.0,"[359, 2079, 23, 22, 22, 22, 22, 22, 22, 22, 22, 23]","[1697548696088, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698389]"
5317,5317,313,29,[],200,llama-7b,128,1,1772.0,1.0,1,A100,1697548643005,1697548644777,120,20.0,1.0,"[45, 1726]","[1697548643050, 1697548644776]"
5318,5318,513,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688908,1697548691086,120,,,"[62, 1875]","[1697548688970, 1697548690845]"
5319,5319,77,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548678244,1697548683204,120,,,"[22, 2090, 1615, 92, 85, 81, 80]","[1697548678266, 1697548680356, 1697548681971, 1697548682063, 1697548682148, 1697548682229, 1697548682309]"
5320,5320,759,30,[],200,llama-7b,128,1,6734.0,1.0,1,A100,1697548644780,1697548651514,120,92.0,20.0,"[27, 2269, 549, 96, 96, 93, 72, 93, 71, 891, 92, 89, 86, 731, 87, 85, 75, 74, 74, 993, 91]","[1697548644807, 1697548647076, 1697548647625, 1697548647721, 1697548647817, 1697548647910, 1697548647982, 1697548648075, 1697548648146, 1697548649037, 1697548649129, 1697548649218, 1697548649304, 1697548650035, 1697548650122, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651423, 1697548651514]"
5321,5321,240,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659865,120,,,"[207, 1826]","[1697548657753, 1697548659579]"
5322,5322,221,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548688901,120,,,"[31, 2480]","[1697548685061, 1697548687541]"
5323,5323,582,43,[],200,llama-7b,128,1,1936.0,1.0,1,A100,1697548688909,1697548690845,120,19.0,1.0,"[226, 1710]","[1697548689135, 1697548690845]"
5324,5324,7,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690854,1697548693450,120,,,"[16, 982]","[1697548690870, 1697548691852]"
5325,5325,405,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683215,1697548686719,120,,,"[130, 1681, 236, 93, 88, 89, 72, 86, 85, 69]","[1697548683345, 1697548685026, 1697548685262, 1697548685355, 1697548685443, 1697548685532, 1697548685604, 1697548685690, 1697548685775, 1697548685844]"
5326,5326,599,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661857,120,,,[188],[1697548660060]
5327,5327,25,34,[],200,llama-7b,128,1,1909.0,1.0,1,A100,1697548661868,1697548663777,120,12.0,1.0,"[125, 1784]","[1697548661993, 1697548663777]"
5328,5328,763,42,[],200,llama-7b,128,1,2872.0,1.0,1,A100,1697548686732,1697548689604,120,20.0,1.0,"[416, 2456]","[1697548687148, 1697548689604]"
5329,5329,367,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695718,120,,,"[78, 1895]","[1697548693537, 1697548695432]"
5330,5330,724,46,[],200,llama-7b,128,1,2412.0,1.0,1,A100,1697548695727,1697548698139,120,11.0,1.0,"[365, 2047]","[1697548696092, 1697548698139]"
5331,5331,361,28,[],200,llama-7b,128,1,4113.0,1.0,1,A100,1697548649309,1697548653422,120,67.0,7.0,"[11, 2858, 267, 320, 87, 70, 403, 97]","[1697548649320, 1697548652178, 1697548652445, 1697548652765, 1697548652852, 1697548652922, 1697548653325, 1697548653422]"
5332,5332,193,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548689608,1697548693448,120,,,[7],[1697548689615]
5333,5333,157,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548643012,1697548655266,120,,,"[162, 1603, 434, 86, 64, 953, 94, 91, 89, 68, 968, 96, 97, 92, 72, 93, 71, 894, 91, 89, 86, 730, 86, 85, 75, 74, 74, 992, 91, 75, 858, 319, 88, 70, 403, 97, 89, 84, 650, 96, 85, 63, 80]","[1697548643174, 1697548644777, 1697548645211, 1697548645297, 1697548645361, 1697548646314, 1697548646408, 1697548646499, 1697548646588, 1697548646656, 1697548647624, 1697548647720, 1697548647817, 1697548647909, 1697548647981, 1697548648074, 1697548648145, 1697548649039, 1697548649130, 1697548649219, 1697548649305, 1697548650035, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421, 1697548651512, 1697548651587, 1697548652445, 1697548652764, 1697548652852, 1697548652922, 1697548653325, 1697548653422, 1697548653511, 1697548653595, 1697548654245, 1697548654341, 1697548654426, 1697548654489, 1697548654569]"
5334,5334,355,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663782,1697548667371,120,,,"[21, 919, 1744, 83, 66, 85]","[1697548663803, 1697548664722, 1697548666466, 1697548666549, 1697548666615, 1697548666700]"
5335,5335,188,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548651517,1697548655269,120,,,"[25, 2351, 355, 96, 83, 63, 81]","[1697548651542, 1697548653893, 1697548654248, 1697548654344, 1697548654427, 1697548654490, 1697548654571]"
5336,5336,550,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695717,120,,,"[11, 1961]","[1697548693470, 1697548695431]"
5337,5337,216,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667373,120,,,"[32, 1786, 432, 82, 67, 85]","[1697548664249, 1697548666035, 1697548666467, 1697548666549, 1697548666616, 1697548666701]"
5338,5338,909,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695725,1697548697834,120,,,"[220, 1738]","[1697548695945, 1697548697683]"
5339,5339,100,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.68 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 8.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548599951,1697548602108,120,,,"[100, 1833]","[1697548600051, 1697548601884]"
5340,5340,716,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667379,1697548671230,120,,,"[368, 2431, 84, 64, 65, 80]","[1697548667747, 1697548670178, 1697548670262, 1697548670326, 1697548670391, 1697548670471]"
5341,5341,546,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655282,1697548657536,120,,,"[71, 2072]","[1697548655353, 1697548657425]"
5342,5342,51,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688898,120,,,"[238, 1745]","[1697548686967, 1697548688712]"
5343,5343,573,33,[],200,llama-7b,128,1,2799.0,1.0,1,A100,1697548667379,1697548670178,120,874.0,2.0,"[377, 2422]","[1697548667756, 1697548670178]"
5344,5344,905,33,[],200,llama-7b,128,1,2034.0,1.0,1,A100,1697548657544,1697548659578,120,11.0,1.0,"[49, 1985]","[1697548657593, 1697548659578]"
5345,5345,330,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664206,120,,,"[54, 1064, 1196, 57, 1027]","[1697548659636, 1697548660700, 1697548661896, 1697548661953, 1697548662980]"
5346,5346,405,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[12, 1925]","[1697548688919, 1697548690844]"
5347,5347,145,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674837,120,,,"[240, 1592, 538, 80, 60]","[1697548671483, 1697548673075, 1697548673613, 1697548673693, 1697548673753]"
5348,5348,7,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548670181,1697548671229,120,,,"[6, 1017]","[1697548670187, 1697548671204]"
5349,5349,730,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691095,1697548693453,120,,,"[54, 2097]","[1697548691149, 1697548693246]"
5350,5350,663,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667374,120,,,"[57, 2193, 82, 67, 85]","[1697548664274, 1697548666467, 1697548666549, 1697548666616, 1697548666701]"
5351,5351,160,42,[],200,llama-7b,128,1,1970.0,1.0,1,A100,1697548693463,1697548695433,120,13.0,1.0,"[298, 1672]","[1697548693761, 1697548695433]"
5352,5352,92,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667381,1697548671225,120,,,"[491, 1837, 470, 84, 64, 64, 81]","[1697548667872, 1697548669709, 1697548670179, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
5353,5353,521,43,[],200,llama-7b,128,1,1291.0,1.0,1,A100,1697548695437,1697548696728,120,18.0,1.0,"[41, 1250]","[1697548695478, 1697548696728]"
5354,5354,291,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548651518,1697548655270,120,,,[29],[1697548651547]
5355,5355,287,28,[],200,llama-7b,128,1,1790.0,1.0,1,A100,1697548648150,1697548649940,120,10.0,1.0,"[25, 1764]","[1697548648175, 1697548649939]"
5356,5356,615,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649942,1697548655266,120,,,"[34, 3251, 100, 96, 89, 84, 650, 96, 84, 63, 80]","[1697548649976, 1697548653227, 1697548653327, 1697548653423, 1697548653512, 1697548653596, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
5357,5357,362,35,[],200,llama-7b,128,1,1834.0,1.0,1,A100,1697548671242,1697548673076,120,14.0,1.0,"[107, 1727]","[1697548671349, 1697548673076]"
5358,5358,619,31,[],200,llama-7b,128,1,2145.0,1.0,1,A100,1697548655281,1697548657426,120,10.0,1.0,"[266, 1879]","[1697548655547, 1697548657426]"
5359,5359,51,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657429,1697548659864,120,,,"[7, 852]","[1697548657436, 1697548658288]"
5360,5360,690,36,[],200,llama-7b,128,1,1237.0,1.0,1,A100,1697548673083,1697548674320,120,39.0,1.0,"[99, 1137]","[1697548673182, 1697548674319]"
5361,5361,121,37,[],200,llama-7b,128,1,1150.0,1.0,1,A100,1697548674323,1697548675473,120,13.0,1.0,"[53, 1097]","[1697548674376, 1697548675473]"
5362,5362,411,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661853,120,,,[271],[1697548660143]
5363,5363,47,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657535,120,,,"[54, 2090]","[1697548655335, 1697548657425]"
5364,5364,478,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548675477,1697548679648,120,,,"[24, 2741]","[1697548675501, 1697548678242]"
5365,5365,407,31,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548657545,1697548659578,120,16.0,1.0,"[108, 1925]","[1697548657653, 1697548659578]"
5366,5366,835,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683209,120,,,"[192, 1840, 283, 91, 85, 82, 80]","[1697548679849, 1697548681689, 1697548681972, 1697548682063, 1697548682148, 1697548682230, 1697548682310]"
5367,5367,760,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659583,1697548664206,120,,,"[48, 1069, 1196, 57, 1027]","[1697548659631, 1697548660700, 1697548661896, 1697548661953, 1697548662980]"
5368,5368,396,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695437,1697548697836,120,,,"[66, 1225]","[1697548695503, 1697548696728]"
5369,5369,772,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664207,120,,,"[290, 1622]","[1697548662157, 1697548663779]"
5370,5370,334,35,[],200,llama-7b,128,1,1910.0,1.0,1,A100,1697548661868,1697548663778,120,15.0,1.0,"[267, 1643]","[1697548662135, 1697548663778]"
5371,5371,453,37,[],200,llama-7b,128,1,1839.0,1.0,1,A100,1697548671238,1697548673077,120,26.0,1.0,"[327, 1512]","[1697548671565, 1697548673077]"
5372,5372,809,38,[],200,llama-7b,128,1,1239.0,1.0,1,A100,1697548673081,1697548674320,120,16.0,1.0,"[88, 1150]","[1697548673169, 1697548674319]"
5373,5373,208,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674323,1697548677257,120,,,"[44, 1106]","[1697548674367, 1697548675473]"
5374,5374,693,36,[],200,llama-7b,128,1,2684.0,1.0,1,A100,1697548663782,1697548666466,120,67.0,2.0,"[11, 929, 1744]","[1697548663793, 1697548664722, 1697548666466]"
5375,5375,562,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677272,1697548679647,120,,,"[108, 2094]","[1697548677380, 1697548679474]"
5376,5376,197,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667369,120,,,"[220, 1601, 428, 84, 65, 85]","[1697548664437, 1697548666038, 1697548666466, 1697548666550, 1697548666615, 1697548666700]"
5377,5377,121,37,[],200,llama-7b,128,1,877.0,1.0,1,A100,1697548666470,1697548667347,120,13.0,1.0,"[14, 863]","[1697548666484, 1697548667347]"
5378,5378,480,38,[],200,llama-7b,128,1,674.0,1.0,1,A100,1697548667351,1697548668025,120,26.0,1.0,"[39, 635]","[1697548667390, 1697548668025]"
5379,5379,395,29,[],200,llama-7b,128,1,7683.0,1.0,1,A100,1697548646660,1697548654343,120,88.0,20.0,"[6, 3272, 97, 86, 86, 74, 74, 75, 991, 91, 74, 860, 319, 88, 70, 403, 97, 88, 84, 651, 96]","[1697548646666, 1697548649938, 1697548650035, 1697548650121, 1697548650207, 1697548650281, 1697548650355, 1697548650430, 1697548651421, 1697548651512, 1697548651586, 1697548652446, 1697548652765, 1697548652853, 1697548652923, 1697548653326, 1697548653423, 1697548653511, 1697548653595, 1697548654246, 1697548654342]"
5380,5380,526,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671227,120,,,"[109, 2219, 472, 84, 65, 64, 80]","[1697548667487, 1697548669706, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
5381,5381,352,40,[],200,llama-7b,128,1,2141.0,1.0,1,A100,1697548683214,1697548685355,120,11.0,3.0,"[122, 1690, 236, 93]","[1697548683336, 1697548685026, 1697548685262, 1697548685355]"
5382,5382,710,41,[],200,llama-7b,128,1,2183.0,1.0,1,A100,1697548685360,1697548687543,120,14.0,1.0,"[16, 2166]","[1697548685376, 1697548687542]"
5383,5383,809,39,[],200,llama-7b,128,1,1677.0,1.0,1,A100,1697548668033,1697548669710,120,16.0,1.0,"[22, 1655]","[1697548668055, 1697548669710]"
5384,5384,457,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677278,1697548679650,120,,,[295],[1697548677573]
5385,5385,140,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548687548,1697548691085,120,,,"[33, 2021]","[1697548687581, 1697548689602]"
5386,5386,820,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683207,120,,,"[301, 1728, 286, 91, 85, 81, 81]","[1697548679959, 1697548681687, 1697548681973, 1697548682064, 1697548682149, 1697548682230, 1697548682311]"
5387,5387,234,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671226,120,,,"[16, 1473]","[1697548669729, 1697548671202]"
5388,5388,736,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655292,1697548657536,120,,,"[295, 1840]","[1697548655587, 1697548657427]"
5389,5389,498,43,[],200,llama-7b,128,1,2152.0,1.0,1,A100,1697548691096,1697548693248,120,9.0,1.0,"[167, 1985]","[1697548691263, 1697548693248]"
5390,5390,134,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657545,1697548659869,120,,,"[118, 1915]","[1697548657663, 1697548659578]"
5391,5391,859,44,[],200,llama-7b,128,1,890.0,1.0,1,A100,1697548693253,1697548694143,120,23.0,1.0,"[36, 854]","[1697548693289, 1697548694143]"
5392,5392,488,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661856,120,,,[365],[1697548660242]
5393,5393,257,45,[],200,llama-7b,128,1,2581.0,1.0,1,A100,1697548694146,1697548696727,120,14.0,1.0,"[8, 2573]","[1697548694154, 1697548696727]"
5394,5394,597,41,[],200,llama-7b,128,1,1832.0,1.0,1,A100,1697548671244,1697548673076,120,39.0,1.0,"[291, 1541]","[1697548671535, 1697548673076]"
5395,5395,245,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683215,1697548686705,120,,,"[147, 1664, 236, 91, 90, 90, 69, 87, 86, 69]","[1697548683362, 1697548685026, 1697548685262, 1697548685353, 1697548685443, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
5396,5396,845,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661868,1697548664208,120,,,"[367, 1543]","[1697548662235, 1697548663778]"
5397,5397,276,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667371,120,,,"[350, 1899, 85, 65, 84]","[1697548664568, 1697548666467, 1697548666552, 1697548666617, 1697548666701]"
5398,5398,656,41,[],200,llama-7b,128,1,2199.0,1.0,1,A100,1697548677274,1697548679473,120,26.0,1.0,"[204, 1995]","[1697548677478, 1697548679473]"
5399,5399,461,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548654348,1697548657502,120,,,[33],[1697548654381]
5400,5400,573,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686723,1697548688902,120,,,[157],[1697548686880]
5401,5401,640,36,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667378,1697548669708,120,15.0,1.0,"[478, 1852]","[1697548667856, 1697548669708]"
5402,5402,786,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657547,1697548659866,120,,,"[332, 1699]","[1697548657879, 1697548659578]"
5403,5403,881,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674838,120,,,"[333, 1506, 537, 79, 61]","[1697548671571, 1697548673077, 1697548673614, 1697548673693, 1697548673754]"
5404,5404,39,37,[],200,llama-7b,128,1,1490.0,1.0,1,A100,1697548669713,1697548671203,120,8.0,1.0,"[31, 1459]","[1697548669744, 1697548671203]"
5405,5405,7,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671225,120,,,"[404, 1925, 471, 84, 65, 64, 81]","[1697548667782, 1697548669707, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670472]"
5406,5406,81,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679477,1697548683205,120,,,"[21, 858, 1615, 91, 85, 83, 80]","[1697548679498, 1697548680356, 1697548681971, 1697548682062, 1697548682147, 1697548682230, 1697548682310]"
5407,5407,311,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674843,1697548677258,120,,,"[30, 1923]","[1697548674873, 1697548676796]"
5408,5408,365,37,[],200,llama-7b,128,1,1839.0,1.0,1,A100,1697548671238,1697548673077,120,23.0,1.0,"[343, 1496]","[1697548671581, 1697548673077]"
5409,5409,697,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673080,1697548674840,120,,,"[17, 1222]","[1697548673097, 1697548674319]"
5410,5410,128,39,[],200,llama-7b,128,1,1949.0,1.0,1,A100,1697548674848,1697548676797,120,9.0,1.0,"[302, 1647]","[1697548675150, 1697548676797]"
5411,5411,489,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548676802,1697548679648,120,,,[39],[1697548676841]
5412,5412,843,41,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679658,1697548681688,120,14.0,1.0,"[321, 1709]","[1697548679979, 1697548681688]"
5413,5413,4,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688911,1697548691088,120,,,"[339, 1604]","[1697548689250, 1697548690854]"
5414,5414,188,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679647,120,,,"[413, 2980]","[1697548675261, 1697548678241]"
5415,5415,398,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671206,1697548674837,120,,,"[19, 1849, 538, 80, 60]","[1697548671225, 1697548673074, 1697548673612, 1697548673692, 1697548673752]"
5416,5416,361,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693451,120,,,"[274, 1876]","[1697548691371, 1697548693247]"
5417,5417,749,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683218,1697548686707,120,,,"[320, 1488, 237, 91, 90, 90, 69, 87, 86, 69]","[1697548683538, 1697548685026, 1697548685263, 1697548685354, 1697548685444, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685845]"
5418,5418,541,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683204,120,,,"[196, 1836, 282, 91, 85, 82, 80]","[1697548679854, 1697548681690, 1697548681972, 1697548682063, 1697548682148, 1697548682230, 1697548682310]"
5419,5419,548,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683208,120,,,"[377, 1653, 285, 91, 84, 83, 80]","[1697548680035, 1697548681688, 1697548681973, 1697548682064, 1697548682148, 1697548682231, 1697548682311]"
5420,5420,719,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695714,120,,,[183],[1697548693643]
5421,5421,150,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688899,120,,,[243],[1697548686972]
5422,5422,120,46,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695724,1697548697683,120,17.0,1.0,"[120, 1839]","[1697548695844, 1697548697683]"
5423,5423,508,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[28, 1909]","[1697548688935, 1697548690844]"
5424,5424,880,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686732,1697548691085,120,,,"[411, 2461]","[1697548687143, 1697548689604]"
5425,5425,881,44,[],200,llama-7b,128,1,2390.0,1.0,1,A100,1697548683214,1697548685604,120,58.0,6.0,"[68, 1743, 239, 90, 91, 87, 72]","[1697548683282, 1697548685025, 1697548685264, 1697548685354, 1697548685445, 1697548685532, 1697548685604]"
5426,5426,749,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677255,120,,,"[127, 1825]","[1697548674972, 1697548676797]"
5427,5427,873,50,[],200,llama-7b,128,1,2150.0,1.0,1,A100,1697548691097,1697548693247,120,6.0,1.0,"[284, 1866]","[1697548691381, 1697548693247]"
5428,5428,330,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671227,120,,,"[26, 1464]","[1697548669739, 1697548671203]"
5429,5429,307,45,[],200,llama-7b,128,1,1935.0,1.0,1,A100,1697548685608,1697548687543,120,26.0,1.0,"[18, 1917]","[1697548685626, 1697548687543]"
5430,5430,302,51,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693252,1697548695715,120,,,"[27, 865]","[1697548693279, 1697548694144]"
5431,5431,665,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548687546,1697548691085,120,,,"[25, 2031]","[1697548687571, 1697548689602]"
5432,5432,665,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671244,1697548674838,120,,,"[312, 1521, 537, 79, 60]","[1697548671556, 1697548673077, 1697548673614, 1697548673693, 1697548673753]"
5433,5433,90,47,[],200,llama-7b,128,1,2152.0,1.0,1,A100,1697548691095,1697548693247,120,19.0,1.0,"[80, 2072]","[1697548691175, 1697548693247]"
5434,5434,452,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693253,1697548695716,120,,,"[56, 835]","[1697548693309, 1697548694144]"
5435,5435,828,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674840,120,,,"[141, 2230, 78, 61]","[1697548671384, 1697548673614, 1697548673692, 1697548673753]"
5436,5436,190,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657545,1697548659868,120,,,"[106, 1927]","[1697548657651, 1697548659578]"
5437,5437,870,51,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693451,120,,,"[271, 1880]","[1697548691367, 1697548693247]"
5438,5438,270,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681700,1697548686705,120,,,"[78, 2265, 1219, 91, 90, 89, 70, 87, 86, 68]","[1697548681778, 1697548684043, 1697548685262, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
5439,5439,305,52,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695714,120,,,"[179, 1795]","[1697548693638, 1697548695433]"
5440,5440,478,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659584,1697548664208,120,,,"[85, 1033, 1194, 58, 1027]","[1697548659669, 1697548660702, 1697548661896, 1697548661954, 1697548662981]"
5441,5441,922,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683209,120,,,"[186, 1846, 283, 91, 85, 82, 80]","[1697548679843, 1697548681689, 1697548681972, 1697548682063, 1697548682148, 1697548682230, 1697548682310]"
5442,5442,549,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661857,120,,,[377],[1697548660254]
5443,5443,904,34,[],200,llama-7b,128,1,1115.0,1.0,1,A100,1697548661866,1697548662981,120,563.0,2.0,"[33, 853, 228]","[1697548661899, 1697548662752, 1697548662980]"
5444,5444,354,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686707,120,,,"[261, 1548, 238, 91, 90, 89, 70, 87, 86, 68]","[1697548683477, 1697548685025, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
5445,5445,928,31,[],200,llama-7b,128,1,1818.0,1.0,1,A100,1697548664218,1697548666036,120,20.0,1.0,"[238, 1580]","[1697548664456, 1697548666036]"
5446,5446,359,32,[],200,llama-7b,128,1,1301.0,1.0,1,A100,1697548666046,1697548667347,120,10.0,1.0,"[38, 1263]","[1697548666084, 1697548667347]"
5447,5447,632,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548632639,1697548635906,120,,,"[20, 1772, 58]","[1697548632659, 1697548634431, 1697548634489]"
5448,5448,636,53,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695724,1697548697683,120,31.0,1.0,"[151, 1808]","[1697548695875, 1697548697683]"
5449,5449,719,33,[],200,llama-7b,128,1,3040.0,1.0,1,A100,1697548667351,1697548670391,120,182.0,6.0,"[33, 676, 2117, 85, 64, 64]","[1697548667384, 1697548668060, 1697548670177, 1697548670262, 1697548670326, 1697548670390]"
5450,5450,60,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635916,1697548637695,120,,,[75],[1697548635991]
5451,5451,629,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688900,120,,,"[107, 1877]","[1697548686836, 1697548688713]"
5452,5452,144,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548670393,1697548674836,120,,,"[6, 1192, 2021, 80, 60]","[1697548670399, 1697548671591, 1697548673612, 1697548673692, 1697548673752]"
5453,5453,273,42,[],200,llama-7b,128,1,2342.0,1.0,1,A100,1697548681702,1697548684044,120,19.0,1.0,"[86, 2255]","[1697548681788, 1697548684043]"
5454,5454,714,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548684047,1697548686709,120,,,"[19, 2113]","[1697548684066, 1697548686179]"
5455,5455,414,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637703,1697548639646,120,,,"[337, 1260]","[1697548638040, 1697548639300]"
5456,5456,58,45,[],200,llama-7b,128,1,1938.0,1.0,1,A100,1697548688907,1697548690845,120,15.0,1.0,"[47, 1891]","[1697548688954, 1697548690845]"
5457,5457,745,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641375,120,,,[317],[1697548639972]
5458,5458,670,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664204,120,,,"[86, 799, 229]","[1697548661953, 1697548662752, 1697548662981]"
5459,5459,106,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664216,1697548667373,120,,,"[23, 1796, 432, 82, 67, 84]","[1697548664239, 1697548666035, 1697548666467, 1697548666549, 1697548666616, 1697548666700]"
5460,5460,146,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686732,1697548691085,120,,,"[363, 2508]","[1697548687095, 1697548689603]"
5461,5461,505,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693448,120,,,[143],[1697548691239]
5462,5462,464,31,[],200,llama-7b,128,1,2328.0,1.0,1,A100,1697548667381,1697548669709,120,12.0,1.0,"[502, 1826]","[1697548667883, 1697548669709]"
5463,5463,863,46,[],200,llama-7b,128,1,1973.0,1.0,1,A100,1697548693459,1697548695432,120,10.0,1.0,"[80, 1893]","[1697548693539, 1697548695432]"
5464,5464,268,47,[],200,llama-7b,128,1,1289.0,1.0,1,A100,1697548695439,1697548696728,120,19.0,1.0,"[58, 1231]","[1697548695497, 1697548696728]"
5465,5465,794,32,[],200,llama-7b,128,1,1489.0,1.0,1,A100,1697548669714,1697548671203,120,11.0,1.0,"[85, 1404]","[1697548669799, 1697548671203]"
5466,5466,807,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624974,1697548627100,120,,,"[203, 1843]","[1697548625177, 1697548627020]"
5467,5467,168,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655295,1697548657536,120,,,"[340, 1792]","[1697548655635, 1697548657427]"
5468,5468,219,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671206,1697548674837,120,,,"[16, 369, 2021, 80, 60]","[1697548671222, 1697548671591, 1697548673612, 1697548673692, 1697548673752]"
5469,5469,481,36,[],200,llama-7b,128,1,2328.0,1.0,1,A100,1697548667381,1697548669709,120,10.0,1.0,"[482, 1846]","[1697548667863, 1697548669709]"
5470,5470,831,37,[],200,llama-7b,128,1,1488.0,1.0,1,A100,1697548669716,1697548671204,120,11.0,1.0,"[102, 1386]","[1697548669818, 1697548671204]"
5471,5471,292,30,[],200,llama-7b,128,1,1832.0,1.0,1,A100,1697548654347,1697548656179,120,286.0,1.0,"[7, 1825]","[1697548654354, 1697548656179]"
5472,5472,622,31,[],200,llama-7b,128,1,2105.0,1.0,1,A100,1697548656183,1697548658288,120,20.0,1.0,"[29, 2076]","[1697548656212, 1697548658288]"
5473,5473,531,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659865,120,,,"[221, 1810]","[1697548657767, 1697548659577]"
5474,5474,233,19,[],200,llama-7b,128,1,2232.0,1.0,1,A100,1697548627106,1697548629338,120,6.0,1.0,"[137, 2095]","[1697548627243, 1697548629338]"
5475,5475,262,38,[],200,llama-7b,128,1,1866.0,1.0,1,A100,1697548671209,1697548673075,120,39.0,1.0,"[42, 1824]","[1697548671251, 1697548673075]"
5476,5476,380,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685848,1697548688904,120,,,[13],[1697548685861]
5477,5477,590,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629351,1697548631887,120,,,"[32, 889]","[1697548629383, 1697548630272]"
5478,5478,708,43,[],200,llama-7b,128,1,1944.0,1.0,1,A100,1697548688910,1697548690854,120,140.0,1.0,"[256, 1687]","[1697548689166, 1697548690853]"
5479,5479,47,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548658293,1697548664204,120,,,"[33, 2374, 1195, 58, 1026]","[1697548658326, 1697548660700, 1697548661895, 1697548661953, 1697548662979]"
5480,5480,619,39,[],200,llama-7b,128,1,1238.0,1.0,1,A100,1697548673080,1697548674318,120,10.0,1.0,"[41, 1197]","[1697548673121, 1697548674318]"
5481,5481,888,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661852,120,,,[19],[1697548659890]
5482,5482,20,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631895,1697548633849,120,,,[349],[1697548632244]
5483,5483,51,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674321,1697548677256,120,,,"[30, 1121]","[1697548674351, 1697548675472]"
5484,5484,377,22,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548633855,1697548635602,120,13.0,1.0,"[285, 1462]","[1697548634140, 1697548635602]"
5485,5485,133,44,[],200,llama-7b,128,1,994.0,1.0,1,A100,1697548690859,1697548691853,120,15.0,1.0,"[36, 958]","[1697548690895, 1697548691853]"
5486,5486,496,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691858,1697548695714,120,,,"[29, 2256]","[1697548691887, 1697548694143]"
5487,5487,385,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677272,1697548679646,120,,,"[93, 2109]","[1697548677365, 1697548679474]"
5488,5488,108,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548633853,1697548635893,120,,,"[108, 1636]","[1697548633961, 1697548635597]"
5489,5489,854,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695723,1697548697836,120,,,"[18, 1941]","[1697548695741, 1697548697682]"
5490,5490,742,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683207,120,,,"[84, 1946, 285, 91, 85, 83, 81]","[1697548679741, 1697548681687, 1697548681972, 1697548682063, 1697548682148, 1697548682231, 1697548682312]"
5491,5491,405,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667369,120,,,"[145, 1675, 431, 82, 67, 85]","[1697548664362, 1697548666037, 1697548666468, 1697548666550, 1697548666617, 1697548666702]"
5492,5492,559,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635907,1697548637692,120,,,[12],[1697548635919]
5493,5493,727,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627024,1697548629597,120,,,"[11, 903]","[1697548627035, 1697548627938]"
5494,5494,244,22,[],200,llama-7b,128,1,2016.0,1.0,1,A100,1697548629601,1697548631617,120,9.0,1.0,"[17, 1998]","[1697548629618, 1697548631616]"
5495,5495,598,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631623,1697548635894,120,,,"[12, 1001, 1249, 57, 547]","[1697548631635, 1697548632636, 1697548633885, 1697548633942, 1697548634489]"
5496,5496,31,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635912,1697548637693,120,,,[40],[1697548635952]
5497,5497,185,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667369,120,,,"[224, 1594, 431, 84, 65, 85]","[1697548664441, 1697548666035, 1697548666466, 1697548666550, 1697548666615, 1697548666700]"
5498,5498,390,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637700,1697548639637,120,,,"[161, 1439]","[1697548637861, 1697548639300]"
5499,5499,215,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661855,120,,,[100],[1697548659971]
5500,5500,336,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548662985,1697548667370,120,,,"[11, 1726, 1744, 83, 66, 84]","[1697548662996, 1697548664722, 1697548666466, 1697548666549, 1697548666615, 1697548666699]"
5501,5501,25,42,[],200,llama-7b,128,1,1238.0,1.0,1,A100,1697548673081,1697548674319,120,12.0,1.0,"[72, 1166]","[1697548673153, 1697548674319]"
5502,5502,719,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639652,1697548641381,120,,,[222],[1697548639874]
5503,5503,569,33,[],200,llama-7b,128,1,1911.0,1.0,1,A100,1697548661868,1697548663779,120,16.0,1.0,"[272, 1639]","[1697548662140, 1697548663779]"
5504,5504,147,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548642998,120,,,[86],[1697548641472]
5505,5505,506,28,[],200,llama-7b,128,1,1766.0,1.0,1,A100,1697548643012,1697548644778,120,16.0,1.0,"[226, 1539]","[1697548643238, 1697548644777]"
5506,5506,863,29,[],200,llama-7b,128,1,2295.0,1.0,1,A100,1697548644781,1697548647076,120,10.0,1.0,"[38, 2257]","[1697548644819, 1697548647076]"
5507,5507,387,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674322,1697548677256,120,,,"[30, 1120]","[1697548674352, 1697548675472]"
5508,5508,292,30,[],200,llama-7b,128,1,2861.0,1.0,1,A100,1697548647078,1697548649939,120,286.0,1.0,"[24, 2837]","[1697548647102, 1697548649939]"
5509,5509,622,31,[],200,llama-7b,128,1,2237.0,1.0,1,A100,1697548649942,1697548652179,120,20.0,1.0,"[21, 2216]","[1697548649963, 1697548652179]"
5510,5510,51,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548652182,1697548655271,120,,,"[24, 2860]","[1697548652206, 1697548655066]"
5511,5511,2,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663782,1697548667371,120,,,"[31, 909, 1744, 83, 66, 85]","[1697548663813, 1697548664722, 1697548666466, 1697548666549, 1697548666615, 1697548666700]"
5512,5512,148,28,[],200,llama-7b,128,1,1955.0,1.0,1,A100,1697548647984,1697548649939,120,16.0,1.0,"[7, 1948]","[1697548647991, 1697548649939]"
5513,5513,410,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657538,120,,,"[183, 1961]","[1697548655464, 1697548657425]"
5514,5514,509,29,[],200,llama-7b,128,1,2825.0,1.0,1,A100,1697548649941,1697548652766,120,286.0,3.0,"[13, 2225, 267, 320]","[1697548649954, 1697548652179, 1697548652446, 1697548652766]"
5515,5515,413,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683215,1697548686710,120,,,"[127, 1684, 236, 93, 88, 89, 72, 86, 85, 68]","[1697548683342, 1697548685026, 1697548685262, 1697548685355, 1697548685443, 1697548685532, 1697548685604, 1697548685690, 1697548685775, 1697548685843]"
5516,5516,717,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679651,120,,,"[71, 2133]","[1697548677340, 1697548679473]"
5517,5517,870,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548652769,1697548655271,120,,,"[10, 2288]","[1697548652779, 1697548655067]"
5518,5518,141,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683208,120,,,"[341, 1689, 285, 91, 84, 83, 80]","[1697548679999, 1697548681688, 1697548681973, 1697548682064, 1697548682148, 1697548682231, 1697548682311]"
5519,5519,300,31,[],200,llama-7b,128,1,2144.0,1.0,1,A100,1697548655283,1697548657427,120,9.0,1.0,"[157, 1987]","[1697548655440, 1697548657427]"
5520,5520,361,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671225,120,,,"[418, 1911, 471, 85, 64, 64, 81]","[1697548667796, 1697548669707, 1697548670178, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
5521,5521,170,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641386,1697548643002,120,,,[360],[1697548641746]
5522,5522,530,28,[],200,llama-7b,128,1,2724.0,1.0,1,A100,1697548643013,1697548645737,120,26.0,1.0,"[334, 2390]","[1697548643347, 1697548645737]"
5523,5523,889,29,[],200,llama-7b,128,1,7683.0,1.0,1,A100,1697548645740,1697548653423,120,86.0,20.0,"[11, 2687, 600, 92, 88, 86, 730, 87, 85, 75, 74, 74, 992, 90, 74, 861, 319, 88, 69, 404, 97]","[1697548645751, 1697548648438, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651421, 1697548651511, 1697548651585, 1697548652446, 1697548652765, 1697548652853, 1697548652922, 1697548653326, 1697548653423]"
5524,5524,422,46,[],200,llama-7b,128,1,998.0,1.0,1,A100,1697548690854,1697548691852,120,26.0,1.0,"[13, 985]","[1697548690867, 1697548691852]"
5525,5525,495,46,[],200,llama-7b,128,1,1811.0,1.0,1,A100,1697548683216,1697548685027,120,13.0,1.0,"[337, 1474]","[1697548683553, 1697548685027]"
5526,5526,654,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657430,1697548659865,120,,,"[20, 838]","[1697548657450, 1697548658288]"
5527,5527,855,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548686724,120,,,"[26, 1123]","[1697548685056, 1697548686179]"
5528,5528,631,52,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697834,120,,,[211],[1697548695935]
5529,5529,179,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677268,1697548679649,120,,,"[10, 2194]","[1697548677278, 1697548679472]"
5530,5530,753,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691856,1697548695714,120,,,"[6, 2281]","[1697548691862, 1697548694143]"
5531,5531,287,48,[],200,llama-7b,128,1,2870.0,1.0,1,A100,1697548686732,1697548689602,120,10.0,1.0,"[343, 2527]","[1697548687075, 1697548689602]"
5532,5532,537,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683207,120,,,"[79, 1951, 285, 91, 85, 83, 81]","[1697548679736, 1697548681687, 1697548681972, 1697548682063, 1697548682148, 1697548682231, 1697548682312]"
5533,5533,810,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671237,1697548674839,120,,,"[34, 1804, 538, 80, 59]","[1697548671271, 1697548673075, 1697548673613, 1697548673693, 1697548673752]"
5534,5534,181,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695723,1697548697836,120,,,"[24, 1935]","[1697548695747, 1697548697682]"
5535,5535,617,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548689609,1697548693449,120,,,"[46, 2198]","[1697548689655, 1697548691853]"
5536,5536,226,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693449,120,,,"[162, 1990]","[1697548691258, 1697548693248]"
5537,5537,50,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695714,120,,,"[188, 1786]","[1697548693648, 1697548695434]"
5538,5538,240,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677256,120,,,"[200, 1751]","[1697548675045, 1697548676796]"
5539,5539,407,51,[],200,llama-7b,128,1,1960.0,1.0,1,A100,1697548695723,1697548697683,120,16.0,1.0,"[29, 1930]","[1697548695752, 1697548697682]"
5540,5540,594,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677274,1697548679651,120,,,"[317, 1884]","[1697548677591, 1697548679475]"
5541,5541,781,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697834,120,,,"[140, 1819]","[1697548695864, 1697548697683]"
5542,5542,587,47,[],200,llama-7b,128,1,1974.0,1.0,1,A100,1697548693460,1697548695434,120,13.0,1.0,"[183, 1791]","[1697548693643, 1697548695434]"
5543,5543,356,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613010,1697548615232,120,,,"[274, 1747]","[1697548613284, 1697548615031]"
5544,5544,665,46,[],200,llama-7b,128,1,2818.0,1.0,1,A100,1697548695727,1697548698545,120,90.0,20.0,"[360, 2052, 28, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 20, 19, 19, 19, 19, 19, 19]","[1697548696087, 1697548698139, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388, 1697548698411, 1697548698431, 1697548698450, 1697548698469, 1697548698488, 1697548698507, 1697548698526, 1697548698545]"
5545,5545,19,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683208,120,,,"[331, 1699, 285, 91, 85, 82, 80]","[1697548679989, 1697548681688, 1697548681973, 1697548682064, 1697548682149, 1697548682231, 1697548682311]"
5546,5546,574,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674843,1697548677258,120,,,"[21, 1932]","[1697548674864, 1697548676796]"
5547,5547,681,15,[],200,llama-7b,128,1,2023.0,1.0,1,A100,1697548615243,1697548617266,120,23.0,1.0,"[130, 1893]","[1697548615373, 1697548617266]"
5548,5548,625,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548656184,1697548659863,120,,,"[26, 2078]","[1697548656210, 1697548658288]"
5549,5549,378,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686706,120,,,"[262, 1785, 91, 90, 89, 70, 87, 86, 68]","[1697548683478, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
5550,5550,113,16,[],200,llama-7b,128,1,685.0,1.0,1,A100,1697548617271,1697548617956,120,13.0,1.0,"[49, 636]","[1697548617320, 1697548617956]"
5551,5551,3,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677270,1697548679646,120,,,"[100, 2104]","[1697548677370, 1697548679474]"
5552,5552,248,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548677260,120,,,"[287, 1661]","[1697548675135, 1697548676796]"
5553,5553,472,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548617958,1697548620356,120,,,"[11, 2353]","[1697548617969, 1697548620322]"
5554,5554,364,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679661,1697548683209,120,,,"[404, 2843]","[1697548680065, 1697548682908]"
5555,5555,814,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686707,120,,,"[327, 1483, 237, 91, 91, 89, 69, 87, 86, 69]","[1697548683543, 1697548685026, 1697548685263, 1697548685354, 1697548685445, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685845]"
5556,5556,833,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.96 GiB. GPU 0 has a total capacty of 39.39 GiB of which 462.06 MiB is free. Process 1412106 has 38.94 GiB memory in use. Of the allocated memory 23.00 GiB is allocated by PyTorch, and 14.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548620369,1697548622600,120,,,"[176, 1786]","[1697548620545, 1697548622331]"
5557,5557,577,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677278,1697548679648,120,,,"[199, 1996]","[1697548677477, 1697548679473]"
5558,5558,243,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686728,1697548688900,120,,,"[46, 1938]","[1697548686774, 1697548688712]"
5559,5559,7,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683205,120,,,"[211, 1822, 283, 90, 85, 82, 80]","[1697548679868, 1697548681690, 1697548681973, 1697548682063, 1697548682148, 1697548682230, 1697548682310]"
5560,5560,27,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659874,1697548661852,120,,,[196],[1697548660070]
5561,5561,710,41,[],200,llama-7b,128,1,1986.0,1.0,1,A100,1697548686729,1697548688715,120,14.0,1.0,"[326, 1660]","[1697548687055, 1697548688715]"
5562,5562,313,29,[],200,llama-7b,128,1,885.0,1.0,1,A100,1697548661867,1697548662752,120,20.0,1.0,"[81, 804]","[1697548661948, 1697548662752]"
5563,5563,597,39,[],200,llama-7b,128,1,1937.0,1.0,1,A100,1697548688909,1697548690846,120,39.0,1.0,"[75, 1862]","[1697548688984, 1697548690846]"
5564,5564,388,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664211,120,,,"[37, 848, 228]","[1697548661904, 1697548662752, 1697548662980]"
5565,5565,27,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690858,1697548693451,120,,,"[34, 960]","[1697548690892, 1697548691852]"
5566,5566,644,30,[],200,llama-7b,128,1,1024.0,1.0,1,A100,1697548662756,1697548663780,120,19.0,1.0,"[23, 1001]","[1697548662779, 1697548663780]"
5567,5567,69,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663784,1697548667372,120,,,"[53, 885, 1745, 82, 67, 84]","[1697548663837, 1697548664722, 1697548666467, 1697548666549, 1697548666616, 1697548666700]"
5568,5568,142,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688717,1697548691086,120,,,"[31, 854]","[1697548688748, 1697548689602]"
5569,5569,381,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695718,120,,,"[103, 1870]","[1697548693562, 1697548695432]"
5570,5570,501,43,[],200,llama-7b,128,1,2152.0,1.0,1,A100,1697548691097,1697548693249,120,19.0,1.0,"[176, 1976]","[1697548691273, 1697548693249]"
5571,5571,372,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683213,1697548686708,120,,,"[29, 1782, 239, 91, 90, 89, 70, 87, 86, 68]","[1697548683242, 1697548685024, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
5572,5572,859,44,[],200,llama-7b,128,1,889.0,1.0,1,A100,1697548693255,1697548694144,120,23.0,1.0,"[80, 809]","[1697548693335, 1697548694144]"
5573,5573,280,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548694147,1697548697835,120,,,"[12, 2569]","[1697548694159, 1697548696728]"
5574,5574,732,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686723,1697548688899,120,,,"[36, 1953]","[1697548686759, 1697548688712]"
5575,5575,428,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667379,1697548671231,120,,,"[382, 1945, 472, 84, 65, 64, 80]","[1697548667761, 1697548669706, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
5576,5576,86,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548687545,1697548691085,120,,,"[20, 2039]","[1697548687565, 1697548689604]"
5577,5577,131,48,[],200,llama-7b,128,1,1938.0,1.0,1,A100,1697548688906,1697548690844,120,8.0,1.0,"[8, 1930]","[1697548688914, 1697548690844]"
5578,5578,450,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691095,1697548693454,120,,,"[59, 2092]","[1697548691154, 1697548693246]"
5579,5579,742,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664219,1697548667371,120,,,"[327, 1491, 430, 84, 65, 85]","[1697548664546, 1697548666037, 1697548666467, 1697548666551, 1697548666616, 1697548666701]"
5580,5580,485,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690854,1697548693450,120,,,"[18, 980]","[1697548690872, 1697548691852]"
5581,5581,841,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695717,120,,,[312],[1697548693776]
5582,5582,789,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674840,120,,,"[131, 1703, 537, 78, 61]","[1697548671374, 1697548673077, 1697548673614, 1697548673692, 1697548673753]"
5583,5583,808,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695718,120,,,"[381, 1589]","[1697548693845, 1697548695434]"
5584,5584,699,37,[],200,llama-7b,128,1,1834.0,1.0,1,A100,1697548671243,1697548673077,120,39.0,1.0,"[136, 1698]","[1697548671379, 1697548673077]"
5585,5585,238,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,"[62, 1897]","[1697548695786, 1697548697683]"
5586,5586,130,38,[],200,llama-7b,128,1,1238.0,1.0,1,A100,1697548673081,1697548674319,120,14.0,1.0,"[71, 1167]","[1697548673152, 1697548674319]"
5587,5587,172,35,[],200,llama-7b,128,1,2328.0,1.0,1,A100,1697548667378,1697548669706,120,19.0,1.0,"[115, 2213]","[1697548667493, 1697548669706]"
5588,5588,526,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671229,120,,,"[96, 1394]","[1697548669809, 1697548671203]"
5589,5589,706,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655283,1697548657502,120,,,"[253, 1889]","[1697548655536, 1697548657425]"
5590,5590,75,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657537,120,,,"[99, 2046]","[1697548655380, 1697548657426]"
5591,5591,131,31,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548657548,1697548659578,120,8.0,1.0,"[316, 1714]","[1697548657864, 1697548659578]"
5592,5592,432,29,[],200,llama-7b,128,1,2034.0,1.0,1,A100,1697548657545,1697548659579,120,13.0,1.0,"[133, 1901]","[1697548657678, 1697548659579]"
5593,5593,630,44,[],200,llama-7b,128,1,1985.0,1.0,1,A100,1697548686729,1697548688714,120,6.0,1.0,"[306, 1679]","[1697548687035, 1697548688714]"
5594,5594,491,32,[],200,llama-7b,128,1,1118.0,1.0,1,A100,1697548659583,1697548660701,120,14.0,1.0,"[63, 1055]","[1697548659646, 1697548660701]"
5595,5595,61,45,[],200,llama-7b,128,1,885.0,1.0,1,A100,1697548688718,1697548689603,120,9.0,1.0,"[40, 844]","[1697548688758, 1697548689602]"
5596,5596,786,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659583,1697548664207,120,,,"[78, 1040, 1195, 58, 1027]","[1697548659661, 1697548660701, 1697548661896, 1697548661954, 1697548662981]"
5597,5597,422,46,[],200,llama-7b,128,1,2245.0,1.0,1,A100,1697548689608,1697548691853,120,26.0,1.0,"[27, 2217]","[1697548689635, 1697548691852]"
5598,5598,776,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691856,1697548695713,120,,,"[11, 2276]","[1697548691867, 1697548694143]"
5599,5599,816,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548660704,1697548664209,120,,,"[12, 2036, 227]","[1697548660716, 1697548662752, 1697548662979]"
5600,5600,248,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667371,120,,,"[346, 1474, 429, 85, 65, 84]","[1697548664564, 1697548666038, 1697548666467, 1697548666552, 1697548666617, 1697548666701]"
5601,5601,14,29,[],200,llama-7b,128,1,7938.0,1.0,1,A100,1697548643575,1697548651513,120,90.0,20.0,"[11, 3489, 550, 96, 96, 93, 71, 94, 71, 891, 92, 88, 87, 731, 87, 85, 75, 73, 75, 991, 92]","[1697548643586, 1697548647075, 1697548647625, 1697548647721, 1697548647817, 1697548647910, 1697548647981, 1697548648075, 1697548648146, 1697548649037, 1697548649129, 1697548649217, 1697548649304, 1697548650035, 1697548650122, 1697548650207, 1697548650282, 1697548650355, 1697548650430, 1697548651421, 1697548651513]"
5602,5602,183,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667368,120,,,"[126, 1694, 430, 83, 67, 84]","[1697548664343, 1697548666037, 1697548666467, 1697548666550, 1697548666617, 1697548666701]"
5603,5603,574,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695716,120,,,"[213, 1758]","[1697548693673, 1697548695431]"
5604,5604,4,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697834,120,,,"[136, 1823]","[1697548695860, 1697548697683]"
5605,5605,607,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671225,120,,,"[468, 1862, 470, 85, 64, 64, 81]","[1697548667846, 1697548669708, 1697548670178, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
5606,5606,720,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653427,1697548657538,120,,,"[7, 2745]","[1697548653434, 1697548656179]"
5607,5607,375,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548651516,1697548655268,120,,,"[6, 2725, 96, 84, 63, 80]","[1697548651522, 1697548654247, 1697548654343, 1697548654427, 1697548654490, 1697548654570]"
5608,5608,618,30,[],200,llama-7b,128,1,3279.0,1.0,1,A100,1697548646660,1697548649939,120,9.0,1.0,"[24, 3255]","[1697548646684, 1697548649939]"
5609,5609,52,31,[],200,llama-7b,128,1,3385.0,1.0,1,A100,1697548649941,1697548653326,120,58.0,6.0,"[7, 2230, 268, 319, 89, 69, 403]","[1697548649948, 1697548652178, 1697548652446, 1697548652765, 1697548652854, 1697548652923, 1697548653326]"
5610,5610,156,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659865,120,,,"[230, 1801]","[1697548657776, 1697548659577]"
5611,5611,602,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659877,1697548661855,120,,,[300],[1697548660177]
5612,5612,666,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671226,120,,,"[75, 572, 35, 2118, 84, 64, 65, 79]","[1697548667453, 1697548668025, 1697548668060, 1697548670178, 1697548670262, 1697548670326, 1697548670391, 1697548670470]"
5613,5613,31,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661866,1697548664209,120,,,"[17, 869, 228]","[1697548661883, 1697548662752, 1697548662980]"
5614,5614,410,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653330,1697548657538,120,,,"[7, 2841]","[1697548653337, 1697548656178]"
5615,5615,515,34,[],200,llama-7b,128,1,2328.0,1.0,1,A100,1697548667379,1697548669707,120,11.0,1.0,"[412, 1916]","[1697548667791, 1697548669707]"
5616,5616,385,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667372,120,,,"[365, 1456, 428, 85, 65, 85]","[1697548664583, 1697548666039, 1697548666467, 1697548666552, 1697548666617, 1697548666702]"
5617,5617,764,33,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548657546,1697548659577,120,39.0,1.0,"[220, 1811]","[1697548657766, 1697548659577]"
5618,5618,190,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664204,120,,,"[23, 1096, 1195, 57, 1026]","[1697548659605, 1697548660701, 1697548661896, 1697548661953, 1697548662979]"
5619,5619,740,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667379,1697548671225,120,,,"[459, 1870, 470, 85, 64, 64, 81]","[1697548667838, 1697548669708, 1697548670178, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
5620,5620,877,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671227,120,,,"[51, 1439]","[1697548669764, 1697548671203]"
5621,5621,518,35,[],200,llama-7b,128,1,1820.0,1.0,1,A100,1697548664216,1697548666036,120,23.0,1.0,"[28, 1791]","[1697548664244, 1697548666035]"
5622,5622,309,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674840,120,,,"[121, 1712, 538, 79, 60]","[1697548671364, 1697548673076, 1697548673614, 1697548673693, 1697548673753]"
5623,5623,95,37,[],200,llama-7b,128,1,1839.0,1.0,1,A100,1697548671238,1697548673077,120,12.0,1.0,"[317, 1522]","[1697548671555, 1697548673077]"
5624,5624,497,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677255,120,,,"[108, 1844]","[1697548674952, 1697548676796]"
5625,5625,169,35,[],200,llama-7b,128,1,1838.0,1.0,1,A100,1697548671238,1697548673076,120,10.0,1.0,"[44, 1794]","[1697548671282, 1697548673076]"
5626,5626,508,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679645,120,,,"[326, 3068]","[1697548675174, 1697548678242]"
5627,5627,457,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673081,1697548674841,120,,,[21],[1697548673102]
5628,5628,828,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677273,1697548679649,120,,,[273],[1697548677546]
5629,5629,869,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683213,1697548686708,120,,,"[39, 1773, 239, 90, 90, 89, 70, 87, 86, 68]","[1697548683252, 1697548685025, 1697548685264, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
5630,5630,95,36,[],200,llama-7b,128,1,3394.0,1.0,1,A100,1697548674848,1697548678242,120,12.0,1.0,"[389, 3005]","[1697548675237, 1697548678242]"
5631,5631,257,37,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679658,1697548681688,120,14.0,1.0,"[326, 1704]","[1697548679984, 1697548681688]"
5632,5632,51,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661852,120,,,[29],[1697548659900]
5633,5633,836,39,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548679656,1697548681689,120,11.0,1.0,"[114, 1919]","[1697548679770, 1697548681689]"
5634,5634,713,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695725,1697548697834,120,,,[238],[1697548695963]
5635,5635,409,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664205,120,,,"[181, 1729]","[1697548662048, 1697548663777]"
5636,5636,770,35,[],200,llama-7b,128,1,1821.0,1.0,1,A100,1697548664217,1697548666038,120,13.0,1.0,"[165, 1655]","[1697548664382, 1697548666037]"
5637,5637,303,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686732,1697548691085,120,,,"[348, 2522]","[1697548687080, 1697548689602]"
5638,5638,267,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548686704,120,,,"[75, 2270, 1219, 91, 90, 89, 70, 87, 86, 68]","[1697548681773, 1697548684043, 1697548685262, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
5639,5639,660,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548695713,120,,,"[358, 2688]","[1697548691454, 1697548694142]"
5640,5640,201,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666047,1697548667369,120,,,"[61, 1240]","[1697548666108, 1697548667348]"
5641,5641,649,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667377,1697548671228,120,,,"[185, 2145, 472, 84, 64, 64, 80]","[1697548667562, 1697548669707, 1697548670179, 1697548670263, 1697548670327, 1697548670391, 1697548670471]"
5642,5642,269,51,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695724,1697548697683,120,11.0,1.0,"[28, 1931]","[1697548695752, 1697548697683]"
5643,5643,74,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674839,120,,,"[358, 2722]","[1697548671596, 1697548674318]"
5644,5644,432,39,[],200,llama-7b,128,1,3394.0,1.0,1,A100,1697548674848,1697548678242,120,13.0,1.0,"[398, 2996]","[1697548675246, 1697548678242]"
5645,5645,617,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688899,120,,,"[29, 1954]","[1697548686758, 1697548688712]"
5646,5646,293,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653427,1697548657538,120,,,"[34, 2718]","[1697548653461, 1697548656179]"
5647,5647,487,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674322,1697548677256,120,,,[24],[1697548674346]
5648,5648,647,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659865,120,,,"[216, 1814]","[1697548657762, 1697548659576]"
5649,5649,837,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679645,120,,,"[81, 2123]","[1697548677350, 1697548679473]"
5650,5650,77,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661856,120,,,[109],[1697548659981]
5651,5651,238,41,[],200,llama-7b,128,1,2655.0,1.0,1,A100,1697548679656,1697548682311,120,563.0,6.0,"[8, 2022, 286, 91, 85, 82, 81]","[1697548679664, 1697548681686, 1697548681972, 1697548682063, 1697548682148, 1697548682230, 1697548682311]"
5652,5652,435,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661866,1697548664210,120,,,"[22, 864, 228]","[1697548661888, 1697548662752, 1697548662980]"
5653,5653,596,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682315,1697548686706,120,,,"[6, 1722, 1219, 91, 91, 89, 69, 87, 86, 69]","[1697548682321, 1697548684043, 1697548685262, 1697548685353, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
5654,5654,38,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674838,120,,,"[322, 1517, 537, 79, 60]","[1697548671560, 1697548673077, 1697548673614, 1697548673693, 1697548673753]"
5655,5655,715,43,[],200,llama-7b,128,1,1984.0,1.0,1,A100,1697548686729,1697548688713,120,20.0,1.0,"[113, 1871]","[1697548686842, 1697548688713]"
5656,5656,767,34,[],200,llama-7b,128,1,2332.0,1.0,1,A100,1697548667378,1697548669710,120,11.0,1.0,"[364, 1968]","[1697548667742, 1697548669710]"
5657,5657,200,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669714,1697548671228,120,,,"[74, 1415]","[1697548669788, 1697548671203]"
5658,5658,602,32,[],200,llama-7b,128,1,2145.0,1.0,1,A100,1697548655281,1697548657426,120,15.0,1.0,"[89, 2056]","[1697548655370, 1697548657426]"
5659,5659,30,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657429,1697548659865,120,,,"[41, 818]","[1697548657470, 1697548658288]"
5660,5660,116,44,[],200,llama-7b,128,1,885.0,1.0,1,A100,1697548688717,1697548689602,120,23.0,1.0,"[46, 839]","[1697548688763, 1697548689602]"
5661,5661,470,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548689609,1697548693449,120,,,"[36, 2208]","[1697548689645, 1697548691853]"
5662,5662,390,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661853,120,,,[38],[1697548659909]
5663,5663,829,46,[],200,llama-7b,128,1,1972.0,1.0,1,A100,1697548693459,1697548695431,120,20.0,1.0,"[30, 1942]","[1697548693489, 1697548695431]"
5664,5664,393,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677256,120,,,"[195, 1757]","[1697548675040, 1697548676797]"
5665,5665,28,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686723,1697548688900,120,,,"[261, 1729]","[1697548686984, 1697548688713]"
5666,5666,646,36,[],200,llama-7b,128,1,1833.0,1.0,1,A100,1697548671243,1697548673076,120,14.0,1.0,"[246, 1587]","[1697548671489, 1697548673076]"
5667,5667,747,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664203,120,,,"[76, 809, 229]","[1697548661943, 1697548662752, 1697548662981]"
5668,5668,255,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695437,1697548697835,120,,,"[40, 1251]","[1697548695477, 1697548696728]"
5669,5669,71,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673080,1697548674841,120,,,"[27, 1212]","[1697548673107, 1697548674319]"
5670,5670,175,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664216,1697548667369,120,,,"[156, 2096, 82, 65, 85]","[1697548664372, 1697548666468, 1697548666550, 1697548666615, 1697548666700]"
5671,5671,167,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683215,1697548686705,120,,,"[223, 1586, 238, 92, 90, 89, 69, 87, 87, 68]","[1697548683438, 1697548685024, 1697548685262, 1697548685354, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685776, 1697548685844]"
5672,5672,757,30,[],200,llama-7b,128,1,1833.0,1.0,1,A100,1697548654347,1697548656180,120,20.0,1.0,"[19, 1813]","[1697548654366, 1697548656179]"
5673,5673,391,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691086,120,,,[57],[1697548688964]
5674,5674,156,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548656185,1697548659864,120,,,"[40, 2063]","[1697548656225, 1697548658288]"
5675,5675,877,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666041,1697548667368,120,,,"[22, 1283]","[1697548666063, 1697548667346]"
5676,5676,768,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657546,1697548659865,120,,,"[215, 1815]","[1697548657761, 1697548659576]"
5677,5677,720,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691095,1697548693449,120,,,"[153, 2000]","[1697548691248, 1697548693248]"
5678,5678,150,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695717,120,,,[33],[1697548693492]
5679,5679,773,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686732,1697548691085,120,,,"[403, 2468]","[1697548687135, 1697548689603]"
5680,5680,197,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661853,120,,,[44],[1697548659915]
5681,5681,500,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697836,120,,,"[9, 1949]","[1697548695733, 1697548697682]"
5682,5682,515,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661852,120,,,[198],[1697548660070]
5683,5683,866,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661867,1697548664206,120,,,"[293, 1619]","[1697548662160, 1697548663779]"
5684,5684,527,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661866,1697548664210,120,,,"[28, 1086]","[1697548661894, 1697548662980]"
5685,5685,907,42,[],200,llama-7b,128,1,1985.0,1.0,1,A100,1697548686729,1697548688714,120,10.0,1.0,"[146, 1839]","[1697548686875, 1697548688714]"
5686,5686,296,34,[],200,llama-7b,128,1,1820.0,1.0,1,A100,1697548664218,1697548666038,120,6.0,1.0,"[164, 1656]","[1697548664382, 1697548666038]"
5687,5687,654,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666045,1697548667368,120,,,"[27, 1274]","[1697548666072, 1697548667346]"
5688,5688,176,36,[],200,llama-7b,128,1,2801.0,1.0,1,A100,1697548667377,1697548670178,120,216.0,2.0,"[102, 2699]","[1697548667479, 1697548670178]"
5689,5689,884,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667372,120,,,"[370, 1451, 428, 84, 66, 85]","[1697548664588, 1697548666039, 1697548666467, 1697548666551, 1697548666617, 1697548666702]"
5690,5690,332,43,[],200,llama-7b,128,1,885.0,1.0,1,A100,1697548688718,1697548689603,120,39.0,1.0,"[48, 836]","[1697548688766, 1697548689602]"
5691,5691,205,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693453,120,,,"[365, 1787]","[1697548691462, 1697548693249]"
5692,5692,690,44,[],200,llama-7b,128,1,2244.0,1.0,1,A100,1697548689609,1697548691853,120,39.0,1.0,"[51, 2193]","[1697548689660, 1697548691853]"
5693,5693,668,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677257,120,,,"[215, 1736]","[1697548675060, 1697548676796]"
5694,5694,732,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655280,1697548657538,120,,,"[199, 1946]","[1697548655479, 1697548657425]"
5695,5695,116,45,[],200,llama-7b,128,1,2286.0,1.0,1,A100,1697548691858,1697548694144,120,23.0,1.0,"[24, 2261]","[1697548691882, 1697548694143]"
5696,5696,312,38,[],200,llama-7b,128,1,2328.0,1.0,1,A100,1697548667381,1697548669709,120,23.0,1.0,"[480, 1847]","[1697548667861, 1697548669708]"
5697,5697,872,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683215,1697548686705,120,,,"[157, 1655, 235, 91, 91, 89, 69, 87, 86, 69]","[1697548683372, 1697548685027, 1697548685262, 1697548685353, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
5698,5698,448,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548694153,1697548697833,120,,,"[16, 2559]","[1697548694169, 1697548696728]"
5699,5699,917,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695437,1697548697835,120,,,"[36, 1255]","[1697548695473, 1697548696728]"
5700,5700,184,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677268,1697548679649,120,,,"[10, 2194]","[1697548677278, 1697548679472]"
5701,5701,687,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674838,120,,,"[323, 1511, 537, 79, 61]","[1697548671566, 1697548673077, 1697548673614, 1697548673693, 1697548673754]"
5702,5702,672,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671227,120,,,"[21, 1468]","[1697548669734, 1697548671202]"
5703,5703,540,39,[],200,llama-7b,128,1,2573.0,1.0,1,A100,1697548679657,1697548682230,120,140.0,5.0,"[201, 1832, 282, 91, 85, 82]","[1697548679858, 1697548681690, 1697548681972, 1697548682063, 1697548682148, 1697548682230]"
5704,5704,97,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671245,1697548674838,120,,,"[331, 1501, 537, 79, 61]","[1697548671576, 1697548673077, 1697548673614, 1697548673693, 1697548673754]"
5705,5705,893,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682233,1697548686706,120,,,"[14, 1796, 1219, 91, 91, 89, 69, 87, 86, 69]","[1697548682247, 1697548684043, 1697548685262, 1697548685353, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
5706,5706,304,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686721,1697548688899,120,,,"[256, 1736]","[1697548686977, 1697548688713]"
5707,5707,321,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688901,120,,,"[126, 1858]","[1697548686855, 1697548688713]"
5708,5708,258,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622608,1697548624958,120,,,"[335, 1737]","[1697548622943, 1697548624680]"
5709,5709,112,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674846,1697548677259,120,,,"[229, 1721]","[1697548675075, 1697548676796]"
5710,5710,685,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691087,120,,,"[129, 1808]","[1697548689038, 1697548690846]"
5711,5711,88,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693453,120,,,"[65, 2086]","[1697548691161, 1697548693247]"
5712,5712,705,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 12.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548624968,1697548627100,120,,,[110],[1697548625078]
5713,5713,134,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627105,1697548629598,120,,,"[49, 2183]","[1697548627154, 1697548629337]"
5714,5714,446,44,[],200,llama-7b,128,1,1968.0,1.0,1,A100,1697548693463,1697548695431,120,26.0,1.0,"[210, 1758]","[1697548693673, 1697548695431]"
5715,5715,706,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635605,1697548639637,120,,,"[72, 832, 1222, 417]","[1697548635677, 1697548636509, 1697548637731, 1697548638148]"
5716,5716,805,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695436,1697548697833,120,,,"[7, 1285]","[1697548695443, 1697548696728]"
5717,5717,136,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639655,1697548641376,120,,,[328],[1697548639983]
5718,5718,787,40,[],200,llama-7b,128,1,4065.0,1.0,1,A100,1697548678245,1697548682310,120,123.0,6.0,"[31, 2080, 1615, 92, 85, 81, 81]","[1697548678276, 1697548680356, 1697548681971, 1697548682063, 1697548682148, 1697548682229, 1697548682310]"
5719,5719,494,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642997,120,,,[260],[1697548641645]
5720,5720,219,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682315,1697548686706,120,,,"[6, 1722, 1220, 90, 91, 89, 69, 87, 86, 69]","[1697548682321, 1697548684043, 1697548685263, 1697548685353, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
5721,5721,467,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548662986,1697548667371,120,,,"[10, 1726, 1744, 83, 66, 84]","[1697548662996, 1697548664722, 1697548666466, 1697548666549, 1697548666615, 1697548666699]"
5722,5722,305,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674846,1697548677259,120,,,"[243, 1707]","[1697548675089, 1697548676796]"
5723,5723,508,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677259,120,,,"[103, 1849]","[1697548674947, 1697548676796]"
5724,5724,821,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671225,120,,,"[470, 1860, 471, 84, 64, 64, 81]","[1697548667848, 1697548669708, 1697548670179, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
5725,5725,861,39,[],200,llama-7b,128,1,2196.0,1.0,1,A100,1697548677278,1697548679474,120,10.0,1.0,"[258, 1937]","[1697548677536, 1697548679473]"
5726,5726,377,40,[],200,llama-7b,128,1,875.0,1.0,1,A100,1697548679482,1697548680357,120,13.0,1.0,"[52, 823]","[1697548679534, 1697548680357]"
5727,5727,667,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677279,1697548679650,120,,,"[299, 1897]","[1697548677578, 1697548679475]"
5728,5728,733,41,[],200,llama-7b,128,1,2548.0,1.0,1,A100,1697548680361,1697548682909,120,31.0,1.0,"[20, 2528]","[1697548680381, 1697548682909]"
5729,5729,165,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682912,1697548686707,120,,,"[19, 1113, 1219, 91, 90, 89, 70, 86, 87, 68]","[1697548682931, 1697548684044, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685689, 1697548685776, 1697548685844]"
5730,5730,549,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686724,1697548688900,120,,,"[265, 1724]","[1697548686989, 1697548688713]"
5731,5731,92,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679659,1697548683209,120,,,"[396, 2853]","[1697548680055, 1697548682908]"
5732,5732,459,12,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.45 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.55 GiB is free. Process 1412106 has 32.84 GiB memory in use. Of the allocated memory 22.96 GiB is allocated by PyTorch, and 8.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548602127,1697548605265,120,,,"[127, 1599, 260, 80, 74]","[1697548602254, 1697548603853, 1697548604113, 1697548604193, 1697548604267]"
5733,5733,524,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686724,1697548688902,120,,,"[161, 1827]","[1697548686885, 1697548688712]"
5734,5734,907,43,[],200,llama-7b,128,1,1937.0,1.0,1,A100,1697548688909,1697548690846,120,10.0,1.0,"[70, 1867]","[1697548688979, 1697548690846]"
5735,5735,853,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691090,120,,,"[231, 1705]","[1697548689140, 1697548690845]"
5736,5736,450,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683214,1697548686709,120,,,"[78, 1734, 238, 91, 88, 89, 72, 86, 85, 68]","[1697548683292, 1697548685026, 1697548685264, 1697548685355, 1697548685443, 1697548685532, 1697548685604, 1697548685690, 1697548685775, 1697548685843]"
5737,5737,282,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693450,120,,,"[241, 1909]","[1697548691337, 1697548693246]"
5738,5738,788,13,[],200,llama-7b,128,1,1946.0,1.0,1,A100,1697548605283,1697548607229,120,31.0,1.0,"[213, 1733]","[1697548605496, 1697548607229]"
5739,5739,208,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.36 GiB is free. Process 1412106 has 32.03 GiB memory in use. Of the allocated memory 24.12 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548607236,1697548608965,120,,,"[7, 1527]","[1697548607243, 1697548608770]"
5740,5740,641,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695714,120,,,"[117, 1856]","[1697548693577, 1697548695433]"
5741,5741,341,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690860,1697548693451,120,,,"[42, 951]","[1697548690902, 1697548691853]"
5742,5742,568,15,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.12 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.29 GiB is free. Process 1412106 has 37.10 GiB memory in use. Of the allocated memory 24.25 GiB is allocated by PyTorch, and 11.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548608974,1697548610832,120,,,[202],[1697548609176]
5743,5743,69,47,[],200,llama-7b,128,1,2819.0,1.0,1,A100,1697548695726,1697548698545,120,85.0,20.0,"[332, 2081, 28, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 19, 19, 19, 19, 19, 19]","[1697548696058, 1697548698139, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388, 1697548698410, 1697548698431, 1697548698450, 1697548698469, 1697548698488, 1697548698507, 1697548698526, 1697548698545]"
5744,5744,202,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548675477,1697548679647,120,,,"[11, 2753]","[1697548675488, 1697548678241]"
5745,5745,889,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657535,120,,,"[285, 1860]","[1697548655566, 1697548657426]"
5746,5746,560,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679656,1697548683207,120,,,"[7, 2309, 91, 85, 82, 81]","[1697548679663, 1697548681972, 1697548682063, 1697548682148, 1697548682230, 1697548682311]"
5747,5747,341,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674838,120,,,"[18, 1819, 538, 79, 61]","[1697548671256, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
5748,5748,695,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695715,120,,,"[203, 1771]","[1697548693663, 1697548695434]"
5749,5749,290,29,[],200,llama-7b,128,1,2034.0,1.0,1,A100,1697548657545,1697548659579,120,14.0,1.0,"[126, 1908]","[1697548657671, 1697548659579]"
5750,5750,918,39,[],200,llama-7b,128,1,1809.0,1.0,1,A100,1697548683216,1697548685025,120,23.0,1.0,"[247, 1562]","[1697548683463, 1697548685025]"
5751,5751,124,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695726,1697548697835,120,,,[257],[1697548695983]
5752,5752,700,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677257,120,,,[210],[1697548675055]
5753,5753,346,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548688901,120,,,"[36, 2475]","[1697548685066, 1697548687541]"
5754,5754,677,41,[],200,llama-7b,128,1,1944.0,1.0,1,A100,1697548688909,1697548690853,120,9.0,1.0,"[80, 1857]","[1697548688989, 1697548690846]"
5755,5755,129,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677279,1697548679651,120,,,"[357, 1839]","[1697548677636, 1697548679475]"
5756,5756,486,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679659,1697548683208,120,,,"[406, 1624, 284, 91, 84, 83, 81]","[1697548680065, 1697548681689, 1697548681973, 1697548682064, 1697548682148, 1697548682231, 1697548682312]"
5757,5757,648,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659584,1697548664207,120,,,"[75, 1042, 1195, 57, 1028]","[1697548659659, 1697548660701, 1697548661896, 1697548661953, 1697548662981]"
5758,5758,916,27,[],200,llama-7b,128,1,1597.0,1.0,1,A100,1697548637703,1697548639300,120,8.0,1.0,"[280, 1317]","[1697548637983, 1697548639300]"
5759,5759,341,28,[],200,llama-7b,128,1,8841.0,1.0,1,A100,1697548639304,1697548648145,120,87.0,20.0,"[34, 845, 1231, 365, 1261, 583, 1588, 85, 64, 953, 94, 91, 89, 68, 969, 96, 95, 94, 72, 93, 71]","[1697548639338, 1697548640183, 1697548641414, 1697548641779, 1697548643040, 1697548643623, 1697548645211, 1697548645296, 1697548645360, 1697548646313, 1697548646407, 1697548646498, 1697548646587, 1697548646655, 1697548647624, 1697548647720, 1697548647815, 1697548647909, 1697548647981, 1697548648074, 1697548648145]"
5760,5760,845,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683213,1697548686709,120,,,"[63, 1749, 239, 90, 91, 89, 69, 87, 86, 68]","[1697548683276, 1697548685025, 1697548685264, 1697548685354, 1697548685445, 1697548685534, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
5761,5761,307,45,[],200,llama-7b,128,1,2151.0,1.0,1,A100,1697548691096,1697548693247,120,26.0,1.0,"[60, 2090]","[1697548691156, 1697548693246]"
5762,5762,125,39,[],200,llama-7b,128,1,1833.0,1.0,1,A100,1697548671243,1697548673076,120,13.0,1.0,"[235, 1597]","[1697548671478, 1697548673075]"
5763,5763,69,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667373,120,,,"[12, 1806, 432, 82, 67, 84]","[1697548664229, 1697548666035, 1697548666467, 1697548666549, 1697548666616, 1697548666700]"
5764,5764,486,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673081,1697548674842,120,,,"[50, 1187]","[1697548673131, 1697548674318]"
5765,5765,558,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695719,120,,,"[386, 1584]","[1697548693850, 1697548695434]"
5766,5766,428,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671225,120,,,"[480, 1850, 471, 84, 64, 64, 81]","[1697548667858, 1697548669708, 1697548670179, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
5767,5767,701,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548648150,1697548655268,120,,,"[8, 1781, 97, 86, 85, 75, 74, 74, 1000, 83, 74, 860, 318, 86, 70, 406, 97, 88, 84, 651, 97, 83, 62, 80]","[1697548648158, 1697548649939, 1697548650036, 1697548650122, 1697548650207, 1697548650282, 1697548650356, 1697548650430, 1697548651430, 1697548651513, 1697548651587, 1697548652447, 1697548652765, 1697548652851, 1697548652921, 1697548653327, 1697548653424, 1697548653512, 1697548653596, 1697548654247, 1697548654344, 1697548654427, 1697548654489, 1697548654569]"
5768,5768,245,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686730,1697548691084,120,,,"[330, 2542]","[1697548687060, 1697548689602]"
5769,5769,888,47,[],200,llama-7b,128,1,2413.0,1.0,1,A100,1697548695727,1697548698140,120,19.0,1.0,"[351, 2062]","[1697548696078, 1697548698140]"
5770,5770,846,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679645,120,,,"[316, 3077]","[1697548675164, 1697548678241]"
5771,5771,248,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679656,1697548683208,120,,,"[89, 1942, 285, 91, 85, 83, 81]","[1697548679745, 1697548681687, 1697548681972, 1697548682063, 1697548682148, 1697548682231, 1697548682312]"
5772,5772,600,43,[],200,llama-7b,128,1,2151.0,1.0,1,A100,1697548691095,1697548693246,120,23.0,1.0,"[8, 2143]","[1697548691103, 1697548693246]"
5773,5773,786,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674839,120,,,"[404, 2676]","[1697548671642, 1697548674318]"
5774,5774,446,45,[],200,llama-7b,128,1,2197.0,1.0,1,A100,1697548677278,1697548679475,120,26.0,1.0,"[308, 1889]","[1697548677586, 1697548679475]"
5775,5775,33,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693252,1697548695715,120,,,"[22, 870]","[1697548693274, 1697548694144]"
5776,5776,803,46,[],200,llama-7b,128,1,877.0,1.0,1,A100,1697548679480,1697548680357,120,20.0,1.0,"[49, 828]","[1697548679529, 1697548680357]"
5777,5777,602,43,[],200,llama-7b,128,1,1809.0,1.0,1,A100,1697548683216,1697548685025,120,15.0,1.0,"[236, 1572]","[1697548683452, 1697548685024]"
5778,5778,236,47,[],200,llama-7b,128,1,2547.0,1.0,1,A100,1697548680362,1697548682909,120,8.0,1.0,"[24, 2523]","[1697548680386, 1697548682909]"
5779,5779,32,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685031,1697548688903,120,,,"[60, 2451]","[1697548685091, 1697548687542]"
5780,5780,547,41,[],200,llama-7b,128,1,1951.0,1.0,1,A100,1697548674845,1697548676796,120,12.0,1.0,"[204, 1747]","[1697548675049, 1697548676796]"
5781,5781,905,42,[],200,llama-7b,128,1,1441.0,1.0,1,A100,1697548676800,1697548678241,120,11.0,1.0,"[11, 1430]","[1697548676811, 1697548678241]"
5782,5782,85,45,[],200,llama-7b,128,1,2818.0,1.0,1,A100,1697548695727,1697548698545,120,88.0,20.0,"[316, 2096, 28, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 19, 19, 19, 18, 19, 19]","[1697548696043, 1697548698139, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388, 1697548698410, 1697548698431, 1697548698450, 1697548698469, 1697548698488, 1697548698506, 1697548698525, 1697548698544]"
5783,5783,337,43,[],200,llama-7b,128,1,2111.0,1.0,1,A100,1697548678245,1697548680356,120,12.0,1.0,"[30, 2081]","[1697548678275, 1697548680356]"
5784,5784,692,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548680359,1697548683209,120,,,"[17, 2532]","[1697548680376, 1697548682908]"
5785,5785,390,45,[],200,llama-7b,128,1,2818.0,1.0,1,A100,1697548695727,1697548698545,120,84.0,20.0,"[326, 2086, 28, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 19, 19, 19, 19, 18, 20]","[1697548696053, 1697548698139, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388, 1697548698410, 1697548698431, 1697548698450, 1697548698469, 1697548698488, 1697548698507, 1697548698525, 1697548698545]"
5786,5786,91,45,[],200,llama-7b,128,1,1809.0,1.0,1,A100,1697548683216,1697548685025,120,23.0,1.0,"[257, 1552]","[1697548683473, 1697548685025]"
5787,5787,351,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664219,1697548667371,120,,,"[342, 1477, 429, 85, 65, 84]","[1697548664561, 1697548666038, 1697548666467, 1697548666552, 1697548666617, 1697548666701]"
5788,5788,446,46,[],200,llama-7b,128,1,2513.0,1.0,1,A100,1697548685030,1697548687543,120,26.0,1.0,"[71, 2442]","[1697548685101, 1697548687543]"
5789,5789,443,36,[],200,llama-7b,128,1,939.0,1.0,1,A100,1697548663783,1697548664722,120,19.0,1.0,"[54, 885]","[1697548663837, 1697548664722]"
5790,5790,805,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548687547,1697548691085,120,,,"[28, 2027]","[1697548687575, 1697548689602]"
5791,5791,805,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664726,1697548667373,120,,,"[27, 2592]","[1697548664753, 1697548667345]"
5792,5792,237,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693449,120,,,"[182, 1971]","[1697548691278, 1697548693249]"
5793,5793,853,26,[],200,llama-7b,128,1,7117.0,1.0,1,A100,1697548643004,1697548650121,120,364.0,22.0,"[40, 1732, 434, 86, 64, 955, 94, 90, 90, 68, 968, 97, 94, 94, 72, 93, 71, 892, 91, 89, 85, 731, 87]","[1697548643044, 1697548644776, 1697548645210, 1697548645296, 1697548645360, 1697548646315, 1697548646409, 1697548646499, 1697548646589, 1697548646657, 1697548647625, 1697548647722, 1697548647816, 1697548647910, 1697548647982, 1697548648075, 1697548648146, 1697548649038, 1697548649129, 1697548649218, 1697548649303, 1697548650034, 1697548650121]"
5794,5794,206,38,[],200,llama-7b,128,1,2331.0,1.0,1,A100,1697548667379,1697548669710,120,16.0,1.0,"[373, 1958]","[1697548667752, 1697548669710]"
5795,5795,567,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669714,1697548671228,120,,,"[45, 1444]","[1697548669759, 1697548671203]"
5796,5796,599,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695718,120,,,"[93, 1880]","[1697548693552, 1697548695432]"
5797,5797,839,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677268,1697548679650,120,,,"[21, 2183]","[1697548677289, 1697548679472]"
5798,5798,268,39,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679659,1697548681689,120,19.0,1.0,"[401, 1629]","[1697548680060, 1697548681689]"
5799,5799,102,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690856,1697548693451,120,,,"[26, 970]","[1697548690882, 1697548691852]"
5800,5800,805,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686732,1697548691085,120,,,"[354, 2517]","[1697548687086, 1697548689603]"
5801,5801,463,43,[],200,llama-7b,128,1,1970.0,1.0,1,A100,1697548693464,1697548695434,120,39.0,1.0,"[317, 1652]","[1697548693781, 1697548695433]"
5802,5802,0,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697836,120,,,"[43, 1916]","[1697548695767, 1697548697683]"
5803,5803,734,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655283,1697548657502,120,,,"[259, 1883]","[1697548655542, 1697548657425]"
5804,5804,209,39,[],200,llama-7b,128,1,2150.0,1.0,1,A100,1697548691096,1697548693246,120,20.0,1.0,"[51, 2099]","[1697548691147, 1697548693246]"
5805,5805,626,40,[],200,llama-7b,128,1,1211.0,1.0,1,A100,1697548681698,1697548682909,120,10.0,1.0,"[35, 1176]","[1697548681733, 1697548682909]"
5806,5806,822,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695438,1697548697835,120,,,"[34, 1256]","[1697548695472, 1697548696728]"
5807,5807,661,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693252,1697548695714,120,,,"[12, 880]","[1697548693264, 1697548694144]"
5808,5808,56,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682912,1697548686707,120,,,"[10, 1122, 1219, 91, 90, 89, 70, 86, 87, 68]","[1697548682922, 1697548684044, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685689, 1697548685776, 1697548685844]"
5809,5809,89,47,[],200,llama-7b,128,1,2819.0,1.0,1,A100,1697548695726,1697548698545,120,52.0,20.0,"[327, 2086, 28, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 19, 19, 19, 19, 18, 20]","[1697548696053, 1697548698139, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388, 1697548698410, 1697548698431, 1697548698450, 1697548698469, 1697548698488, 1697548698507, 1697548698525, 1697548698545]"
5810,5810,160,32,[],200,llama-7b,128,1,2034.0,1.0,1,A100,1697548657543,1697548659577,120,13.0,1.0,"[11, 2022]","[1697548657554, 1697548659576]"
5811,5811,514,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664206,120,,,"[47, 1071, 1196, 57, 1027]","[1697548659629, 1697548660700, 1697548661896, 1697548661953, 1697548662980]"
5812,5812,453,37,[],200,llama-7b,128,1,2111.0,1.0,1,A100,1697548678246,1697548680357,120,26.0,1.0,"[39, 2071]","[1697548678285, 1697548680356]"
5813,5813,595,48,[],200,llama-7b,128,1,1131.0,1.0,1,A100,1697548682913,1697548684044,120,8.0,1.0,"[19, 1112]","[1697548682932, 1697548684044]"
5814,5814,803,38,[],200,llama-7b,128,1,2548.0,1.0,1,A100,1697548680361,1697548682909,120,20.0,1.0,"[10, 2537]","[1697548680371, 1697548682908]"
5815,5815,664,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691088,120,,,"[145, 1800]","[1697548689054, 1697548690854]"
5816,5816,676,48,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695725,1697548697684,120,19.0,1.0,"[230, 1729]","[1697548695955, 1697548697684]"
5817,5817,233,39,[],200,llama-7b,128,1,1132.0,1.0,1,A100,1697548682912,1697548684044,120,6.0,1.0,"[15, 1117]","[1697548682927, 1697548684044]"
5818,5818,46,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,[22],[1697548688929]
5819,5819,95,44,[],200,llama-7b,128,1,2151.0,1.0,1,A100,1697548691096,1697548693247,120,12.0,1.0,"[63, 2088]","[1697548691159, 1697548693247]"
5820,5820,385,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686720,1697548688899,120,,,"[23, 1969]","[1697548686743, 1697548688712]"
5821,5821,420,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693253,1697548695716,120,,,"[51, 840]","[1697548693304, 1697548694144]"
5822,5822,562,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548684047,1697548686709,120,,,"[21, 2111]","[1697548684068, 1697548686179]"
5823,5823,405,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691095,1697548693454,120,,,"[57, 2094]","[1697548691152, 1697548693246]"
5824,5824,921,41,[],200,llama-7b,128,1,2871.0,1.0,1,A100,1697548686731,1697548689602,120,31.0,1.0,"[334, 2537]","[1697548687065, 1697548689602]"
5825,5825,744,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[37, 1900]","[1697548688944, 1697548690844]"
5826,5826,357,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548689608,1697548693448,120,,,"[17, 2227]","[1697548689625, 1697548691852]"
5827,5827,853,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693463,1697548695717,120,,,"[283, 1686]","[1697548693746, 1697548695432]"
5828,5828,172,44,[],200,llama-7b,128,1,2151.0,1.0,1,A100,1697548691096,1697548693247,120,19.0,1.0,"[261, 1890]","[1697548691357, 1697548693247]"
5829,5829,529,45,[],200,llama-7b,128,1,891.0,1.0,1,A100,1697548693253,1697548694144,120,10.0,1.0,"[41, 850]","[1697548693294, 1697548694144]"
5830,5830,774,46,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695725,1697548697684,120,8.0,1.0,"[228, 1730]","[1697548695953, 1697548697683]"
5831,5831,286,45,[],200,llama-7b,128,1,2662.0,1.0,1,A100,1697548695727,1697548698389,120,161.0,12.0,"[356, 2084, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22]","[1697548696083, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388]"
5832,5832,714,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667379,1697548671230,120,,,"[358, 1973, 469, 85, 62, 64, 81]","[1697548667737, 1697548669710, 1697548670179, 1697548670264, 1697548670326, 1697548670390, 1697548670471]"
5833,5833,142,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671244,1697548674837,120,,,"[307, 1526, 537, 79, 60]","[1697548671551, 1697548673077, 1697548673614, 1697548673693, 1697548673753]"
5834,5834,889,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548694156,1697548697833,120,,,"[23, 2549]","[1697548694179, 1697548696728]"
5835,5835,672,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677270,1697548679647,120,,,"[168, 2036]","[1697548677438, 1697548679474]"
5836,5836,790,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667370,120,,,"[271, 1547, 430, 85, 65, 84]","[1697548664489, 1697548666036, 1697548666466, 1697548666551, 1697548666616, 1697548666700]"
5837,5837,503,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677256,120,,,"[128, 1825]","[1697548674972, 1697548676797]"
5838,5838,536,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548670181,1697548671229,120,,,"[16, 1007]","[1697548670197, 1697548671204]"
5839,5839,72,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683206,120,,,"[297, 1733, 286, 91, 85, 81, 81]","[1697548679954, 1697548681687, 1697548681973, 1697548682064, 1697548682149, 1697548682230, 1697548682311]"
5840,5840,435,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683213,1697548686708,120,,,"[34, 1777, 239, 91, 90, 89, 70, 87, 86, 68]","[1697548683247, 1697548685024, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
5841,5841,830,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679646,120,,,"[101, 2104]","[1697548677370, 1697548679474]"
5842,5842,893,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674841,120,,,"[155, 1679, 537, 78, 61]","[1697548671398, 1697548673077, 1697548673614, 1697548673692, 1697548673753]"
5843,5843,789,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688901,120,,,"[138, 1846]","[1697548686867, 1697548688713]"
5844,5844,259,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683207,120,,,"[41, 1989, 285, 91, 85, 83, 80]","[1697548679698, 1697548681687, 1697548681972, 1697548682063, 1697548682148, 1697548682231, 1697548682311]"
5845,5845,217,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691089,120,,,"[159, 1776]","[1697548689068, 1697548690844]"
5846,5846,501,36,[],200,llama-7b,128,1,1237.0,1.0,1,A100,1697548673081,1697548674318,120,19.0,1.0,"[60, 1177]","[1697548673141, 1697548674318]"
5847,5847,282,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548650123,1697548655266,120,,,"[11, 3093, 100, 96, 89, 84, 651, 96, 83, 63, 80]","[1697548650134, 1697548653227, 1697548653327, 1697548653423, 1697548653512, 1697548653596, 1697548654247, 1697548654343, 1697548654426, 1697548654489, 1697548654569]"
5848,5848,612,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683212,1697548686708,120,,,"[20, 1792, 239, 91, 90, 89, 70, 87, 86, 68]","[1697548683232, 1697548685024, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
5849,5849,638,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548650209,1697548655267,120,,,"[21, 2997, 101, 96, 88, 84, 651, 96, 84, 62, 81]","[1697548650230, 1697548653227, 1697548653328, 1697548653424, 1697548653512, 1697548653596, 1697548654247, 1697548654343, 1697548654427, 1697548654489, 1697548654570]"
5850,5850,622,38,[],200,llama-7b,128,1,1211.0,1.0,1,A100,1697548681698,1697548682909,120,20.0,1.0,"[34, 1177]","[1697548681732, 1697548682909]"
5851,5851,52,39,[],200,llama-7b,128,1,2692.0,1.0,1,A100,1697548682911,1697548685603,120,58.0,6.0,"[10, 1123, 1219, 91, 90, 89, 70]","[1697548682921, 1697548684044, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603]"
5852,5852,920,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548684047,1697548686709,120,,,"[15, 2117]","[1697548684062, 1697548686179]"
5853,5853,31,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664217,1697548667369,120,,,"[160, 1660, 431, 82, 65, 85]","[1697548664377, 1697548666037, 1697548666468, 1697548666550, 1697548666615, 1697548666700]"
5854,5854,921,40,[],200,llama-7b,128,1,1834.0,1.0,1,A100,1697548671243,1697548673077,120,31.0,1.0,"[146, 1688]","[1697548671389, 1697548673077]"
5855,5855,351,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688900,120,,,"[310, 1675]","[1697548687039, 1697548688714]"
5856,5856,425,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674846,1697548677259,120,,,"[238, 1712]","[1697548675084, 1697548676796]"
5857,5857,784,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677273,1697548679649,120,,,"[275, 1926]","[1697548677548, 1697548679474]"
5858,5858,212,40,[],200,llama-7b,128,1,2029.0,1.0,1,A100,1697548679658,1697548681687,120,31.0,1.0,"[295, 1734]","[1697548679953, 1697548681687]"
5859,5859,549,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681697,1697548683210,120,,,"[25, 1187]","[1697548681722, 1697548682909]"
5860,5860,351,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673081,1697548674843,120,,,"[82, 1156]","[1697548673163, 1697548674319]"
5861,5861,908,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686706,120,,,"[252, 1557, 238, 91, 90, 89, 70, 87, 86, 68]","[1697548683468, 1697548685025, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
5862,5862,389,35,[],200,llama-7b,128,1,2331.0,1.0,1,A100,1697548667378,1697548669709,120,8.0,1.0,"[296, 2035]","[1697548667674, 1697548669709]"
5863,5863,67,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655291,1697548657536,120,,,"[295, 1841]","[1697548655586, 1697548657427]"
5864,5864,752,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669714,1697548671228,120,,,"[80, 1409]","[1697548669794, 1697548671203]"
5865,5865,318,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679646,120,,,"[332, 3061]","[1697548675180, 1697548678241]"
5866,5866,422,28,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548657546,1697548659577,120,26.0,1.0,"[225, 1806]","[1697548657771, 1697548659577]"
5867,5867,754,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659582,1697548664205,120,,,"[29, 2285, 57, 1027]","[1697548659611, 1697548661896, 1697548661953, 1697548662980]"
5868,5868,573,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693452,120,,,[12],[1697548691108]
5869,5869,864,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674321,1697548677256,120,,,"[6, 1146]","[1697548674327, 1697548675473]"
5870,5870,92,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693463,1697548695716,120,,,"[273, 1696]","[1697548693736, 1697548695432]"
5871,5871,293,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677270,1697548679646,120,,,"[85, 2118]","[1697548677355, 1697548679473]"
5872,5872,451,46,[],200,llama-7b,128,1,2412.0,1.0,1,A100,1697548695727,1697548698139,120,286.0,1.0,"[321, 2091]","[1697548696048, 1697548698139]"
5873,5873,184,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674838,120,,,"[347, 1492, 537, 79, 61]","[1697548671585, 1697548673077, 1697548673614, 1697548673693, 1697548673754]"
5874,5874,674,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683206,120,,,"[283, 1747, 286, 90, 86, 81, 81]","[1697548679940, 1697548681687, 1697548681973, 1697548682063, 1697548682149, 1697548682230, 1697548682311]"
5875,5875,811,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548677260,120,,,"[306, 1643]","[1697548675154, 1697548676797]"
5876,5876,74,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686706,120,,,"[227, 1581, 239, 91, 90, 89, 69, 87, 87, 68]","[1697548683443, 1697548685024, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685776, 1697548685844]"
5877,5877,240,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677278,1697548679651,120,,,"[315, 1882]","[1697548677593, 1697548679475]"
5878,5878,647,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683206,120,,,"[26, 2003, 286, 91, 85, 83, 80]","[1697548679683, 1697548681686, 1697548681972, 1697548682063, 1697548682148, 1697548682231, 1697548682311]"
5879,5879,567,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679658,1697548683207,120,,,"[311, 1718, 286, 91, 85, 82, 80]","[1697548679969, 1697548681687, 1697548681973, 1697548682064, 1697548682149, 1697548682231, 1697548682311]"
5880,5880,126,30,[],200,llama-7b,128,1,2145.0,1.0,1,A100,1697548655280,1697548657425,120,19.0,1.0,"[15, 2130]","[1697548655295, 1697548657425]"
5881,5881,458,31,[],200,llama-7b,128,1,858.0,1.0,1,A100,1697548657430,1697548658288,120,11.0,1.0,"[25, 833]","[1697548657455, 1697548658288]"
5882,5882,816,32,[],200,llama-7b,128,1,4688.0,1.0,1,A100,1697548658291,1697548662979,120,182.0,4.0,"[20, 2389, 1196, 57, 1026]","[1697548658311, 1697548660700, 1697548661896, 1697548661953, 1697548662979]"
5883,5883,928,42,[],200,llama-7b,128,1,1810.0,1.0,1,A100,1697548683216,1697548685026,120,20.0,1.0,"[267, 1542]","[1697548683483, 1697548685025]"
5884,5884,356,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548688902,120,,,"[46, 2466]","[1697548685076, 1697548687542]"
5885,5885,716,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691089,120,,,[155],[1697548689064]
5886,5886,512,38,[],200,llama-7b,128,1,1949.0,1.0,1,A100,1697548674848,1697548676797,120,11.0,1.0,"[307, 1642]","[1697548675155, 1697548676797]"
5887,5887,249,33,[],200,llama-7b,128,1,3716.0,1.0,1,A100,1697548662984,1697548666700,120,874.0,5.0,"[7, 1730, 1745, 83, 66, 84]","[1697548662991, 1697548664721, 1697548666466, 1697548666549, 1697548666615, 1697548666699]"
5888,5888,77,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683214,1697548686709,120,,,"[73, 1739, 238, 91, 88, 89, 72, 86, 85, 68]","[1697548683287, 1697548685026, 1697548685264, 1697548685355, 1697548685443, 1697548685532, 1697548685604, 1697548685690, 1697548685775, 1697548685843]"
5889,5889,43,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688902,120,,,"[233, 1750]","[1697548686962, 1697548688712]"
5890,5890,144,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693451,120,,,"[291, 1860]","[1697548691387, 1697548693247]"
5891,5891,505,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671226,120,,,"[94, 2706, 84, 65, 64, 80]","[1697548667472, 1697548670178, 1697548670262, 1697548670327, 1697548670391, 1697548670471]"
5892,5892,404,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688910,1697548691087,120,,,"[255, 1688]","[1697548689165, 1697548690853]"
5893,5893,863,38,[],200,llama-7b,128,1,1833.0,1.0,1,A100,1697548671244,1697548673077,120,10.0,1.0,"[331, 1502]","[1697548671575, 1697548673077]"
5894,5894,293,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673081,1697548674842,120,,,"[55, 1182]","[1697548673136, 1697548674318]"
5895,5895,603,34,[],200,llama-7b,128,1,769.0,1.0,1,A100,1697548666704,1697548667473,120,9.0,1.0,"[16, 753]","[1697548666720, 1697548667473]"
5896,5896,33,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667475,1697548671226,120,,,"[412, 1822, 470, 84, 64, 65, 80]","[1697548667887, 1697548669709, 1697548670179, 1697548670263, 1697548670327, 1697548670392, 1697548670472]"
5897,5897,402,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688900,120,,,"[121, 1863]","[1697548686850, 1697548688713]"
5898,5898,851,43,[],200,llama-7b,128,1,2149.0,1.0,1,A100,1697548691097,1697548693246,120,23.0,1.0,"[254, 1895]","[1697548691351, 1697548693246]"
5899,5899,651,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679646,120,,,"[387, 3006]","[1697548675235, 1697548678241]"
5900,5900,80,41,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679657,1697548681687,120,13.0,1.0,"[32, 1998]","[1697548679689, 1697548681687]"
5901,5901,761,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[33, 1904]","[1697548688940, 1697548690844]"
5902,5902,590,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695714,120,,,"[120, 1854]","[1697548693579, 1697548695433]"
5903,5903,410,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548686704,120,,,"[70, 2275, 1218, 92, 90, 89, 70, 87, 86, 68]","[1697548681768, 1697548684043, 1697548685261, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
5904,5904,282,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693252,1697548695715,120,,,"[16, 876]","[1697548693268, 1697548694144]"
5905,5905,294,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695726,1697548697835,120,,,[332],[1697548696058]
5906,5906,362,36,[],200,llama-7b,128,1,1834.0,1.0,1,A100,1697548671243,1697548673077,120,14.0,1.0,"[150, 1684]","[1697548671393, 1697548673077]"
5907,5907,19,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697836,120,,,"[32, 1926]","[1697548695756, 1697548697682]"
5908,5908,636,45,[],200,llama-7b,128,1,1959.0,1.0,1,A100,1697548695725,1697548697684,120,31.0,1.0,"[245, 1714]","[1697548695970, 1697548697684]"
5909,5909,925,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.63 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 22.90 GiB is allocated by PyTorch, and 7.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548610853,1697548613000,120,,,"[386, 1578]","[1697548611239, 1697548612817]"
5910,5910,541,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667377,1697548671228,120,,,"[201, 2129, 472, 84, 64, 65, 80]","[1697548667578, 1697548669707, 1697548670179, 1697548670263, 1697548670327, 1697548670392, 1697548670472]"
5911,5911,357,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.51 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.37 GiB is free. Process 1412106 has 32.02 GiB memory in use. Of the allocated memory 21.79 GiB is allocated by PyTorch, and 8.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548613008,1697548615234,120,,,"[78, 1946]","[1697548613086, 1697548615032]"
5912,5912,903,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671245,1697548674837,120,,,"[305, 1527, 537, 79, 60]","[1697548671550, 1697548673077, 1697548673614, 1697548673693, 1697548673753]"
5913,5913,808,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 588.06 MiB is free. Process 1412106 has 38.81 GiB memory in use. Of the allocated memory 21.93 GiB is allocated by PyTorch, and 15.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548615244,1697548617368,120,,,"[318, 1703]","[1697548615562, 1697548617265]"
5914,5914,236,19,[],200,llama-7b,128,1,1906.0,1.0,1,A100,1697548617379,1697548619285,120,8.0,1.0,"[173, 1732]","[1697548617552, 1697548619284]"
5915,5915,591,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacty of 39.39 GiB of which 886.06 MiB is free. Process 1412106 has 38.52 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 13.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548619290,1697548620358,120,,,"[35, 997]","[1697548619325, 1697548620322]"
5916,5916,704,51,[],200,llama-7b,128,1,1945.0,1.0,1,A100,1697548688909,1697548690854,120,14.0,1.0,"[139, 1805]","[1697548689048, 1697548690853]"
5917,5917,136,52,[],200,llama-7b,128,1,993.0,1.0,1,A100,1697548690860,1697548691853,120,31.0,1.0,"[37, 956]","[1697548690897, 1697548691853]"
5918,5918,21,21,[],200,llama-7b,128,1,1963.0,1.0,1,A100,1697548620368,1697548622331,120,15.0,1.0,"[407, 1556]","[1697548620775, 1697548622331]"
5919,5919,374,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.77 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.64 GiB is free. Process 1412106 has 37.75 GiB memory in use. Of the allocated memory 24.11 GiB is allocated by PyTorch, and 11.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548622336,1697548624952,120,,,"[33, 1088]","[1697548622369, 1697548623457]"
5920,5920,496,53,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691856,1697548695714,120,,,"[16, 2271]","[1697548691872, 1697548694143]"
5921,5921,523,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686720,1697548688899,120,,,"[23, 1968]","[1697548686743, 1697548688711]"
5922,5922,183,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667370,120,,,"[263, 1555, 430, 85, 65, 84]","[1697548664481, 1697548666036, 1697548666466, 1697548666551, 1697548666616, 1697548666700]"
5923,5923,435,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686719,1697548688900,120,,,"[138, 1856]","[1697548686857, 1697548688713]"
5924,5924,868,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548676800,1697548679648,120,,,"[16, 1426]","[1697548676816, 1697548678242]"
5925,5925,15,54,[],200,llama-7b,128,1,2818.0,1.0,1,A100,1697548695727,1697548698545,120,100.0,20.0,"[340, 2072, 28, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 19, 19, 19, 18, 20, 19]","[1697548696067, 1697548698139, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388, 1697548698410, 1697548698431, 1697548698450, 1697548698469, 1697548698488, 1697548698506, 1697548698526, 1697548698545]"
5926,5926,704,23,[],200,llama-7b,128,1,2048.0,1.0,1,A100,1697548624975,1697548627023,120,14.0,1.0,"[361, 1686]","[1697548625336, 1697548627022]"
5927,5927,794,43,[],200,llama-7b,128,1,1944.0,1.0,1,A100,1697548688909,1697548690853,120,11.0,1.0,"[149, 1795]","[1697548689058, 1697548690853]"
5928,5928,881,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[27, 1910]","[1697548688934, 1697548690844]"
5929,5929,282,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693454,120,,,"[70, 2081]","[1697548691166, 1697548693247]"
5930,5930,226,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690856,1697548693450,120,,,"[19, 977]","[1697548690875, 1697548691852]"
5931,5931,336,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686719,1697548688901,120,,,"[133, 1861]","[1697548686852, 1697548688713]"
5932,5932,568,40,[],200,llama-7b,128,1,892.0,1.0,1,A100,1697548693252,1697548694144,120,11.0,1.0,"[7, 885]","[1697548693259, 1697548694144]"
5933,5933,926,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548694146,1697548697835,120,,,"[8, 2573]","[1697548694154, 1697548696727]"
5934,5934,690,44,[],200,llama-7b,128,1,1944.0,1.0,1,A100,1697548688910,1697548690854,120,39.0,1.0,"[271, 1673]","[1697548689181, 1697548690854]"
5935,5935,614,28,[],200,llama-7b,128,1,2144.0,1.0,1,A100,1697548655283,1697548657427,120,15.0,1.0,"[293, 1851]","[1697548655576, 1697548657427]"
5936,5936,556,45,[],200,llama-7b,128,1,1970.0,1.0,1,A100,1697548693463,1697548695433,120,9.0,1.0,"[288, 1681]","[1697548693751, 1697548695432]"
5937,5937,542,31,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667378,1697548669708,120,11.0,1.0,"[464, 1866]","[1697548667842, 1697548669708]"
5938,5938,116,45,[],200,llama-7b,128,1,992.0,1.0,1,A100,1697548690861,1697548691853,120,23.0,1.0,"[39, 953]","[1697548690900, 1697548691853]"
5939,5939,715,43,[],200,llama-7b,128,1,1973.0,1.0,1,A100,1697548693459,1697548695432,120,20.0,1.0,"[43, 1930]","[1697548693502, 1697548695432]"
5940,5940,900,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671227,120,,,"[21, 1468]","[1697548669734, 1697548671202]"
5941,5941,910,46,[],200,llama-7b,128,1,1289.0,1.0,1,A100,1697548695439,1697548696728,120,8.0,1.0,"[79, 1210]","[1697548695518, 1697548696728]"
5942,5942,331,33,[],200,llama-7b,128,1,3072.0,1.0,1,A100,1697548671246,1697548674318,120,26.0,1.0,"[401, 2671]","[1697548671647, 1697548674318]"
5943,5943,232,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695437,1697548697835,120,,,"[30, 1261]","[1697548695467, 1697548696728]"
5944,5944,658,34,[],200,llama-7b,128,1,1151.0,1.0,1,A100,1697548674322,1697548675473,120,11.0,1.0,"[40, 1111]","[1697548674362, 1697548675473]"
5945,5945,445,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691856,1697548695713,120,,,"[16, 2271]","[1697548691872, 1697548694143]"
5946,5946,87,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548675476,1697548679647,120,,,"[13, 2752]","[1697548675489, 1697548678241]"
5947,5947,489,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.38 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.03 GiB is free. Process 1412106 has 36.36 GiB memory in use. Of the allocated memory 22.70 GiB is allocated by PyTorch, and 11.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548629607,1697548631888,120,,,[176],[1697548629783]
5948,5948,804,47,[],200,llama-7b,128,1,1958.0,1.0,1,A100,1697548695726,1697548697684,120,20.0,1.0,"[317, 1641]","[1697548696043, 1697548697684]"
5949,5949,851,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.36 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.05 GiB is free. Process 1412106 has 35.34 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 9.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548631901,1697548633847,120,,,[134],[1697548632035]
5950,5950,250,24,[],200,llama-7b,128,1,1747.0,1.0,1,A100,1697548633855,1697548635602,120,31.0,1.0,"[281, 1466]","[1697548634136, 1697548635602]"
5951,5951,446,36,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548679658,1697548681689,120,26.0,1.0,"[383, 1648]","[1697548680041, 1697548681689]"
5952,5952,612,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635605,1697548639637,120,,,"[69, 835, 1222, 417]","[1697548635674, 1697548636509, 1697548637731, 1697548638148]"
5953,5953,804,37,[],200,llama-7b,128,1,1212.0,1.0,1,A100,1697548681698,1697548682910,120,20.0,1.0,"[19, 1192]","[1697548681717, 1697548682909]"
5954,5954,229,38,[],200,llama-7b,128,1,1130.0,1.0,1,A100,1697548682914,1697548684044,120,15.0,1.0,"[27, 1103]","[1697548682941, 1697548684044]"
5955,5955,380,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685606,1697548688903,120,,,[8],[1697548685614]
5956,5956,705,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674849,1697548679646,120,,,[335],[1697548675184]
5957,5957,37,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639654,1697548641375,120,,,[303],[1697548639957]
5958,5958,730,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691088,120,,,"[150, 1794]","[1697548689059, 1697548690853]"
5959,5959,108,43,[],200,llama-7b,128,1,2316.0,1.0,1,A100,1697548679656,1697548681972,120,182.0,2.0,"[23, 2007, 286]","[1697548679679, 1697548681686, 1697548681972]"
5960,5960,157,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657545,1697548659864,120,,,"[131, 1903]","[1697548657676, 1697548659579]"
5961,5961,487,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659871,1697548661854,120,,,[90],[1697548659961]
5962,5962,395,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548643001,120,,,[195],[1697548641580]
5963,5963,849,34,[],200,llama-7b,128,1,1911.0,1.0,1,A100,1697548661868,1697548663779,120,10.0,1.0,"[372, 1538]","[1697548662240, 1697548663778]"
5964,5964,160,42,[],200,llama-7b,128,1,2153.0,1.0,1,A100,1697548691095,1697548693248,120,13.0,1.0,"[143, 2010]","[1697548691238, 1697548693248]"
5965,5965,279,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548663783,1697548667371,120,,,"[15, 924, 1744, 83, 66, 84]","[1697548663798, 1697548664722, 1697548666466, 1697548666549, 1697548666615, 1697548666699]"
5966,5966,563,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548684047,1697548686710,120,,,"[24, 2108]","[1697548684071, 1697548686179]"
5967,5967,518,43,[],200,llama-7b,128,1,891.0,1.0,1,A100,1697548693254,1697548694145,120,23.0,1.0,"[83, 807]","[1697548693337, 1697548694144]"
5968,5968,386,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688911,1697548691087,120,,,"[334, 1609]","[1697548689245, 1697548690854]"
5969,5969,467,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681974,1697548686705,120,,,"[11, 2058, 1219, 91, 90, 89, 70, 87, 86, 68]","[1697548681985, 1697548684043, 1697548685262, 1697548685353, 1697548685443, 1697548685532, 1697548685602, 1697548685689, 1697548685775, 1697548685843]"
5970,5970,858,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674840,120,,,"[131, 2240, 79, 60]","[1697548671374, 1697548673614, 1697548673693, 1697548673753]"
5971,5971,333,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677255,120,,,"[108, 1844]","[1697548674952, 1697548676796]"
5972,5972,665,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677268,1697548679650,120,,,"[34, 2171]","[1697548677302, 1697548679473]"
5973,5973,288,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548677260,120,,,"[292, 1657]","[1697548675140, 1697548676797]"
5974,5974,95,36,[],200,llama-7b,128,1,2033.0,1.0,1,A100,1697548679657,1697548681690,120,12.0,1.0,"[216, 1817]","[1697548679873, 1697548681690]"
5975,5975,746,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693451,120,,,"[264, 1886]","[1697548691361, 1697548693247]"
5976,5976,449,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548681698,1697548683211,120,,,"[45, 1166]","[1697548681743, 1697548682909]"
5977,5977,310,37,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667377,1697548669707,120,26.0,1.0,"[166, 2164]","[1697548667543, 1697548669707]"
5978,5978,771,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686724,1697548688900,120,,,"[55, 1934]","[1697548686779, 1697548688713]"
5979,5979,149,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695717,120,,,"[327, 1643]","[1697548693791, 1697548695434]"
5980,5980,649,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677279,1697548679650,120,,,"[302, 1894]","[1697548677581, 1697548679475]"
5981,5981,196,44,[],200,llama-7b,128,1,1937.0,1.0,1,A100,1697548688907,1697548690844,120,13.0,1.0,"[32, 1905]","[1697548688939, 1697548690844]"
5982,5982,80,40,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548679659,1697548681689,120,13.0,1.0,"[391, 1639]","[1697548680050, 1697548681689]"
5983,5983,825,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688900,120,,,"[270, 1715]","[1697548686999, 1697548688714]"
5984,5984,671,38,[],200,llama-7b,128,1,1491.0,1.0,1,A100,1697548669713,1697548671204,120,12.0,1.0,"[51, 1439]","[1697548669764, 1697548671203]"
5985,5985,189,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671206,1697548674838,120,,,"[21, 1847, 539, 79, 60]","[1697548671227, 1697548673074, 1697548673613, 1697548673692, 1697548673752]"
5986,5986,803,38,[],200,llama-7b,128,1,1809.0,1.0,1,A100,1697548683218,1697548685027,120,20.0,1.0,"[348, 1461]","[1697548683566, 1697548685027]"
5987,5987,292,40,[],200,llama-7b,128,1,2031.0,1.0,1,A100,1697548679657,1697548681688,120,286.0,1.0,"[94, 1937]","[1697548679751, 1697548681688]"
5988,5988,254,46,[],200,llama-7b,128,1,1935.0,1.0,1,A100,1697548688909,1697548690844,120,58.0,1.0,"[160, 1775]","[1697548689069, 1697548690844]"
5989,5989,650,41,[],200,llama-7b,128,1,1208.0,1.0,1,A100,1697548681701,1697548682909,120,13.0,1.0,"[77, 1131]","[1697548681778, 1697548682909]"
5990,5990,510,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695725,1697548697834,120,,,[155],[1697548695880]
5991,5991,80,42,[],200,llama-7b,128,1,1130.0,1.0,1,A100,1697548682914,1697548684044,120,13.0,1.0,"[23, 1107]","[1697548682937, 1697548684044]"
5992,5992,643,47,[],200,llama-7b,128,1,1969.0,1.0,1,A100,1697548693463,1697548695432,120,18.0,1.0,"[216, 1752]","[1697548693679, 1697548695431]"
5993,5993,233,39,[],200,llama-7b,128,1,1149.0,1.0,1,A100,1697548685031,1697548686180,120,6.0,1.0,"[70, 1078]","[1697548685101, 1697548686179]"
5994,5994,75,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695436,1697548697834,120,,,"[17, 1275]","[1697548695453, 1697548696728]"
5995,5995,417,43,[],200,llama-7b,128,1,2133.0,1.0,1,A100,1697548684047,1697548686180,120,17.0,1.0,"[29, 2103]","[1697548684076, 1697548686179]"
5996,5996,681,40,[],200,llama-7b,128,1,1359.0,1.0,1,A100,1697548686184,1697548687543,120,23.0,1.0,"[21, 1338]","[1697548686205, 1697548687543]"
5997,5997,521,41,[],200,llama-7b,128,1,2343.0,1.0,1,A100,1697548681700,1697548684043,120,18.0,1.0,"[73, 2270]","[1697548681773, 1697548684043]"
5998,5998,110,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548687546,1697548691085,120,,,"[24, 2032]","[1697548687570, 1697548689602]"
5999,5999,555,45,[],200,llama-7b,128,1,999.0,1.0,1,A100,1697548690854,1697548691853,120,11.0,1.0,"[23, 975]","[1697548690877, 1697548691852]"
6000,6000,193,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693453,120,,,[41],[1697548691137]
6001,6001,909,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691856,1697548695714,120,,,"[21, 2266]","[1697548691877, 1697548694143]"
6002,6002,776,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686182,1697548688898,120,,,"[18, 1343]","[1697548686200, 1697548687543]"
6003,6003,473,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693451,120,,,"[266, 1885]","[1697548691362, 1697548693247]"
6004,6004,204,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688907,1697548691085,120,,,"[23, 1914]","[1697548688930, 1697548690844]"
6005,6005,554,44,[],200,llama-7b,128,1,1970.0,1.0,1,A100,1697548693463,1697548695433,120,26.0,1.0,"[293, 1677]","[1697548693756, 1697548695433]"
6006,6006,877,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548694150,1697548697835,120,,,"[17, 2561]","[1697548694167, 1697548696728]"
6007,6007,827,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695714,120,,,"[112, 1861]","[1697548693572, 1697548695433]"
6008,6008,885,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695439,1697548697836,120,,,"[74, 1215]","[1697548695513, 1697548696728]"
6009,6009,558,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693453,120,,,"[362, 1790]","[1697548691459, 1697548693249]"
6010,6010,257,44,[],200,llama-7b,128,1,1958.0,1.0,1,A100,1697548695725,1697548697683,120,14.0,1.0,"[160, 1798]","[1697548695885, 1697548697683]"
6011,6011,921,40,[],200,llama-7b,128,1,2871.0,1.0,1,A100,1697548686732,1697548689603,120,31.0,1.0,"[364, 2507]","[1697548687096, 1697548689603]"
6012,6012,194,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667378,1697548671228,120,,,"[189, 2140, 472, 84, 64, 64, 81]","[1697548667567, 1697548669707, 1697548670179, 1697548670263, 1697548670327, 1697548670391, 1697548670472]"
6013,6013,314,47,[],200,llama-7b,128,1,2685.0,1.0,1,A100,1697548695726,1697548698411,120,335.0,13.0,"[337, 2076, 28, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23]","[1697548696063, 1697548698139, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388, 1697548698411]"
6014,6014,353,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548689608,1697548693448,120,,,"[12, 2231]","[1697548689620, 1697548691851]"
6015,6015,192,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677256,120,,,"[199, 1753]","[1697548675044, 1697548676797]"
6016,6016,137,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacty of 39.39 GiB of which 1.71 GiB is free. Process 1412106 has 37.68 GiB memory in use. Of the allocated memory 24.47 GiB is allocated by PyTorch, and 11.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548627025,1697548629598,120,,,"[59, 855]","[1697548627084, 1697548627939]"
6017,6017,498,25,[],200,llama-7b,128,1,2010.0,1.0,1,A100,1697548629608,1697548631618,120,9.0,1.0,"[259, 1751]","[1697548629867, 1697548631618]"
6018,6018,41,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657430,1697548659866,120,,,"[45, 813]","[1697548657475, 1697548658288]"
6019,6019,857,26,[],200,llama-7b,128,1,1012.0,1.0,1,A100,1697548631625,1697548632637,120,18.0,1.0,"[23, 989]","[1697548631648, 1697548632637]"
6020,6020,397,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.18 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.13 GiB is free. Process 1412106 has 36.26 GiB memory in use. Of the allocated memory 23.55 GiB is allocated by PyTorch, and 11.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659872,1697548661855,120,,,[295],[1697548660167]
6021,6021,752,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548661866,1697548664211,120,,,"[43, 843, 228]","[1697548661909, 1697548662752, 1697548662980]"
6022,6022,552,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677269,1697548679647,120,,,"[106, 2099]","[1697548677375, 1697548679474]"
6023,6023,708,42,[],200,llama-7b,128,1,1974.0,1.0,1,A100,1697548693458,1697548695432,120,140.0,1.0,"[19, 1954]","[1697548693477, 1697548695431]"
6024,6024,718,37,[],200,llama-7b,128,1,1238.0,1.0,1,A100,1697548673081,1697548674319,120,13.0,1.0,"[66, 1172]","[1697548673147, 1697548674319]"
6025,6025,186,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548664218,1697548667370,120,,,"[253, 1565, 430, 85, 65, 84]","[1697548664471, 1697548666036, 1697548666466, 1697548666551, 1697548666616, 1697548666700]"
6026,6026,224,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695438,1697548697836,120,,,"[70, 1220]","[1697548695508, 1697548696728]"
6027,6027,909,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683207,120,,,"[37, 1993, 285, 91, 85, 83, 80]","[1697548679694, 1697548681687, 1697548681972, 1697548682063, 1697548682148, 1697548682231, 1697548682311]"
6028,6028,613,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690853,1697548693449,120,,,"[14, 986]","[1697548690867, 1697548691853]"
6029,6029,630,33,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667378,1697548669708,120,6.0,1.0,"[199, 2131]","[1697548667577, 1697548669708]"
6030,6030,129,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693460,1697548695714,120,,,"[125, 1848]","[1697548693585, 1697548695433]"
6031,6031,63,34,[],200,llama-7b,128,1,1489.0,1.0,1,A100,1697548669714,1697548671203,120,39.0,1.0,"[89, 1400]","[1697548669803, 1697548671203]"
6032,6032,913,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695718,120,,,"[332, 1638]","[1697548693796, 1697548695434]"
6033,6033,484,49,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697836,120,,,[17],[1697548695741]
6034,6034,313,48,[],200,llama-7b,128,1,1957.0,1.0,1,A100,1697548695727,1697548697684,120,20.0,1.0,"[258, 1699]","[1697548695985, 1697548697684]"
6035,6035,422,35,[],200,llama-7b,128,1,1867.0,1.0,1,A100,1697548671208,1697548673075,120,26.0,1.0,"[49, 1818]","[1697548671257, 1697548673075]"
6036,6036,150,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674322,1697548677256,120,,,[25],[1697548674347]
6037,6037,339,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683216,1697548686706,120,,,"[251, 1558, 238, 91, 90, 89, 70, 87, 86, 68]","[1697548683467, 1697548685025, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
6038,6038,507,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677273,1697548679648,120,,,"[175, 2026]","[1697548677448, 1697548679474]"
6039,6039,776,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548673080,1697548674842,120,,,"[60, 1178]","[1697548673140, 1697548674318]"
6040,6040,841,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683209,120,,,"[178, 2136, 92, 85, 81, 81]","[1697548679835, 1697548681971, 1697548682063, 1697548682148, 1697548682229, 1697548682310]"
6041,6041,543,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674850,1697548679647,120,,,"[412, 2979]","[1697548675262, 1697548678241]"
6042,6042,177,37,[],200,llama-7b,128,1,3391.0,1.0,1,A100,1697548674851,1697548678242,120,14.0,1.0,"[416, 2974]","[1697548675267, 1697548678241]"
6043,6043,531,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548678244,1697548683204,120,,,"[17, 2094, 1616, 92, 85, 81, 80]","[1697548678261, 1697548680355, 1697548681971, 1697548682063, 1697548682148, 1697548682229, 1697548682309]"
6044,6044,266,41,[],200,llama-7b,128,1,1812.0,1.0,1,A100,1697548683214,1697548685026,120,9.0,1.0,"[72, 1740]","[1697548683286, 1697548685026]"
6045,6045,282,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 23.39 GiB is allocated by PyTorch, and 10.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548632639,1697548635903,120,,,"[10, 1782, 58]","[1697548632649, 1697548634431, 1697548634489]"
6046,6046,611,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.40 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.36 GiB is free. Process 1412106 has 36.03 GiB memory in use. Of the allocated memory 24.40 GiB is allocated by PyTorch, and 9.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548635912,1697548637695,120,,,[72],[1697548635984]
6047,6047,900,41,[],200,llama-7b,128,1,2655.0,1.0,1,A100,1697548679657,1697548682312,120,67.0,6.0,"[109, 1922, 283, 92, 85, 83, 81]","[1697548679766, 1697548681688, 1697548681971, 1697548682063, 1697548682148, 1697548682231, 1697548682312]"
6048,6048,38,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 24.07 GiB is allocated by PyTorch, and 10.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548637702,1697548639645,120,,,"[43, 1554]","[1697548637745, 1697548639299]"
6049,6049,875,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548684046,1697548686709,120,,,"[8, 2125]","[1697548684054, 1697548686179]"
6050,6050,398,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.10 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 22.95 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548639648,1697548641376,120,,,[112],[1697548639760]
6051,6051,305,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688900,120,,,"[111, 1873]","[1697548686840, 1697548688713]"
6052,6052,757,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 5.35 GiB. GPU 0 has a total capacty of 39.39 GiB of which 3.20 GiB is free. Process 1412106 has 36.19 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 11.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548641385,1697548642999,120,,,[289],[1697548641674]
6053,6053,666,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691090,120,,,"[165, 1770]","[1697548689074, 1697548690844]"
6054,6054,160,32,[],200,llama-7b,128,1,2724.0,1.0,1,A100,1697548643013,1697548645737,120,13.0,1.0,"[264, 2460]","[1697548643277, 1697548645737]"
6055,6055,514,33,[],200,llama-7b,128,1,7684.0,1.0,1,A100,1697548645739,1697548653423,120,85.0,20.0,"[7, 2691, 601, 92, 88, 86, 730, 87, 85, 75, 74, 74, 1001, 81, 76, 858, 320, 88, 69, 404, 97]","[1697548645746, 1697548648437, 1697548649038, 1697548649130, 1697548649218, 1697548649304, 1697548650034, 1697548650121, 1697548650206, 1697548650281, 1697548650355, 1697548650429, 1697548651430, 1697548651511, 1697548651587, 1697548652445, 1697548652765, 1697548652853, 1697548652922, 1697548653326, 1697548653423]"
6056,6056,326,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682315,1697548686707,120,,,"[16, 1712, 1220, 91, 90, 89, 69, 87, 87, 68]","[1697548682331, 1697548684043, 1697548685263, 1697548685354, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685776, 1697548685844]"
6057,6057,98,45,[],200,llama-7b,128,1,2152.0,1.0,1,A100,1697548691097,1697548693249,120,14.0,1.0,"[360, 1792]","[1697548691457, 1697548693249]"
6058,6058,428,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693254,1697548695716,120,,,"[59, 831]","[1697548693313, 1697548694144]"
6059,6059,690,38,[],200,llama-7b,128,1,1983.0,1.0,1,A100,1697548686729,1697548688712,120,39.0,1.0,"[34, 1949]","[1697548686763, 1697548688712]"
6060,6060,206,39,[],200,llama-7b,128,1,886.0,1.0,1,A100,1697548688716,1697548689602,120,16.0,1.0,"[22, 864]","[1697548688738, 1697548689602]"
6061,6061,786,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,"[131, 1828]","[1697548695855, 1697548697683]"
6062,6062,565,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548689609,1697548693448,120,,,"[26, 2217]","[1697548689635, 1697548691852]"
6063,6063,623,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548685030,1697548688903,120,,,[61],[1697548685091]
6064,6064,874,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548653427,1697548657538,120,,,"[19, 2733]","[1697548653446, 1697548656179]"
6065,6065,926,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693459,1697548695717,120,,,"[35, 1938]","[1697548693494, 1697548695432]"
6066,6066,754,28,[],200,llama-7b,128,1,4617.0,1.0,1,A100,1697548643008,1697548647625,120,88.0,7.0,"[274, 3033, 93, 92, 88, 69, 968]","[1697548643282, 1697548646315, 1697548646408, 1697548646500, 1697548646588, 1697548646657, 1697548647625]"
6067,6067,53,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688910,1697548691087,120,,,"[251, 1692]","[1697548689161, 1697548690853]"
6068,6068,303,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.49 GiB. GPU 0 has a total capacty of 39.39 GiB of which 834.06 MiB is free. Process 1412106 has 38.57 GiB memory in use. Of the allocated memory 22.08 GiB is allocated by PyTorch, and 14.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548657543,1697548659867,120,,,"[10, 2023]","[1697548657553, 1697548659576]"
6069,6069,411,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693449,120,,,[181],[1697548691278]
6070,6070,660,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548659878,1697548664208,120,,,"[359, 2515, 229]","[1697548660237, 1697548662752, 1697548662981]"
6071,6071,858,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695717,120,,,[307],[1697548693771]
6072,6072,178,37,[],200,llama-7b,128,1,1820.0,1.0,1,A100,1697548664217,1697548666037,120,11.0,1.0,"[68, 1751]","[1697548664285, 1697548666036]"
6073,6073,640,36,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667378,1697548669708,120,15.0,1.0,"[266, 2064]","[1697548667644, 1697548669708]"
6074,6074,358,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695725,1697548697834,120,,,"[225, 1733]","[1697548695950, 1697548697683]"
6075,6075,892,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683215,1697548686715,120,,,"[135, 1676, 236, 93, 88, 89, 72, 87, 84, 69]","[1697548683350, 1697548685026, 1697548685262, 1697548685355, 1697548685443, 1697548685532, 1697548685604, 1697548685691, 1697548685775, 1697548685844]"
6076,6076,687,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686723,1697548688900,120,,,"[54, 1935]","[1697548686777, 1697548688712]"
6077,6077,535,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666047,1697548667370,120,,,"[88, 1211]","[1697548666135, 1697548667346]"
6078,6078,892,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548667377,1697548671227,120,,,"[175, 2155, 471, 85, 64, 64, 80]","[1697548667552, 1697548669707, 1697548670178, 1697548670263, 1697548670327, 1697548670391, 1697548670471]"
6079,6079,89,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691089,120,,,"[154, 1781]","[1697548689063, 1697548690844]"
6080,6080,64,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671227,120,,,"[26, 1464]","[1697548669739, 1697548671203]"
6081,6081,322,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671243,1697548674841,120,,,"[205, 1626, 539, 79, 61]","[1697548671448, 1697548673074, 1697548673613, 1697548673692, 1697548673753]"
6082,6082,680,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674848,1697548679646,120,,,"[346, 3048]","[1697548675194, 1697548678242]"
6083,6083,395,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674836,120,,,"[236, 1601, 538, 79, 61]","[1697548671474, 1697548673075, 1697548673613, 1697548673692, 1697548673753]"
6084,6084,450,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691097,1697548693452,120,,,"[355, 1796]","[1697548691452, 1697548693248]"
6085,6085,550,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671238,1697548674839,120,,,"[101, 1737, 538, 79, 60]","[1697548671339, 1697548673076, 1697548673614, 1697548673693, 1697548673753]"
6086,6086,82,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679656,1697548683207,120,,,"[22, 2008, 286, 91, 85, 82, 81]","[1697548679678, 1697548681686, 1697548681972, 1697548682063, 1697548682148, 1697548682230, 1697548682311]"
6087,6087,748,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674844,1697548677255,120,,,[123],[1697548674967]
6088,6088,804,46,[],200,llama-7b,128,1,1969.0,1.0,1,A100,1697548693463,1697548695432,120,20.0,1.0,"[273, 1696]","[1697548693736, 1697548695432]"
6089,6089,286,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695723,1697548697836,120,,,[43],[1697548695766]
6090,6090,439,43,[],200,llama-7b,128,1,2228.0,1.0,1,A100,1697548683216,1697548685444,120,13.0,4.0,"[168, 1640, 238, 92, 90]","[1697548683384, 1697548685024, 1697548685262, 1697548685354, 1697548685444]"
6091,6091,234,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695437,1697548697834,120,,,"[31, 1259]","[1697548695468, 1697548696727]"
6092,6092,797,44,[],200,llama-7b,128,1,2095.0,1.0,1,A100,1697548685448,1697548687543,120,26.0,1.0,"[10, 2085]","[1697548685458, 1697548687543]"
6093,6093,322,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686731,1697548691084,120,,,"[324, 2546]","[1697548687055, 1697548689601]"
6094,6094,154,29,[],200,llama-7b,128,1,2310.0,1.0,1,A100,1697548647629,1697548649939,120,13.0,1.0,"[9, 2300]","[1697548647638, 1697548649938]"
6095,6095,513,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.98 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.85 GiB is allocated by PyTorch, and 14.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548649942,1697548655272,120,,,"[11, 2226, 267, 319, 89, 69, 404, 96, 88, 85, 650, 96, 84, 63, 80]","[1697548649953, 1697548652179, 1697548652446, 1697548652765, 1697548652854, 1697548652923, 1697548653327, 1697548653423, 1697548653511, 1697548653596, 1697548654246, 1697548654342, 1697548654426, 1697548654489, 1697548654569]"
6096,6096,683,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693454,120,,,[75],[1697548691171]
6097,6097,227,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548687546,1697548691085,120,,,"[20, 2035]","[1697548687566, 1697548689601]"
6098,6098,83,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693464,1697548695718,120,,,"[379, 1591]","[1697548693843, 1697548695434]"
6099,6099,181,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677268,1697548679649,120,,,"[24, 2180]","[1697548677292, 1697548679472]"
6100,6100,584,46,[],200,llama-7b,128,1,2152.0,1.0,1,A100,1697548691096,1697548693248,120,10.0,1.0,"[339, 1813]","[1697548691435, 1697548693248]"
6101,6101,439,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695725,1697548697834,120,,,"[218, 1740]","[1697548695943, 1697548697683]"
6102,6102,913,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693253,1697548695715,120,,,"[46, 845]","[1697548693299, 1697548694144]"
6103,6103,540,41,[],200,llama-7b,128,1,2573.0,1.0,1,A100,1697548679657,1697548682230,120,140.0,5.0,"[178, 1854, 282, 92, 85, 82]","[1697548679835, 1697548681689, 1697548681971, 1697548682063, 1697548682148, 1697548682230]"
6104,6104,343,48,[],200,llama-7b,128,1,2819.0,1.0,1,A100,1697548695726,1697548698545,120,84.0,20.0,"[322, 2091, 28, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 21, 19, 19, 19, 19, 18, 20]","[1697548696048, 1697548698139, 1697548698167, 1697548698190, 1697548698212, 1697548698234, 1697548698256, 1697548698278, 1697548698300, 1697548698322, 1697548698344, 1697548698366, 1697548698388, 1697548698410, 1697548698431, 1697548698450, 1697548698469, 1697548698488, 1697548698507, 1697548698525, 1697548698545]"
6105,6105,871,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacty of 39.39 GiB of which 330.06 MiB is free. Process 1412106 has 39.07 GiB memory in use. Of the allocated memory 22.13 GiB is allocated by PyTorch, and 15.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548655281,1697548657538,120,,,"[164, 1982]","[1697548655445, 1697548657427]"
6106,6106,56,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548682233,1697548686706,120,,,"[6, 1805, 1218, 91, 91, 89, 69, 87, 86, 69]","[1697548682239, 1697548684044, 1697548685262, 1697548685353, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
6107,6107,914,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.95 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 24.26 GiB is allocated by PyTorch, and 5.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548674845,1697548677258,120,,,"[219, 1732]","[1697548675064, 1697548676796]"
6108,6108,338,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.71 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.85 GiB is allocated by PyTorch, and 6.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548677273,1697548679648,120,,,"[190, 2012]","[1697548677463, 1697548679475]"
6109,6109,414,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686723,1697548688902,120,,,"[240, 1749]","[1697548686963, 1697548688712]"
6110,6110,696,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548679657,1697548683206,120,,,"[236, 1794, 286, 90, 86, 81, 81]","[1697548679893, 1697548681687, 1697548681973, 1697548682063, 1697548682149, 1697548682230, 1697548682311]"
6111,6111,296,32,[],200,llama-7b,128,1,2030.0,1.0,1,A100,1697548657549,1697548659579,120,6.0,1.0,"[335, 1695]","[1697548657884, 1697548659579]"
6112,6112,658,33,[],200,llama-7b,128,1,1117.0,1.0,1,A100,1697548659584,1697548660701,120,11.0,1.0,"[70, 1047]","[1697548659654, 1697548660701]"
6113,6113,214,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683213,1697548686708,120,,,"[44, 1768, 239, 90, 90, 89, 70, 87, 86, 68]","[1697548683257, 1697548685025, 1697548685264, 1697548685354, 1697548685444, 1697548685533, 1697548685603, 1697548685690, 1697548685776, 1697548685844]"
6114,6114,773,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691088,120,,,"[144, 1800]","[1697548689053, 1697548690853]"
6115,6115,58,34,[],200,llama-7b,128,1,2046.0,1.0,1,A100,1697548660706,1697548662752,120,15.0,1.0,"[22, 2024]","[1697548660728, 1697548662752]"
6116,6116,419,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.78 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.71 GiB is free. Process 1412106 has 31.67 GiB memory in use. Of the allocated memory 23.30 GiB is allocated by PyTorch, and 6.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548662755,1697548664209,120,,,"[18, 1006]","[1697548662773, 1697548663779]"
6117,6117,775,36,[],200,llama-7b,128,1,1817.0,1.0,1,A100,1697548664218,1697548666035,120,17.0,1.0,"[232, 1585]","[1697548664450, 1697548666035]"
6118,6118,201,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548691096,1697548693448,120,,,"[147, 2005]","[1697548691243, 1697548693248]"
6119,6119,203,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.73 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.96 GiB is free. Process 1412106 has 36.43 GiB memory in use. Of the allocated memory 24.03 GiB is allocated by PyTorch, and 10.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548666043,1697548667368,120,,,"[21, 1282]","[1697548666064, 1697548667346]"
6120,6120,557,46,[],200,llama-7b,128,1,1973.0,1.0,1,A100,1697548693459,1697548695432,120,31.0,1.0,"[40, 1933]","[1697548693499, 1697548695432]"
6121,6121,889,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695437,1697548697834,120,,,"[15, 1276]","[1697548695452, 1697548696728]"
6122,6122,557,38,[],200,llama-7b,128,1,2330.0,1.0,1,A100,1697548667377,1697548669707,120,31.0,1.0,"[107, 2222]","[1697548667484, 1697548669706]"
6123,6123,571,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686729,1697548688901,120,,,"[316, 1669]","[1697548687045, 1697548688714]"
6124,6124,75,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.82 GiB. GPU 0 has a total capacty of 39.39 GiB of which 2.79 GiB is free. Process 1412106 has 36.60 GiB memory in use. Of the allocated memory 23.11 GiB is allocated by PyTorch, and 11.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548669713,1697548671227,120,,,"[31, 1459]","[1697548669744, 1697548671203]"
6125,6125,2,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.62 GiB. GPU 0 has a total capacty of 39.39 GiB of which 940.06 MiB is free. Process 1412106 has 38.47 GiB memory in use. Of the allocated memory 23.10 GiB is allocated by PyTorch, and 13.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548688909,1697548691088,120,,,"[330, 1614]","[1697548689239, 1697548690853]"
6126,6126,436,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.73 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.35 GiB is allocated by PyTorch, and 6.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548671245,1697548674838,120,,,"[341, 1492, 536, 79, 61]","[1697548671586, 1697548673078, 1697548673614, 1697548673693, 1697548673754]"
6127,6127,359,43,[],200,llama-7b,128,1,2151.0,1.0,1,A100,1697548691097,1697548693248,120,10.0,1.0,"[161, 1990]","[1697548691258, 1697548693248]"
6128,6128,795,41,[],200,llama-7b,128,1,3393.0,1.0,1,A100,1697548674848,1697548678241,120,12.0,1.0,"[347, 3046]","[1697548675195, 1697548678241]"
6129,6129,716,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.41 GiB. GPU 0 has a total capacty of 39.39 GiB of which 132.06 MiB is free. Process 1412106 has 39.26 GiB memory in use. Of the allocated memory 24.15 GiB is allocated by PyTorch, and 13.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548693252,1697548695715,120,,,[17],[1697548693269]
6130,6130,227,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.93 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.54 GiB is free. Process 1412106 has 31.85 GiB memory in use. Of the allocated memory 23.95 GiB is allocated by PyTorch, and 6.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548678244,1697548683205,120,,,"[20, 2091, 1616, 92, 85, 81, 80]","[1697548678264, 1697548680355, 1697548681971, 1697548682063, 1697548682148, 1697548682229, 1697548682309]"
6131,6131,118,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695724,1697548697833,120,,,"[52, 1907]","[1697548695776, 1697548697683]"
6132,6132,552,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.42 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548683215,1697548686705,120,,,"[165, 1647, 235, 92, 90, 89, 69, 87, 86, 69]","[1697548683380, 1697548685027, 1697548685262, 1697548685354, 1697548685444, 1697548685533, 1697548685602, 1697548685689, 1697548685775, 1697548685844]"
6133,6133,911,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 24.98 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548686721,1697548688900,120,,,"[48, 1943]","[1697548686769, 1697548688712]"
6134,6134,337,45,[],200,llama-7b,128,1,1943.0,1.0,1,A100,1697548688911,1697548690854,120,12.0,1.0,"[345, 1597]","[1697548689256, 1697548690853]"
6135,6135,694,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.58 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 23.76 GiB is allocated by PyTorch, and 6.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548690862,1697548693452,120,,,[43],[1697548690905]
6136,6136,128,47,[],200,llama-7b,128,1,1970.0,1.0,1,A100,1697548693463,1697548695433,120,9.0,1.0,"[308, 1662]","[1697548693771, 1697548695433]"
6137,6137,460,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.72 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.49 GiB is free. Process 1412106 has 31.90 GiB memory in use. Of the allocated memory 25.17 GiB is allocated by PyTorch, and 5.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,128,1,,,1,A100,1697548695436,1697548697834,120,,,"[21, 1271]","[1697548695457, 1697548696728]"
