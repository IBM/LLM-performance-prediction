,Unnamed: 0,smpnum,reqnum,errors,status,model,num_users,requests,latency_ms,records,n_gpus,gpu_type,start_timestamp,end_timestamp,experiment_duration_s,n_input_tokens,n_output_tokens,latency_ms_per_token,timestamps_per_token
0,0,563,0,[],200,llama-7b,64,1,3923.0,1.0,1,A100,1697548434173,1697548438096,120,874.0,18.0,"[83, 1324, 198, 60, 61, 59, 57, 57, 740, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66]","[1697548434256, 1697548435580, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436072, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096]"
1,1,752,0,[],200,llama-7b,64,1,1632.0,1.0,1,A100,1697548434207,1697548435839,120,39.0,3.0,"[150, 1223, 198, 60]","[1697548434357, 1697548435580, 1697548435778, 1697548435838]"
2,2,258,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548434175,1697548441611,120,,,"[168, 1238, 197, 60, 61, 59, 57, 56, 742, 252, 240, 242, 67, 66, 62, 50, 186, 53, 65, 64, 54, 386, 51, 40, 50, 943, 69, 63, 46, 62, 58, 724, 70, 53, 69, 64, 49, 63]","[1697548434343, 1697548435581, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436813, 1697548437065, 1697548437305, 1697548437547, 1697548437614, 1697548437680, 1697548437742, 1697548437792, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438214, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439753, 1697548439816, 1697548439862, 1697548439924, 1697548439982, 1697548440706, 1697548440776, 1697548440829, 1697548440898, 1697548440962, 1697548441011, 1697548441074]"
3,3,591,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548434174,1697548441613,120,,,"[85, 1321, 198, 61, 60, 59, 57, 57, 740, 252, 241, 241, 67, 66, 64, 49, 185, 54, 65, 64, 53, 387, 51, 40, 50, 943, 69, 62, 47, 61, 59, 724, 70, 53, 69, 64, 49, 63]","[1697548434259, 1697548435580, 1697548435778, 1697548435839, 1697548435899, 1697548435958, 1697548436015, 1697548436072, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438031, 1697548438096, 1697548438160, 1697548438213, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439753, 1697548439815, 1697548439862, 1697548439923, 1697548439982, 1697548440706, 1697548440776, 1697548440829, 1697548440898, 1697548440962, 1697548441011, 1697548441074]"
4,4,778,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548434207,1697548441612,120,,,"[160, 1213, 198, 61, 60, 59, 57, 56, 741, 253, 240, 241, 67, 66, 64, 49, 186, 52, 66, 64, 54, 386, 51, 40, 50, 943, 70, 61, 48, 61, 58, 724, 70, 53, 70, 63, 49, 63]","[1697548434367, 1697548435580, 1697548435778, 1697548435839, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437065, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438030, 1697548438096, 1697548438160, 1697548438214, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439754, 1697548439815, 1697548439863, 1697548439924, 1697548439982, 1697548440706, 1697548440776, 1697548440829, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
5,5,13,0,[],200,llama-7b,64,1,3986.0,1.0,1,A100,1697548434174,1697548438160,120,90.0,20.0,"[50, 542, 38, 974, 60, 61, 59, 57, 56, 741, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 64]","[1697548434224, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438160]"
6,6,778,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548434207,1697548441612,120,,,"[171, 1203, 198, 60, 60, 59, 57, 56, 741, 252, 241, 241, 67, 66, 64, 49, 186, 52, 66, 63, 54, 387, 51, 41, 49, 943, 70, 61, 48, 61, 59, 723, 71, 52, 70, 63, 49, 63]","[1697548434378, 1697548435581, 1697548435779, 1697548435839, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438030, 1697548438096, 1697548438159, 1697548438213, 1697548438600, 1697548438651, 1697548438692, 1697548438741, 1697548439684, 1697548439754, 1697548439815, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440829, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
7,7,11,0,[],200,llama-7b,64,1,3855.0,1.0,1,A100,1697548434176,1697548438031,120,732.0,17.0,"[113, 1291, 198, 61, 60, 59, 57, 56, 742, 252, 240, 241, 68, 65, 65, 49, 185, 53]","[1697548434289, 1697548435580, 1697548435778, 1697548435839, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436813, 1697548437065, 1697548437305, 1697548437546, 1697548437614, 1697548437679, 1697548437744, 1697548437793, 1697548437978, 1697548438031]"
8,8,698,0,[],200,llama-7b,64,1,1784.0,1.0,1,A100,1697548434174,1697548435958,120,182.0,6.0,"[23, 569, 38, 974, 60, 60, 60]","[1697548434197, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435898, 1697548435958]"
9,9,718,0,[],200,llama-7b,64,1,592.0,1.0,1,A100,1697548434174,1697548434766,120,13.0,1.0,"[70, 522]","[1697548434244, 1697548434766]"
10,10,553,0,[],200,llama-7b,64,1,3989.0,1.0,1,A100,1697548434171,1697548438160,120,88.0,20.0,"[81, 514, 38, 974, 60, 61, 59, 57, 57, 740, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 64]","[1697548434252, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436072, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438160]"
11,11,339,0,[],200,llama-7b,64,1,3986.0,1.0,1,A100,1697548434173,1697548438159,120,87.0,20.0,"[41, 552, 38, 974, 60, 61, 59, 57, 56, 741, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 63]","[1697548434214, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438159]"
12,12,175,0,[],200,llama-7b,64,1,1899.0,1.0,1,A100,1697548434173,1697548436072,120,140.0,8.0,"[56, 575, 974, 60, 61, 59, 57, 57]","[1697548434229, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436072]"
13,13,262,0,[],200,llama-7b,64,1,1405.0,1.0,1,A100,1697548434176,1697548435581,120,39.0,1.0,"[166, 1239]","[1697548434342, 1697548435581]"
14,14,39,0,[],200,llama-7b,64,1,592.0,1.0,1,A100,1697548434174,1697548434766,120,8.0,1.0,"[25, 567]","[1697548434199, 1697548434766]"
15,15,693,0,[],200,llama-7b,64,1,631.0,1.0,1,A100,1697548434173,1697548434804,120,67.0,2.0,"[66, 527, 38]","[1697548434239, 1697548434766, 1697548434804]"
16,16,744,1,[],200,llama-7b,64,1,1247.0,1.0,1,A100,1697548434769,1697548436016,120,161.0,6.0,"[18, 794, 198, 60, 60, 59, 57]","[1697548434787, 1697548435581, 1697548435779, 1697548435839, 1697548435899, 1697548435958, 1697548436015]"
17,17,507,0,[],200,llama-7b,64,1,3986.0,1.0,1,A100,1697548434173,1697548438159,120,83.0,20.0,"[34, 559, 38, 974, 60, 61, 59, 57, 56, 741, 252, 240, 241, 68, 66, 64, 49, 185, 53, 66, 63]","[1697548434207, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437064, 1697548437304, 1697548437545, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438159]"
18,18,396,2,[],200,llama-7b,64,1,3737.0,1.0,1,A100,1697548436018,1697548439755,120,89.0,20.0,"[13, 387, 395, 252, 240, 242, 67, 66, 64, 49, 185, 53, 65, 64, 54, 387, 50, 41, 49, 943, 71]","[1697548436031, 1697548436418, 1697548436813, 1697548437065, 1697548437305, 1697548437547, 1697548437614, 1697548437680, 1697548437744, 1697548437793, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438214, 1697548438601, 1697548438651, 1697548438692, 1697548438741, 1697548439684, 1697548439755]"
19,19,466,1,[],200,llama-7b,64,1,3446.0,1.0,1,A100,1697548434767,1697548438213,120,457.0,20.0,"[14, 800, 198, 60, 60, 59, 57, 56, 741, 252, 241, 241, 67, 66, 64, 49, 186, 52, 66, 63, 54]","[1697548434781, 1697548435581, 1697548435779, 1697548435839, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438030, 1697548438096, 1697548438159, 1697548438213]"
20,20,837,0,[],200,llama-7b,64,1,4040.0,1.0,1,A100,1697548434173,1697548438213,120,85.0,20.0,"[91, 1316, 198, 61, 60, 60, 56, 57, 740, 252, 241, 241, 67, 66, 64, 49, 186, 53, 65, 64, 53]","[1697548434264, 1697548435580, 1697548435778, 1697548435839, 1697548435899, 1697548435959, 1697548436015, 1697548436072, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438213]"
21,21,462,1,[],200,llama-7b,64,1,1611.0,1.0,1,A100,1697548434807,1697548436418,120,52.0,1.0,"[7, 1604]","[1697548434814, 1697548436418]"
22,22,175,0,[],200,llama-7b,64,1,1900.0,1.0,1,A100,1697548434171,1697548436071,120,140.0,8.0,"[19, 614, 974, 60, 60, 59, 57, 57]","[1697548434190, 1697548434804, 1697548435778, 1697548435838, 1697548435898, 1697548435957, 1697548436014, 1697548436071]"
23,23,118,2,[],200,llama-7b,64,1,4478.0,1.0,1,A100,1697548436421,1697548440899,120,85.0,20.0,"[16, 1470, 71, 53, 65, 64, 54, 387, 50, 40, 50, 943, 71, 60, 48, 61, 59, 723, 71, 52, 70]","[1697548436437, 1697548437907, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438214, 1697548438601, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439755, 1697548439815, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440829, 1697548440899]"
24,24,536,0,[],200,llama-7b,64,1,4039.0,1.0,1,A100,1697548434174,1697548438213,120,83.0,20.0,"[92, 1314, 198, 61, 60, 59, 57, 57, 740, 252, 241, 241, 68, 65, 64, 49, 186, 53, 65, 64, 53]","[1697548434266, 1697548435580, 1697548435778, 1697548435839, 1697548435899, 1697548435958, 1697548436015, 1697548436072, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437614, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438213]"
25,25,208,0,[],200,llama-7b,64,1,3987.0,1.0,1,A100,1697548434173,1697548438160,120,96.0,20.0,"[59, 534, 38, 974, 60, 61, 59, 57, 57, 740, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 64]","[1697548434232, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436072, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438160]"
26,26,158,0,[],200,llama-7b,64,1,4037.0,1.0,1,A100,1697548434177,1697548438214,120,85.0,20.0,"[109, 1294, 198, 61, 60, 59, 57, 56, 742, 252, 240, 241, 68, 65, 65, 49, 185, 53, 65, 64, 54]","[1697548434286, 1697548435580, 1697548435778, 1697548435839, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436813, 1697548437065, 1697548437305, 1697548437546, 1697548437614, 1697548437679, 1697548437744, 1697548437793, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438214]"
27,27,444,0,[],200,llama-7b,64,1,1787.0,1.0,1,A100,1697548434171,1697548435958,120,457.0,6.0,"[14, 67, 552, 974, 60, 60, 59]","[1697548434185, 1697548434252, 1697548434804, 1697548435778, 1697548435838, 1697548435898, 1697548435957]"
28,28,231,0,[],200,llama-7b,64,1,1373.0,1.0,1,A100,1697548434207,1697548435580,120,13.0,1.0,"[156, 1217]","[1697548434363, 1697548435580]"
29,29,233,2,[],200,llama-7b,64,1,1061.0,1.0,1,A100,1697548438217,1697548439278,120,6.0,1.0,"[33, 1027]","[1697548438250, 1697548439277]"
30,30,823,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439281,1697548441612,120,,,"[20, 929, 476, 71, 53, 69, 63, 49, 63]","[1697548439301, 1697548440230, 1697548440706, 1697548440777, 1697548440830, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
31,31,861,0,[],200,llama-7b,64,1,593.0,1.0,1,A100,1697548434173,1697548434766,120,10.0,1.0,"[64, 529]","[1697548434237, 1697548434766]"
32,32,221,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548435961,1697548441612,120,,,"[10, 447, 395, 251, 241, 242, 67, 66, 64, 49, 185, 53, 65, 64, 53, 388, 50, 41, 49, 943, 71, 60, 48, 61, 59, 723, 71, 53, 69, 63, 49, 63]","[1697548435971, 1697548436418, 1697548436813, 1697548437064, 1697548437305, 1697548437547, 1697548437614, 1697548437680, 1697548437744, 1697548437793, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438213, 1697548438601, 1697548438651, 1697548438692, 1697548438741, 1697548439684, 1697548439755, 1697548439815, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440830, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
33,33,593,4,[],200,llama-7b,64,1,1915.0,1.0,1,A100,1697548441617,1697548443532,120,335.0,9.0,"[118, 1023, 57, 59, 48, 45, 38, 418, 53, 56]","[1697548441735, 1697548442758, 1697548442815, 1697548442874, 1697548442922, 1697548442967, 1697548443005, 1697548443423, 1697548443476, 1697548443532]"
34,34,836,0,[],200,llama-7b,64,1,1408.0,1.0,1,A100,1697548434172,1697548435580,120,11.0,1.0,"[104, 1304]","[1697548434276, 1697548435580]"
35,35,753,0,[],200,llama-7b,64,1,3986.0,1.0,1,A100,1697548434174,1697548438160,120,83.0,20.0,"[45, 547, 38, 974, 60, 61, 59, 56, 57, 741, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 63]","[1697548434219, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436014, 1697548436071, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438159]"
36,36,609,1,[],200,llama-7b,64,1,3444.0,1.0,1,A100,1697548434769,1697548438213,120,88.0,20.0,"[6, 806, 197, 61, 60, 59, 57, 56, 741, 252, 241, 241, 67, 66, 64, 49, 186, 52, 66, 63, 54]","[1697548434775, 1697548435581, 1697548435778, 1697548435839, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438030, 1697548438096, 1697548438159, 1697548438213]"
37,37,202,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548434173,1697548441611,120,,,"[21, 572, 38, 974, 60, 60, 59, 57, 57, 741, 252, 240, 241, 68, 66, 64, 49, 185, 53, 65, 64, 53, 388, 51, 40, 50, 942, 70, 62, 47, 61, 59, 724, 70, 53, 69, 64, 49, 63]","[1697548434194, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435898, 1697548435957, 1697548436014, 1697548436071, 1697548436812, 1697548437064, 1697548437304, 1697548437545, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438095, 1697548438159, 1697548438212, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439683, 1697548439753, 1697548439815, 1697548439862, 1697548439923, 1697548439982, 1697548440706, 1697548440776, 1697548440829, 1697548440898, 1697548440962, 1697548441011, 1697548441074]"
38,38,673,0,[],200,llama-7b,64,1,3986.0,1.0,1,A100,1697548434174,1697548438160,120,93.0,20.0,"[53, 539, 38, 974, 60, 61, 59, 57, 56, 741, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 64]","[1697548434227, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438160]"
39,39,590,0,[],200,llama-7b,64,1,3986.0,1.0,1,A100,1697548434173,1697548438159,120,88.0,20.0,"[19, 574, 38, 974, 60, 60, 59, 57, 57, 741, 252, 240, 241, 68, 66, 63, 50, 185, 53, 66, 63]","[1697548434192, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435898, 1697548435957, 1697548436014, 1697548436071, 1697548436812, 1697548437064, 1697548437304, 1697548437545, 1697548437613, 1697548437679, 1697548437742, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438159]"
40,40,424,0,[],200,llama-7b,64,1,4038.0,1.0,1,A100,1697548434176,1697548438214,120,88.0,20.0,"[162, 1243, 197, 60, 61, 59, 57, 56, 742, 252, 240, 242, 67, 65, 65, 48, 186, 53, 65, 64, 54]","[1697548434338, 1697548435581, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436813, 1697548437065, 1697548437305, 1697548437547, 1697548437614, 1697548437679, 1697548437744, 1697548437792, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438214]"
41,41,427,0,[],200,llama-7b,64,1,1726.0,1.0,1,A100,1697548434173,1697548435899,120,58.0,5.0,"[31, 562, 37, 975, 60, 61]","[1697548434204, 1697548434766, 1697548434803, 1697548435778, 1697548435838, 1697548435899]"
42,42,81,1,[],200,llama-7b,64,1,2258.0,1.0,1,A100,1697548435902,1697548438160,120,732.0,13.0,"[12, 504, 395, 251, 241, 242, 67, 66, 64, 49, 185, 53, 65, 64]","[1697548435914, 1697548436418, 1697548436813, 1697548437064, 1697548437305, 1697548437547, 1697548437614, 1697548437680, 1697548437744, 1697548437793, 1697548437978, 1697548438031, 1697548438096, 1697548438160]"
43,43,613,0,[],200,llama-7b,64,1,4037.0,1.0,1,A100,1697548434176,1697548438213,120,90.0,20.0,"[95, 1309, 198, 61, 61, 58, 57, 57, 741, 251, 241, 241, 68, 65, 64, 49, 186, 53, 65, 64, 53]","[1697548434271, 1697548435580, 1697548435778, 1697548435839, 1697548435900, 1697548435958, 1697548436015, 1697548436072, 1697548436813, 1697548437064, 1697548437305, 1697548437546, 1697548437614, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438213]"
44,44,533,0,[],200,llama-7b,64,1,1607.0,1.0,1,A100,1697548434171,1697548435778,120,216.0,2.0,"[122, 1485]","[1697548434293, 1697548435778]"
45,45,371,0,[],200,llama-7b,64,1,1374.0,1.0,1,A100,1697548434207,1697548435581,120,13.0,1.0,"[176, 1198]","[1697548434383, 1697548435581]"
46,46,165,1,[],200,llama-7b,64,1,3974.0,1.0,1,A100,1697548435781,1697548439755,120,83.0,20.0,"[16, 621, 395, 251, 241, 242, 67, 66, 63, 50, 185, 52, 66, 64, 53, 388, 50, 41, 49, 943, 71]","[1697548435797, 1697548436418, 1697548436813, 1697548437064, 1697548437305, 1697548437547, 1697548437614, 1697548437680, 1697548437743, 1697548437793, 1697548437978, 1697548438030, 1697548438096, 1697548438160, 1697548438213, 1697548438601, 1697548438651, 1697548438692, 1697548438741, 1697548439684, 1697548439755]"
47,47,119,0,[],200,llama-7b,64,1,1404.0,1.0,1,A100,1697548434177,1697548435581,120,31.0,1.0,"[114, 1290]","[1697548434291, 1697548435581]"
48,48,781,2,[],200,llama-7b,64,1,1762.0,1.0,1,A100,1697548438162,1697548439924,120,335.0,10.0,"[18, 364, 56, 52, 40, 49, 944, 68, 63, 46, 62]","[1697548438180, 1697548438544, 1697548438600, 1697548438652, 1697548438692, 1697548438741, 1697548439685, 1697548439753, 1697548439816, 1697548439862, 1697548439924]"
49,49,413,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438163,1697548441611,120,,,"[28, 353, 57, 51, 40, 49, 944, 69, 62, 46, 62, 58, 725, 70, 53, 70, 62, 50, 62]","[1697548438191, 1697548438544, 1697548438601, 1697548438652, 1697548438692, 1697548438741, 1697548439685, 1697548439754, 1697548439816, 1697548439862, 1697548439924, 1697548439982, 1697548440707, 1697548440777, 1697548440830, 1697548440900, 1697548440962, 1697548441012, 1697548441074]"
50,50,821,1,[],200,llama-7b,64,1,4171.0,1.0,1,A100,1697548435584,1697548439755,120,85.0,20.0,"[29, 805, 395, 251, 241, 241, 68, 66, 63, 50, 184, 53, 66, 64, 53, 388, 50, 41, 49, 943, 71]","[1697548435613, 1697548436418, 1697548436813, 1697548437064, 1697548437305, 1697548437546, 1697548437614, 1697548437680, 1697548437743, 1697548437793, 1697548437977, 1697548438030, 1697548438096, 1697548438160, 1697548438213, 1697548438601, 1697548438651, 1697548438692, 1697548438741, 1697548439684, 1697548439755]"
51,51,435,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439926,1697548441611,120,,,"[14, 290, 477, 70, 54, 69, 63, 49, 63]","[1697548439940, 1697548440230, 1697548440707, 1697548440777, 1697548440831, 1697548440900, 1697548440963, 1697548441012, 1697548441075]"
52,52,533,0,[],200,llama-7b,64,1,634.0,1.0,1,A100,1697548434171,1697548434805,120,216.0,2.0,"[78, 555]","[1697548434249, 1697548434804]"
53,53,69,0,[],200,llama-7b,64,1,4043.0,1.0,1,A100,1697548434171,1697548438214,120,85.0,20.0,"[112, 1297, 198, 61, 60, 59, 57, 56, 742, 252, 240, 241, 68, 65, 64, 50, 185, 53, 65, 64, 54]","[1697548434283, 1697548435580, 1697548435778, 1697548435839, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436813, 1697548437065, 1697548437305, 1697548437546, 1697548437614, 1697548437679, 1697548437743, 1697548437793, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438214]"
54,54,280,1,[],200,llama-7b,64,1,4947.0,1.0,1,A100,1697548434807,1697548439754,120,91.0,20.0,"[7, 1604, 394, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 63, 54, 387, 51, 41, 49, 943, 70]","[1697548434814, 1697548436418, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438159, 1697548438213, 1697548438600, 1697548438651, 1697548438692, 1697548438741, 1697548439684, 1697548439754]"
55,55,211,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548441623,1697548488024,120,,,"[224, 911, 57, 59, 48, 46, 38, 417, 54, 54, 51, 41, 48, 679, 58, 46, 57, 55, 54, 43, 50, 825, 310, 55, 41, 40, 368, 61, 61, 58, 46, 55, 359, 45, 44, 44, 57, 801, 63, 61, 59, 57, 56, 770, 66, 58, 46, 45, 56, 45, 306, 60, 45, 54, 53, 556, 56, 53, 52, 405, 64, 61, 55, 41, 298, 49, 48, 53, 48, 58, 245, 58, 45, 58, 54, 1016, 68, 52, 62, 875, 66, 71, 53, 63, 59, 1117, 73, 76, 72, 69, 69, 52, 68, 247, 69, 52, 70, 66, 60, 359, 62, 60, 48, 58, 54, 331, 51, 61, 62, 49, 48, 62, 525, 73, 61, 49, 48, 50, 205, 46, 58, 49, 57, 56, 619, 64, 50, 62, 61, 60, 427, 69, 52, 67, 63, 63, 49, 236, 50, 62, 62, 61, 61, 525, 176, 50, 57, 57, 53, 382, 56, 58, 53, 821, 62, 61, 57, 55, 730, 459, 65, 49, 60, 45, 702, 72, 69, 53, 65, 63, 48, 689, 74, 71, 68, 67, 51, 63, 359, 73, 54, 70, 70, 65, 535, 65, 51, 67, 64, 62, 60, 569, 69, 52, 67, 51, 52, 65, 64, 845, 73, 71, 70, 67, 49, 66, 311, 74, 73, 73, 67, 64, 48, 305, 70, 68, 69, 52, 65, 49, 530, 67, 50, 50, 64, 59, 346, 68, 67, 51, 66, 65, 62, 594, 72, 71, 70, 69, 67, 64, 861, 512, 288, 74, 57, 63, 618, 251, 54, 253, 71, 68, 62, 613, 255, 71, 55, 71, 67, 63, 912, 76, 74, 72, 72, 67, 67, 588, 269, 63, 74, 264, 69, 51, 68, 839, 75, 55, 73, 66, 65, 50, 338, 76, 75, 75, 70, 57, 70, 838, 313, 309, 76, 75, 72, 70, 323, 60, 59, 79, 306, 72, 69]","[1697548441847, 1697548442758, 1697548442815, 1697548442874, 1697548442922, 1697548442968, 1697548443006, 1697548443423, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443671, 1697548444350, 1697548444408, 1697548444454, 1697548444511, 1697548444566, 1697548444620, 1697548444663, 1697548444713, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446352, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446633, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448107, 1697548448166, 1697548448223, 1697548448279, 1697548449049, 1697548449115, 1697548449173, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449671, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450439, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451780, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453256, 1697548453324, 1697548453376, 1697548453438, 1697548454313, 1697548454379, 1697548454450, 1697548454503, 1697548454566, 1697548454625, 1697548455742, 1697548455815, 1697548455891, 1697548455963, 1697548456032, 1697548456101, 1697548456153, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456659, 1697548456725, 1697548456785, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457372, 1697548457426, 1697548457757, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458028, 1697548458090, 1697548458615, 1697548458688, 1697548458749, 1697548458798, 1697548458846, 1697548458896, 1697548459101, 1697548459147, 1697548459205, 1697548459254, 1697548459311, 1697548459367, 1697548459986, 1697548460050, 1697548460100, 1697548460162, 1697548460223, 1697548460283, 1697548460710, 1697548460779, 1697548460831, 1697548460898, 1697548460961, 1697548461024, 1697548461073, 1697548461309, 1697548461359, 1697548461421, 1697548461483, 1697548461544, 1697548461605, 1697548462130, 1697548462306, 1697548462356, 1697548462413, 1697548462470, 1697548462523, 1697548462905, 1697548462961, 1697548463019, 1697548463072, 1697548463893, 1697548463955, 1697548464016, 1697548464073, 1697548464128, 1697548464858, 1697548465317, 1697548465382, 1697548465431, 1697548465491, 1697548465536, 1697548466238, 1697548466310, 1697548466379, 1697548466432, 1697548466497, 1697548466560, 1697548466608, 1697548467297, 1697548467371, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468247, 1697548468317, 1697548468382, 1697548468917, 1697548468982, 1697548469033, 1697548469100, 1697548469164, 1697548469226, 1697548469286, 1697548469855, 1697548469924, 1697548469976, 1697548470043, 1697548470094, 1697548470146, 1697548470211, 1697548470275, 1697548471120, 1697548471193, 1697548471264, 1697548471334, 1697548471401, 1697548471450, 1697548471516, 1697548471827, 1697548471901, 1697548471974, 1697548472047, 1697548472114, 1697548472178, 1697548472226, 1697548472531, 1697548472601, 1697548472669, 1697548472738, 1697548472790, 1697548472855, 1697548472904, 1697548473434, 1697548473501, 1697548473551, 1697548473601, 1697548473665, 1697548473724, 1697548474070, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474449, 1697548475043, 1697548475115, 1697548475186, 1697548475256, 1697548475325, 1697548475392, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477191, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478234, 1697548478487, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479556, 1697548479627, 1697548479682, 1697548479753, 1697548479820, 1697548479883, 1697548480795, 1697548480871, 1697548480945, 1697548481017, 1697548481089, 1697548481156, 1697548481223, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711, 1697548483777, 1697548483842, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484526, 1697548484583, 1697548484653, 1697548485491, 1697548485804, 1697548486113, 1697548486189, 1697548486264, 1697548486336, 1697548486406, 1697548486729, 1697548486789, 1697548486848, 1697548486927, 1697548487233, 1697548487305, 1697548487374]"
56,56,921,0,[],200,llama-7b,64,1,1405.0,1.0,1,A100,1697548434175,1697548435580,120,31.0,1.0,"[94, 1311]","[1697548434269, 1697548435580]"
57,57,858,0,[],200,llama-7b,64,1,3406.0,1.0,1,A100,1697548434207,1697548437613,120,182.0,12.0,"[151, 1420, 60, 61, 59, 57, 56, 741, 253, 240, 241, 67]","[1697548434358, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437065, 1697548437305, 1697548437546, 1697548437613]"
58,58,476,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439759,1697548441611,120,,,"[19, 452, 477, 70, 53, 69, 64, 48, 63]","[1697548439778, 1697548440230, 1697548440707, 1697548440777, 1697548440830, 1697548440899, 1697548440963, 1697548441011, 1697548441074]"
59,59,202,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548434207,1697548441612,120,,,"[145, 1229, 197, 60, 61, 59, 57, 56, 741, 253, 239, 242, 68, 65, 64, 49, 186, 52, 66, 64, 54, 386, 51, 40, 50, 943, 69, 62, 47, 62, 58, 724, 70, 53, 69, 64, 49, 63]","[1697548434352, 1697548435581, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437065, 1697548437304, 1697548437546, 1697548437614, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438030, 1697548438096, 1697548438160, 1697548438214, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439753, 1697548439815, 1697548439862, 1697548439924, 1697548439982, 1697548440706, 1697548440776, 1697548440829, 1697548440898, 1697548440962, 1697548441011, 1697548441074]"
60,60,841,1,[],200,llama-7b,64,1,3018.0,1.0,1,A100,1697548435583,1697548438601,120,123.0,15.0,"[25, 1204, 252, 241, 241, 67, 67, 63, 50, 184, 53, 66, 64, 53, 387]","[1697548435608, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437680, 1697548437743, 1697548437793, 1697548437977, 1697548438030, 1697548438096, 1697548438160, 1697548438213, 1697548438600]"
61,61,609,0,[],200,llama-7b,64,1,4042.0,1.0,1,A100,1697548434171,1697548438213,120,88.0,20.0,"[83, 1326, 198, 60, 61, 59, 57, 57, 740, 252, 241, 241, 67, 66, 64, 49, 185, 54, 65, 64, 53]","[1697548434254, 1697548435580, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436072, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438031, 1697548438096, 1697548438160, 1697548438213]"
62,62,13,0,[],200,llama-7b,64,1,3986.0,1.0,1,A100,1697548434174,1697548438160,120,90.0,20.0,"[43, 549, 38, 974, 60, 61, 59, 56, 57, 741, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 63]","[1697548434217, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436014, 1697548436071, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438159]"
63,63,879,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548434176,1697548441611,120,,,"[172, 1233, 197, 60, 61, 59, 57, 56, 741, 253, 239, 241, 69, 65, 64, 49, 186, 53, 65, 64, 54, 386, 51, 40, 50, 943, 69, 63, 46, 62, 58, 724, 70, 53, 70, 63, 49, 63]","[1697548434348, 1697548435581, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437065, 1697548437304, 1697548437545, 1697548437614, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438214, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439753, 1697548439816, 1697548439862, 1697548439924, 1697548439982, 1697548440706, 1697548440776, 1697548440829, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
64,64,490,1,[],200,llama-7b,64,1,1964.0,1.0,1,A100,1697548435583,1697548437547,120,11.0,5.0,"[25, 810, 394, 252, 241, 241]","[1697548435608, 1697548436418, 1697548436812, 1697548437064, 1697548437305, 1697548437546]"
65,65,823,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548440903,1697548441612,120,,,"[16, 484]","[1697548440919, 1697548441403]"
66,66,427,0,[],200,llama-7b,64,1,1728.0,1.0,1,A100,1697548434171,1697548435899,120,58.0,5.0,"[71, 524, 38, 974, 60, 61]","[1697548434242, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899]"
67,67,477,4,[],200,llama-7b,64,1,8054.0,1.0,1,A100,1697548441617,1697548449671,120,244.0,50.0,"[150, 993, 54, 60, 48, 46, 37, 418, 54, 55, 51, 41, 46, 680, 57, 47, 57, 55, 54, 47, 46, 825, 310, 54, 41, 41, 368, 61, 61, 58, 46, 55, 359, 45, 44, 44, 57, 801, 62, 62, 59, 57, 56, 770, 66, 58, 45, 46, 56, 45, 306]","[1697548441767, 1697548442760, 1697548442814, 1697548442874, 1697548442922, 1697548442968, 1697548443005, 1697548443423, 1697548443477, 1697548443532, 1697548443583, 1697548443624, 1697548443670, 1697548444350, 1697548444407, 1697548444454, 1697548444511, 1697548444566, 1697548444620, 1697548444667, 1697548444713, 1697548445538, 1697548445848, 1697548445902, 1697548445943, 1697548445984, 1697548446352, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446633, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448045, 1697548448107, 1697548448166, 1697548448223, 1697548448279, 1697548449049, 1697548449115, 1697548449173, 1697548449218, 1697548449264, 1697548449320, 1697548449365, 1697548449671]"
68,68,199,1,[],200,llama-7b,64,1,515.0,1.0,1,A100,1697548435904,1697548436419,120,13.0,1.0,"[13, 501]","[1697548435917, 1697548436418]"
69,69,711,1,[],200,llama-7b,64,1,530.0,1.0,1,A100,1697548438162,1697548438692,120,457.0,4.0,"[24, 358, 56, 52, 40]","[1697548438186, 1697548438544, 1697548438600, 1697548438652, 1697548438692]"
70,70,626,1,[],200,llama-7b,64,1,291.0,1.0,1,A100,1697548437616,1697548437907,120,10.0,1.0,"[6, 285]","[1697548437622, 1697548437907]"
71,71,286,2,[],200,llama-7b,64,1,2795.0,1.0,1,A100,1697548437912,1697548440707,120,161.0,12.0,"[27, 661, 52, 40, 49, 944, 70, 61, 47, 61, 59, 724]","[1697548437939, 1697548438600, 1697548438652, 1697548438692, 1697548438741, 1697548439685, 1697548439755, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440707]"
72,72,807,2,[],200,llama-7b,64,1,3097.0,1.0,1,A100,1697548441616,1697548444713,120,90.0,20.0,"[64, 1078, 57, 59, 48, 46, 37, 417, 55, 54, 51, 41, 48, 678, 58, 46, 57, 56, 53, 44, 49]","[1697548441680, 1697548442758, 1697548442815, 1697548442874, 1697548442922, 1697548442968, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443671, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444619, 1697548444663, 1697548444712]"
73,73,372,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438695,1697548441612,120,,,"[13, 570, 407, 70, 61, 47, 61, 59, 723, 71, 53, 69, 64, 48, 63]","[1697548438708, 1697548439278, 1697548439685, 1697548439755, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440830, 1697548440899, 1697548440963, 1697548441011, 1697548441074]"
74,74,69,0,[],200,llama-7b,64,1,4037.0,1.0,1,A100,1697548434177,1697548438214,120,85.0,20.0,"[119, 1285, 197, 60, 61, 59, 57, 56, 742, 252, 240, 242, 67, 65, 65, 48, 186, 53, 65, 64, 54]","[1697548434296, 1697548435581, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436813, 1697548437065, 1697548437305, 1697548437547, 1697548437614, 1697548437679, 1697548437744, 1697548437792, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438214]"
75,75,343,0,[],200,llama-7b,64,1,4040.0,1.0,1,A100,1697548434174,1697548438214,120,84.0,20.0,"[104, 1302, 198, 61, 61, 59, 56, 56, 742, 252, 240, 241, 68, 65, 64, 49, 186, 53, 65, 64, 53]","[1697548434278, 1697548435580, 1697548435778, 1697548435839, 1697548435900, 1697548435959, 1697548436015, 1697548436071, 1697548436813, 1697548437065, 1697548437305, 1697548437546, 1697548437614, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438213]"
76,76,659,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438223,1697548441612,120,,,"[47, 1008, 406, 70, 62, 47, 61, 59, 723, 70, 53, 70, 64, 48, 63]","[1697548438270, 1697548439278, 1697548439684, 1697548439754, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440776, 1697548440829, 1697548440899, 1697548440963, 1697548441011, 1697548441074]"
77,77,641,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548434207,1697548441612,120,,,"[166, 1207, 199, 60, 60, 59, 57, 56, 741, 253, 240, 241, 67, 66, 64, 49, 186, 52, 66, 63, 54, 387, 51, 40, 50, 943, 70, 61, 48, 61, 59, 723, 70, 53, 70, 63, 49, 63]","[1697548434373, 1697548435580, 1697548435779, 1697548435839, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437065, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438030, 1697548438096, 1697548438159, 1697548438213, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439754, 1697548439815, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440776, 1697548440829, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
78,78,143,3,[],200,llama-7b,64,1,2008.0,1.0,1,A100,1697548441615,1697548443623,120,6.0,12.0,"[40, 420, 33, 706, 60, 48, 45, 38, 417, 55, 54, 51, 41]","[1697548441655, 1697548442075, 1697548442108, 1697548442814, 1697548442874, 1697548442922, 1697548442967, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623]"
79,79,248,1,[],200,llama-7b,64,1,2850.0,1.0,1,A100,1697548438162,1697548441012,120,182.0,17.0,"[8, 374, 56, 52, 40, 49, 944, 68, 63, 47, 61, 59, 724, 70, 53, 70, 62, 50]","[1697548438170, 1697548438544, 1697548438600, 1697548438652, 1697548438692, 1697548438741, 1697548439685, 1697548439753, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440707, 1697548440777, 1697548440830, 1697548440900, 1697548440962, 1697548441012]"
80,80,928,1,[],200,llama-7b,64,1,1060.0,1.0,1,A100,1697548438218,1697548439278,120,20.0,1.0,"[48, 1012]","[1697548438266, 1697548439278]"
81,81,861,2,[],200,llama-7b,64,1,473.0,1.0,1,A100,1697548439758,1697548440231,120,10.0,1.0,"[15, 458]","[1697548439773, 1697548440231]"
82,82,697,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439279,1697548441612,120,,,"[17, 934, 476, 71, 53, 69, 63, 49, 63]","[1697548439296, 1697548440230, 1697548440706, 1697548440777, 1697548440830, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
83,83,595,1,[],200,llama-7b,64,1,381.0,1.0,1,A100,1697548438163,1697548438544,120,8.0,1.0,"[28, 353]","[1697548438191, 1697548438544]"
84,84,372,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438547,1697548441612,120,,,"[19, 711, 408, 70, 61, 47, 61, 59, 723, 71, 53, 69, 64, 48, 63]","[1697548438566, 1697548439277, 1697548439685, 1697548439755, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440830, 1697548440899, 1697548440963, 1697548441011, 1697548441074]"
85,85,284,0,[],200,llama-7b,64,1,6530.0,1.0,1,A100,1697548434176,1697548440706,120,90.0,31.0,"[105, 1497, 61, 59, 59, 58, 56, 742, 252, 240, 241, 68, 65, 64, 50, 185, 53, 65, 64, 53, 387, 51, 40, 50, 943, 69, 62, 47, 62, 58, 724]","[1697548434281, 1697548435778, 1697548435839, 1697548435898, 1697548435957, 1697548436015, 1697548436071, 1697548436813, 1697548437065, 1697548437305, 1697548437546, 1697548437614, 1697548437679, 1697548437743, 1697548437793, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438213, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439753, 1697548439815, 1697548439862, 1697548439924, 1697548439982, 1697548440706]"
86,86,329,3,[],200,llama-7b,64,1,461.0,1.0,1,A100,1697548441614,1697548442075,120,15.0,1.0,"[21, 440]","[1697548441635, 1697548442075]"
87,87,24,3,[],200,llama-7b,64,1,1863.0,1.0,1,A100,1697548441614,1697548443477,120,79.0,9.0,"[26, 468, 706, 60, 47, 46, 38, 417, 55]","[1697548441640, 1697548442108, 1697548442814, 1697548442874, 1697548442921, 1697548442967, 1697548443005, 1697548443422, 1697548443477]"
88,88,99,4,[],200,llama-7b,64,1,680.0,1.0,1,A100,1697548442080,1697548442760,120,10.0,1.0,"[14, 665]","[1697548442094, 1697548442759]"
89,89,600,1,[],200,llama-7b,64,1,509.0,1.0,1,A100,1697548438035,1697548438544,120,23.0,1.0,"[6, 503]","[1697548438041, 1697548438544]"
90,90,690,5,[],200,llama-7b,64,1,585.0,1.0,1,A100,1697548442768,1697548443353,120,39.0,1.0,"[42, 543]","[1697548442810, 1697548443353]"
91,91,228,0,[],200,llama-7b,64,1,4007.0,1.0,1,A100,1697548434207,1697548438214,120,100.0,20.0,"[155, 1218, 198, 60, 61, 59, 57, 56, 741, 253, 239, 242, 67, 66, 64, 49, 186, 52, 66, 64, 54]","[1697548434362, 1697548435580, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437065, 1697548437304, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438030, 1697548438096, 1697548438160, 1697548438214]"
92,92,704,4,[],200,llama-7b,64,1,662.0,1.0,1,A100,1697548443479,1697548444141,120,14.0,1.0,"[11, 651]","[1697548443490, 1697548444141]"
93,93,474,5,[],200,llama-7b,64,1,5632.0,1.0,1,A100,1697548444144,1697548449776,120,109.0,33.0,"[11, 1384, 309, 55, 41, 40, 368, 62, 61, 58, 46, 56, 358, 44, 44, 44, 57, 801, 63, 62, 58, 57, 57, 769, 66, 58, 46, 45, 56, 45, 307, 59, 45]","[1697548444155, 1697548445539, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446579, 1697548446635, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448280, 1697548449049, 1697548449115, 1697548449173, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776]"
94,94,460,6,[],200,llama-7b,64,1,3278.0,1.0,1,A100,1697548443356,1697548446634,120,87.0,20.0,"[17, 767, 209, 58, 46, 57, 56, 53, 44, 50, 825, 310, 55, 41, 41, 367, 61, 61, 58, 46, 56]","[1697548443373, 1697548444140, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444619, 1697548444663, 1697548444713, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446634]"
95,95,120,7,[],200,llama-7b,64,1,838.0,1.0,1,A100,1697548446637,1697548447475,120,17.0,1.0,"[6, 832]","[1697548446643, 1697548447475]"
96,96,821,8,[],200,llama-7b,64,1,3652.0,1.0,1,A100,1697548447478,1697548451130,120,85.0,20.0,"[17, 1026, 528, 67, 58, 45, 45, 56, 45, 307, 60, 44, 54, 53, 558, 54, 53, 52, 405, 64, 61]","[1697548447495, 1697548448521, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449732, 1697548449776, 1697548449830, 1697548449883, 1697548450441, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130]"
97,97,575,1,[],200,llama-7b,64,1,4172.0,1.0,1,A100,1697548435583,1697548439755,120,86.0,20.0,"[35, 800, 395, 251, 241, 242, 67, 66, 63, 50, 184, 54, 65, 64, 53, 387, 51, 41, 49, 943, 71]","[1697548435618, 1697548436418, 1697548436813, 1697548437064, 1697548437305, 1697548437547, 1697548437614, 1697548437680, 1697548437743, 1697548437793, 1697548437977, 1697548438031, 1697548438096, 1697548438160, 1697548438213, 1697548438600, 1697548438651, 1697548438692, 1697548438741, 1697548439684, 1697548439755]"
98,98,52,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548440709,1697548441612,120,,,"[15, 679]","[1697548440724, 1697548441403]"
99,99,655,1,[],200,llama-7b,64,1,2684.0,1.0,1,A100,1697548438216,1697548440900,120,335.0,11.0,"[28, 1033, 407, 70, 62, 47, 61, 59, 724, 70, 53, 70]","[1697548438244, 1697548439277, 1697548439684, 1697548439754, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440707, 1697548440777, 1697548440830, 1697548440900]"
100,100,645,2,[],200,llama-7b,64,1,3096.0,1.0,1,A100,1697548441617,1697548444713,120,86.0,20.0,"[148, 995, 54, 60, 48, 46, 37, 418, 54, 55, 51, 41, 46, 680, 57, 47, 56, 56, 54, 47, 45]","[1697548441765, 1697548442760, 1697548442814, 1697548442874, 1697548442922, 1697548442968, 1697548443005, 1697548443423, 1697548443477, 1697548443532, 1697548443583, 1697548443624, 1697548443670, 1697548444350, 1697548444407, 1697548444454, 1697548444510, 1697548444566, 1697548444620, 1697548444667, 1697548444712]"
101,101,474,9,[],200,llama-7b,64,1,5458.0,1.0,1,A100,1697548451132,1697548456590,120,109.0,33.0,"[7, 386, 48, 49, 52, 48, 59, 245, 58, 45, 57, 54, 1016, 69, 51, 63, 875, 66, 71, 54, 61, 59, 1118, 73, 76, 71, 70, 68, 53, 67, 247, 70, 52]","[1697548451139, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451722, 1697548451781, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453256, 1697548453325, 1697548453376, 1697548453439, 1697548454314, 1697548454380, 1697548454451, 1697548454505, 1697548454566, 1697548454625, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456538, 1697548456590]"
102,102,764,1,[],200,llama-7b,64,1,1831.0,1.0,1,A100,1697548436076,1697548437907,120,39.0,1.0,"[14, 1817]","[1697548436090, 1697548437907]"
103,103,764,1,[],200,llama-7b,64,1,1833.0,1.0,1,A100,1697548436074,1697548437907,120,39.0,1.0,"[14, 1819]","[1697548436088, 1697548437907]"
104,104,430,2,[],200,llama-7b,64,1,500.0,1.0,1,A100,1697548440903,1697548441403,120,15.0,1.0,"[18, 482]","[1697548440921, 1697548441403]"
105,105,84,3,[],200,llama-7b,64,1,268.0,1.0,1,A100,1697548441408,1697548441676,120,26.0,1.0,"[23, 245]","[1697548441431, 1697548441676]"
106,106,534,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548437910,1697548441611,120,,,"[16, 618, 56, 51, 41, 49, 944, 70, 61, 47, 61, 59, 723, 71, 53, 70, 62, 49, 63]","[1697548437926, 1697548438544, 1697548438600, 1697548438651, 1697548438692, 1697548438741, 1697548439685, 1697548439755, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440830, 1697548440900, 1697548440962, 1697548441011, 1697548441074]"
107,107,789,4,[],200,llama-7b,64,1,7993.0,1.0,1,A100,1697548441679,1697548449672,120,6.0,50.0,"[178, 902, 56, 60, 47, 46, 38, 417, 54, 54, 51, 41, 48, 679, 57, 47, 57, 54, 55, 43, 50, 825, 310, 54, 42, 40, 368, 61, 61, 58, 46, 55, 359, 45, 44, 44, 57, 801, 63, 62, 58, 57, 56, 770, 66, 58, 45, 46, 56, 45, 306]","[1697548441857, 1697548442759, 1697548442815, 1697548442875, 1697548442922, 1697548442968, 1697548443006, 1697548443423, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443671, 1697548444350, 1697548444407, 1697548444454, 1697548444511, 1697548444565, 1697548444620, 1697548444663, 1697548444713, 1697548445538, 1697548445848, 1697548445902, 1697548445944, 1697548445984, 1697548446352, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446633, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448279, 1697548449049, 1697548449115, 1697548449173, 1697548449218, 1697548449264, 1697548449320, 1697548449365, 1697548449671]"
108,108,822,1,[],200,llama-7b,64,1,4172.0,1.0,1,A100,1697548435582,1697548439754,120,88.0,20.0,"[14, 822, 394, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 63, 54, 387, 51, 41, 49, 943, 70]","[1697548435596, 1697548436418, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438159, 1697548438213, 1697548438600, 1697548438651, 1697548438692, 1697548438741, 1697548439684, 1697548439754]"
109,109,251,10,[],200,llama-7b,64,1,484.0,1.0,1,A100,1697548456593,1697548457077,120,31.0,1.0,"[6, 478]","[1697548456599, 1697548457077]"
110,110,161,1,[],200,llama-7b,64,1,1592.0,1.0,1,A100,1697548438162,1697548439754,120,109.0,7.0,"[19, 419, 52, 40, 49, 944, 69]","[1697548438181, 1697548438600, 1697548438652, 1697548438692, 1697548438741, 1697548439685, 1697548439754]"
111,111,592,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439757,1697548441613,120,,,"[6, 467, 477, 70, 53, 69, 63, 49, 63]","[1697548439763, 1697548440230, 1697548440707, 1697548440777, 1697548440830, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
112,112,837,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439757,1697548441613,120,,,"[11, 462, 477, 70, 53, 69, 63, 49, 63]","[1697548439768, 1697548440230, 1697548440707, 1697548440777, 1697548440830, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
113,113,493,3,[],200,llama-7b,64,1,3096.0,1.0,1,A100,1697548441617,1697548444713,120,83.0,20.0,"[140, 1002, 55, 60, 48, 45, 38, 418, 54, 55, 51, 41, 46, 680, 58, 46, 56, 56, 54, 47, 45]","[1697548441757, 1697548442759, 1697548442814, 1697548442874, 1697548442922, 1697548442967, 1697548443005, 1697548443423, 1697548443477, 1697548443532, 1697548443583, 1697548443624, 1697548443670, 1697548444350, 1697548444408, 1697548444454, 1697548444510, 1697548444566, 1697548444620, 1697548444667, 1697548444712]"
114,114,252,3,[],200,llama-7b,64,1,4232.0,1.0,1,A100,1697548441616,1697548445848,120,182.0,22.0,"[78, 1064, 57, 60, 47, 46, 37, 417, 56, 53, 51, 42, 47, 679, 57, 46, 57, 56, 53, 44, 49, 826, 310]","[1697548441694, 1697548442758, 1697548442815, 1697548442875, 1697548442922, 1697548442968, 1697548443005, 1697548443422, 1697548443478, 1697548443531, 1697548443582, 1697548443624, 1697548443671, 1697548444350, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444619, 1697548444663, 1697548444712, 1697548445538, 1697548445848]"
115,115,832,11,[],200,llama-7b,64,1,612.0,1.0,1,A100,1697548457081,1697548457693,120,15.0,1.0,"[28, 583]","[1697548457109, 1697548457692]"
116,116,99,0,[],200,llama-7b,64,1,593.0,1.0,1,A100,1697548434173,1697548434766,120,10.0,1.0,"[61, 532]","[1697548434234, 1697548434766]"
117,117,248,5,[],200,llama-7b,64,1,2941.0,1.0,1,A100,1697548443534,1697548446475,120,182.0,17.0,"[8, 599, 209, 58, 46, 57, 55, 54, 43, 50, 826, 309, 55, 41, 40, 368, 62, 61]","[1697548443542, 1697548444141, 1697548444350, 1697548444408, 1697548444454, 1697548444511, 1697548444566, 1697548444620, 1697548444663, 1697548444713, 1697548445539, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446352, 1697548446414, 1697548446475]"
118,118,679,1,[],200,llama-7b,64,1,810.0,1.0,1,A100,1697548434771,1697548435581,120,15.0,1.0,"[14, 796]","[1697548434785, 1697548435581]"
119,119,262,4,[],200,llama-7b,64,1,1558.0,1.0,1,A100,1697548444716,1697548446274,120,39.0,1.0,"[26, 1532]","[1697548444742, 1697548446274]"
120,120,854,0,[],200,llama-7b,64,1,5688.0,1.0,1,A100,1697548434174,1697548439862,120,67.0,29.0,"[38, 554, 38, 974, 60, 61, 59, 56, 57, 741, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 63, 53, 388, 51, 40, 50, 943, 69, 62, 47]","[1697548434212, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436014, 1697548436071, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438159, 1697548438212, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439753, 1697548439815, 1697548439862]"
121,121,449,2,[],200,llama-7b,64,1,4172.0,1.0,1,A100,1697548435583,1697548439755,120,86.0,20.0,"[20, 815, 394, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 63, 54, 387, 51, 41, 49, 943, 70]","[1697548435603, 1697548436418, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438159, 1697548438213, 1697548438600, 1697548438651, 1697548438692, 1697548438741, 1697548439684, 1697548439754]"
122,122,852,5,[],200,llama-7b,64,1,3395.0,1.0,1,A100,1697548446277,1697548449672,120,100.0,20.0,"[7, 647, 62, 44, 44, 44, 58, 800, 63, 62, 58, 57, 56, 771, 67, 57, 45, 45, 56, 45, 307]","[1697548446284, 1697548446931, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447183, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448279, 1697548449050, 1697548449117, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672]"
123,123,576,12,[],200,llama-7b,64,1,648.0,1.0,1,A100,1697548457696,1697548458344,120,14.0,1.0,"[21, 627]","[1697548457717, 1697548458344]"
124,124,25,6,[],200,llama-7b,64,1,454.0,1.0,1,A100,1697548446478,1697548446932,120,12.0,1.0,"[9, 445]","[1697548446487, 1697548446932]"
125,125,346,13,[],200,llama-7b,64,1,2727.0,1.0,1,A100,1697548458347,1697548461074,120,85.0,20.0,"[11, 680, 63, 47, 61, 45, 57, 57, 619, 64, 49, 62, 61, 60, 429, 68, 52, 67, 63, 63, 49]","[1697548458358, 1697548459038, 1697548459101, 1697548459148, 1697548459209, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460100, 1697548460162, 1697548460223, 1697548460283, 1697548460712, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074]"
126,126,39,0,[],200,llama-7b,64,1,594.0,1.0,1,A100,1697548434172,1697548434766,120,8.0,1.0,"[15, 579]","[1697548434187, 1697548434766]"
127,127,394,0,[],200,llama-7b,64,1,1404.0,1.0,1,A100,1697548434177,1697548435581,120,11.0,1.0,"[160, 1244]","[1697548434337, 1697548435581]"
128,128,611,7,[],200,llama-7b,64,1,541.0,1.0,1,A100,1697548446935,1697548447476,120,14.0,1.0,"[19, 522]","[1697548446954, 1697548447476]"
129,129,57,3,[],200,llama-7b,64,1,692.0,1.0,1,A100,1697548440711,1697548441403,120,13.0,1.0,"[17, 675]","[1697548440728, 1697548441403]"
130,130,55,1,[],200,llama-7b,64,1,835.0,1.0,1,A100,1697548435583,1697548436418,120,12.0,1.0,"[13, 822]","[1697548435596, 1697548436418]"
131,131,738,1,[],200,llama-7b,64,1,1246.0,1.0,1,A100,1697548434769,1697548436015,120,79.0,6.0,"[11, 999, 60, 60, 59, 57]","[1697548434780, 1697548435779, 1697548435839, 1697548435899, 1697548435958, 1697548436015]"
132,132,640,4,[],200,llama-7b,64,1,269.0,1.0,1,A100,1697548441407,1697548441676,120,15.0,1.0,"[17, 252]","[1697548441424, 1697548441676]"
133,133,25,1,[],200,llama-7b,64,1,835.0,1.0,1,A100,1697548435584,1697548436419,120,12.0,1.0,"[39, 795]","[1697548435623, 1697548436418]"
134,134,757,2,[],200,llama-7b,64,1,1485.0,1.0,1,A100,1697548436422,1697548437907,120,20.0,1.0,"[20, 1465]","[1697548436442, 1697548437907]"
135,135,430,2,[],200,llama-7b,64,1,1143.0,1.0,1,A100,1697548441617,1697548442760,120,15.0,1.0,"[143, 999]","[1697548441760, 1697548442759]"
136,136,417,3,[],200,llama-7b,64,1,633.0,1.0,1,A100,1697548437910,1697548438543,120,17.0,1.0,"[22, 611]","[1697548437932, 1697548438543]"
137,137,701,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548436424,1697548441611,120,,,"[23, 1460, 71, 53, 66, 63, 54, 386, 51, 40, 50, 943, 71, 61, 47, 61, 59, 723, 71, 53, 69, 63, 49, 63]","[1697548436447, 1697548437907, 1697548437978, 1697548438031, 1697548438097, 1697548438160, 1697548438214, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439755, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440830, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
138,138,84,3,[],200,llama-7b,64,1,587.0,1.0,1,A100,1697548442767,1697548443354,120,26.0,1.0,"[30, 557]","[1697548442797, 1697548443354]"
139,139,186,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438545,1697548441612,120,,,"[6, 727, 406, 70, 62, 47, 61, 59, 723, 70, 53, 70, 64, 48, 63]","[1697548438551, 1697548439278, 1697548439684, 1697548439754, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440776, 1697548440829, 1697548440899, 1697548440963, 1697548441011, 1697548441074]"
140,140,789,4,[],200,llama-7b,64,1,7774.0,1.0,1,A100,1697548443357,1697548451131,120,6.0,50.0,"[40, 743, 210, 58, 46, 56, 56, 54, 43, 50, 825, 311, 54, 41, 41, 367, 62, 61, 58, 45, 56, 358, 45, 44, 44, 57, 801, 63, 62, 58, 57, 57, 769, 66, 58, 46, 45, 56, 45, 307, 59, 45, 54, 53, 557, 55, 53, 52, 405, 64, 61]","[1697548443397, 1697548444140, 1697548444350, 1697548444408, 1697548444454, 1697548444510, 1697548444566, 1697548444620, 1697548444663, 1697548444713, 1697548445538, 1697548445849, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448280, 1697548449049, 1697548449115, 1697548449173, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450440, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130]"
141,141,444,1,[],200,llama-7b,64,1,1653.0,1.0,1,A100,1697548435961,1697548437614,120,457.0,6.0,"[6, 451, 395, 251, 241, 242, 67]","[1697548435967, 1697548436418, 1697548436813, 1697548437064, 1697548437305, 1697548437547, 1697548437614]"
142,142,98,2,[],200,llama-7b,64,1,289.0,1.0,1,A100,1697548437618,1697548437907,120,14.0,1.0,"[6, 283]","[1697548437624, 1697548437907]"
143,143,928,1,[],200,llama-7b,64,1,380.0,1.0,1,A100,1697548438164,1697548438544,120,20.0,1.0,"[36, 344]","[1697548438200, 1697548438544]"
144,144,697,2,[],200,llama-7b,64,1,2283.0,1.0,1,A100,1697548438547,1697548440830,120,123.0,10.0,"[6, 725, 406, 70, 62, 47, 61, 59, 723, 70, 53]","[1697548438553, 1697548439278, 1697548439684, 1697548439754, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440776, 1697548440829]"
145,145,803,3,[],200,llama-7b,64,1,635.0,1.0,1,A100,1697548437909,1697548438544,120,20.0,1.0,"[15, 620]","[1697548437924, 1697548438544]"
146,146,459,4,[],200,llama-7b,64,1,1316.0,1.0,1,A100,1697548438547,1697548439863,120,58.0,5.0,"[9, 722, 406, 70, 62, 47]","[1697548438556, 1697548439278, 1697548439684, 1697548439754, 1697548439816, 1697548439863]"
147,147,775,5,[],200,llama-7b,64,1,1141.0,1.0,1,A100,1697548441618,1697548442759,120,17.0,1.0,"[221, 919]","[1697548441839, 1697548442758]"
148,148,414,3,[],200,llama-7b,64,1,4401.0,1.0,1,A100,1697548444716,1697548449117,120,87.0,20.0,"[31, 1527, 78, 62, 61, 58, 45, 56, 358, 45, 44, 44, 57, 802, 62, 61, 60, 57, 56, 770, 67]","[1697548444747, 1697548446274, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447984, 1697548448046, 1697548448107, 1697548448167, 1697548448224, 1697548448280, 1697548449050, 1697548449117]"
149,149,805,0,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548434207,1697548441611,120,,,"[146, 1227, 198, 60, 61, 59, 57, 56, 741, 253, 239, 242, 67, 66, 64, 49, 186, 52, 66, 64, 54, 386, 51, 40, 50, 943, 69, 62, 47, 62, 59, 723, 70, 53, 70, 63, 49, 63]","[1697548434353, 1697548435580, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437065, 1697548437304, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438030, 1697548438096, 1697548438160, 1697548438214, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439753, 1697548439815, 1697548439862, 1697548439924, 1697548439983, 1697548440706, 1697548440776, 1697548440829, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
150,150,164,3,[],200,llama-7b,64,1,1143.0,1.0,1,A100,1697548441617,1697548442760,120,15.0,1.0,"[155, 988]","[1697548441772, 1697548442760]"
151,151,866,4,[],200,llama-7b,64,1,3222.0,1.0,1,A100,1697548442763,1697548445985,120,93.0,20.0,"[23, 567, 69, 55, 54, 51, 42, 47, 678, 58, 46, 58, 54, 54, 44, 50, 825, 310, 54, 42, 40]","[1697548442786, 1697548443353, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443624, 1697548443671, 1697548444349, 1697548444407, 1697548444453, 1697548444511, 1697548444565, 1697548444619, 1697548444663, 1697548444713, 1697548445538, 1697548445848, 1697548445902, 1697548445944, 1697548445984]"
152,152,70,4,[],200,llama-7b,64,1,484.0,1.0,1,A100,1697548449121,1697548449605,120,39.0,1.0,"[19, 465]","[1697548449140, 1697548449605]"
153,153,788,1,[],200,llama-7b,64,1,381.0,1.0,1,A100,1697548438163,1697548438544,120,31.0,1.0,"[38, 343]","[1697548438201, 1697548438544]"
154,154,371,0,[],200,llama-7b,64,1,1374.0,1.0,1,A100,1697548434207,1697548435581,120,13.0,1.0,"[161, 1212]","[1697548434368, 1697548435580]"
155,155,536,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438547,1697548441612,120,,,"[24, 706, 408, 70, 61, 47, 61, 59, 723, 71, 53, 69, 64, 48, 63]","[1697548438571, 1697548439277, 1697548439685, 1697548439755, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440830, 1697548440899, 1697548440963, 1697548441011, 1697548441074]"
156,156,141,1,[],200,llama-7b,64,1,4172.0,1.0,1,A100,1697548435583,1697548439755,120,89.0,20.0,"[30, 805, 394, 252, 241, 241, 68, 66, 63, 50, 184, 53, 66, 64, 53, 387, 51, 41, 49, 943, 70]","[1697548435613, 1697548436418, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437614, 1697548437680, 1697548437743, 1697548437793, 1697548437977, 1697548438030, 1697548438096, 1697548438160, 1697548438213, 1697548438600, 1697548438651, 1697548438692, 1697548438741, 1697548439684, 1697548439754]"
157,157,527,5,[],200,llama-7b,64,1,8392.0,1.0,1,A100,1697548445988,1697548454380,120,732.0,50.0,"[6, 999, 44, 44, 44, 57, 801, 63, 62, 58, 57, 56, 771, 67, 57, 45, 45, 56, 45, 307, 59, 45, 54, 53, 556, 56, 53, 52, 405, 64, 61, 55, 41, 298, 49, 48, 53, 48, 59, 244, 59, 44, 58, 54, 1016, 68, 52, 63, 875, 66]","[1697548445994, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448279, 1697548449050, 1697548449117, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450439, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451781, 1697548452025, 1697548452084, 1697548452128, 1697548452186, 1697548452240, 1697548453256, 1697548453324, 1697548453376, 1697548453439, 1697548454314, 1697548454380]"
158,158,413,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548435841,1697548441612,120,,,"[7, 570, 395, 251, 241, 242, 67, 66, 63, 50, 185, 53, 65, 64, 53, 388, 50, 41, 49, 943, 71, 60, 48, 61, 59, 723, 71, 52, 70, 63, 49, 63]","[1697548435848, 1697548436418, 1697548436813, 1697548437064, 1697548437305, 1697548437547, 1697548437614, 1697548437680, 1697548437743, 1697548437793, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438213, 1697548438601, 1697548438651, 1697548438692, 1697548438741, 1697548439684, 1697548439755, 1697548439815, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440829, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
159,159,724,2,[],200,llama-7b,64,1,471.0,1.0,1,A100,1697548439759,1697548440230,120,11.0,1.0,"[24, 447]","[1697548439783, 1697548440230]"
160,160,473,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548440234,1697548441611,120,,,"[6, 1163]","[1697548440240, 1697548441403]"
161,161,126,4,[],200,llama-7b,64,1,1141.0,1.0,1,A100,1697548441618,1697548442759,120,19.0,1.0,"[219, 922]","[1697548441837, 1697548442759]"
162,162,306,3,[],200,llama-7b,64,1,1390.0,1.0,1,A100,1697548441616,1697548443006,120,140.0,6.0,"[129, 1070, 59, 49, 44, 38]","[1697548441745, 1697548442815, 1697548442874, 1697548442923, 1697548442967, 1697548443005]"
163,163,828,5,[],200,llama-7b,64,1,863.0,1.0,1,A100,1697548442761,1697548443624,120,182.0,6.0,"[15, 646, 55, 54, 51, 41]","[1697548442776, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623]"
164,164,480,6,[],200,llama-7b,64,1,513.0,1.0,1,A100,1697548443628,1697548444141,120,26.0,1.0,"[14, 499]","[1697548443642, 1697548444141]"
165,165,908,1,[],200,llama-7b,64,1,8056.0,1.0,1,A100,1697548441616,1697548449672,120,6.0,50.0,"[74, 1068, 57, 60, 47, 46, 37, 417, 55, 54, 51, 42, 47, 679, 57, 46, 57, 56, 54, 44, 48, 826, 309, 55, 41, 41, 368, 61, 61, 58, 46, 55, 359, 45, 44, 44, 57, 801, 63, 61, 59, 57, 56, 770, 66, 58, 45, 46, 56, 45, 306]","[1697548441690, 1697548442758, 1697548442815, 1697548442875, 1697548442922, 1697548442968, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443624, 1697548443671, 1697548444350, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444620, 1697548444664, 1697548444712, 1697548445538, 1697548445847, 1697548445902, 1697548445943, 1697548445984, 1697548446352, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446633, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448107, 1697548448166, 1697548448223, 1697548448279, 1697548449049, 1697548449115, 1697548449173, 1697548449218, 1697548449264, 1697548449320, 1697548449365, 1697548449671]"
166,166,256,7,[],200,llama-7b,64,1,7636.0,1.0,1,A100,1697548444144,1697548451780,120,47.0,50.0,"[16, 1128, 250, 310, 55, 41, 40, 369, 61, 61, 58, 46, 56, 358, 44, 44, 44, 57, 801, 63, 62, 58, 57, 57, 769, 67, 58, 45, 45, 56, 45, 307, 59, 45, 54, 53, 557, 56, 52, 52, 405, 65, 60, 55, 41, 298, 49, 48, 53, 48, 58]","[1697548444160, 1697548445288, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446353, 1697548446414, 1697548446475, 1697548446533, 1697548446579, 1697548446635, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448280, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450440, 1697548450496, 1697548450548, 1697548450600, 1697548451005, 1697548451070, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451780]"
167,167,618,2,[],200,llama-7b,64,1,673.0,1.0,1,A100,1697548438605,1697548439278,120,9.0,1.0,"[6, 667]","[1697548438611, 1697548439278]"
168,168,273,3,[],200,llama-7b,64,1,948.0,1.0,1,A100,1697548439282,1697548440230,120,19.0,1.0,"[22, 926]","[1697548439304, 1697548440230]"
169,169,43,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548440233,1697548441611,120,,,"[7, 1163]","[1697548440240, 1697548441403]"
170,170,267,2,[],200,llama-7b,64,1,3351.0,1.0,1,A100,1697548437549,1697548440900,120,83.0,20.0,"[16, 342, 71, 53, 66, 63, 54, 386, 51, 40, 50, 944, 70, 61, 47, 61, 59, 723, 71, 53, 70]","[1697548437565, 1697548437907, 1697548437978, 1697548438031, 1697548438097, 1697548438160, 1697548438214, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439685, 1697548439755, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440830, 1697548440900]"
171,171,632,5,[],200,llama-7b,64,1,3096.0,1.0,1,A100,1697548441617,1697548444713,120,91.0,20.0,"[153, 990, 54, 60, 48, 46, 38, 417, 54, 55, 51, 41, 46, 680, 57, 47, 57, 55, 54, 45, 48]","[1697548441770, 1697548442760, 1697548442814, 1697548442874, 1697548442922, 1697548442968, 1697548443006, 1697548443423, 1697548443477, 1697548443532, 1697548443583, 1697548443624, 1697548443670, 1697548444350, 1697548444407, 1697548444454, 1697548444511, 1697548444566, 1697548444620, 1697548444665, 1697548444713]"
172,172,110,3,[],200,llama-7b,64,1,1073.0,1.0,1,A100,1697548439757,1697548440830,120,96.0,4.0,"[16, 458, 476, 70, 53]","[1697548439773, 1697548440231, 1697548440707, 1697548440777, 1697548440830]"
173,173,806,4,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548440833,1697548441612,120,,,"[10, 560]","[1697548440843, 1697548441403]"
174,174,467,5,[],200,llama-7b,64,1,3096.0,1.0,1,A100,1697548441616,1697548444712,120,93.0,20.0,"[69, 1073, 57, 60, 47, 46, 37, 417, 55, 54, 51, 42, 47, 679, 57, 46, 57, 56, 53, 44, 49]","[1697548441685, 1697548442758, 1697548442815, 1697548442875, 1697548442922, 1697548442968, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443624, 1697548443671, 1697548444350, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444619, 1697548444663, 1697548444712]"
175,175,237,6,[],200,llama-7b,64,1,4401.0,1.0,1,A100,1697548444716,1697548449117,120,87.0,20.0,"[37, 1521, 79, 61, 61, 58, 45, 56, 359, 44, 44, 44, 57, 802, 62, 61, 60, 57, 55, 771, 67]","[1697548444753, 1697548446274, 1697548446353, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447984, 1697548448046, 1697548448107, 1697548448167, 1697548448224, 1697548448279, 1697548449050, 1697548449117]"
176,176,262,2,[],200,llama-7b,64,1,1061.0,1.0,1,A100,1697548438217,1697548439278,120,39.0,1.0,"[38, 1023]","[1697548438255, 1697548439278]"
177,177,38,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439281,1697548441612,120,,,"[25, 924, 477, 70, 53, 69, 63, 49, 63]","[1697548439306, 1697548440230, 1697548440707, 1697548440777, 1697548440830, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
178,178,252,5,[],200,llama-7b,64,1,3583.0,1.0,1,A100,1697548449674,1697548453257,120,182.0,22.0,"[13, 680, 73, 56, 53, 51, 405, 64, 62, 54, 42, 297, 49, 48, 53, 48, 59, 244, 58, 45, 58, 54, 1017]","[1697548449687, 1697548450367, 1697548450440, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451131, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453257]"
179,179,620,4,[],200,llama-7b,64,1,1808.0,1.0,1,A100,1697548441615,1697548443423,120,100.0,8.0,"[60, 401, 32, 707, 59, 48, 46, 37, 417]","[1697548441675, 1697548442076, 1697548442108, 1697548442815, 1697548442874, 1697548442922, 1697548442968, 1697548443005, 1697548443422]"
180,180,398,5,[],200,llama-7b,64,1,3211.0,1.0,1,A100,1697548443424,1697548446635,120,87.0,20.0,"[12, 704, 210, 58, 46, 56, 56, 54, 43, 50, 825, 311, 54, 41, 40, 368, 62, 61, 58, 45, 57]","[1697548443436, 1697548444140, 1697548444350, 1697548444408, 1697548444454, 1697548444510, 1697548444566, 1697548444620, 1697548444663, 1697548444713, 1697548445538, 1697548445849, 1697548445903, 1697548445944, 1697548445984, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446635]"
181,181,31,1,[],200,llama-7b,64,1,3049.0,1.0,1,A100,1697548441614,1697548444663,120,84.0,20.0,"[6, 455, 33, 706, 59, 48, 46, 38, 417, 54, 55, 51, 41, 47, 679, 58, 46, 57, 55, 54, 44]","[1697548441620, 1697548442075, 1697548442108, 1697548442814, 1697548442873, 1697548442921, 1697548442967, 1697548443005, 1697548443422, 1697548443476, 1697548443531, 1697548443582, 1697548443623, 1697548443670, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444565, 1697548444619, 1697548444663]"
182,182,448,0,[],200,llama-7b,64,1,3373.0,1.0,1,A100,1697548434173,1697548437546,120,335.0,12.0,"[29, 564, 38, 974, 60, 61, 59, 57, 56, 741, 252, 240, 242]","[1697548434202, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437064, 1697548437304, 1697548437546]"
183,183,612,2,[],200,llama-7b,64,1,3439.0,1.0,1,A100,1697548444670,1697548448109,120,93.0,20.0,"[22, 596, 250, 311, 54, 41, 41, 367, 61, 62, 57, 46, 56, 359, 45, 44, 43, 57, 801, 63, 63]","[1697548444692, 1697548445288, 1697548445538, 1697548445849, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446413, 1697548446475, 1697548446532, 1697548446578, 1697548446634, 1697548446993, 1697548447038, 1697548447082, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448109]"
184,184,821,7,[],200,llama-7b,64,1,2602.0,1.0,1,A100,1697548449120,1697548451722,120,85.0,20.0,"[15, 470, 68, 59, 45, 53, 53, 557, 56, 53, 51, 405, 64, 62, 54, 42, 297, 49, 48, 52, 49]","[1697548449135, 1697548449605, 1697548449673, 1697548449732, 1697548449777, 1697548449830, 1697548449883, 1697548450440, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451131, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451673, 1697548451722]"
185,185,835,6,[],200,llama-7b,64,1,3466.0,1.0,1,A100,1697548453260,1697548456726,120,87.0,20.0,"[11, 575, 468, 66, 71, 53, 62, 60, 1117, 73, 76, 71, 70, 68, 53, 67, 249, 68, 52, 70, 66]","[1697548453271, 1697548453846, 1697548454314, 1697548454380, 1697548454451, 1697548454504, 1697548454566, 1697548454626, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456470, 1697548456538, 1697548456590, 1697548456660, 1697548456726]"
186,186,389,0,[],200,llama-7b,64,1,1406.0,1.0,1,A100,1697548434174,1697548435580,120,8.0,1.0,"[99, 1307]","[1697548434273, 1697548435580]"
187,187,369,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438547,1697548441612,120,,,"[14, 716, 408, 70, 61, 47, 61, 59, 723, 70, 54, 69, 64, 48, 63]","[1697548438561, 1697548439277, 1697548439685, 1697548439755, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440776, 1697548440830, 1697548440899, 1697548440963, 1697548441011, 1697548441074]"
188,188,430,1,[],200,llama-7b,64,1,461.0,1.0,1,A100,1697548441615,1697548442076,120,15.0,1.0,"[30, 431]","[1697548441645, 1697548442076]"
189,189,207,2,[],200,llama-7b,64,1,680.0,1.0,1,A100,1697548442079,1697548442759,120,10.0,1.0,"[10, 670]","[1697548442089, 1697548442759]"
190,190,793,3,[],200,llama-7b,64,1,4420.0,1.0,1,A100,1697548442762,1697548447182,120,92.0,31.0,"[19, 641, 55, 54, 51, 42, 47, 678, 58, 46, 58, 54, 54, 44, 50, 825, 310, 54, 42, 40, 368, 61, 61, 58, 46, 56, 358, 45, 44, 44, 57]","[1697548442781, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443624, 1697548443671, 1697548444349, 1697548444407, 1697548444453, 1697548444511, 1697548444565, 1697548444619, 1697548444663, 1697548444713, 1697548445538, 1697548445848, 1697548445902, 1697548445944, 1697548445984, 1697548446352, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446634, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182]"
191,191,161,1,[],200,llama-7b,64,1,2097.0,1.0,1,A100,1697548435583,1697548437680,120,109.0,7.0,"[20, 1209, 252, 241, 241, 67, 67]","[1697548435603, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437680]"
192,192,926,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438217,1697548441611,120,,,"[44, 1017, 406, 70, 62, 47, 61, 59, 723, 71, 52, 70, 64, 49, 62]","[1697548438261, 1697548439278, 1697548439684, 1697548439754, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440829, 1697548440899, 1697548440963, 1697548441012, 1697548441074]"
193,193,837,2,[],200,llama-7b,64,1,3217.0,1.0,1,A100,1697548437683,1697548440900,120,85.0,20.0,"[11, 213, 71, 53, 66, 63, 54, 386, 51, 41, 49, 944, 70, 61, 47, 61, 59, 724, 70, 53, 70]","[1697548437694, 1697548437907, 1697548437978, 1697548438031, 1697548438097, 1697548438160, 1697548438214, 1697548438600, 1697548438651, 1697548438692, 1697548438741, 1697548439685, 1697548439755, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440707, 1697548440777, 1697548440830, 1697548440900]"
194,194,565,4,[],200,llama-7b,64,1,3944.0,1.0,1,A100,1697548447186,1697548451130,120,91.0,20.0,"[10, 1325, 528, 67, 58, 45, 45, 56, 45, 307, 60, 44, 54, 53, 557, 55, 54, 51, 405, 64, 61]","[1697548447196, 1697548448521, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449732, 1697548449776, 1697548449830, 1697548449883, 1697548450440, 1697548450495, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451130]"
195,195,312,5,[],200,llama-7b,64,1,325.0,1.0,1,A100,1697548451134,1697548451459,120,23.0,1.0,"[20, 305]","[1697548451154, 1697548451459]"
196,196,228,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439864,1697548441611,120,,,"[7, 359, 477, 70, 53, 69, 64, 49, 63]","[1697548439871, 1697548440230, 1697548440707, 1697548440777, 1697548440830, 1697548440899, 1697548440963, 1697548441012, 1697548441075]"
197,197,895,6,[],200,llama-7b,64,1,499.0,1.0,1,A100,1697548451463,1697548451962,120,15.0,1.0,"[37, 462]","[1697548451500, 1697548451962]"
198,198,1,3,[],200,llama-7b,64,1,7431.0,1.0,1,A100,1697548441618,1697548449049,120,47.0,43.0,"[224, 916, 56, 60, 48, 46, 38, 417, 54, 54, 52, 40, 47, 680, 58, 46, 57, 55, 54, 47, 46, 825, 310, 55, 41, 40, 368, 61, 61, 58, 46, 55, 359, 45, 44, 44, 57, 801, 63, 61, 59, 57, 56, 770]","[1697548441842, 1697548442758, 1697548442814, 1697548442874, 1697548442922, 1697548442968, 1697548443006, 1697548443423, 1697548443477, 1697548443531, 1697548443583, 1697548443623, 1697548443670, 1697548444350, 1697548444408, 1697548444454, 1697548444511, 1697548444566, 1697548444620, 1697548444667, 1697548444713, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446352, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446633, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448107, 1697548448166, 1697548448223, 1697548448279, 1697548449049]"
199,199,671,7,[],200,llama-7b,64,1,748.0,1.0,1,A100,1697548451964,1697548452712,120,12.0,1.0,"[14, 733]","[1697548451978, 1697548452711]"
200,200,144,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439759,1697548441613,120,,,"[19, 453, 476, 70, 53, 69, 63, 49, 63]","[1697548439778, 1697548440231, 1697548440707, 1697548440777, 1697548440830, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
201,201,818,6,[],200,llama-7b,64,1,1142.0,1.0,1,A100,1697548441618,1697548442760,120,13.0,1.0,"[161, 981]","[1697548441779, 1697548442760]"
202,202,547,6,[],200,llama-7b,64,1,589.0,1.0,1,A100,1697548442765,1697548443354,120,12.0,1.0,"[41, 548]","[1697548442806, 1697548443354]"
203,203,200,7,[],200,llama-7b,64,1,1356.0,1.0,1,A100,1697548443357,1697548444713,120,6.0,9.0,"[35, 748, 209, 59, 46, 56, 56, 54, 43, 50]","[1697548443392, 1697548444140, 1697548444349, 1697548444408, 1697548444454, 1697548444510, 1697548444566, 1697548444620, 1697548444663, 1697548444713]"
204,204,582,7,[],200,llama-7b,64,1,590.0,1.0,1,A100,1697548442764,1697548443354,120,19.0,1.0,"[32, 558]","[1697548442796, 1697548443354]"
205,205,722,4,[],200,llama-7b,64,1,1143.0,1.0,1,A100,1697548441616,1697548442759,120,39.0,1.0,"[139, 1004]","[1697548441755, 1697548442759]"
206,206,878,8,[],200,llama-7b,64,1,4401.0,1.0,1,A100,1697548444716,1697548449117,120,83.0,20.0,"[27, 1531, 78, 62, 61, 58, 45, 56, 358, 45, 44, 44, 57, 802, 62, 61, 60, 57, 56, 770, 66]","[1697548444743, 1697548446274, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447984, 1697548448046, 1697548448107, 1697548448167, 1697548448224, 1697548448280, 1697548449050, 1697548449116]"
207,207,776,5,[],200,llama-7b,64,1,833.0,1.0,1,A100,1697548449607,1697548450440,120,67.0,2.0,"[32, 728, 73]","[1697548449639, 1697548450367, 1697548450440]"
208,208,185,2,[],200,llama-7b,64,1,3097.0,1.0,1,A100,1697548441616,1697548444713,120,93.0,20.0,"[89, 1053, 57, 59, 48, 45, 38, 418, 55, 54, 51, 41, 47, 679, 57, 46, 57, 56, 54, 43, 49]","[1697548441705, 1697548442758, 1697548442815, 1697548442874, 1697548442922, 1697548442967, 1697548443005, 1697548443423, 1697548443478, 1697548443532, 1697548443583, 1697548443624, 1697548443671, 1697548444350, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444620, 1697548444663, 1697548444712]"
209,209,430,6,[],200,llama-7b,64,1,489.0,1.0,1,A100,1697548450446,1697548450935,120,15.0,1.0,"[26, 463]","[1697548450472, 1697548450935]"
210,210,175,7,[],200,llama-7b,64,1,1088.0,1.0,1,A100,1697548450938,1697548452026,120,140.0,8.0,"[13, 574, 48, 49, 52, 48, 59, 245]","[1697548450951, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451722, 1697548451781, 1697548452026]"
211,211,777,2,[],200,llama-7b,64,1,1486.0,1.0,1,A100,1697548436421,1697548437907,120,9.0,1.0,"[16, 1470]","[1697548436437, 1697548437907]"
212,212,554,3,[],200,llama-7b,64,1,633.0,1.0,1,A100,1697548437911,1697548438544,120,26.0,1.0,"[23, 610]","[1697548437934, 1697548438544]"
213,213,206,4,[],200,llama-7b,64,1,731.0,1.0,1,A100,1697548438547,1697548439278,120,16.0,1.0,"[21, 709]","[1697548438568, 1697548439277]"
214,214,912,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439280,1697548441612,120,,,"[14, 936, 476, 71, 53, 69, 64, 48, 63]","[1697548439294, 1697548440230, 1697548440706, 1697548440777, 1697548440830, 1697548440899, 1697548440963, 1697548441011, 1697548441074]"
215,215,630,1,[],200,llama-7b,64,1,461.0,1.0,1,A100,1697548441615,1697548442076,120,6.0,1.0,"[60, 401]","[1697548441675, 1697548442076]"
216,216,244,1,[],200,llama-7b,64,1,1143.0,1.0,1,A100,1697548441617,1697548442760,120,9.0,1.0,"[145, 998]","[1697548441762, 1697548442760]"
217,217,21,2,[],200,llama-7b,64,1,591.0,1.0,1,A100,1697548442763,1697548443354,120,15.0,1.0,"[28, 563]","[1697548442791, 1697548443354]"
218,218,398,2,[],200,llama-7b,64,1,2635.0,1.0,1,A100,1697548442078,1697548444713,120,87.0,20.0,"[6, 675, 56, 60, 47, 46, 37, 418, 54, 54, 51, 41, 48, 678, 58, 47, 57, 54, 54, 44, 50]","[1697548442084, 1697548442759, 1697548442815, 1697548442875, 1697548442922, 1697548442968, 1697548443005, 1697548443423, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443671, 1697548444349, 1697548444407, 1697548444454, 1697548444511, 1697548444565, 1697548444619, 1697548444663, 1697548444713]"
219,219,600,3,[],200,llama-7b,64,1,783.0,1.0,1,A100,1697548443357,1697548444140,120,23.0,1.0,"[35, 748]","[1697548443392, 1697548444140]"
220,220,375,4,[],200,llama-7b,64,1,3039.0,1.0,1,A100,1697548444144,1697548447183,120,874.0,17.0,"[6, 1389, 309, 55, 41, 40, 368, 62, 61, 58, 46, 56, 358, 44, 44, 44, 57]","[1697548444150, 1697548445539, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446579, 1697548446635, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447182]"
221,221,298,6,[],200,llama-7b,64,1,677.0,1.0,1,A100,1697548454383,1697548455060,120,17.0,1.0,"[7, 670]","[1697548454390, 1697548455060]"
222,222,881,7,[],200,llama-7b,64,1,1665.0,1.0,1,A100,1697548455061,1697548456726,120,58.0,6.0,"[16, 1316, 76, 69, 52, 70, 65]","[1697548455077, 1697548456393, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725]"
223,223,266,1,[],200,llama-7b,64,1,1062.0,1.0,1,A100,1697548438216,1697548439278,120,9.0,1.0,"[40, 1022]","[1697548438256, 1697548439278]"
224,224,38,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439281,1697548441612,120,,,"[18, 931, 476, 71, 53, 69, 63, 49, 63]","[1697548439299, 1697548440230, 1697548440706, 1697548440777, 1697548440830, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
225,225,183,2,[],200,llama-7b,64,1,8055.0,1.0,1,A100,1697548441617,1697548449672,120,17.0,50.0,"[135, 1007, 55, 60, 47, 46, 38, 418, 54, 55, 51, 41, 46, 680, 57, 46, 57, 56, 54, 44, 48, 826, 310, 54, 41, 41, 368, 61, 61, 58, 46, 55, 359, 45, 44, 44, 57, 801, 63, 61, 59, 57, 56, 770, 66, 58, 45, 46, 56, 45, 307]","[1697548441752, 1697548442759, 1697548442814, 1697548442874, 1697548442921, 1697548442967, 1697548443005, 1697548443423, 1697548443477, 1697548443532, 1697548443583, 1697548443624, 1697548443670, 1697548444350, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444620, 1697548444664, 1697548444712, 1697548445538, 1697548445848, 1697548445902, 1697548445943, 1697548445984, 1697548446352, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446633, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448107, 1697548448166, 1697548448223, 1697548448279, 1697548449049, 1697548449115, 1697548449173, 1697548449218, 1697548449264, 1697548449320, 1697548449365, 1697548449672]"
226,226,631,3,[],200,llama-7b,64,1,7750.0,1.0,1,A100,1697548441615,1697548449365,120,216.0,50.0,"[15, 478, 706, 60, 47, 46, 38, 417, 55, 54, 51, 41, 47, 679, 58, 46, 57, 55, 54, 44, 49, 826, 310, 54, 41, 41, 368, 61, 61, 58, 46, 55, 359, 45, 44, 44, 57, 801, 63, 61, 59, 57, 56, 770, 66, 58, 45, 46, 56, 45]","[1697548441630, 1697548442108, 1697548442814, 1697548442874, 1697548442921, 1697548442967, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443670, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444565, 1697548444619, 1697548444663, 1697548444712, 1697548445538, 1697548445848, 1697548445902, 1697548445943, 1697548445984, 1697548446352, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446633, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448107, 1697548448166, 1697548448223, 1697548448279, 1697548449049, 1697548449115, 1697548449173, 1697548449218, 1697548449264, 1697548449320, 1697548449365]"
227,227,536,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548437910,1697548441611,120,,,"[19, 615, 56, 51, 41, 49, 944, 70, 61, 47, 61, 59, 724, 70, 53, 70, 62, 49, 63]","[1697548437929, 1697548438544, 1697548438600, 1697548438651, 1697548438692, 1697548438741, 1697548439685, 1697548439755, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440707, 1697548440777, 1697548440830, 1697548440900, 1697548440962, 1697548441011, 1697548441074]"
228,228,324,8,[],200,llama-7b,64,1,7448.0,1.0,1,A100,1697548452715,1697548460163,120,17.0,50.0,"[11, 1120, 468, 66, 70, 54, 62, 59, 1119, 73, 75, 72, 70, 68, 52, 68, 247, 69, 52, 70, 65, 61, 358, 62, 60, 48, 59, 53, 332, 50, 61, 62, 49, 49, 61, 525, 73, 62, 48, 49, 49, 205, 47, 57, 49, 57, 57, 618, 65, 49, 63]","[1697548452726, 1697548453846, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455744, 1697548455817, 1697548455892, 1697548455964, 1697548456034, 1697548456102, 1697548456154, 1697548456222, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456786, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457426, 1697548457758, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459205, 1697548459254, 1697548459311, 1697548459368, 1697548459986, 1697548460051, 1697548460100, 1697548460163]"
229,229,572,8,[],200,llama-7b,64,1,236.0,1.0,1,A100,1697548451726,1697548451962,120,16.0,1.0,"[10, 226]","[1697548451736, 1697548451962]"
230,230,490,1,[],200,llama-7b,64,1,1647.0,1.0,1,A100,1697548438216,1697548439863,120,11.0,5.0,"[11, 1050, 408, 69, 62, 47]","[1697548438227, 1697548439277, 1697548439685, 1697548439754, 1697548439816, 1697548439863]"
231,231,767,3,[],200,llama-7b,64,1,694.0,1.0,1,A100,1697548449674,1697548450368,120,11.0,1.0,"[9, 684]","[1697548449683, 1697548450367]"
232,232,267,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439865,1697548441611,120,,,"[11, 354, 477, 70, 53, 69, 64, 49, 63]","[1697548439876, 1697548440230, 1697548440707, 1697548440777, 1697548440830, 1697548440899, 1697548440963, 1697548441012, 1697548441075]"
233,233,304,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438216,1697548441611,120,,,"[33, 1028, 407, 70, 62, 47, 61, 59, 724, 70, 53, 70, 63, 49, 62]","[1697548438249, 1697548439277, 1697548439684, 1697548439754, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440707, 1697548440777, 1697548440830, 1697548440900, 1697548440963, 1697548441012, 1697548441074]"
234,234,100,9,[],200,llama-7b,64,1,1440.0,1.0,1,A100,1697548460166,1697548461606,120,732.0,14.0,"[9, 448, 88, 69, 52, 67, 63, 63, 49, 236, 50, 62, 62, 60, 61]","[1697548460175, 1697548460623, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461544, 1697548461605]"
235,235,849,3,[],200,llama-7b,64,1,1142.0,1.0,1,A100,1697548441617,1697548442759,120,10.0,1.0,"[121, 1021]","[1697548441738, 1697548442759]"
236,236,679,10,[],200,llama-7b,64,1,1224.0,1.0,1,A100,1697548461610,1697548462834,120,15.0,1.0,"[25, 1199]","[1697548461635, 1697548462834]"
237,237,625,4,[],200,llama-7b,64,1,662.0,1.0,1,A100,1697548442761,1697548443423,120,364.0,2.0,"[7, 585, 70]","[1697548442768, 1697548443353, 1697548443423]"
238,238,278,5,[],200,llama-7b,64,1,716.0,1.0,1,A100,1697548443425,1697548444141,120,13.0,1.0,"[14, 701]","[1697548443439, 1697548444140]"
239,239,865,2,[],200,llama-7b,64,1,1143.0,1.0,1,A100,1697548441617,1697548442760,120,9.0,1.0,"[157, 986]","[1697548441774, 1697548442760]"
240,240,451,11,[],200,llama-7b,64,1,744.0,1.0,1,A100,1697548462837,1697548463581,120,286.0,1.0,"[15, 729]","[1697548462852, 1697548463581]"
241,241,111,12,[],200,llama-7b,64,1,1853.0,1.0,1,A100,1697548463583,1697548465436,120,79.0,5.0,"[22, 1253, 460, 64, 50]","[1697548463605, 1697548464858, 1697548465318, 1697548465382, 1697548465432]"
242,242,55,6,[],200,llama-7b,64,1,1144.0,1.0,1,A100,1697548444144,1697548445288,120,12.0,1.0,"[12, 1132]","[1697548444156, 1697548445288]"
243,243,231,0,[],200,llama-7b,64,1,1374.0,1.0,1,A100,1697548434207,1697548435581,120,13.0,1.0,"[164, 1209]","[1697548434371, 1697548435580]"
244,244,818,1,[],200,llama-7b,64,1,836.0,1.0,1,A100,1697548435583,1697548436419,120,13.0,1.0,"[35, 800]","[1697548435618, 1697548436418]"
245,245,810,13,[],200,llama-7b,64,1,2871.0,1.0,1,A100,1697548465448,1697548468319,120,91.0,20.0,"[19, 435, 337, 72, 69, 53, 65, 63, 48, 689, 74, 70, 68, 67, 51, 63, 361, 72, 55, 69, 71]","[1697548465467, 1697548465902, 1697548466239, 1697548466311, 1697548466380, 1697548466433, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468052, 1697548468124, 1697548468179, 1697548468248, 1697548468319]"
246,246,635,7,[],200,llama-7b,64,1,982.0,1.0,1,A100,1697548445292,1697548446274,120,23.0,1.0,"[9, 973]","[1697548445301, 1697548446274]"
247,247,93,0,[],200,llama-7b,64,1,3987.0,1.0,1,A100,1697548434173,1697548438160,120,88.0,20.0,"[49, 544, 38, 974, 60, 61, 59, 57, 56, 741, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 63]","[1697548434222, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438159]"
248,248,565,6,[],200,llama-7b,64,1,3048.0,1.0,1,A100,1697548441615,1697548444663,120,91.0,20.0,"[35, 425, 33, 706, 60, 47, 46, 38, 417, 55, 54, 51, 41, 47, 679, 58, 46, 57, 55, 54, 44]","[1697548441650, 1697548442075, 1697548442108, 1697548442814, 1697548442874, 1697548442921, 1697548442967, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443670, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444565, 1697548444619, 1697548444663]"
249,249,29,5,[],200,llama-7b,64,1,2079.0,1.0,1,A100,1697548447185,1697548449264,120,161.0,6.0,"[7, 1329, 528, 67, 58, 45, 45]","[1697548447192, 1697548448521, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264]"
250,250,705,6,[],200,llama-7b,64,1,3989.0,1.0,1,A100,1697548449268,1697548453257,120,79.0,27.0,"[15, 390, 59, 45, 53, 53, 557, 56, 53, 51, 405, 64, 62, 54, 41, 298, 49, 48, 53, 48, 59, 244, 59, 44, 58, 54, 1017]","[1697548449283, 1697548449673, 1697548449732, 1697548449777, 1697548449830, 1697548449883, 1697548450440, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451131, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451781, 1697548452025, 1697548452084, 1697548452128, 1697548452186, 1697548452240, 1697548453257]"
251,251,609,0,[],200,llama-7b,64,1,3986.0,1.0,1,A100,1697548434173,1697548438159,120,88.0,20.0,"[36, 557, 38, 974, 60, 61, 59, 56, 57, 741, 252, 240, 241, 68, 66, 64, 49, 185, 53, 66, 63]","[1697548434209, 1697548434766, 1697548434804, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436014, 1697548436071, 1697548436812, 1697548437064, 1697548437304, 1697548437545, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438159]"
252,252,840,8,[],200,llama-7b,64,1,929.0,1.0,1,A100,1697548451783,1697548452712,120,17.0,1.0,"[8, 920]","[1697548451791, 1697548452711]"
253,253,386,8,[],200,llama-7b,64,1,1786.0,1.0,1,A100,1697548447479,1697548449265,120,140.0,6.0,"[21, 1021, 528, 67, 58, 45, 45]","[1697548447500, 1697548448521, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264]"
254,254,85,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438217,1697548441612,120,,,"[43, 1018, 406, 70, 62, 47, 61, 59, 724, 70, 53, 70, 63, 49, 62]","[1697548438260, 1697548439278, 1697548439684, 1697548439754, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440707, 1697548440777, 1697548440830, 1697548440900, 1697548440963, 1697548441012, 1697548441074]"
255,255,578,3,[],200,llama-7b,64,1,1557.0,1.0,1,A100,1697548444717,1697548446274,120,31.0,1.0,"[41, 1516]","[1697548444758, 1697548446274]"
256,256,239,4,[],200,llama-7b,64,1,4271.0,1.0,1,A100,1697548446277,1697548450548,120,39.0,27.0,"[26, 628, 62, 45, 44, 44, 57, 800, 63, 62, 58, 57, 57, 770, 67, 57, 45, 45, 57, 44, 307, 59, 45, 54, 53, 556, 56, 53]","[1697548446303, 1697548446931, 1697548446993, 1697548447038, 1697548447082, 1697548447126, 1697548447183, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448280, 1697548449050, 1697548449117, 1697548449174, 1697548449219, 1697548449264, 1697548449321, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450439, 1697548450495, 1697548450548]"
257,257,418,5,[],200,llama-7b,64,1,1198.0,1.0,1,A100,1697548441677,1697548442875,120,286.0,3.0,"[167, 914, 56, 60]","[1697548441844, 1697548442758, 1697548442814, 1697548442874]"
258,258,781,2,[],200,llama-7b,64,1,1915.0,1.0,1,A100,1697548441616,1697548443531,120,335.0,10.0,"[54, 405, 33, 706, 60, 48, 46, 37, 417, 55, 54]","[1697548441670, 1697548442075, 1697548442108, 1697548442814, 1697548442874, 1697548442922, 1697548442968, 1697548443005, 1697548443422, 1697548443477, 1697548443531]"
259,259,477,7,[],200,llama-7b,64,1,6905.0,1.0,1,A100,1697548453259,1697548460164,120,244.0,50.0,"[7, 580, 468, 66, 71, 53, 62, 60, 1116, 73, 77, 71, 70, 68, 53, 67, 249, 68, 52, 70, 66, 60, 358, 62, 60, 48, 59, 53, 332, 50, 61, 62, 49, 49, 61, 525, 73, 62, 48, 49, 49, 205, 47, 57, 49, 57, 57, 618, 65, 49, 63]","[1697548453266, 1697548453846, 1697548454314, 1697548454380, 1697548454451, 1697548454504, 1697548454566, 1697548454626, 1697548455742, 1697548455815, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456470, 1697548456538, 1697548456590, 1697548456660, 1697548456726, 1697548456786, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457426, 1697548457758, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459205, 1697548459254, 1697548459311, 1697548459368, 1697548459986, 1697548460051, 1697548460100, 1697548460163]"
260,260,522,3,[],200,llama-7b,64,1,1169.0,1.0,1,A100,1697548440234,1697548441403,120,20.0,1.0,"[11, 1158]","[1697548440245, 1697548441403]"
261,261,292,4,[],200,llama-7b,64,1,270.0,1.0,1,A100,1697548441406,1697548441676,120,286.0,1.0,"[8, 262]","[1697548441414, 1697548441676]"
262,262,435,3,[],200,llama-7b,64,1,4513.0,1.0,1,A100,1697548443533,1697548448046,120,563.0,27.0,"[14, 594, 209, 58, 46, 57, 55, 54, 43, 50, 826, 309, 55, 41, 40, 368, 62, 61, 58, 45, 56, 359, 44, 44, 44, 57, 801, 63]","[1697548443547, 1697548444141, 1697548444350, 1697548444408, 1697548444454, 1697548444511, 1697548444566, 1697548444620, 1697548444663, 1697548444713, 1697548445539, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046]"
263,263,615,7,[],200,llama-7b,64,1,2169.0,1.0,1,A100,1697548456727,1697548458896,120,93.0,20.0,"[21, 329, 68, 62, 60, 47, 59, 54, 331, 50, 62, 61, 49, 49, 61, 526, 73, 61, 48, 48, 50]","[1697548456748, 1697548457077, 1697548457145, 1697548457207, 1697548457267, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458689, 1697548458750, 1697548458798, 1697548458846, 1697548458896]"
264,264,271,8,[],200,llama-7b,64,1,2707.0,1.0,1,A100,1697548458899,1697548461606,120,87.0,20.0,"[23, 815, 249, 65, 49, 63, 60, 60, 429, 68, 52, 68, 62, 63, 49, 236, 49, 63, 62, 61, 60]","[1697548458922, 1697548459737, 1697548459986, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460712, 1697548460780, 1697548460832, 1697548460900, 1697548460962, 1697548461025, 1697548461074, 1697548461310, 1697548461359, 1697548461422, 1697548461484, 1697548461545, 1697548461605]"
265,265,864,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439757,1697548441612,120,,,"[6, 467, 477, 70, 53, 69, 63, 49, 63]","[1697548439763, 1697548440230, 1697548440707, 1697548440777, 1697548440830, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
266,266,212,4,[],200,llama-7b,64,1,473.0,1.0,1,A100,1697548448049,1697548448522,120,31.0,1.0,"[6, 467]","[1697548448055, 1697548448522]"
267,267,40,9,[],200,llama-7b,64,1,4824.0,1.0,1,A100,1697548461609,1697548466433,120,86.0,20.0,"[16, 1208, 73, 56, 57, 54, 821, 61, 62, 56, 56, 729, 460, 64, 56, 53, 45, 703, 71, 70, 53]","[1697548461625, 1697548462833, 1697548462906, 1697548462962, 1697548463019, 1697548463073, 1697548463894, 1697548463955, 1697548464017, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465438, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466433]"
268,268,792,5,[],200,llama-7b,64,1,1079.0,1.0,1,A100,1697548448525,1697548449604,120,11.0,1.0,"[20, 1059]","[1697548448545, 1697548449604]"
269,269,540,6,[],200,llama-7b,64,1,994.0,1.0,1,A100,1697548449607,1697548450601,120,140.0,5.0,"[17, 744, 72, 56, 53, 51]","[1697548449624, 1697548450368, 1697548450440, 1697548450496, 1697548450549, 1697548450600]"
270,270,193,7,[],200,llama-7b,64,1,3902.0,1.0,1,A100,1697548450602,1697548454504,120,79.0,20.0,"[10, 913, 48, 49, 52, 48, 59, 245, 58, 45, 57, 54, 1017, 68, 52, 62, 875, 66, 70, 54]","[1697548450612, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451722, 1697548451781, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454504]"
271,271,635,3,[],200,llama-7b,64,1,460.0,1.0,1,A100,1697548441615,1697548442075,120,23.0,1.0,"[15, 445]","[1697548441630, 1697548442075]"
272,272,14,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548441014,1697548441612,120,,,"[7, 382]","[1697548441021, 1697548441403]"
273,273,295,4,[],200,llama-7b,64,1,737.0,1.0,1,A100,1697548442078,1697548442815,120,52.0,2.0,"[6, 675, 56]","[1697548442084, 1697548442759, 1697548442815]"
274,274,130,8,[],200,llama-7b,64,1,456.0,1.0,1,A100,1697548460167,1697548460623,120,14.0,1.0,"[19, 437]","[1697548460186, 1697548460623]"
275,275,63,5,[],200,llama-7b,64,1,535.0,1.0,1,A100,1697548442819,1697548443354,120,39.0,1.0,"[26, 509]","[1697548442845, 1697548443354]"
276,276,649,6,[],200,llama-7b,64,1,3277.0,1.0,1,A100,1697548443357,1697548446634,120,244.0,20.0,"[25, 758, 209, 58, 47, 56, 56, 54, 43, 49, 826, 310, 55, 41, 41, 367, 61, 62, 58, 45, 56]","[1697548443382, 1697548444140, 1697548444349, 1697548444407, 1697548444454, 1697548444510, 1697548444566, 1697548444620, 1697548444663, 1697548444712, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446413, 1697548446475, 1697548446533, 1697548446578, 1697548446634]"
277,277,836,9,[],200,llama-7b,64,1,617.0,1.0,1,A100,1697548460626,1697548461243,120,11.0,1.0,"[12, 604]","[1697548460638, 1697548461242]"
278,278,491,10,[],200,llama-7b,64,1,689.0,1.0,1,A100,1697548461246,1697548461935,120,14.0,1.0,"[16, 673]","[1697548461262, 1697548461935]"
279,279,419,7,[],200,llama-7b,64,1,3803.0,1.0,1,A100,1697548446637,1697548450440,120,88.0,20.0,"[25, 813, 508, 63, 62, 59, 57, 56, 769, 66, 59, 45, 46, 55, 45, 308, 59, 44, 54, 53, 557]","[1697548446662, 1697548447475, 1697548447983, 1697548448046, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449115, 1697548449174, 1697548449219, 1697548449265, 1697548449320, 1697548449365, 1697548449673, 1697548449732, 1697548449776, 1697548449830, 1697548449883, 1697548450440]"
280,280,895,8,[],200,llama-7b,64,1,540.0,1.0,1,A100,1697548454520,1697548455060,120,15.0,1.0,"[24, 516]","[1697548454544, 1697548455060]"
281,281,604,3,[],200,llama-7b,64,1,1259.0,1.0,1,A100,1697548441615,1697548442874,120,161.0,4.0,"[35, 426, 32, 706, 60]","[1697548441650, 1697548442076, 1697548442108, 1697548442814, 1697548442874]"
282,282,559,9,[],200,llama-7b,64,1,3028.0,1.0,1,A100,1697548455063,1697548458091,120,86.0,20.0,"[23, 1307, 76, 69, 52, 70, 65, 61, 359, 62, 59, 48, 60, 53, 331, 50, 62, 62, 49, 48, 62]","[1697548455086, 1697548456393, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456786, 1697548457145, 1697548457207, 1697548457266, 1697548457314, 1697548457374, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457981, 1697548458029, 1697548458091]"
283,283,74,8,[],200,llama-7b,64,1,2935.0,1.0,1,A100,1697548450442,1697548453377,120,88.0,20.0,"[10, 483, 71, 64, 61, 54, 42, 297, 49, 48, 53, 49, 58, 244, 58, 45, 58, 54, 1017, 68, 52]","[1697548450452, 1697548450935, 1697548451006, 1697548451070, 1697548451131, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451723, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453377]"
284,284,375,4,[],200,llama-7b,64,1,2971.0,1.0,1,A100,1697548442877,1697548445848,120,874.0,17.0,"[18, 527, 55, 55, 51, 41, 47, 678, 58, 46, 57, 56, 53, 44, 50, 825, 310]","[1697548442895, 1697548443422, 1697548443477, 1697548443532, 1697548443583, 1697548443624, 1697548443671, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444619, 1697548444663, 1697548444713, 1697548445538, 1697548445848]"
285,285,268,11,[],200,llama-7b,64,1,897.0,1.0,1,A100,1697548461937,1697548462834,120,19.0,1.0,"[6, 891]","[1697548461943, 1697548462834]"
286,286,29,5,[],200,llama-7b,64,1,728.0,1.0,1,A100,1697548445851,1697548446579,120,161.0,6.0,"[7, 416, 79, 61, 61, 58, 46]","[1697548445858, 1697548446274, 1697548446353, 1697548446414, 1697548446475, 1697548446533, 1697548446579]"
287,287,851,12,[],200,llama-7b,64,1,741.0,1.0,1,A100,1697548462839,1697548463580,120,23.0,1.0,"[19, 722]","[1697548462858, 1697548463580]"
288,288,705,6,[],200,llama-7b,64,1,3967.0,1.0,1,A100,1697548446582,1697548450549,120,79.0,27.0,"[18, 393, 45, 44, 44, 57, 800, 63, 62, 58, 57, 57, 769, 68, 57, 45, 45, 57, 44, 307, 59, 45, 54, 53, 556, 56, 54]","[1697548446600, 1697548446993, 1697548447038, 1697548447082, 1697548447126, 1697548447183, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448280, 1697548449049, 1697548449117, 1697548449174, 1697548449219, 1697548449264, 1697548449321, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450439, 1697548450495, 1697548450549]"
289,289,738,1,[],200,llama-7b,64,1,1708.0,1.0,1,A100,1697548438216,1697548439924,120,79.0,6.0,"[22, 1447, 69, 62, 46, 62]","[1697548438238, 1697548439685, 1697548439754, 1697548439816, 1697548439862, 1697548439924]"
290,290,622,13,[],200,llama-7b,64,1,971.0,1.0,1,A100,1697548463584,1697548464555,120,20.0,1.0,"[26, 945]","[1697548463610, 1697548464555]"
291,291,283,14,[],200,llama-7b,64,1,3761.0,1.0,1,A100,1697548464558,1697548468319,120,85.0,20.0,"[14, 1329, 337, 73, 69, 53, 64, 64, 48, 689, 74, 70, 68, 67, 51, 63, 360, 73, 54, 70, 70]","[1697548464572, 1697548465901, 1697548466238, 1697548466311, 1697548466380, 1697548466433, 1697548466497, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468124, 1697548468178, 1697548468248, 1697548468318]"
292,292,445,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548440832,1697548441612,120,,,"[7, 564]","[1697548440839, 1697548441403]"
293,293,100,4,[],200,llama-7b,64,1,2735.0,1.0,1,A100,1697548441614,1697548444349,120,732.0,14.0,"[6, 455, 33, 706, 60, 47, 46, 38, 417, 55, 54, 51, 41, 47, 679]","[1697548441620, 1697548442075, 1697548442108, 1697548442814, 1697548442874, 1697548442921, 1697548442967, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443670, 1697548444349]"
294,294,358,7,[],200,llama-7b,64,1,518.0,1.0,1,A100,1697548450552,1697548451070,120,216.0,3.0,"[11, 372, 71, 64]","[1697548450563, 1697548450935, 1697548451006, 1697548451070]"
295,295,537,4,[],200,llama-7b,64,1,3006.0,1.0,1,A100,1697548450371,1697548453377,120,83.0,20.0,"[20, 544, 71, 64, 60, 55, 42, 297, 49, 48, 53, 49, 58, 244, 58, 45, 58, 54, 1017, 68, 52]","[1697548450391, 1697548450935, 1697548451006, 1697548451070, 1697548451130, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451723, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453377]"
296,296,134,8,[],200,llama-7b,64,1,3432.0,1.0,1,A100,1697548451073,1697548454505,120,86.0,20.0,"[15, 371, 66, 48, 49, 52, 48, 59, 245, 58, 45, 57, 54, 1017, 68, 51, 62, 876, 66, 70, 55]","[1697548451088, 1697548451459, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451722, 1697548451781, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453376, 1697548453438, 1697548454314, 1697548454380, 1697548454450, 1697548454505]"
297,297,717,9,[],200,llama-7b,64,1,2866.0,1.0,1,A100,1697548454508,1697548457374,120,89.0,20.0,"[21, 531, 683, 74, 75, 72, 69, 69, 52, 68, 246, 70, 52, 70, 65, 60, 359, 63, 60, 48, 59]","[1697548454529, 1697548455060, 1697548455743, 1697548455817, 1697548455892, 1697548455964, 1697548456033, 1697548456102, 1697548456154, 1697548456222, 1697548456468, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457207, 1697548457267, 1697548457315, 1697548457374]"
298,298,52,15,[],200,llama-7b,64,1,843.0,1.0,1,A100,1697548468323,1697548469166,120,58.0,6.0,"[25, 323, 246, 67, 51, 66, 65]","[1697548468348, 1697548468671, 1697548468917, 1697548468984, 1697548469035, 1697548469101, 1697548469166]"
299,299,611,16,[],200,llama-7b,64,1,413.0,1.0,1,A100,1697548469169,1697548469582,120,14.0,1.0,"[11, 402]","[1697548469180, 1697548469582]"
300,300,802,5,[],200,llama-7b,64,1,934.0,1.0,1,A100,1697548444354,1697548445288,120,9.0,1.0,"[10, 924]","[1697548444364, 1697548445288]"
301,301,463,6,[],200,llama-7b,64,1,982.0,1.0,1,A100,1697548445292,1697548446274,120,39.0,1.0,"[19, 963]","[1697548445311, 1697548446274]"
302,302,377,17,[],200,llama-7b,64,1,1117.0,1.0,1,A100,1697548469586,1697548470703,120,13.0,1.0,"[25, 1092]","[1697548469611, 1697548470703]"
303,303,634,3,[],200,llama-7b,64,1,589.0,1.0,1,A100,1697548442765,1697548443354,120,13.0,1.0,"[28, 561]","[1697548442793, 1697548443354]"
304,304,443,5,[],200,llama-7b,64,1,325.0,1.0,1,A100,1697548451134,1697548451459,120,19.0,1.0,"[19, 306]","[1697548451153, 1697548451459]"
305,305,492,10,[],200,llama-7b,64,1,1992.0,1.0,1,A100,1697548457376,1697548459368,120,47.0,20.0,"[21, 295, 66, 50, 62, 62, 49, 49, 61, 525, 72, 62, 48, 49, 50, 204, 46, 58, 49, 57, 57]","[1697548457397, 1697548457692, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457981, 1697548458030, 1697548458091, 1697548458616, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458897, 1697548459101, 1697548459147, 1697548459205, 1697548459254, 1697548459311, 1697548459368]"
306,306,289,4,[],200,llama-7b,64,1,3278.0,1.0,1,A100,1697548443356,1697548446634,120,89.0,20.0,"[14, 770, 209, 58, 46, 57, 56, 53, 44, 49, 826, 310, 54, 42, 41, 367, 61, 61, 58, 46, 56]","[1697548443370, 1697548444140, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444619, 1697548444663, 1697548444712, 1697548445538, 1697548445848, 1697548445902, 1697548445944, 1697548445985, 1697548446352, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446634]"
307,307,625,6,[],200,llama-7b,64,1,766.0,1.0,1,A100,1697548449674,1697548450440,120,364.0,2.0,"[7, 686, 73]","[1697548449681, 1697548450367, 1697548450440]"
308,308,527,9,[],200,llama-7b,64,1,7666.0,1.0,1,A100,1697548449119,1697548456785,120,732.0,50.0,"[6, 547, 59, 45, 54, 53, 558, 54, 54, 51, 405, 64, 62, 54, 41, 298, 49, 48, 53, 48, 59, 244, 59, 44, 58, 54, 1017, 68, 51, 63, 875, 66, 70, 54, 62, 59, 1117, 73, 77, 71, 70, 68, 53, 67, 247, 69, 52, 71, 65, 60]","[1697548449125, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450441, 1697548450495, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451131, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451781, 1697548452025, 1697548452084, 1697548452128, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453376, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455742, 1697548455815, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456660, 1697548456725, 1697548456785]"
309,309,405,8,[],200,llama-7b,64,1,3396.0,1.0,1,A100,1697548446277,1697548449673,120,87.0,20.0,"[21, 633, 62, 45, 43, 45, 57, 800, 63, 62, 58, 57, 56, 771, 67, 57, 45, 45, 57, 44, 307]","[1697548446298, 1697548446931, 1697548446993, 1697548447038, 1697548447081, 1697548447126, 1697548447183, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448279, 1697548449050, 1697548449117, 1697548449174, 1697548449219, 1697548449264, 1697548449321, 1697548449365, 1697548449672]"
310,310,386,1,[],200,llama-7b,64,1,1708.0,1.0,1,A100,1697548438216,1697548439924,120,140.0,6.0,"[26, 1035, 409, 68, 62, 46, 62]","[1697548438242, 1697548439277, 1697548439686, 1697548439754, 1697548439816, 1697548439862, 1697548439924]"
311,311,284,7,[],200,llama-7b,64,1,5520.0,1.0,1,A100,1697548450443,1697548455963,120,90.0,31.0,"[15, 547, 65, 60, 55, 42, 297, 49, 48, 53, 49, 58, 244, 58, 45, 58, 54, 1017, 68, 52, 62, 875, 66, 70, 54, 62, 59, 1118, 73, 76, 71]","[1697548450458, 1697548451005, 1697548451070, 1697548451130, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451723, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455743, 1697548455816, 1697548455892, 1697548455963]"
312,312,37,9,[],200,llama-7b,64,1,687.0,1.0,1,A100,1697548449681,1697548450368,120,20.0,1.0,"[14, 672]","[1697548449695, 1697548450367]"
313,313,734,10,[],200,llama-7b,64,1,858.0,1.0,1,A100,1697548450370,1697548451228,120,100.0,6.0,"[9, 556, 70, 65, 60, 55, 42]","[1697548450379, 1697548450935, 1697548451005, 1697548451070, 1697548451130, 1697548451185, 1697548451227]"
314,314,38,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439925,1697548441611,120,,,"[14, 291, 477, 70, 54, 69, 63, 49, 63]","[1697548439939, 1697548440230, 1697548440707, 1697548440777, 1697548440831, 1697548440900, 1697548440963, 1697548441012, 1697548441075]"
315,315,302,10,[],200,llama-7b,64,1,2580.0,1.0,1,A100,1697548456788,1697548459368,120,85.0,20.0,"[6, 898, 66, 50, 62, 61, 49, 49, 61, 526, 73, 61, 48, 49, 49, 205, 47, 58, 48, 57, 57]","[1697548456794, 1697548457692, 1697548457758, 1697548457808, 1697548457870, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458689, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459206, 1697548459254, 1697548459311, 1697548459368]"
316,316,96,0,[],200,llama-7b,64,1,593.0,1.0,1,A100,1697548434174,1697548434767,120,31.0,1.0,"[73, 519]","[1697548434247, 1697548434766]"
317,317,763,3,[],200,llama-7b,64,1,1557.0,1.0,1,A100,1697548444717,1697548446274,120,20.0,1.0,"[35, 1522]","[1697548444752, 1697548446274]"
318,318,791,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548434770,1697548441612,120,,,"[22, 987, 60, 60, 59, 57, 56, 741, 252, 241, 241, 67, 66, 64, 49, 185, 53, 66, 63, 54, 387, 51, 41, 49, 943, 70, 61, 48, 61, 59, 723, 70, 53, 70, 63, 49, 63]","[1697548434792, 1697548435779, 1697548435839, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438030, 1697548438096, 1697548438159, 1697548438213, 1697548438600, 1697548438651, 1697548438692, 1697548438741, 1697548439684, 1697548439754, 1697548439815, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440776, 1697548440829, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
319,319,540,4,[],200,llama-7b,64,1,849.0,1.0,1,A100,1697548446277,1697548447126,120,140.0,5.0,"[16, 638, 62, 44, 44, 45]","[1697548446293, 1697548446931, 1697548446993, 1697548447037, 1697548447081, 1697548447126]"
320,320,39,9,[],200,llama-7b,64,1,337.0,1.0,1,A100,1697548449268,1697548449605,120,8.0,1.0,"[6, 330]","[1697548449274, 1697548449604]"
321,321,672,0,[],200,llama-7b,64,1,4039.0,1.0,1,A100,1697548434174,1697548438213,120,93.0,20.0,"[87, 1319, 198, 61, 60, 59, 57, 57, 740, 252, 241, 241, 67, 66, 64, 49, 185, 54, 65, 64, 53]","[1697548434261, 1697548435580, 1697548435778, 1697548435839, 1697548435899, 1697548435958, 1697548436015, 1697548436072, 1697548436812, 1697548437064, 1697548437305, 1697548437546, 1697548437613, 1697548437679, 1697548437743, 1697548437792, 1697548437977, 1697548438031, 1697548438096, 1697548438160, 1697548438213]"
322,322,717,10,[],200,llama-7b,64,1,2580.0,1.0,1,A100,1697548449606,1697548452186,120,89.0,20.0,"[18, 743, 73, 56, 53, 51, 405, 65, 61, 54, 42, 297, 49, 48, 53, 48, 59, 244, 58, 45, 58]","[1697548449624, 1697548450367, 1697548450440, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451070, 1697548451131, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186]"
323,323,193,5,[],200,llama-7b,64,1,3311.0,1.0,1,A100,1697548447129,1697548450440,120,79.0,20.0,"[6, 849, 63, 61, 59, 57, 56, 769, 67, 58, 45, 45, 56, 45, 307, 60, 44, 54, 53, 557]","[1697548447135, 1697548447984, 1697548448047, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449732, 1697548449776, 1697548449830, 1697548449883, 1697548450440]"
324,324,610,9,[],200,llama-7b,64,1,4010.0,1.0,1,A100,1697548452715,1697548456725,120,89.0,20.0,"[11, 1120, 468, 66, 70, 54, 62, 59, 1119, 71, 76, 73, 70, 68, 52, 67, 248, 69, 52, 70, 65]","[1697548452726, 1697548453846, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455744, 1697548455815, 1697548455891, 1697548455964, 1697548456034, 1697548456102, 1697548456154, 1697548456221, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725]"
325,325,400,2,[],200,llama-7b,64,1,1662.0,1.0,1,A100,1697548436018,1697548437680,120,123.0,7.0,"[9, 391, 395, 252, 240, 242, 67, 66]","[1697548436027, 1697548436418, 1697548436813, 1697548437065, 1697548437305, 1697548437547, 1697548437614, 1697548437680]"
326,326,169,3,[],200,llama-7b,64,1,223.0,1.0,1,A100,1697548437684,1697548437907,120,10.0,1.0,"[12, 211]","[1697548437696, 1697548437907]"
327,327,840,4,[],200,llama-7b,64,1,633.0,1.0,1,A100,1697548437911,1697548438544,120,17.0,1.0,"[26, 607]","[1697548437937, 1697548438544]"
328,328,494,5,[],200,llama-7b,64,1,2283.0,1.0,1,A100,1697548438547,1697548440830,120,6.0,10.0,"[11, 719, 408, 69, 62, 47, 61, 59, 723, 70, 54]","[1697548438558, 1697548439277, 1697548439685, 1697548439754, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440776, 1697548440830]"
329,329,271,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548440832,1697548441611,120,,,"[6, 565]","[1697548440838, 1697548441403]"
330,330,452,2,[],200,llama-7b,64,1,1306.0,1.0,1,A100,1697548441617,1697548442923,120,216.0,4.0,"[125, 1017, 56, 59, 49]","[1697548441742, 1697548442759, 1697548442815, 1697548442874, 1697548442923]"
331,331,855,7,[],200,llama-7b,64,1,3048.0,1.0,1,A100,1697548441615,1697548444663,120,83.0,20.0,"[25, 435, 33, 706, 60, 47, 46, 38, 417, 55, 54, 51, 41, 47, 679, 58, 46, 57, 55, 54, 44]","[1697548441640, 1697548442075, 1697548442108, 1697548442814, 1697548442874, 1697548442921, 1697548442967, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443670, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444565, 1697548444619, 1697548444663]"
332,332,624,8,[],200,llama-7b,64,1,16682.0,1.0,1,A100,1697548444677,1697548461359,120,563.0,119.0,"[19, 592, 251, 310, 54, 41, 41, 367, 61, 62, 57, 46, 56, 359, 45, 44, 43, 57, 801, 63, 63, 58, 57, 56, 769, 67, 58, 45, 45, 56, 45, 307, 59, 45, 54, 53, 556, 56, 53, 52, 405, 64, 61, 55, 41, 298, 49, 48, 53, 48, 58, 245, 58, 45, 58, 54, 1016, 68, 52, 62, 875, 66, 71, 53, 63, 59, 1117, 73, 76, 72, 69, 69, 53, 67, 247, 69, 52, 71, 65, 60, 359, 62, 60, 48, 59, 53, 331, 51, 61, 62, 49, 49, 61, 525, 73, 61, 49, 48, 50, 205, 46, 58, 49, 57, 56, 619, 65, 49, 62, 61, 60, 427, 69, 52, 67, 63, 63, 49, 236, 50]","[1697548444696, 1697548445288, 1697548445539, 1697548445849, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446413, 1697548446475, 1697548446532, 1697548446578, 1697548446634, 1697548446993, 1697548447038, 1697548447082, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448109, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450439, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451780, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453256, 1697548453324, 1697548453376, 1697548453438, 1697548454313, 1697548454379, 1697548454450, 1697548454503, 1697548454566, 1697548454625, 1697548455742, 1697548455815, 1697548455891, 1697548455963, 1697548456032, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457426, 1697548457757, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458749, 1697548458798, 1697548458846, 1697548458896, 1697548459101, 1697548459147, 1697548459205, 1697548459254, 1697548459311, 1697548459367, 1697548459986, 1697548460051, 1697548460100, 1697548460162, 1697548460223, 1697548460283, 1697548460710, 1697548460779, 1697548460831, 1697548460898, 1697548460961, 1697548461024, 1697548461073, 1697548461309, 1697548461359]"
333,333,219,3,[],200,llama-7b,64,1,3060.0,1.0,1,A100,1697548442925,1697548445985,120,90.0,20.0,"[6, 423, 68, 55, 55, 51, 41, 47, 678, 58, 46, 57, 56, 53, 44, 49, 826, 310, 55, 41, 41]","[1697548442931, 1697548443354, 1697548443422, 1697548443477, 1697548443532, 1697548443583, 1697548443624, 1697548443671, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444619, 1697548444663, 1697548444712, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445985]"
334,334,389,3,[],200,llama-7b,64,1,410.0,1.0,1,A100,1697548448112,1697548448522,120,8.0,1.0,"[23, 387]","[1697548448135, 1697548448522]"
335,335,301,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438163,1697548441611,120,,,"[33, 348, 57, 51, 40, 49, 944, 69, 62, 46, 62, 58, 725, 70, 53, 70, 62, 50, 62]","[1697548438196, 1697548438544, 1697548438601, 1697548438652, 1697548438692, 1697548438741, 1697548439685, 1697548439754, 1697548439816, 1697548439862, 1697548439924, 1697548439982, 1697548440707, 1697548440777, 1697548440830, 1697548440900, 1697548440962, 1697548441012, 1697548441074]"
336,336,42,4,[],200,llama-7b,64,1,1080.0,1.0,1,A100,1697548448525,1697548449605,120,10.0,1.0,"[20, 1059]","[1697548448545, 1697548449604]"
337,337,810,4,[],200,llama-7b,64,1,3685.0,1.0,1,A100,1697548445987,1697548449672,120,91.0,20.0,"[14, 930, 62, 44, 44, 44, 58, 800, 63, 62, 58, 57, 56, 771, 67, 57, 45, 45, 56, 45, 307]","[1697548446001, 1697548446931, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447183, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448279, 1697548449050, 1697548449117, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672]"
338,338,742,5,[],200,llama-7b,64,1,2579.0,1.0,1,A100,1697548449607,1697548452186,120,89.0,20.0,"[27, 734, 72, 56, 53, 51, 405, 65, 61, 54, 42, 297, 49, 48, 53, 48, 59, 244, 58, 45, 58]","[1697548449634, 1697548450368, 1697548450440, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451070, 1697548451131, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186]"
339,339,579,5,[],200,llama-7b,64,1,689.0,1.0,1,A100,1697548449679,1697548450368,120,19.0,1.0,"[14, 675]","[1697548449693, 1697548450368]"
340,340,469,3,[],200,llama-7b,64,1,462.0,1.0,1,A100,1697548441614,1697548442076,120,17.0,1.0,"[56, 406]","[1697548441670, 1697548442076]"
341,341,234,6,[],200,llama-7b,64,1,4133.0,1.0,1,A100,1697548450371,1697548454504,120,457.0,25.0,"[22, 542, 70, 65, 60, 55, 42, 297, 49, 48, 53, 49, 58, 244, 58, 45, 58, 54, 1017, 68, 52, 62, 875, 66, 70, 54]","[1697548450393, 1697548450935, 1697548451005, 1697548451070, 1697548451130, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451723, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454504]"
342,342,129,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548449785,1697548488027,120,,,"[6, 576, 73, 56, 53, 51, 405, 64, 62, 54, 42, 297, 49, 48, 53, 49, 58, 244, 58, 45, 58, 54, 1017, 68, 51, 63, 875, 66, 70, 54, 62, 59, 1118, 73, 76, 71, 70, 68, 53, 67, 247, 69, 52, 71, 65, 60, 359, 62, 60, 48, 59, 53, 332, 50, 61, 62, 49, 49, 61, 525, 73, 62, 48, 48, 50, 205, 46, 58, 49, 57, 56, 619, 64, 50, 63, 60, 60, 427, 69, 52, 67, 64, 62, 49, 237, 49, 62, 62, 61, 61, 525, 176, 50, 57, 57, 54, 381, 57, 57, 53, 821, 62, 60, 58, 56, 729, 460, 64, 49, 60, 45, 702, 72, 69, 53, 65, 63, 49, 689, 74, 70, 68, 67, 51, 63, 359, 73, 54, 70, 70, 65, 535, 65, 52, 66, 64, 62, 60, 569, 69, 52, 68, 50, 52, 65, 65, 844, 73, 71, 70, 67, 49, 66, 312, 73, 74, 72, 67, 64, 48, 305, 70, 69, 68, 52, 66, 48, 530, 67, 50, 50, 65, 58, 346, 68, 67, 51, 66, 65, 62, 594, 72, 71, 70, 69, 67, 64, 861, 512, 288, 74, 57, 63, 618, 251, 54, 253, 71, 68, 62, 613, 255, 71, 55, 71, 68, 62, 912, 76, 74, 73, 71, 67, 68, 587, 269, 63, 74, 264, 69, 51, 68, 839, 75, 55, 73, 66, 66, 49, 338, 76, 79, 71, 70, 57, 70, 838, 313, 308, 77, 76, 71, 70, 324, 59, 59, 79, 306, 72, 69]","[1697548449791, 1697548450367, 1697548450440, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451131, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451723, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453376, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457426, 1697548457758, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458750, 1697548458798, 1697548458846, 1697548458896, 1697548459101, 1697548459147, 1697548459205, 1697548459254, 1697548459311, 1697548459367, 1697548459986, 1697548460050, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460710, 1697548460779, 1697548460831, 1697548460898, 1697548460962, 1697548461024, 1697548461073, 1697548461310, 1697548461359, 1697548461421, 1697548461483, 1697548461544, 1697548461605, 1697548462130, 1697548462306, 1697548462356, 1697548462413, 1697548462470, 1697548462524, 1697548462905, 1697548462962, 1697548463019, 1697548463072, 1697548463893, 1697548463955, 1697548464015, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465431, 1697548465491, 1697548465536, 1697548466238, 1697548466310, 1697548466379, 1697548466432, 1697548466497, 1697548466560, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468247, 1697548468317, 1697548468382, 1697548468917, 1697548468982, 1697548469034, 1697548469100, 1697548469164, 1697548469226, 1697548469286, 1697548469855, 1697548469924, 1697548469976, 1697548470044, 1697548470094, 1697548470146, 1697548470211, 1697548470276, 1697548471120, 1697548471193, 1697548471264, 1697548471334, 1697548471401, 1697548471450, 1697548471516, 1697548471828, 1697548471901, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472226, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472904, 1697548473434, 1697548473501, 1697548473551, 1697548473601, 1697548473666, 1697548473724, 1697548474070, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474449, 1697548475043, 1697548475115, 1697548475186, 1697548475256, 1697548475325, 1697548475392, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477191, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478234, 1697548478487, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479556, 1697548479627, 1697548479682, 1697548479753, 1697548479821, 1697548479883, 1697548480795, 1697548480871, 1697548480945, 1697548481018, 1697548481089, 1697548481156, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484385, 1697548484456, 1697548484526, 1697548484583, 1697548484653, 1697548485491, 1697548485804, 1697548486112, 1697548486189, 1697548486265, 1697548486336, 1697548486406, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487233, 1697548487305, 1697548487374]"
343,343,130,4,[],200,llama-7b,64,1,680.0,1.0,1,A100,1697548442079,1697548442759,120,14.0,1.0,"[15, 665]","[1697548442094, 1697548442759]"
344,344,342,9,[],200,llama-7b,64,1,3928.0,1.0,1,A100,1697548451964,1697548455892,120,364.0,14.0,"[20, 727, 546, 68, 52, 62, 875, 65, 72, 53, 63, 59, 1118, 72, 76]","[1697548451984, 1697548452711, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454379, 1697548454451, 1697548454504, 1697548454567, 1697548454626, 1697548455744, 1697548455816, 1697548455892]"
345,345,825,5,[],200,llama-7b,64,1,3221.0,1.0,1,A100,1697548442764,1697548445985,120,96.0,20.0,"[37, 553, 68, 55, 54, 51, 42, 47, 678, 58, 46, 58, 54, 54, 44, 50, 825, 310, 55, 41, 41]","[1697548442801, 1697548443354, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443624, 1697548443671, 1697548444349, 1697548444407, 1697548444453, 1697548444511, 1697548444565, 1697548444619, 1697548444663, 1697548444713, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445985]"
346,346,1,10,[],200,llama-7b,64,1,5066.0,1.0,1,A100,1697548455896,1697548460962,120,47.0,43.0,"[6, 491, 76, 69, 52, 70, 66, 60, 359, 62, 59, 48, 59, 54, 331, 51, 61, 62, 49, 48, 62, 525, 72, 62, 48, 49, 49, 205, 47, 58, 48, 57, 57, 619, 64, 49, 64, 59, 60, 427, 70, 52, 67, 63]","[1697548455902, 1697548456393, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456726, 1697548456786, 1697548457145, 1697548457207, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457809, 1697548457870, 1697548457932, 1697548457981, 1697548458029, 1697548458091, 1697548458616, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459206, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460100, 1697548460164, 1697548460223, 1697548460283, 1697548460710, 1697548460780, 1697548460832, 1697548460899, 1697548460962]"
347,347,780,9,[],200,llama-7b,64,1,3346.0,1.0,1,A100,1697548453380,1697548456726,120,85.0,20.0,"[18, 448, 469, 65, 71, 54, 62, 59, 1117, 73, 76, 71, 70, 68, 53, 67, 247, 70, 52, 70, 66]","[1697548453398, 1697548453846, 1697548454315, 1697548454380, 1697548454451, 1697548454505, 1697548454567, 1697548454626, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456538, 1697548456590, 1697548456660, 1697548456726]"
348,348,404,6,[],200,llama-7b,64,1,4280.0,1.0,1,A100,1697548452189,1697548456469,120,87.0,20.0,"[16, 507, 545, 68, 52, 62, 875, 66, 71, 53, 62, 59, 1119, 73, 75, 72, 70, 68, 52, 68, 247]","[1697548452205, 1697548452712, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454380, 1697548454451, 1697548454504, 1697548454566, 1697548454625, 1697548455744, 1697548455817, 1697548455892, 1697548455964, 1697548456034, 1697548456102, 1697548456154, 1697548456222, 1697548456469]"
349,349,401,4,[],200,llama-7b,64,1,2819.0,1.0,1,A100,1697548449367,1697548452186,120,84.0,20.0,"[11, 989, 73, 56, 53, 51, 405, 64, 62, 54, 42, 297, 49, 48, 53, 48, 59, 244, 58, 45, 58]","[1697548449378, 1697548450367, 1697548450440, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451131, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186]"
350,350,494,5,[],200,llama-7b,64,1,1691.0,1.0,1,A100,1697548442762,1697548444453,120,6.0,10.0,"[11, 580, 69, 55, 54, 51, 41, 48, 678, 58, 46]","[1697548442773, 1697548443353, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443671, 1697548444349, 1697548444407, 1697548444453]"
351,351,155,6,[],200,llama-7b,64,1,3652.0,1.0,1,A100,1697548444456,1697548448108,120,90.0,20.0,"[15, 817, 250, 310, 55, 41, 40, 369, 60, 61, 58, 46, 55, 360, 45, 43, 44, 58, 800, 63, 62]","[1697548444471, 1697548445288, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446353, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446633, 1697548446993, 1697548447038, 1697548447081, 1697548447125, 1697548447183, 1697548447983, 1697548448046, 1697548448108]"
352,352,218,6,[],200,llama-7b,64,1,1795.0,1.0,1,A100,1697548451462,1697548453257,120,109.0,7.0,"[10, 554, 58, 45, 57, 54, 1017]","[1697548451472, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453257]"
353,353,575,1,[],200,llama-7b,64,1,3049.0,1.0,1,A100,1697548441614,1697548444663,120,86.0,20.0,"[11, 450, 33, 706, 59, 48, 46, 38, 417, 54, 55, 51, 41, 47, 679, 58, 46, 57, 55, 54, 44]","[1697548441625, 1697548442075, 1697548442108, 1697548442814, 1697548442873, 1697548442921, 1697548442967, 1697548443005, 1697548443422, 1697548443476, 1697548443531, 1697548443582, 1697548443623, 1697548443670, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444565, 1697548444619, 1697548444663]"
354,354,149,5,[],200,llama-7b,64,1,2377.0,1.0,1,A100,1697548452190,1697548454567,120,563.0,10.0,"[20, 502, 545, 68, 52, 62, 875, 66, 71, 53, 62]","[1697548452210, 1697548452712, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454380, 1697548454451, 1697548454504, 1697548454566]"
355,355,699,11,[],200,llama-7b,64,1,278.0,1.0,1,A100,1697548460965,1697548461243,120,39.0,1.0,"[14, 264]","[1697548460979, 1697548461243]"
356,356,801,7,[],200,llama-7b,64,1,3466.0,1.0,1,A100,1697548453260,1697548456726,120,47.0,20.0,"[15, 571, 468, 66, 71, 54, 62, 59, 1117, 73, 76, 71, 70, 68, 53, 67, 247, 70, 52, 70, 66]","[1697548453275, 1697548453846, 1697548454314, 1697548454380, 1697548454451, 1697548454505, 1697548454567, 1697548454626, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456538, 1697548456590, 1697548456660, 1697548456726]"
357,357,361,12,[],200,llama-7b,64,1,1280.0,1.0,1,A100,1697548461245,1697548462525,120,67.0,7.0,"[7, 683, 196, 175, 51, 57, 57, 54]","[1697548461252, 1697548461935, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462525]"
358,358,747,3,[],200,llama-7b,64,1,5565.0,1.0,1,A100,1697548441617,1697548447182,120,140.0,36.0,"[167, 976, 54, 60, 48, 46, 38, 417, 54, 54, 52, 40, 47, 680, 58, 46, 57, 55, 54, 47, 46, 825, 310, 55, 40, 41, 367, 62, 61, 58, 46, 55, 359, 45, 44, 44, 57]","[1697548441784, 1697548442760, 1697548442814, 1697548442874, 1697548442922, 1697548442968, 1697548443006, 1697548443423, 1697548443477, 1697548443531, 1697548443583, 1697548443623, 1697548443670, 1697548444350, 1697548444408, 1697548444454, 1697548444511, 1697548444566, 1697548444620, 1697548444667, 1697548444713, 1697548445538, 1697548445848, 1697548445903, 1697548445943, 1697548445984, 1697548446351, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446633, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182]"
359,359,132,13,[],200,llama-7b,64,1,4771.0,1.0,1,A100,1697548462528,1697548467299,120,100.0,20.0,"[17, 1035, 313, 63, 60, 57, 56, 729, 461, 64, 49, 59, 45, 703, 71, 70, 54, 64, 63, 48, 690]","[1697548462545, 1697548463580, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465319, 1697548465383, 1697548465432, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466434, 1697548466498, 1697548466561, 1697548466609, 1697548467299]"
360,360,907,1,[],200,llama-7b,64,1,460.0,1.0,1,A100,1697548441615,1697548442075,120,10.0,1.0,"[45, 415]","[1697548441660, 1697548442075]"
361,361,545,8,[],200,llama-7b,64,1,586.0,1.0,1,A100,1697548456729,1697548457315,120,216.0,5.0,"[30, 318, 68, 62, 60, 48]","[1697548456759, 1697548457077, 1697548457145, 1697548457207, 1697548457267, 1697548457315]"
362,362,561,2,[],200,llama-7b,64,1,2634.0,1.0,1,A100,1697548442079,1697548444713,120,87.0,20.0,"[10, 670, 56, 60, 47, 46, 37, 418, 54, 54, 51, 41, 48, 678, 58, 46, 58, 54, 54, 44, 50]","[1697548442089, 1697548442759, 1697548442815, 1697548442875, 1697548442922, 1697548442968, 1697548443005, 1697548443423, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443671, 1697548444349, 1697548444407, 1697548444453, 1697548444511, 1697548444565, 1697548444619, 1697548444663, 1697548444713]"
363,363,200,9,[],200,llama-7b,64,1,1299.0,1.0,1,A100,1697548457317,1697548458616,120,6.0,9.0,"[13, 363, 65, 50, 62, 62, 49, 48, 62, 525]","[1697548457330, 1697548457693, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457981, 1697548458029, 1697548458091, 1697548458616]"
364,364,54,8,[],200,llama-7b,64,1,2127.0,1.0,1,A100,1697548455965,1697548458092,120,87.0,20.0,"[16, 412, 76, 69, 52, 70, 66, 60, 359, 62, 59, 48, 59, 54, 331, 51, 61, 62, 49, 48, 62]","[1697548455981, 1697548456393, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456726, 1697548456786, 1697548457145, 1697548457207, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457809, 1697548457870, 1697548457932, 1697548457981, 1697548458029, 1697548458091]"
365,365,874,8,[],200,llama-7b,64,1,7340.0,1.0,1,A100,1697548452028,1697548459368,120,140.0,50.0,"[10, 674, 545, 68, 52, 62, 875, 65, 72, 53, 63, 58, 1119, 72, 76, 72, 70, 68, 52, 68, 247, 69, 52, 70, 65, 61, 358, 62, 60, 48, 59, 53, 332, 50, 61, 62, 49, 49, 61, 525, 73, 62, 48, 49, 49, 205, 46, 58, 49, 57, 57]","[1697548452038, 1697548452712, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454379, 1697548454451, 1697548454504, 1697548454567, 1697548454625, 1697548455744, 1697548455816, 1697548455892, 1697548455964, 1697548456034, 1697548456102, 1697548456154, 1697548456222, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456786, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457426, 1697548457758, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459147, 1697548459205, 1697548459254, 1697548459311, 1697548459368]"
366,366,1,14,[],200,llama-7b,64,1,7840.0,1.0,1,A100,1697548461077,1697548468917,120,47.0,43.0,"[21, 837, 196, 175, 51, 57, 57, 53, 382, 55, 59, 52, 821, 63, 60, 57, 56, 729, 460, 64, 50, 59, 45, 703, 71, 70, 53, 65, 63, 48, 689, 74, 70, 68, 67, 51, 63, 360, 72, 54, 71, 70, 64, 535]","[1697548461098, 1697548461935, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462524, 1697548462906, 1697548462961, 1697548463020, 1697548463072, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466433, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468318, 1697548468382, 1697548468917]"
367,367,899,10,[],200,llama-7b,64,1,2456.0,1.0,1,A100,1697548458618,1697548461074,120,100.0,20.0,"[6, 414, 64, 46, 61, 45, 58, 56, 619, 64, 49, 63, 60, 60, 429, 68, 52, 68, 62, 63, 49]","[1697548458624, 1697548459038, 1697548459102, 1697548459148, 1697548459209, 1697548459254, 1697548459312, 1697548459368, 1697548459987, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460712, 1697548460780, 1697548460832, 1697548460900, 1697548460962, 1697548461025, 1697548461074]"
368,368,52,6,[],200,llama-7b,64,1,1587.0,1.0,1,A100,1697548446637,1697548448224,120,58.0,6.0,"[16, 822, 508, 63, 62, 58, 58]","[1697548446653, 1697548447475, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448224]"
369,369,217,1,[],200,llama-7b,64,1,3350.0,1.0,1,A100,1697548437549,1697548440899,120,85.0,20.0,"[16, 342, 71, 53, 65, 64, 54, 386, 51, 40, 50, 943, 71, 61, 47, 61, 59, 723, 71, 53, 69]","[1697548437565, 1697548437907, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438214, 1697548438600, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439755, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440830, 1697548440899]"
370,370,747,7,[],200,llama-7b,64,1,5150.0,1.0,1,A100,1697548448227,1697548453377,120,140.0,36.0,"[6, 289, 528, 66, 58, 46, 45, 56, 44, 307, 59, 45, 54, 53, 557, 55, 53, 52, 405, 64, 61, 55, 41, 298, 49, 48, 52, 49, 59, 244, 59, 44, 58, 54, 1017, 68, 51]","[1697548448233, 1697548448522, 1697548449050, 1697548449116, 1697548449174, 1697548449220, 1697548449265, 1697548449321, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450440, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451673, 1697548451722, 1697548451781, 1697548452025, 1697548452084, 1697548452128, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453376]"
371,371,559,11,[],200,llama-7b,64,1,4355.0,1.0,1,A100,1697548461077,1697548465432,120,86.0,20.0,"[11, 847, 196, 175, 50, 58, 56, 54, 382, 56, 58, 53, 820, 63, 60, 57, 56, 729, 460, 64, 50]","[1697548461088, 1697548461935, 1697548462131, 1697548462306, 1697548462356, 1697548462414, 1697548462470, 1697548462524, 1697548462906, 1697548462962, 1697548463020, 1697548463073, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465432]"
372,372,673,2,[],200,llama-7b,64,1,3052.0,1.0,1,A100,1697548441615,1697548444667,120,93.0,20.0,"[50, 410, 33, 706, 60, 48, 46, 37, 417, 55, 54, 51, 41, 47, 679, 58, 46, 57, 56, 53, 44]","[1697548441665, 1697548442075, 1697548442108, 1697548442814, 1697548442874, 1697548442922, 1697548442968, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443670, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444619, 1697548444663]"
373,373,242,8,[],200,llama-7b,64,1,1356.0,1.0,1,A100,1697548443357,1697548444713,120,345.0,9.0,"[31, 752, 210, 58, 46, 56, 56, 54, 43, 50]","[1697548443388, 1697548444140, 1697548444350, 1697548444408, 1697548444454, 1697548444510, 1697548444566, 1697548444620, 1697548444663, 1697548444713]"
374,374,253,3,[],200,llama-7b,64,1,3097.0,1.0,1,A100,1697548441616,1697548444713,120,67.0,20.0,"[134, 1009, 55, 60, 47, 46, 38, 418, 54, 55, 51, 41, 47, 679, 57, 46, 57, 56, 54, 47, 45]","[1697548441750, 1697548442759, 1697548442814, 1697548442874, 1697548442921, 1697548442967, 1697548443005, 1697548443423, 1697548443477, 1697548443532, 1697548443583, 1697548443624, 1697548443671, 1697548444350, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444620, 1697548444667, 1697548444712]"
375,375,394,11,[],200,llama-7b,64,1,730.0,1.0,1,A100,1697548451232,1697548451962,120,11.0,1.0,"[7, 723]","[1697548451239, 1697548451962]"
376,376,439,14,[],200,llama-7b,64,1,713.0,1.0,1,A100,1697548468321,1697548469034,120,13.0,4.0,"[11, 339, 246, 66, 51]","[1697548468332, 1697548468671, 1697548468917, 1697548468983, 1697548469034]"
377,377,231,7,[],200,llama-7b,64,1,652.0,1.0,1,A100,1697548446279,1697548446931,120,13.0,1.0,"[19, 633]","[1697548446298, 1697548446931]"
378,378,821,8,[],200,llama-7b,64,1,3507.0,1.0,1,A100,1697548446933,1697548450440,120,85.0,20.0,"[6, 536, 509, 62, 62, 59, 57, 56, 769, 67, 58, 45, 46, 55, 45, 308, 59, 44, 54, 53, 557]","[1697548446939, 1697548447475, 1697548447984, 1697548448046, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449265, 1697548449320, 1697548449365, 1697548449673, 1697548449732, 1697548449776, 1697548449830, 1697548449883, 1697548450440]"
379,379,38,18,[],200,llama-7b,64,1,2961.0,1.0,1,A100,1697548470705,1697548473666,120,88.0,20.0,"[14, 1032, 77, 74, 73, 72, 68, 63, 49, 304, 71, 68, 68, 52, 66, 49, 529, 68, 49, 50, 65]","[1697548470719, 1697548471751, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472905, 1697548473434, 1697548473502, 1697548473551, 1697548473601, 1697548473666]"
380,380,168,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548453386,1697548488024,120,,,"[23, 437, 469, 65, 71, 54, 62, 59, 1117, 73, 76, 71, 70, 68, 53, 67, 247, 69, 52, 71, 66, 60, 358, 62, 60, 48, 59, 54, 331, 50, 62, 62, 48, 49, 61, 525, 73, 62, 48, 49, 49, 205, 47, 58, 48, 57, 57, 618, 65, 49, 63, 60, 60, 427, 70, 52, 67, 63, 63, 48, 236, 50, 63, 61, 61, 61, 526, 175, 50, 58, 56, 54, 381, 57, 57, 53, 821, 62, 61, 57, 56, 729, 459, 65, 49, 60, 45, 702, 72, 69, 54, 64, 64, 48, 689, 73, 71, 68, 67, 51, 63, 359, 73, 54, 70, 70, 65, 535, 65, 52, 66, 64, 63, 59, 569, 69, 52, 68, 51, 52, 64, 65, 844, 73, 71, 70, 67, 49, 66, 312, 74, 73, 72, 67, 64, 48, 305, 70, 69, 68, 52, 65, 49, 531, 66, 50, 50, 65, 58, 346, 68, 67, 51, 66, 65, 62, 594, 72, 71, 70, 69, 68, 63, 861, 512, 288, 74, 57, 63, 618, 251, 54, 254, 70, 68, 62, 613, 255, 72, 54, 71, 68, 63, 911, 76, 75, 71, 72, 68, 67, 587, 269, 63, 74, 264, 69, 51, 68, 839, 75, 55, 73, 66, 66, 49, 338, 76, 75, 75, 70, 57, 71, 837, 314, 308, 76, 76, 71, 70, 324, 59, 59, 79, 306, 72, 70]","[1697548453409, 1697548453846, 1697548454315, 1697548454380, 1697548454451, 1697548454505, 1697548454567, 1697548454626, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456660, 1697548456726, 1697548456786, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459206, 1697548459254, 1697548459311, 1697548459368, 1697548459986, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460710, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461309, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462356, 1697548462414, 1697548462470, 1697548462524, 1697548462905, 1697548462962, 1697548463019, 1697548463072, 1697548463893, 1697548463955, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465317, 1697548465382, 1697548465431, 1697548465491, 1697548465536, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466561, 1697548466609, 1697548467298, 1697548467371, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468247, 1697548468317, 1697548468382, 1697548468917, 1697548468982, 1697548469034, 1697548469100, 1697548469164, 1697548469227, 1697548469286, 1697548469855, 1697548469924, 1697548469976, 1697548470044, 1697548470095, 1697548470147, 1697548470211, 1697548470276, 1697548471120, 1697548471193, 1697548471264, 1697548471334, 1697548471401, 1697548471450, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472226, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472855, 1697548472904, 1697548473435, 1697548473501, 1697548473551, 1697548473601, 1697548473666, 1697548473724, 1697548474070, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474449, 1697548475043, 1697548475115, 1697548475186, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477191, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479556, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481017, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484526, 1697548484583, 1697548484654, 1697548485491, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486336, 1697548486406, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487233, 1697548487305, 1697548487375]"
381,381,164,12,[],200,llama-7b,64,1,746.0,1.0,1,A100,1697548451966,1697548452712,120,15.0,1.0,"[38, 708]","[1697548452004, 1697548452712]"
382,382,9,5,[],200,llama-7b,64,1,2828.0,1.0,1,A100,1697548450550,1697548453378,120,85.0,20.0,"[6, 379, 71, 64, 61, 54, 42, 298, 48, 49, 52, 49, 58, 244, 58, 45, 58, 54, 1017, 68, 52]","[1697548450556, 1697548450935, 1697548451006, 1697548451070, 1697548451131, 1697548451185, 1697548451227, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451723, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453377]"
383,383,189,3,[],200,llama-7b,64,1,3048.0,1.0,1,A100,1697548441615,1697548444663,120,88.0,20.0,"[40, 421, 32, 706, 60, 47, 46, 38, 417, 55, 54, 51, 41, 47, 679, 58, 46, 57, 55, 54, 44]","[1697548441655, 1697548442076, 1697548442108, 1697548442814, 1697548442874, 1697548442921, 1697548442967, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443670, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444565, 1697548444619, 1697548444663]"
384,384,328,1,[],200,llama-7b,64,1,1524.0,1.0,1,A100,1697548438162,1697548439686,120,109.0,6.0,"[24, 358, 57, 51, 40, 49, 944]","[1697548438186, 1697548438544, 1697548438601, 1697548438652, 1697548438692, 1697548438741, 1697548439685]"
385,385,60,3,[],200,llama-7b,64,1,6353.0,1.0,1,A100,1697548444716,1697548451069,120,93.0,36.0,"[16, 1620, 62, 61, 58, 45, 56, 358, 45, 45, 43, 57, 802, 62, 63, 58, 57, 56, 770, 66, 58, 45, 45, 56, 45, 307, 59, 45, 54, 53, 556, 56, 53, 52, 405, 64]","[1697548444732, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634, 1697548446992, 1697548447037, 1697548447082, 1697548447125, 1697548447182, 1697548447984, 1697548448046, 1697548448109, 1697548448167, 1697548448224, 1697548448280, 1697548449050, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450439, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069]"
386,386,866,4,[],200,llama-7b,64,1,3439.0,1.0,1,A100,1697548444670,1697548448109,120,93.0,20.0,"[16, 602, 250, 310, 55, 41, 41, 367, 61, 61, 58, 46, 55, 360, 45, 44, 44, 57, 800, 63, 62]","[1697548444686, 1697548445288, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446633, 1697548446993, 1697548447038, 1697548447082, 1697548447126, 1697548447183, 1697548447983, 1697548448046, 1697548448108]"
387,387,729,6,[],200,llama-7b,64,1,1174.0,1.0,1,A100,1697548454570,1697548455744,120,874.0,2.0,"[6, 1168]","[1697548454576, 1697548455744]"
388,388,507,7,[],200,llama-7b,64,1,2344.0,1.0,1,A100,1697548455747,1697548458091,120,83.0,20.0,"[13, 633, 76, 69, 52, 70, 66, 60, 359, 62, 59, 48, 59, 54, 331, 51, 61, 62, 49, 48, 62]","[1697548455760, 1697548456393, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456726, 1697548456786, 1697548457145, 1697548457207, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457809, 1697548457870, 1697548457932, 1697548457981, 1697548458029, 1697548458091]"
389,389,152,11,[],200,llama-7b,64,1,3153.0,1.0,1,A100,1697548459371,1697548462524,120,87.0,20.0,"[33, 1219, 88, 69, 52, 67, 63, 63, 49, 237, 48, 63, 61, 61, 61, 526, 176, 50, 57, 57, 53]","[1697548459404, 1697548460623, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074, 1697548461311, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462307, 1697548462357, 1697548462414, 1697548462471, 1697548462524]"
390,390,590,9,[],200,llama-7b,64,1,2935.0,1.0,1,A100,1697548450443,1697548453378,120,88.0,20.0,"[24, 467, 72, 64, 60, 55, 41, 299, 48, 48, 53, 49, 58, 244, 59, 45, 57, 54, 1017, 68, 52]","[1697548450467, 1697548450934, 1697548451006, 1697548451070, 1697548451130, 1697548451185, 1697548451226, 1697548451525, 1697548451573, 1697548451621, 1697548451674, 1697548451723, 1697548451781, 1697548452025, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453377]"
391,391,643,9,[],200,llama-7b,64,1,943.0,1.0,1,A100,1697548458095,1697548459038,120,18.0,1.0,"[15, 927]","[1697548458110, 1697548459037]"
392,392,415,10,[],200,llama-7b,64,1,3980.0,1.0,1,A100,1697548459040,1697548463020,120,109.0,29.0,"[7, 940, 64, 50, 62, 60, 61, 426, 69, 53, 66, 63, 63, 50, 236, 50, 62, 62, 61, 61, 525, 175, 51, 57, 57, 53, 381, 57, 57]","[1697548459047, 1697548459987, 1697548460051, 1697548460101, 1697548460163, 1697548460223, 1697548460284, 1697548460710, 1697548460779, 1697548460832, 1697548460898, 1697548460961, 1697548461024, 1697548461074, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461606, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462524, 1697548462905, 1697548462962, 1697548463019]"
393,393,336,7,[],200,llama-7b,64,1,1682.0,1.0,1,A100,1697548444670,1697548446352,120,58.0,7.0,"[16, 603, 249, 310, 55, 41, 41, 367]","[1697548444686, 1697548445289, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445985, 1697548446352]"
394,394,738,19,[],200,llama-7b,64,1,654.0,1.0,1,A100,1697548473669,1697548474323,120,79.0,6.0,"[11, 391, 68, 67, 51, 66]","[1697548473680, 1697548474071, 1697548474139, 1697548474206, 1697548474257, 1697548474323]"
395,395,246,10,[],200,llama-7b,64,1,6604.0,1.0,1,A100,1697548453383,1697548459987,120,58.0,47.0,"[21, 442, 469, 65, 71, 54, 62, 59, 1117, 73, 76, 71, 70, 68, 53, 67, 247, 69, 52, 71, 66, 60, 358, 62, 60, 48, 59, 53, 332, 50, 62, 62, 48, 49, 61, 525, 73, 62, 48, 49, 49, 205, 47, 57, 49, 57, 57, 618]","[1697548453404, 1697548453846, 1697548454315, 1697548454380, 1697548454451, 1697548454505, 1697548454567, 1697548454626, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456660, 1697548456726, 1697548456786, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457426, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459205, 1697548459254, 1697548459311, 1697548459368, 1697548459986]"
396,396,391,20,[],200,llama-7b,64,1,4302.0,1.0,1,A100,1697548474326,1697548478628,120,79.0,20.0,"[6, 712, 72, 71, 70, 69, 67, 64, 860, 512, 288, 75, 56, 63, 619, 251, 53, 254, 71, 69]","[1697548474332, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475393, 1697548475457, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248, 1697548477311, 1697548477930, 1697548478181, 1697548478234, 1697548478488, 1697548478559, 1697548478628]"
397,397,627,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439864,1697548441611,120,,,"[7, 359, 477, 70, 54, 68, 64, 48, 63]","[1697548439871, 1697548440230, 1697548440707, 1697548440777, 1697548440831, 1697548440899, 1697548440963, 1697548441011, 1697548441074]"
398,398,169,21,[],200,llama-7b,64,1,460.0,1.0,1,A100,1697548478631,1697548479091,120,10.0,1.0,"[28, 432]","[1697548478659, 1697548479091]"
399,399,714,14,[],200,llama-7b,64,1,2845.0,1.0,1,A100,1697548467302,1697548470147,120,83.0,20.0,"[21, 648, 81, 72, 53, 71, 71, 64, 534, 65, 51, 67, 65, 62, 59, 569, 70, 53, 67, 49, 52]","[1697548467323, 1697548467971, 1697548468052, 1697548468124, 1697548468177, 1697548468248, 1697548468319, 1697548468383, 1697548468917, 1697548468982, 1697548469033, 1697548469100, 1697548469165, 1697548469227, 1697548469286, 1697548469855, 1697548469925, 1697548469978, 1697548470045, 1697548470094, 1697548470146]"
400,400,751,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548479094,1697548488025,120,,,"[25, 1677, 76, 74, 72, 72, 67, 67, 587, 270, 62, 74, 265, 68, 52, 67, 839, 76, 55, 72, 66, 66, 49, 338, 76, 79, 71, 70, 58, 69, 839, 314, 308, 76, 76, 72, 69, 324, 59, 59, 79, 306, 71, 70]","[1697548479119, 1697548480796, 1697548480872, 1697548480946, 1697548481018, 1697548481090, 1697548481157, 1697548481224, 1697548481811, 1697548482081, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482602, 1697548482669, 1697548483508, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484385, 1697548484456, 1697548484526, 1697548484584, 1697548484653, 1697548485492, 1697548485806, 1697548486114, 1697548486190, 1697548486266, 1697548486338, 1697548486407, 1697548486731, 1697548486790, 1697548486849, 1697548486928, 1697548487234, 1697548487305, 1697548487375]"
401,401,400,6,[],200,llama-7b,64,1,1918.0,1.0,1,A100,1697548444716,1697548446634,120,123.0,7.0,"[32, 1526, 78, 62, 61, 58, 45, 56]","[1697548444748, 1697548446274, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634]"
402,402,148,7,[],200,llama-7b,64,1,839.0,1.0,1,A100,1697548446637,1697548447476,120,16.0,1.0,"[15, 823]","[1697548446652, 1697548447475]"
403,403,657,8,[],200,llama-7b,64,1,348.0,1.0,1,A100,1697548456729,1697548457077,120,10.0,1.0,"[35, 313]","[1697548456764, 1697548457077]"
404,404,287,2,[],200,llama-7b,64,1,461.0,1.0,1,A100,1697548441615,1697548442076,120,10.0,1.0,"[30, 431]","[1697548441645, 1697548442076]"
405,405,57,3,[],200,llama-7b,64,1,680.0,1.0,1,A100,1697548442080,1697548442760,120,13.0,1.0,"[19, 660]","[1697548442099, 1697548442759]"
406,406,309,9,[],200,llama-7b,64,1,2288.0,1.0,1,A100,1697548457080,1697548459368,120,52.0,20.0,"[24, 588, 66, 50, 62, 62, 49, 48, 62, 525, 73, 61, 48, 49, 50, 204, 47, 57, 49, 57, 57]","[1697548457104, 1697548457692, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457981, 1697548458029, 1697548458091, 1697548458616, 1697548458689, 1697548458750, 1697548458798, 1697548458847, 1697548458897, 1697548459101, 1697548459148, 1697548459205, 1697548459254, 1697548459311, 1697548459368]"
407,407,487,15,[],200,llama-7b,64,1,2453.0,1.0,1,A100,1697548470149,1697548472602,120,123.0,17.0,"[16, 956, 73, 71, 69, 68, 49, 66, 311, 74, 73, 72, 67, 64, 49, 304, 70]","[1697548470165, 1697548471121, 1697548471194, 1697548471265, 1697548471334, 1697548471402, 1697548471451, 1697548471517, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472227, 1697548472531, 1697548472601]"
408,408,646,4,[],200,llama-7b,64,1,590.0,1.0,1,A100,1697548442764,1697548443354,120,14.0,1.0,"[24, 565]","[1697548442788, 1697548443353]"
409,409,141,16,[],200,llama-7b,64,1,2790.0,1.0,1,A100,1697548472604,1697548475394,120,89.0,20.0,"[16, 566, 249, 67, 49, 51, 65, 58, 345, 68, 68, 50, 67, 64, 63, 594, 72, 71, 70, 69, 68]","[1697548472620, 1697548473186, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473667, 1697548473725, 1697548474070, 1697548474138, 1697548474206, 1697548474256, 1697548474323, 1697548474387, 1697548474450, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475394]"
410,410,415,5,[],200,llama-7b,64,1,4810.0,1.0,1,A100,1697548443357,1697548448167,120,109.0,29.0,"[30, 962, 59, 46, 56, 56, 54, 43, 49, 826, 310, 55, 41, 41, 367, 62, 61, 58, 45, 56, 358, 45, 44, 44, 57, 801, 63, 62, 58]","[1697548443387, 1697548444349, 1697548444408, 1697548444454, 1697548444510, 1697548444566, 1697548444620, 1697548444663, 1697548444712, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448108, 1697548448166]"
411,411,75,6,[],200,llama-7b,64,1,2836.0,1.0,1,A100,1697548448169,1697548451005,120,345.0,18.0,"[6, 347, 528, 66, 58, 45, 46, 56, 44, 307, 59, 45, 54, 53, 557, 56, 52, 52, 405]","[1697548448175, 1697548448522, 1697548449050, 1697548449116, 1697548449174, 1697548449219, 1697548449265, 1697548449321, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450440, 1697548450496, 1697548450548, 1697548450600, 1697548451005]"
412,412,777,7,[],200,llama-7b,64,1,452.0,1.0,1,A100,1697548451007,1697548451459,120,9.0,1.0,"[6, 446]","[1697548451013, 1697548451459]"
413,413,432,8,[],200,llama-7b,64,1,500.0,1.0,1,A100,1697548451462,1697548451962,120,13.0,1.0,"[30, 469]","[1697548451492, 1697548451961]"
414,414,180,9,[],200,llama-7b,64,1,3780.0,1.0,1,A100,1697548451964,1697548455744,120,123.0,12.0,"[6, 741, 546, 68, 51, 63, 876, 66, 70, 52, 64, 59, 1117]","[1697548451970, 1697548452711, 1697548453257, 1697548453325, 1697548453376, 1697548453439, 1697548454315, 1697548454381, 1697548454451, 1697548454503, 1697548454567, 1697548454626, 1697548455743]"
415,415,25,4,[],200,llama-7b,64,1,422.0,1.0,1,A100,1697548445852,1697548446274,120,12.0,1.0,"[14, 408]","[1697548445866, 1697548446274]"
416,416,71,6,[],200,llama-7b,64,1,1633.0,1.0,1,A100,1697548442877,1697548444510,120,364.0,11.0,"[13, 464, 68, 55, 55, 50, 42, 47, 678, 58, 46, 57]","[1697548442890, 1697548443354, 1697548443422, 1697548443477, 1697548443532, 1697548443582, 1697548443624, 1697548443671, 1697548444349, 1697548444407, 1697548444453, 1697548444510]"
417,417,763,10,[],200,llama-7b,64,1,646.0,1.0,1,A100,1697548455747,1697548456393,120,20.0,1.0,"[17, 629]","[1697548455764, 1697548456393]"
418,418,380,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438163,1697548441611,120,,,"[8, 429, 52, 40, 49, 944, 68, 63, 46, 61, 60, 724, 70, 53, 70, 62, 50, 62]","[1697548438171, 1697548438600, 1697548438652, 1697548438692, 1697548438741, 1697548439685, 1697548439753, 1697548439816, 1697548439862, 1697548439923, 1697548439983, 1697548440707, 1697548440777, 1697548440830, 1697548440900, 1697548440962, 1697548441012, 1697548441074]"
419,419,537,11,[],200,llama-7b,64,1,2499.0,1.0,1,A100,1697548456398,1697548458897,120,83.0,20.0,"[13, 665, 68, 62, 60, 48, 59, 54, 331, 50, 62, 61, 50, 48, 61, 526, 72, 62, 49, 48, 50]","[1697548456411, 1697548457076, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457931, 1697548457981, 1697548458029, 1697548458090, 1697548458616, 1697548458688, 1697548458750, 1697548458799, 1697548458847, 1697548458897]"
420,420,41,2,[],200,llama-7b,64,1,7433.0,1.0,1,A100,1697548441616,1697548449049,120,39.0,43.0,"[79, 1063, 57, 60, 47, 46, 37, 417, 55, 54, 51, 42, 47, 679, 57, 46, 57, 56, 54, 43, 49, 826, 310, 54, 41, 41, 368, 61, 61, 58, 46, 55, 359, 45, 44, 44, 57, 801, 63, 61, 59, 57, 56, 770]","[1697548441695, 1697548442758, 1697548442815, 1697548442875, 1697548442922, 1697548442968, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443624, 1697548443671, 1697548444350, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444620, 1697548444663, 1697548444712, 1697548445538, 1697548445848, 1697548445902, 1697548445943, 1697548445984, 1697548446352, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446633, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448107, 1697548448166, 1697548448223, 1697548448279, 1697548449049]"
421,421,884,11,[],200,llama-7b,64,1,3154.0,1.0,1,A100,1697548459371,1697548462525,120,90.0,20.0,"[18, 1234, 88, 69, 52, 67, 63, 63, 48, 238, 48, 63, 61, 61, 61, 526, 175, 51, 57, 57, 54]","[1697548459389, 1697548460623, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461311, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462525]"
422,422,191,12,[],200,llama-7b,64,1,2706.0,1.0,1,A100,1697548458900,1697548461606,120,85.0,20.0,"[29, 808, 249, 65, 49, 63, 60, 60, 427, 70, 51, 69, 63, 63, 48, 236, 50, 62, 62, 61, 61]","[1697548458929, 1697548459737, 1697548459986, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460710, 1697548460780, 1697548460831, 1697548460900, 1697548460963, 1697548461026, 1697548461074, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461606]"
423,423,606,5,[],200,llama-7b,64,1,653.0,1.0,1,A100,1697548446278,1697548446931,120,9.0,1.0,"[15, 638]","[1697548446293, 1697548446931]"
424,424,383,6,[],200,llama-7b,64,1,541.0,1.0,1,A100,1697548446935,1697548447476,120,15.0,1.0,"[18, 523]","[1697548446953, 1697548447476]"
425,425,328,10,[],200,llama-7b,64,1,1217.0,1.0,1,A100,1697548458094,1697548459311,120,109.0,6.0,"[6, 937, 64, 47, 61, 45, 57]","[1697548458100, 1697548459037, 1697548459101, 1697548459148, 1697548459209, 1697548459254, 1697548459311]"
426,426,37,7,[],200,llama-7b,64,1,1041.0,1.0,1,A100,1697548447481,1697548448522,120,20.0,1.0,"[25, 1016]","[1697548447506, 1697548448522]"
427,427,531,9,[],200,llama-7b,64,1,3154.0,1.0,1,A100,1697548459371,1697548462525,120,52.0,20.0,"[20, 1232, 88, 69, 52, 67, 63, 63, 48, 238, 48, 63, 61, 61, 61, 526, 175, 51, 57, 57, 54]","[1697548459391, 1697548460623, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461311, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462525]"
428,428,708,8,[],200,llama-7b,64,1,1080.0,1.0,1,A100,1697548448525,1697548449605,120,140.0,1.0,"[15, 1064]","[1697548448540, 1697548449604]"
429,429,918,11,[],200,llama-7b,64,1,421.0,1.0,1,A100,1697548459316,1697548459737,120,23.0,1.0,"[6, 415]","[1697548459322, 1697548459737]"
430,430,362,9,[],200,llama-7b,64,1,761.0,1.0,1,A100,1697548449607,1697548450368,120,14.0,1.0,"[22, 739]","[1697548449629, 1697548450368]"
431,431,869,6,[],200,llama-7b,64,1,1339.0,1.0,1,A100,1697548450442,1697548451781,120,244.0,12.0,"[6, 487, 71, 64, 60, 55, 42, 297, 49, 48, 53, 49, 58]","[1697548450448, 1697548450935, 1697548451006, 1697548451070, 1697548451130, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451723, 1697548451781]"
432,432,303,10,[],200,llama-7b,64,1,4771.0,1.0,1,A100,1697548462528,1697548467299,120,88.0,20.0,"[22, 1030, 313, 63, 60, 57, 57, 728, 461, 63, 50, 59, 45, 704, 70, 70, 54, 64, 64, 47, 690]","[1697548462550, 1697548463580, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464130, 1697548464858, 1697548465319, 1697548465382, 1697548465432, 1697548465491, 1697548465536, 1697548466240, 1697548466310, 1697548466380, 1697548466434, 1697548466498, 1697548466562, 1697548466609, 1697548467299]"
433,433,685,12,[],200,llama-7b,64,1,973.0,1.0,1,A100,1697548459739,1697548460712,120,364.0,2.0,"[14, 870, 88]","[1697548459753, 1697548460623, 1697548460711]"
434,434,347,13,[],200,llama-7b,64,1,11187.0,1.0,1,A100,1697548460715,1697548471902,120,100.0,72.0,"[10, 585, 50, 62, 62, 61, 61, 525, 175, 50, 58, 56, 54, 382, 56, 58, 53, 820, 63, 60, 57, 56, 729, 460, 64, 50, 59, 45, 702, 72, 69, 54, 65, 63, 48, 689, 74, 70, 68, 67, 51, 63, 360, 72, 54, 71, 69, 65, 535, 66, 51, 66, 64, 63, 59, 569, 69, 52, 68, 51, 52, 64, 65, 844, 73, 71, 70, 67, 50, 65, 312, 74]","[1697548460725, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461606, 1697548462131, 1697548462306, 1697548462356, 1697548462414, 1697548462470, 1697548462524, 1697548462906, 1697548462962, 1697548463020, 1697548463073, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465491, 1697548465536, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468317, 1697548468382, 1697548468917, 1697548468983, 1697548469034, 1697548469100, 1697548469164, 1697548469227, 1697548469286, 1697548469855, 1697548469924, 1697548469976, 1697548470044, 1697548470095, 1697548470147, 1697548470211, 1697548470276, 1697548471120, 1697548471193, 1697548471264, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902]"
435,435,885,11,[],200,llama-7b,64,1,5602.0,1.0,1,A100,1697548467303,1697548472905,120,84.0,43.0,"[27, 641, 81, 72, 53, 72, 70, 64, 534, 65, 52, 66, 65, 62, 59, 569, 70, 53, 67, 49, 52, 66, 64, 846, 72, 71, 69, 68, 49, 65, 312, 74, 73, 72, 68, 63, 49, 304, 71, 68, 68, 53, 65, 49]","[1697548467330, 1697548467971, 1697548468052, 1697548468124, 1697548468177, 1697548468249, 1697548468319, 1697548468383, 1697548468917, 1697548468982, 1697548469034, 1697548469100, 1697548469165, 1697548469227, 1697548469286, 1697548469855, 1697548469925, 1697548469978, 1697548470045, 1697548470094, 1697548470146, 1697548470212, 1697548470276, 1697548471122, 1697548471194, 1697548471265, 1697548471334, 1697548471402, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472791, 1697548472856, 1697548472905]"
436,436,748,13,[],200,llama-7b,64,1,3440.0,1.0,1,A100,1697548452715,1697548456155,120,182.0,14.0,"[6, 1593, 66, 70, 54, 62, 59, 1119, 73, 75, 72, 70, 68, 52]","[1697548452721, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455744, 1697548455817, 1697548455892, 1697548455964, 1697548456034, 1697548456102, 1697548456154]"
437,437,515,2,[],200,llama-7b,64,1,303.0,1.0,1,A100,1697548439928,1697548440231,120,11.0,1.0,"[17, 286]","[1697548439945, 1697548440231]"
438,438,169,3,[],200,llama-7b,64,1,1169.0,1.0,1,A100,1697548440234,1697548441403,120,10.0,1.0,"[15, 1154]","[1697548440249, 1697548441403]"
439,439,865,4,[],200,llama-7b,64,1,268.0,1.0,1,A100,1697548441408,1697548441676,120,9.0,1.0,"[21, 247]","[1697548441429, 1697548441676]"
440,440,638,7,[],200,llama-7b,64,1,4685.0,1.0,1,A100,1697548451784,1697548456469,120,88.0,20.0,"[6, 921, 546, 68, 51, 63, 876, 66, 70, 54, 62, 58, 1118, 73, 76, 72, 69, 68, 53, 67, 248]","[1697548451790, 1697548452711, 1697548453257, 1697548453325, 1697548453376, 1697548453439, 1697548454315, 1697548454381, 1697548454451, 1697548454505, 1697548454567, 1697548454625, 1697548455743, 1697548455816, 1697548455892, 1697548455964, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456469]"
441,441,497,5,[],200,llama-7b,64,1,1137.0,1.0,1,A100,1697548441678,1697548442815,120,67.0,2.0,"[171, 910, 56]","[1697548441849, 1697548442759, 1697548442815]"
442,442,528,14,[],200,llama-7b,64,1,1933.0,1.0,1,A100,1697548456157,1697548458090,120,52.0,20.0,"[14, 222, 76, 69, 52, 70, 66, 60, 358, 63, 59, 48, 59, 54, 331, 51, 61, 62, 49, 47, 62]","[1697548456171, 1697548456393, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456726, 1697548456786, 1697548457144, 1697548457207, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457809, 1697548457870, 1697548457932, 1697548457981, 1697548458028, 1697548458090]"
443,443,173,7,[],200,llama-7b,64,1,2425.0,1.0,1,A100,1697548456472,1697548458897,120,96.0,20.0,"[15, 590, 67, 62, 60, 48, 59, 54, 331, 50, 62, 61, 49, 49, 61, 526, 72, 62, 49, 48, 50]","[1697548456487, 1697548457077, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458688, 1697548458750, 1697548458799, 1697548458847, 1697548458897]"
444,444,492,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548440903,1697548441612,120,,,"[21, 479]","[1697548440924, 1697548441403]"
445,445,266,6,[],200,llama-7b,64,1,536.0,1.0,1,A100,1697548442818,1697548443354,120,9.0,1.0,"[19, 516]","[1697548442837, 1697548443353]"
446,446,266,4,[],200,llama-7b,64,1,1142.0,1.0,1,A100,1697548441617,1697548442759,120,9.0,1.0,"[130, 1012]","[1697548441747, 1697548442759]"
447,447,849,5,[],200,llama-7b,64,1,589.0,1.0,1,A100,1697548442764,1697548443353,120,10.0,1.0,"[19, 570]","[1697548442783, 1697548443353]"
448,448,299,8,[],200,llama-7b,64,1,603.0,1.0,1,A100,1697548456474,1697548457077,120,14.0,1.0,"[24, 579]","[1697548456498, 1697548457077]"
449,449,70,9,[],200,llama-7b,64,1,612.0,1.0,1,A100,1697548457081,1697548457693,120,39.0,1.0,"[30, 581]","[1697548457111, 1697548457692]"
450,450,325,3,[],200,llama-7b,64,1,3435.0,1.0,1,A100,1697548444674,1697548448109,120,85.0,20.0,"[23, 591, 250, 311, 54, 41, 41, 367, 61, 62, 57, 46, 56, 358, 45, 45, 43, 57, 802, 62, 63]","[1697548444697, 1697548445288, 1697548445538, 1697548445849, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446413, 1697548446475, 1697548446532, 1697548446578, 1697548446634, 1697548446992, 1697548447037, 1697548447082, 1697548447125, 1697548447182, 1697548447984, 1697548448046, 1697548448109]"
451,451,656,10,[],200,llama-7b,64,1,647.0,1.0,1,A100,1697548457697,1697548458344,120,26.0,1.0,"[36, 611]","[1697548457733, 1697548458344]"
452,452,734,8,[],200,llama-7b,64,1,1324.0,1.0,1,A100,1697548458900,1697548460224,120,100.0,6.0,"[32, 805, 249, 65, 49, 63, 60]","[1697548458932, 1697548459737, 1697548459986, 1697548460051, 1697548460100, 1697548460163, 1697548460223]"
453,453,434,11,[],200,llama-7b,64,1,2727.0,1.0,1,A100,1697548458347,1697548461074,120,85.0,20.0,"[6, 685, 63, 47, 61, 45, 57, 57, 619, 64, 49, 62, 61, 60, 428, 69, 52, 67, 63, 63, 49]","[1697548458353, 1697548459038, 1697548459101, 1697548459148, 1697548459209, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460100, 1697548460162, 1697548460223, 1697548460283, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074]"
454,454,505,9,[],200,llama-7b,64,1,3790.0,1.0,1,A100,1697548460226,1697548464016,120,100.0,27.0,"[9, 476, 69, 52, 67, 63, 63, 49, 236, 50, 62, 62, 60, 62, 526, 175, 50, 57, 56, 54, 381, 57, 57, 54, 820, 63, 60]","[1697548460235, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461544, 1697548461606, 1697548462132, 1697548462307, 1697548462357, 1697548462414, 1697548462470, 1697548462524, 1697548462905, 1697548462962, 1697548463019, 1697548463073, 1697548463893, 1697548463956, 1697548464016]"
455,455,629,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548443363,1697548488024,120,,,"[33, 744, 210, 58, 46, 56, 56, 54, 43, 50, 825, 311, 54, 41, 41, 367, 62, 61, 58, 45, 56, 358, 45, 44, 44, 57, 801, 63, 62, 58, 57, 56, 770, 66, 58, 46, 45, 56, 45, 306, 60, 45, 54, 53, 556, 56, 53, 52, 405, 64, 61, 55, 41, 298, 49, 48, 53, 48, 58, 245, 58, 45, 58, 54, 1016, 68, 52, 62, 875, 66, 71, 53, 63, 59, 1117, 73, 76, 72, 69, 69, 52, 68, 247, 69, 52, 71, 65, 60, 359, 62, 60, 48, 58, 54, 331, 51, 61, 62, 49, 48, 62, 525, 73, 61, 49, 48, 50, 205, 46, 58, 49, 57, 56, 619, 65, 49, 62, 61, 60, 427, 69, 52, 67, 63, 63, 49, 236, 50, 62, 62, 61, 61, 526, 175, 50, 57, 57, 54, 381, 56, 58, 53, 821, 62, 60, 58, 56, 729, 459, 65, 49, 60, 45, 702, 72, 69, 53, 65, 63, 48, 690, 73, 71, 68, 67, 51, 62, 360, 73, 54, 70, 70, 65, 535, 65, 51, 67, 64, 62, 60, 569, 69, 52, 67, 51, 52, 65, 64, 845, 73, 71, 70, 67, 50, 65, 312, 73, 74, 72, 67, 64, 48, 305, 70, 68, 69, 52, 65, 49, 530, 67, 50, 50, 65, 58, 346, 68, 67, 51, 66, 65, 62, 594, 72, 71, 70, 69, 67, 64, 861, 512, 288, 74, 57, 63, 618, 251, 54, 254, 70, 68, 62, 613, 255, 71, 55, 71, 67, 63, 912, 76, 74, 72, 72, 67, 67, 588, 269, 63, 74, 264, 69, 51, 68, 839, 75, 55, 73, 66, 65, 50, 338, 76, 75, 75, 70, 57, 70, 838, 313, 308, 77, 75, 72, 70, 324, 59, 59, 79, 306, 72, 69]","[1697548443396, 1697548444140, 1697548444350, 1697548444408, 1697548444454, 1697548444510, 1697548444566, 1697548444620, 1697548444663, 1697548444713, 1697548445538, 1697548445849, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448279, 1697548449049, 1697548449115, 1697548449173, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449671, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450439, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451780, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453256, 1697548453324, 1697548453376, 1697548453438, 1697548454313, 1697548454379, 1697548454450, 1697548454503, 1697548454566, 1697548454625, 1697548455742, 1697548455815, 1697548455891, 1697548455963, 1697548456032, 1697548456101, 1697548456153, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457372, 1697548457426, 1697548457757, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458028, 1697548458090, 1697548458615, 1697548458688, 1697548458749, 1697548458798, 1697548458846, 1697548458896, 1697548459101, 1697548459147, 1697548459205, 1697548459254, 1697548459311, 1697548459367, 1697548459986, 1697548460051, 1697548460100, 1697548460162, 1697548460223, 1697548460283, 1697548460710, 1697548460779, 1697548460831, 1697548460898, 1697548460961, 1697548461024, 1697548461073, 1697548461309, 1697548461359, 1697548461421, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462356, 1697548462413, 1697548462470, 1697548462524, 1697548462905, 1697548462961, 1697548463019, 1697548463072, 1697548463893, 1697548463955, 1697548464015, 1697548464073, 1697548464129, 1697548464858, 1697548465317, 1697548465382, 1697548465431, 1697548465491, 1697548465536, 1697548466238, 1697548466310, 1697548466379, 1697548466432, 1697548466497, 1697548466560, 1697548466608, 1697548467298, 1697548467371, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467690, 1697548468050, 1697548468123, 1697548468177, 1697548468247, 1697548468317, 1697548468382, 1697548468917, 1697548468982, 1697548469033, 1697548469100, 1697548469164, 1697548469226, 1697548469286, 1697548469855, 1697548469924, 1697548469976, 1697548470043, 1697548470094, 1697548470146, 1697548470211, 1697548470275, 1697548471120, 1697548471193, 1697548471264, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471901, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472226, 1697548472531, 1697548472601, 1697548472669, 1697548472738, 1697548472790, 1697548472855, 1697548472904, 1697548473434, 1697548473501, 1697548473551, 1697548473601, 1697548473666, 1697548473724, 1697548474070, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474449, 1697548475043, 1697548475115, 1697548475186, 1697548475256, 1697548475325, 1697548475392, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477191, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479556, 1697548479627, 1697548479682, 1697548479753, 1697548479820, 1697548479883, 1697548480795, 1697548480871, 1697548480945, 1697548481017, 1697548481089, 1697548481156, 1697548481223, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711, 1697548483777, 1697548483842, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484526, 1697548484583, 1697548484653, 1697548485491, 1697548485804, 1697548486112, 1697548486189, 1697548486264, 1697548486336, 1697548486406, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487233, 1697548487305, 1697548487374]"
456,456,103,4,[],200,llama-7b,64,1,410.0,1.0,1,A100,1697548448112,1697548448522,120,15.0,1.0,"[24, 386]","[1697548448136, 1697548448522]"
457,457,690,5,[],200,llama-7b,64,1,1079.0,1.0,1,A100,1697548448526,1697548449605,120,39.0,1.0,"[24, 1054]","[1697548448550, 1697548449604]"
458,458,460,6,[],200,llama-7b,64,1,2579.0,1.0,1,A100,1697548449607,1697548452186,120,87.0,20.0,"[27, 733, 73, 56, 53, 51, 405, 64, 62, 54, 42, 297, 49, 48, 53, 48, 59, 244, 58, 45, 58]","[1697548449634, 1697548450367, 1697548450440, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451131, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186]"
459,459,410,1,[],200,llama-7b,64,1,2054.0,1.0,1,A100,1697548441617,1697548443671,120,364.0,12.0,"[83, 1058, 57, 60, 47, 45, 38, 418, 55, 53, 51, 42, 47]","[1697548441700, 1697548442758, 1697548442815, 1697548442875, 1697548442922, 1697548442967, 1697548443005, 1697548443423, 1697548443478, 1697548443531, 1697548443582, 1697548443624, 1697548443671]"
460,460,855,7,[],200,llama-7b,64,1,3278.0,1.0,1,A100,1697548443356,1697548446634,120,83.0,20.0,"[20, 764, 209, 58, 46, 57, 56, 53, 44, 49, 826, 310, 55, 41, 41, 367, 61, 62, 57, 46, 56]","[1697548443376, 1697548444140, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444619, 1697548444663, 1697548444712, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446413, 1697548446475, 1697548446532, 1697548446578, 1697548446634]"
461,461,810,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548440901,1697548441612,120,,,"[12, 490]","[1697548440913, 1697548441403]"
462,462,121,7,[],200,llama-7b,64,1,522.0,1.0,1,A100,1697548452190,1697548452712,120,13.0,1.0,"[24, 498]","[1697548452214, 1697548452712]"
463,463,818,8,[],200,llama-7b,64,1,1131.0,1.0,1,A100,1697548452715,1697548453846,120,13.0,1.0,"[16, 1115]","[1697548452731, 1697548453846]"
464,464,479,9,[],200,llama-7b,64,1,5299.0,1.0,1,A100,1697548453849,1697548459148,120,140.0,36.0,"[21, 1190, 683, 73, 76, 71, 70, 69, 52, 67, 247, 69, 53, 70, 66, 59, 359, 62, 61, 47, 59, 54, 331, 50, 62, 62, 48, 49, 61, 525, 73, 62, 48, 49, 49, 205, 47]","[1697548453870, 1697548455060, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456102, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456590, 1697548456660, 1697548456726, 1697548456785, 1697548457144, 1697548457206, 1697548457267, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148]"
465,465,248,10,[],200,llama-7b,64,1,2272.0,1.0,1,A100,1697548459151,1697548461423,120,182.0,17.0,"[12, 574, 250, 64, 50, 62, 60, 61, 426, 69, 53, 67, 63, 63, 48, 237, 50, 63]","[1697548459163, 1697548459737, 1697548459987, 1697548460051, 1697548460101, 1697548460163, 1697548460223, 1697548460284, 1697548460710, 1697548460779, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461310, 1697548461360, 1697548461423]"
466,466,704,4,[],200,llama-7b,64,1,552.0,1.0,1,A100,1697548449053,1697548449605,120,14.0,1.0,"[16, 535]","[1697548449069, 1697548449604]"
467,467,358,5,[],200,llama-7b,64,1,889.0,1.0,1,A100,1697548449607,1697548450496,120,216.0,3.0,"[22, 738, 73, 56]","[1697548449629, 1697548450367, 1697548450440, 1697548450496]"
468,468,833,11,[],200,llama-7b,64,1,1481.0,1.0,1,A100,1697548461425,1697548462906,120,563.0,8.0,"[15, 495, 196, 175, 51, 57, 57, 54, 381]","[1697548461440, 1697548461935, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462525, 1697548462906]"
469,469,68,11,[],200,llama-7b,64,1,558.0,1.0,1,A100,1697548463023,1697548463581,120,12.0,1.0,"[13, 545]","[1697548463036, 1697548463581]"
470,470,745,12,[],200,llama-7b,64,1,973.0,1.0,1,A100,1697548463582,1697548464555,120,17.0,1.0,"[18, 955]","[1697548463600, 1697548464555]"
471,471,135,6,[],200,llama-7b,64,1,509.0,1.0,1,A100,1697548450497,1697548451006,120,52.0,2.0,"[6, 432, 71]","[1697548450503, 1697548450935, 1697548451006]"
472,472,509,13,[],200,llama-7b,64,1,1754.0,1.0,1,A100,1697548464557,1697548466311,120,286.0,3.0,"[8, 1336, 337, 73]","[1697548464565, 1697548465901, 1697548466238, 1697548466311]"
473,473,576,12,[],200,llama-7b,64,1,673.0,1.0,1,A100,1697548462908,1697548463581,120,14.0,1.0,"[6, 666]","[1697548462914, 1697548463580]"
474,474,230,13,[],200,llama-7b,64,1,1849.0,1.0,1,A100,1697548463583,1697548465432,120,86.0,5.0,"[18, 954, 303, 460, 64, 50]","[1697548463601, 1697548464555, 1697548464858, 1697548465318, 1697548465382, 1697548465432]"
475,475,7,14,[],200,llama-7b,64,1,1995.0,1.0,1,A100,1697548465448,1697548467443,120,345.0,11.0,"[13, 440, 338, 72, 69, 53, 65, 63, 48, 689, 74, 70]","[1697548465461, 1697548465901, 1697548466239, 1697548466311, 1697548466380, 1697548466433, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442]"
476,476,170,14,[],200,llama-7b,64,1,2604.0,1.0,1,A100,1697548466314,1697548468918,120,335.0,15.0,"[8, 694, 282, 74, 71, 67, 68, 51, 62, 359, 73, 54, 72, 68, 65, 536]","[1697548466322, 1697548467016, 1697548467298, 1697548467372, 1697548467443, 1697548467510, 1697548467578, 1697548467629, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468249, 1697548468317, 1697548468382, 1697548468918]"
477,477,588,15,[],200,llama-7b,64,1,526.0,1.0,1,A100,1697548467446,1697548467972,120,11.0,1.0,"[6, 520]","[1697548467452, 1697548467972]"
478,478,364,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548467981,1697548488026,120,,,"[21, 669, 246, 66, 51, 67, 64, 62, 60, 568, 69, 53, 67, 51, 51, 65, 65, 846, 72, 71, 70, 67, 49, 66, 312, 74, 72, 72, 68, 63, 49, 304, 71, 68, 68, 53, 65, 49, 530, 67, 49, 51, 64, 59, 346, 68, 66, 51, 66, 65, 63, 593, 73, 70, 70, 69, 68, 63, 861, 512, 288, 75, 56, 63, 618, 251, 55, 253, 70, 68, 62, 613, 256, 71, 54, 71, 68, 63, 911, 76, 75, 72, 71, 68, 67, 587, 269, 63, 74, 264, 69, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 75, 75, 71, 57, 70, 838, 313, 308, 76, 76, 72, 70, 323, 59, 59, 79, 307, 71, 70]","[1697548468002, 1697548468671, 1697548468917, 1697548468983, 1697548469034, 1697548469101, 1697548469165, 1697548469227, 1697548469287, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470146, 1697548470211, 1697548470276, 1697548471122, 1697548471194, 1697548471265, 1697548471335, 1697548471402, 1697548471451, 1697548471517, 1697548471829, 1697548471903, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472791, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473666, 1697548473725, 1697548474071, 1697548474139, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475186, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
479,479,22,11,[],200,llama-7b,64,1,634.0,1.0,1,A100,1697548459990,1697548460624,120,16.0,1.0,"[11, 622]","[1697548460001, 1697548460623]"
480,480,601,12,[],200,llama-7b,64,1,3389.0,1.0,1,A100,1697548460627,1697548464016,120,83.0,20.0,"[21, 594, 68, 50, 62, 62, 61, 61, 526, 175, 49, 57, 57, 54, 382, 56, 58, 53, 820, 63, 60]","[1697548460648, 1697548461242, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461606, 1697548462132, 1697548462307, 1697548462356, 1697548462413, 1697548462470, 1697548462524, 1697548462906, 1697548462962, 1697548463020, 1697548463073, 1697548463893, 1697548463956, 1697548464016]"
481,481,869,15,[],200,llama-7b,64,1,2344.0,1.0,1,A100,1697548468921,1697548471265,120,244.0,12.0,"[15, 646, 274, 68, 53, 67, 51, 52, 65, 64, 844, 74, 71]","[1697548468936, 1697548469582, 1697548469856, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471194, 1697548471265]"
482,482,349,13,[],200,llama-7b,64,1,3610.0,1.0,1,A100,1697548464019,1697548467629,120,88.0,20.0,"[20, 517, 303, 459, 65, 51, 58, 45, 701, 72, 70, 53, 64, 64, 48, 689, 74, 70, 69, 67, 51]","[1697548464039, 1697548464556, 1697548464859, 1697548465318, 1697548465383, 1697548465434, 1697548465492, 1697548465537, 1697548466238, 1697548466310, 1697548466380, 1697548466433, 1697548466497, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467511, 1697548467578, 1697548467629]"
483,483,17,17,[],200,llama-7b,64,1,799.0,1.0,1,A100,1697548488033,1697548488832,120,23.0,1.0,"[129, 670]","[1697548488162, 1697548488832]"
484,484,718,18,[],200,llama-7b,64,1,405.0,1.0,1,A100,1697548488836,1697548489241,120,13.0,1.0,"[31, 374]","[1697548488867, 1697548489241]"
485,485,1,14,[],200,llama-7b,64,1,5273.0,1.0,1,A100,1697548467632,1697548472905,120,47.0,43.0,"[17, 322, 81, 72, 54, 71, 71, 63, 534, 66, 51, 66, 65, 62, 59, 569, 70, 53, 66, 51, 51, 66, 64, 846, 72, 71, 69, 68, 49, 66, 311, 74, 73, 72, 68, 63, 49, 304, 71, 68, 68, 53, 65, 49]","[1697548467649, 1697548467971, 1697548468052, 1697548468124, 1697548468178, 1697548468249, 1697548468320, 1697548468383, 1697548468917, 1697548468983, 1697548469034, 1697548469100, 1697548469165, 1697548469227, 1697548469286, 1697548469855, 1697548469925, 1697548469978, 1697548470044, 1697548470095, 1697548470146, 1697548470212, 1697548470276, 1697548471122, 1697548471194, 1697548471265, 1697548471334, 1697548471402, 1697548471451, 1697548471517, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472791, 1697548472856, 1697548472905]"
486,486,139,10,[],200,llama-7b,64,1,3068.0,1.0,1,A100,1697548450371,1697548453439,120,39.0,21.0,"[17, 547, 70, 65, 60, 55, 42, 297, 49, 48, 53, 49, 58, 244, 58, 45, 58, 54, 1017, 68, 52, 62]","[1697548450388, 1697548450935, 1697548451005, 1697548451070, 1697548451130, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451723, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453377, 1697548453439]"
487,487,382,19,[],200,llama-7b,64,1,2466.0,1.0,1,A100,1697548489244,1697548491710,120,47.0,20.0,"[25, 561, 66, 51, 43, 43, 51, 48, 41, 41, 193, 374, 50, 49, 41, 553, 51, 48, 40, 48, 49]","[1697548489269, 1697548489830, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490214, 1697548490407, 1697548490781, 1697548490831, 1697548490880, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710]"
488,488,85,12,[],200,llama-7b,64,1,4356.0,1.0,1,A100,1697548461077,1697548465433,120,88.0,20.0,"[16, 842, 196, 175, 50, 58, 56, 54, 382, 55, 59, 53, 820, 63, 60, 57, 56, 729, 460, 64, 50]","[1697548461093, 1697548461935, 1697548462131, 1697548462306, 1697548462356, 1697548462414, 1697548462470, 1697548462524, 1697548462906, 1697548462961, 1697548463020, 1697548463073, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465432]"
489,489,716,10,[],200,llama-7b,64,1,4685.0,1.0,1,A100,1697548466436,1697548471121,120,79.0,30.0,"[13, 850, 73, 71, 67, 68, 51, 63, 359, 72, 54, 71, 70, 64, 536, 65, 52, 65, 65, 63, 58, 570, 69, 52, 67, 52, 52, 64, 65, 844]","[1697548466449, 1697548467299, 1697548467372, 1697548467443, 1697548467510, 1697548467578, 1697548467629, 1697548467692, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468318, 1697548468382, 1697548468918, 1697548468983, 1697548469035, 1697548469100, 1697548469165, 1697548469228, 1697548469286, 1697548469856, 1697548469925, 1697548469977, 1697548470044, 1697548470096, 1697548470148, 1697548470212, 1697548470277, 1697548471121]"
490,490,578,3,[],200,llama-7b,64,1,1142.0,1.0,1,A100,1697548441618,1697548442760,120,31.0,1.0,"[164, 978]","[1697548441782, 1697548442760]"
491,491,238,4,[],200,llama-7b,64,1,861.0,1.0,1,A100,1697548442763,1697548443624,120,563.0,6.0,"[15, 575, 69, 55, 54, 51, 42]","[1697548442778, 1697548443353, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443624]"
492,492,9,5,[],200,llama-7b,64,1,3008.0,1.0,1,A100,1697548443627,1697548446635,120,85.0,20.0,"[20, 493, 210, 58, 46, 57, 55, 54, 43, 50, 826, 309, 55, 41, 40, 368, 62, 61, 58, 45, 57]","[1697548443647, 1697548444140, 1697548444350, 1697548444408, 1697548444454, 1697548444511, 1697548444566, 1697548444620, 1697548444663, 1697548444713, 1697548445539, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446635]"
493,493,593,6,[],200,llama-7b,64,1,2478.0,1.0,1,A100,1697548446638,1697548449116,120,335.0,9.0,"[20, 817, 508, 63, 62, 58, 58, 56, 769, 67]","[1697548446658, 1697548447475, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448224, 1697548448280, 1697548449049, 1697548449116]"
494,494,341,7,[],200,llama-7b,64,1,2603.0,1.0,1,A100,1697548449119,1697548451722,120,87.0,20.0,"[6, 479, 68, 59, 45, 54, 53, 558, 55, 52, 52, 405, 64, 61, 55, 41, 298, 49, 48, 53, 48]","[1697548449125, 1697548449604, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450441, 1697548450496, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722]"
495,495,624,8,[],200,llama-7b,64,1,17316.0,1.0,1,A100,1697548446639,1697548463955,120,563.0,119.0,"[28, 808, 508, 63, 62, 59, 57, 56, 769, 66, 59, 45, 46, 55, 45, 308, 58, 45, 54, 53, 557, 55, 53, 52, 405, 64, 61, 55, 41, 298, 49, 48, 52, 49, 59, 244, 59, 44, 58, 54, 1016, 69, 51, 63, 875, 66, 70, 54, 62, 59, 1117, 73, 76, 72, 70, 68, 53, 67, 247, 69, 52, 71, 65, 60, 359, 62, 60, 48, 58, 54, 332, 50, 61, 62, 49, 49, 61, 525, 73, 61, 49, 48, 50, 205, 46, 58, 49, 57, 56, 619, 65, 49, 63, 60, 60, 427, 69, 52, 67, 63, 63, 49, 237, 49, 62, 62, 61, 61, 526, 175, 50, 57, 57, 54, 381, 57, 57, 53, 821, 62]","[1697548446667, 1697548447475, 1697548447983, 1697548448046, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449115, 1697548449174, 1697548449219, 1697548449265, 1697548449320, 1697548449365, 1697548449673, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450440, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451673, 1697548451722, 1697548451781, 1697548452025, 1697548452084, 1697548452128, 1697548452186, 1697548452240, 1697548453256, 1697548453325, 1697548453376, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455742, 1697548455815, 1697548455891, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457372, 1697548457426, 1697548457758, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458749, 1697548458798, 1697548458846, 1697548458896, 1697548459101, 1697548459147, 1697548459205, 1697548459254, 1697548459311, 1697548459367, 1697548459986, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460710, 1697548460779, 1697548460831, 1697548460898, 1697548460961, 1697548461024, 1697548461073, 1697548461310, 1697548461359, 1697548461421, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462356, 1697548462413, 1697548462470, 1697548462524, 1697548462905, 1697548462962, 1697548463019, 1697548463072, 1697548463893, 1697548463955]"
496,496,370,11,[],200,llama-7b,64,1,628.0,1.0,1,A100,1697548471123,1697548471751,120,31.0,1.0,"[17, 611]","[1697548471140, 1697548471751]"
497,497,145,12,[],200,llama-7b,64,1,1682.0,1.0,1,A100,1697548471753,1697548473435,120,161.0,9.0,"[20, 683, 76, 70, 68, 68, 53, 65, 49, 530]","[1697548471773, 1697548472456, 1697548472532, 1697548472602, 1697548472670, 1697548472738, 1697548472791, 1697548472856, 1697548472905, 1697548473435]"
498,498,728,13,[],200,llama-7b,64,1,559.0,1.0,1,A100,1697548473439,1697548473998,120,20.0,1.0,"[19, 540]","[1697548473458, 1697548473998]"
499,499,501,14,[],200,llama-7b,64,1,759.0,1.0,1,A100,1697548474001,1697548474760,120,19.0,1.0,"[10, 749]","[1697548474011, 1697548474760]"
500,500,156,15,[],200,llama-7b,64,1,5059.0,1.0,1,A100,1697548474763,1697548479822,120,86.0,20.0,"[19, 1054, 481, 512, 289, 74, 57, 63, 618, 250, 54, 254, 70, 70, 60, 614, 255, 71, 54, 71, 69]","[1697548474782, 1697548475836, 1697548476317, 1697548476829, 1697548477118, 1697548477192, 1697548477249, 1697548477312, 1697548477930, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478628, 1697548478688, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479822]"
501,501,855,16,[],200,llama-7b,64,1,3887.0,1.0,1,A100,1697548479824,1697548483711,120,83.0,20.0,"[16, 462, 494, 76, 74, 72, 72, 67, 68, 586, 269, 63, 74, 265, 68, 51, 68, 839, 75, 55, 73]","[1697548479840, 1697548480302, 1697548480796, 1697548480872, 1697548480946, 1697548481018, 1697548481090, 1697548481157, 1697548481225, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711]"
502,502,408,8,[],200,llama-7b,64,1,467.0,1.0,1,A100,1697548453380,1697548453847,120,16.0,1.0,"[11, 456]","[1697548453391, 1697548453847]"
503,503,110,8,[],200,llama-7b,64,1,405.0,1.0,1,A100,1697548451724,1697548452129,120,96.0,4.0,"[7, 231, 64, 58, 45]","[1697548451731, 1697548451962, 1697548452026, 1697548452084, 1697548452129]"
504,504,175,9,[],200,llama-7b,64,1,2305.0,1.0,1,A100,1697548453849,1697548456154,120,140.0,8.0,"[19, 1875, 73, 76, 71, 70, 69, 52]","[1697548453868, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456102, 1697548456154]"
505,505,515,17,[],200,llama-7b,64,1,433.0,1.0,1,A100,1697548483716,1697548484149,120,11.0,1.0,"[23, 409]","[1697548483739, 1697548484148]"
506,506,694,9,[],200,llama-7b,64,1,3683.0,1.0,1,A100,1697548452134,1697548455817,120,161.0,13.0,"[8, 1115, 68, 52, 62, 875, 66, 71, 53, 63, 58, 1119, 73]","[1697548452142, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454380, 1697548454451, 1697548454504, 1697548454567, 1697548454625, 1697548455744, 1697548455817]"
507,507,284,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484152,1697548488027,120,,,"[19, 1321, 313, 308, 77, 76, 71, 70, 323, 59, 59, 79, 307, 71, 70]","[1697548484171, 1697548485492, 1697548485805, 1697548486113, 1697548486190, 1697548486266, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
508,508,737,10,[],200,llama-7b,64,1,313.0,1.0,1,A100,1697548456157,1697548456470,120,216.0,2.0,"[15, 221, 77]","[1697548456172, 1697548456393, 1697548456470]"
509,509,844,19,[],200,llama-7b,64,1,801.0,1.0,1,A100,1697548488033,1697548488834,120,10.0,1.0,"[164, 636]","[1697548488197, 1697548488833]"
510,510,614,20,[],200,llama-7b,64,1,404.0,1.0,1,A100,1697548488837,1697548489241,120,15.0,1.0,"[46, 358]","[1697548488883, 1697548489241]"
511,511,272,21,[],200,llama-7b,64,1,2466.0,1.0,1,A100,1697548489244,1697548491710,120,86.0,20.0,"[31, 556, 65, 51, 43, 43, 51, 48, 41, 41, 193, 374, 50, 49, 41, 553, 51, 48, 40, 48, 49]","[1697548489275, 1697548489831, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490214, 1697548490407, 1697548490781, 1697548490831, 1697548490880, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710]"
512,512,464,10,[],200,llama-7b,64,1,572.0,1.0,1,A100,1697548455821,1697548456393,120,12.0,1.0,"[9, 563]","[1697548455830, 1697548456393]"
513,513,50,22,[],200,llama-7b,64,1,1455.0,1.0,1,A100,1697548491713,1697548493168,120,90.0,4.0,"[19, 969, 360, 58, 49]","[1697548491732, 1697548492701, 1697548493061, 1697548493119, 1697548493168]"
514,514,98,2,[],200,llama-7b,64,1,542.0,1.0,1,A100,1697548439689,1697548440231,120,14.0,1.0,"[15, 526]","[1697548439704, 1697548440230]"
515,515,125,11,[],200,llama-7b,64,1,677.0,1.0,1,A100,1697548456400,1697548457077,120,13.0,1.0,"[17, 659]","[1697548456417, 1697548457076]"
516,516,687,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548440234,1697548441611,120,,,"[10, 1159]","[1697548440244, 1697548441403]"
517,517,632,23,[],200,llama-7b,64,1,2239.0,1.0,1,A100,1697548493171,1697548495410,120,91.0,20.0,"[9, 371, 61, 48, 38, 38, 311, 52, 41, 46, 433, 60, 47, 60, 55, 55, 51, 296, 49, 58, 59]","[1697548493180, 1697548493551, 1697548493612, 1697548493660, 1697548493698, 1697548493736, 1697548494047, 1697548494099, 1697548494140, 1697548494186, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494896, 1697548494947, 1697548495243, 1697548495292, 1697548495350, 1697548495409]"
518,518,456,4,[],200,llama-7b,64,1,3048.0,1.0,1,A100,1697548441616,1697548444664,120,90.0,20.0,"[49, 410, 34, 705, 60, 48, 46, 37, 417, 55, 54, 51, 41, 48, 678, 58, 46, 57, 55, 54, 44]","[1697548441665, 1697548442075, 1697548442109, 1697548442814, 1697548442874, 1697548442922, 1697548442968, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443671, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444565, 1697548444619, 1697548444663]"
519,519,407,24,[],200,llama-7b,64,1,315.0,1.0,1,A100,1697548495413,1697548495728,120,16.0,1.0,"[16, 299]","[1697548495429, 1697548495728]"
520,520,825,12,[],200,llama-7b,64,1,2287.0,1.0,1,A100,1697548457080,1697548459367,120,96.0,20.0,"[16, 596, 66, 50, 62, 62, 48, 49, 61, 526, 73, 61, 48, 49, 49, 205, 47, 57, 49, 57, 56]","[1697548457096, 1697548457692, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458689, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459205, 1697548459254, 1697548459311, 1697548459367]"
521,521,61,25,[],200,llama-7b,64,1,593.0,1.0,1,A100,1697548495732,1697548496325,120,9.0,1.0,"[26, 567]","[1697548495758, 1697548496325]"
522,522,766,26,[],200,llama-7b,64,1,602.0,1.0,1,A100,1697548496330,1697548496932,120,11.0,1.0,"[9, 593]","[1697548496339, 1697548496932]"
523,523,422,27,[],200,llama-7b,64,1,636.0,1.0,1,A100,1697548496936,1697548497572,120,26.0,1.0,"[17, 619]","[1697548496953, 1697548497572]"
524,524,187,28,[],200,llama-7b,64,1,879.0,1.0,1,A100,1697548497573,1697548498452,120,161.0,6.0,"[16, 582, 66, 61, 52, 52, 50]","[1697548497589, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498402, 1697548498452]"
525,525,865,29,[],200,llama-7b,64,1,964.0,1.0,1,A100,1697548498456,1697548499420,120,9.0,1.0,"[12, 952]","[1697548498468, 1697548499420]"
526,526,478,13,[],200,llama-7b,64,1,9916.0,1.0,1,A100,1697548459370,1697548469286,120,161.0,62.0,"[9, 1244, 87, 70, 52, 67, 63, 63, 48, 237, 50, 61, 62, 61, 61, 526, 175, 51, 57, 57, 53, 381, 57, 57, 54, 820, 63, 60, 57, 56, 729, 460, 64, 50, 59, 45, 702, 72, 69, 54, 65, 63, 48, 689, 73, 71, 68, 67, 51, 63, 359, 73, 54, 70, 70, 65, 535, 66, 51, 66, 64, 63, 59]","[1697548459379, 1697548460623, 1697548460710, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461310, 1697548461360, 1697548461421, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462524, 1697548462905, 1697548462962, 1697548463019, 1697548463073, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465491, 1697548465536, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467371, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468247, 1697548468317, 1697548468382, 1697548468917, 1697548468983, 1697548469034, 1697548469100, 1697548469164, 1697548469227, 1697548469286]"
527,527,332,3,[],200,llama-7b,64,1,1556.0,1.0,1,A100,1697548444718,1697548446274,120,39.0,1.0,"[39, 1517]","[1697548444757, 1697548446274]"
528,528,209,15,[],200,llama-7b,64,1,546.0,1.0,1,A100,1697548469037,1697548469583,120,20.0,1.0,"[7, 538]","[1697548469044, 1697548469582]"
529,529,80,4,[],200,llama-7b,64,1,654.0,1.0,1,A100,1697548446278,1697548446932,120,13.0,1.0,"[30, 624]","[1697548446308, 1697548446932]"
530,530,659,5,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548446941,1697548488027,120,,,"[18, 517, 508, 63, 61, 59, 57, 56, 769, 67, 57, 46, 45, 56, 45, 307, 60, 44, 54, 53, 557, 55, 53, 52, 405, 64, 61, 55, 41, 298, 49, 48, 52, 49, 59, 244, 59, 44, 58, 54, 1016, 69, 51, 63, 875, 66, 70, 54, 62, 59, 1117, 73, 77, 71, 70, 68, 53, 67, 247, 69, 52, 71, 65, 60, 359, 62, 60, 48, 58, 54, 332, 50, 61, 62, 49, 49, 61, 525, 73, 61, 49, 48, 50, 205, 46, 58, 49, 57, 56, 619, 65, 49, 63, 60, 60, 427, 69, 52, 67, 63, 63, 49, 237, 49, 62, 62, 61, 61, 525, 176, 50, 57, 57, 54, 381, 56, 58, 53, 821, 62, 61, 57, 56, 729, 460, 64, 49, 60, 45, 702, 72, 69, 54, 64, 63, 48, 690, 73, 71, 68, 67, 51, 63, 359, 73, 54, 70, 70, 65, 535, 65, 52, 66, 64, 62, 60, 569, 69, 52, 68, 50, 52, 65, 65, 844, 73, 71, 70, 67, 50, 65, 312, 74, 73, 72, 67, 64, 48, 305, 70, 69, 68, 52, 65, 49, 530, 67, 50, 50, 65, 58, 346, 68, 67, 51, 66, 65, 62, 594, 72, 71, 70, 69, 67, 64, 861, 512, 288, 74, 57, 63, 618, 251, 54, 254, 70, 68, 62, 613, 255, 71, 55, 71, 67, 63, 912, 76, 74, 73, 71, 67, 67, 588, 269, 63, 74, 264, 69, 51, 68, 839, 75, 55, 73, 66, 65, 50, 338, 76, 75, 75, 70, 57, 70, 838, 313, 308, 77, 76, 71, 70, 324, 59, 59, 79, 306, 72, 70]","[1697548446959, 1697548447476, 1697548447984, 1697548448047, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449116, 1697548449173, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449732, 1697548449776, 1697548449830, 1697548449883, 1697548450440, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451673, 1697548451722, 1697548451781, 1697548452025, 1697548452084, 1697548452128, 1697548452186, 1697548452240, 1697548453256, 1697548453325, 1697548453376, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455742, 1697548455815, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457372, 1697548457426, 1697548457758, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458749, 1697548458798, 1697548458846, 1697548458896, 1697548459101, 1697548459147, 1697548459205, 1697548459254, 1697548459311, 1697548459367, 1697548459986, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460710, 1697548460779, 1697548460831, 1697548460898, 1697548460961, 1697548461024, 1697548461073, 1697548461310, 1697548461359, 1697548461421, 1697548461483, 1697548461544, 1697548461605, 1697548462130, 1697548462306, 1697548462356, 1697548462413, 1697548462470, 1697548462524, 1697548462905, 1697548462961, 1697548463019, 1697548463072, 1697548463893, 1697548463955, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465431, 1697548465491, 1697548465536, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466560, 1697548466608, 1697548467298, 1697548467371, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468247, 1697548468317, 1697548468382, 1697548468917, 1697548468982, 1697548469034, 1697548469100, 1697548469164, 1697548469226, 1697548469286, 1697548469855, 1697548469924, 1697548469976, 1697548470044, 1697548470094, 1697548470146, 1697548470211, 1697548470276, 1697548471120, 1697548471193, 1697548471264, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472226, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472855, 1697548472904, 1697548473434, 1697548473501, 1697548473551, 1697548473601, 1697548473666, 1697548473724, 1697548474070, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474449, 1697548475043, 1697548475115, 1697548475186, 1697548475256, 1697548475325, 1697548475392, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477191, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479556, 1697548479627, 1697548479682, 1697548479753, 1697548479820, 1697548479883, 1697548480795, 1697548480871, 1697548480945, 1697548481018, 1697548481089, 1697548481156, 1697548481223, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711, 1697548483777, 1697548483842, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484526, 1697548484583, 1697548484653, 1697548485491, 1697548485804, 1697548486112, 1697548486189, 1697548486265, 1697548486336, 1697548486406, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487233, 1697548487305, 1697548487375]"
531,531,877,5,[],200,llama-7b,64,1,3036.0,1.0,1,A100,1697548441677,1697548444713,120,85.0,20.0,"[175, 907, 56, 59, 48, 46, 38, 417, 54, 54, 51, 41, 47, 680, 58, 46, 57, 54, 55, 43, 50]","[1697548441852, 1697548442759, 1697548442815, 1697548442874, 1697548442922, 1697548442968, 1697548443006, 1697548443423, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443670, 1697548444350, 1697548444408, 1697548444454, 1697548444511, 1697548444565, 1697548444620, 1697548444663, 1697548444713]"
532,532,734,8,[],200,llama-7b,64,1,1786.0,1.0,1,A100,1697548447479,1697548449265,120,100.0,6.0,"[22, 1020, 528, 67, 58, 45, 45]","[1697548447501, 1697548448521, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264]"
533,533,506,9,[],200,llama-7b,64,1,336.0,1.0,1,A100,1697548449269,1697548449605,120,16.0,1.0,"[19, 317]","[1697548449288, 1697548449605]"
534,534,346,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548439757,1697548441613,120,,,"[11, 463, 476, 70, 53, 69, 63, 49, 63]","[1697548439768, 1697548440231, 1697548440707, 1697548440777, 1697548440830, 1697548440899, 1697548440962, 1697548441011, 1697548441074]"
535,535,854,7,[],200,llama-7b,64,1,3914.0,1.0,1,A100,1697548448111,1697548452025,120,67.0,29.0,"[20, 390, 529, 66, 58, 45, 46, 56, 44, 307, 59, 45, 55, 52, 557, 56, 52, 52, 405, 64, 61, 55, 41, 298, 49, 48, 52, 49, 59, 244]","[1697548448131, 1697548448521, 1697548449050, 1697548449116, 1697548449174, 1697548449219, 1697548449265, 1697548449321, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449831, 1697548449883, 1697548450440, 1697548450496, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451673, 1697548451722, 1697548451781, 1697548452025]"
536,536,7,3,[],200,llama-7b,64,1,2008.0,1.0,1,A100,1697548441616,1697548443624,120,345.0,11.0,"[73, 1069, 57, 60, 47, 46, 37, 417, 55, 54, 51, 42]","[1697548441689, 1697548442758, 1697548442815, 1697548442875, 1697548442922, 1697548442968, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443624]"
537,537,86,10,[],200,llama-7b,64,1,2986.0,1.0,1,A100,1697548459371,1697548462357,120,335.0,17.0,"[11, 1241, 88, 69, 52, 67, 63, 63, 48, 237, 49, 63, 61, 61, 61, 526, 175, 51]","[1697548459382, 1697548460623, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461310, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462357]"
538,538,661,12,[],200,llama-7b,64,1,2905.0,1.0,1,A100,1697548462527,1697548465432,120,161.0,10.0,"[16, 1037, 313, 63, 60, 57, 56, 729, 461, 64, 49]","[1697548462543, 1697548463580, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465319, 1697548465383, 1697548465432]"
539,539,526,23,[],200,llama-7b,64,1,2185.0,1.0,1,A100,1697548488029,1697548490214,120,89.0,20.0,"[32, 771, 56, 52, 35, 37, 285, 67, 50, 49, 40, 48, 40, 304, 52, 43, 43, 50, 49, 40, 42]","[1697548488061, 1697548488832, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490214]"
540,540,729,4,[],200,llama-7b,64,1,724.0,1.0,1,A100,1697548443626,1697548444350,120,874.0,2.0,"[6, 718]","[1697548443632, 1697548444350]"
541,541,477,5,[],200,llama-7b,64,1,7427.0,1.0,1,A100,1697548444353,1697548451780,120,244.0,50.0,"[6, 929, 250, 310, 55, 41, 40, 369, 60, 61, 58, 46, 57, 358, 44, 44, 44, 57, 801, 63, 62, 59, 57, 56, 769, 67, 58, 45, 45, 56, 45, 307, 59, 45, 54, 53, 556, 57, 52, 52, 405, 64, 61, 55, 41, 298, 49, 48, 53, 48, 58]","[1697548444359, 1697548445288, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446353, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446635, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450439, 1697548450496, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451780]"
542,542,317,13,[],200,llama-7b,64,1,6780.0,1.0,1,A100,1697548465447,1697548472227,120,244.0,50.0,"[7, 448, 337, 72, 69, 53, 65, 63, 48, 689, 74, 70, 68, 67, 51, 63, 360, 73, 55, 69, 71, 64, 535, 65, 52, 65, 65, 63, 59, 568, 69, 53, 67, 51, 53, 64, 64, 844, 73, 72, 69, 67, 50, 65, 312, 74, 73, 72, 67, 64, 49]","[1697548465454, 1697548465902, 1697548466239, 1697548466311, 1697548466380, 1697548466433, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468124, 1697548468179, 1697548468248, 1697548468319, 1697548468383, 1697548468918, 1697548468983, 1697548469035, 1697548469100, 1697548469165, 1697548469228, 1697548469287, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470148, 1697548470212, 1697548470276, 1697548471120, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472227]"
543,543,183,24,[],200,llama-7b,64,1,5782.0,1.0,1,A100,1697548490219,1697548496001,120,17.0,50.0,"[11, 496, 55, 50, 48, 42, 554, 50, 48, 40, 49, 49, 327, 54, 53, 43, 51, 51, 48, 723, 58, 49, 48, 39, 356, 48, 38, 38, 311, 52, 42, 46, 432, 60, 47, 60, 55, 55, 51, 297, 49, 58, 59, 54, 53, 42, 232, 56, 53, 51, 51]","[1697548490230, 1697548490726, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491711, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492338, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493659, 1697548493697, 1697548493735, 1697548494046, 1697548494098, 1697548494140, 1697548494186, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494840, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495409, 1697548495463, 1697548495516, 1697548495558, 1697548495790, 1697548495846, 1697548495899, 1697548495950, 1697548496001]"
544,544,860,25,[],200,llama-7b,64,1,2855.0,1.0,1,A100,1697548496007,1697548498862,120,85.0,20.0,"[53, 871, 77, 68, 64, 65, 58, 57, 55, 262, 61, 48, 46, 45, 400, 62, 51, 52, 49, 351, 60]","[1697548496060, 1697548496931, 1697548497008, 1697548497076, 1697548497140, 1697548497205, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498862]"
545,545,743,3,[],200,llama-7b,64,1,831.0,1.0,1,A100,1697548449052,1697548449883,120,123.0,6.0,"[14, 538, 68, 60, 44, 54, 53]","[1697548449066, 1697548449604, 1697548449672, 1697548449732, 1697548449776, 1697548449830, 1697548449883]"
546,546,631,26,[],200,llama-7b,64,1,7872.0,1.0,1,A100,1697548498865,1697548506737,120,216.0,50.0,"[21, 902, 62, 61, 60, 54, 51, 421, 65, 50, 49, 58, 55, 724, 64, 60, 58, 55, 55, 620, 65, 64, 62, 59, 55, 876, 63, 58, 57, 52, 42, 704, 62, 47, 60, 59, 59, 56, 352, 65, 51, 61, 60, 60, 283, 45, 55, 51, 679, 65]","[1697548498886, 1697548499788, 1697548499850, 1697548499911, 1697548499971, 1697548500025, 1697548500076, 1697548500497, 1697548500562, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501498, 1697548501562, 1697548501622, 1697548501680, 1697548501735, 1697548501790, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504854, 1697548504910, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505887, 1697548505942, 1697548505993, 1697548506672, 1697548506737]"
547,547,404,4,[],200,llama-7b,64,1,3491.0,1.0,1,A100,1697548449886,1697548453377,120,87.0,20.0,"[13, 1036, 70, 65, 60, 55, 42, 297, 49, 48, 53, 49, 58, 244, 58, 46, 57, 54, 1017, 68, 52]","[1697548449899, 1697548450935, 1697548451005, 1697548451070, 1697548451130, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451723, 1697548451781, 1697548452025, 1697548452083, 1697548452129, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453377]"
548,548,285,27,[],200,llama-7b,64,1,3653.0,1.0,1,A100,1697548506740,1697548510393,120,100.0,27.0,"[18, 524, 194, 64, 61, 59, 56, 57, 324, 65, 64, 60, 59, 55, 42, 306, 56, 55, 50, 40, 40, 580, 56, 54, 53, 53, 49, 559]","[1697548506758, 1697548507282, 1697548507476, 1697548507540, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509569, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509834, 1697548510393]"
549,549,512,8,[],200,llama-7b,64,1,684.0,1.0,1,A100,1697548452028,1697548452712,120,11.0,1.0,"[6, 678]","[1697548452034, 1697548452712]"
550,550,281,9,[],200,llama-7b,64,1,1130.0,1.0,1,A100,1697548452717,1697548453847,120,23.0,1.0,"[29, 1100]","[1697548452746, 1697548453846]"
551,551,872,10,[],200,llama-7b,64,1,3524.0,1.0,1,A100,1697548453849,1697548457373,120,91.0,20.0,"[12, 1199, 683, 73, 76, 71, 70, 68, 53, 67, 247, 69, 52, 71, 66, 59, 359, 62, 60, 48, 59]","[1697548453861, 1697548455060, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456660, 1697548456726, 1697548456785, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373]"
552,552,129,6,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548451787,1697548488024,120,,,"[8, 916, 546, 68, 51, 63, 876, 66, 70, 54, 61, 59, 1118, 73, 76, 72, 69, 68, 53, 68, 247, 69, 52, 70, 65, 60, 359, 62, 60, 48, 59, 53, 332, 50, 61, 62, 49, 49, 61, 525, 73, 62, 48, 49, 49, 205, 46, 58, 49, 57, 56, 619, 65, 49, 63, 60, 60, 427, 69, 53, 67, 63, 63, 48, 236, 50, 63, 61, 61, 61, 526, 175, 50, 57, 57, 54, 381, 57, 57, 53, 821, 62, 61, 57, 56, 729, 460, 64, 49, 60, 45, 702, 72, 69, 54, 64, 64, 47, 690, 73, 71, 68, 67, 51, 63, 359, 73, 54, 70, 70, 65, 535, 65, 52, 66, 64, 63, 59, 569, 69, 52, 67, 52, 52, 64, 65, 844, 73, 71, 70, 67, 50, 65, 312, 74, 73, 72, 67, 64, 48, 305, 70, 69, 68, 52, 65, 49, 531, 67, 49, 50, 65, 58, 346, 68, 67, 51, 66, 65, 62, 594, 72, 71, 70, 69, 68, 63, 861, 512, 288, 74, 57, 63, 618, 251, 54, 254, 70, 68, 62, 613, 255, 71, 55, 70, 68, 63, 912, 76, 75, 71, 72, 68, 66, 588, 269, 63, 74, 264, 69, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 79, 71, 70, 57, 71, 837, 314, 308, 76, 76, 71, 70, 324, 59, 59, 79, 306, 72, 70]","[1697548451795, 1697548452711, 1697548453257, 1697548453325, 1697548453376, 1697548453439, 1697548454315, 1697548454381, 1697548454451, 1697548454505, 1697548454566, 1697548454625, 1697548455743, 1697548455816, 1697548455892, 1697548455964, 1697548456033, 1697548456101, 1697548456154, 1697548456222, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457426, 1697548457758, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459147, 1697548459205, 1697548459254, 1697548459311, 1697548459367, 1697548459986, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460710, 1697548460779, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461309, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462356, 1697548462413, 1697548462470, 1697548462524, 1697548462905, 1697548462962, 1697548463019, 1697548463072, 1697548463893, 1697548463955, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465431, 1697548465491, 1697548465536, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466561, 1697548466608, 1697548467298, 1697548467371, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468247, 1697548468317, 1697548468382, 1697548468917, 1697548468982, 1697548469034, 1697548469100, 1697548469164, 1697548469227, 1697548469286, 1697548469855, 1697548469924, 1697548469976, 1697548470043, 1697548470095, 1697548470147, 1697548470211, 1697548470276, 1697548471120, 1697548471193, 1697548471264, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472226, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472855, 1697548472904, 1697548473435, 1697548473502, 1697548473551, 1697548473601, 1697548473666, 1697548473724, 1697548474070, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474449, 1697548475043, 1697548475115, 1697548475186, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477191, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479556, 1697548479627, 1697548479682, 1697548479752, 1697548479820, 1697548479883, 1697548480795, 1697548480871, 1697548480946, 1697548481017, 1697548481089, 1697548481157, 1697548481223, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484385, 1697548484456, 1697548484526, 1697548484583, 1697548484654, 1697548485491, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486336, 1697548486406, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487233, 1697548487305, 1697548487375]"
553,553,62,28,[],200,llama-7b,64,1,3835.0,1.0,1,A100,1697548510395,1697548514230,120,91.0,20.0,"[12, 432, 65, 56, 42, 43, 52, 51, 653, 58, 53, 53, 53, 751, 350, 59, 56, 43, 52, 835, 66]","[1697548510407, 1697548510839, 1697548510904, 1697548510960, 1697548511002, 1697548511045, 1697548511097, 1697548511148, 1697548511801, 1697548511859, 1697548511912, 1697548511965, 1697548512018, 1697548512769, 1697548513119, 1697548513178, 1697548513234, 1697548513277, 1697548513329, 1697548514164, 1697548514230]"
554,554,707,15,[],200,llama-7b,64,1,1087.0,1.0,1,A100,1697548472910,1697548473997,120,8.0,1.0,"[17, 1070]","[1697548472927, 1697548473997]"
555,555,646,11,[],200,llama-7b,64,1,316.0,1.0,1,A100,1697548457377,1697548457693,120,14.0,1.0,"[22, 293]","[1697548457399, 1697548457692]"
556,556,661,12,[],200,llama-7b,64,1,2210.0,1.0,1,A100,1697548472907,1697548475117,120,161.0,10.0,"[11, 1079, 73, 68, 68, 50, 67, 64, 63, 594, 73]","[1697548472918, 1697548473997, 1697548474070, 1697548474138, 1697548474206, 1697548474256, 1697548474323, 1697548474387, 1697548474450, 1697548475044, 1697548475117]"
557,557,300,12,[],200,llama-7b,64,1,648.0,1.0,1,A100,1697548457696,1697548458344,120,9.0,1.0,"[22, 626]","[1697548457718, 1697548458344]"
558,558,160,8,[],200,llama-7b,64,1,943.0,1.0,1,A100,1697548458095,1697548459038,120,13.0,1.0,"[10, 933]","[1697548458105, 1697548459038]"
559,559,216,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438099,1697548441611,120,,,"[6, 439, 56, 52, 40, 49, 944, 68, 63, 47, 61, 59, 724, 70, 53, 70, 62, 49, 63]","[1697548438105, 1697548438544, 1697548438600, 1697548438652, 1697548438692, 1697548438741, 1697548439685, 1697548439753, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440707, 1697548440777, 1697548440830, 1697548440900, 1697548440962, 1697548441011, 1697548441074]"
560,560,860,9,[],200,llama-7b,64,1,2567.0,1.0,1,A100,1697548459040,1697548461607,120,85.0,20.0,"[12, 685, 250, 64, 50, 62, 60, 61, 426, 69, 53, 67, 63, 62, 50, 236, 50, 62, 62, 61, 61]","[1697548459052, 1697548459737, 1697548459987, 1697548460051, 1697548460101, 1697548460163, 1697548460223, 1697548460284, 1697548460710, 1697548460779, 1697548460832, 1697548460899, 1697548460962, 1697548461024, 1697548461074, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461606]"
561,561,480,6,[],200,llama-7b,64,1,942.0,1.0,1,A100,1697548445989,1697548446931,120,26.0,1.0,"[18, 924]","[1697548446007, 1697548446931]"
562,562,553,0,[],200,llama-7b,64,1,4007.0,1.0,1,A100,1697548434207,1697548438214,120,88.0,20.0,"[140, 1234, 197, 60, 61, 59, 57, 56, 742, 252, 240, 242, 67, 65, 64, 49, 186, 53, 65, 64, 54]","[1697548434347, 1697548435581, 1697548435778, 1697548435838, 1697548435899, 1697548435958, 1697548436015, 1697548436071, 1697548436813, 1697548437065, 1697548437305, 1697548437547, 1697548437614, 1697548437679, 1697548437743, 1697548437792, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438214]"
563,563,116,5,[],200,llama-7b,64,1,620.0,1.0,1,A100,1697548444669,1697548445289,120,23.0,1.0,"[11, 608]","[1697548444680, 1697548445288]"
564,564,811,6,[],200,llama-7b,64,1,11367.0,1.0,1,A100,1697548445293,1697548456660,120,457.0,72.0,"[13, 968, 79, 61, 61, 58, 45, 56, 359, 44, 44, 44, 57, 802, 62, 62, 58, 57, 56, 771, 67, 57, 45, 45, 56, 45, 307, 59, 45, 54, 53, 556, 56, 53, 52, 405, 64, 61, 55, 41, 298, 49, 48, 53, 48, 58, 245, 59, 44, 58, 54, 1016, 68, 52, 62, 875, 66, 71, 54, 62, 59, 1117, 73, 76, 72, 70, 68, 53, 67, 247, 69, 52, 71]","[1697548445306, 1697548446274, 1697548446353, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447984, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448279, 1697548449050, 1697548449117, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450439, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451780, 1697548452025, 1697548452084, 1697548452128, 1697548452186, 1697548452240, 1697548453256, 1697548453324, 1697548453376, 1697548453438, 1697548454313, 1697548454379, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455742, 1697548455815, 1697548455891, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456660]"
565,565,372,11,[],200,llama-7b,64,1,4597.0,1.0,1,A100,1697548452189,1697548456786,120,874.0,25.0,"[15, 508, 545, 68, 52, 62, 875, 66, 71, 53, 62, 59, 1119, 73, 75, 72, 70, 68, 52, 68, 247, 69, 52, 70, 65, 61]","[1697548452204, 1697548452712, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454380, 1697548454451, 1697548454504, 1697548454566, 1697548454625, 1697548455744, 1697548455817, 1697548455892, 1697548455964, 1697548456034, 1697548456102, 1697548456154, 1697548456222, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456786]"
566,566,401,4,[],200,llama-7b,64,1,3945.0,1.0,1,A100,1697548447185,1697548451130,120,84.0,20.0,"[6, 1330, 528, 67, 58, 45, 45, 56, 45, 307, 60, 44, 54, 53, 557, 55, 53, 52, 405, 64, 61]","[1697548447191, 1697548448521, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449732, 1697548449776, 1697548449830, 1697548449883, 1697548450440, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130]"
567,567,172,5,[],200,llama-7b,64,1,325.0,1.0,1,A100,1697548451134,1697548451459,120,19.0,1.0,"[25, 300]","[1697548451159, 1697548451459]"
568,568,732,6,[],200,llama-7b,64,1,2919.0,1.0,1,A100,1697548451462,1697548454381,120,345.0,12.0,"[28, 471, 65, 58, 45, 57, 54, 1017, 68, 51, 63, 876, 66]","[1697548451490, 1697548451961, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453376, 1697548453439, 1697548454315, 1697548454381]"
569,569,503,7,[],200,llama-7b,64,1,2991.0,1.0,1,A100,1697548454383,1697548457374,120,109.0,20.0,"[13, 664, 683, 73, 76, 72, 69, 69, 52, 68, 246, 70, 52, 70, 65, 60, 359, 62, 61, 48, 59]","[1697548454396, 1697548455060, 1697548455743, 1697548455816, 1697548455892, 1697548455964, 1697548456033, 1697548456102, 1697548456154, 1697548456222, 1697548456468, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457206, 1697548457267, 1697548457315, 1697548457374]"
570,570,649,6,[],200,llama-7b,64,1,4400.0,1.0,1,A100,1697548444716,1697548449116,120,244.0,20.0,"[17, 1540, 79, 62, 61, 58, 45, 56, 358, 45, 44, 44, 57, 802, 62, 63, 58, 57, 56, 770, 66]","[1697548444733, 1697548446273, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447984, 1697548448046, 1697548448109, 1697548448167, 1697548448224, 1697548448280, 1697548449050, 1697548449116]"
571,571,141,12,[],200,llama-7b,64,1,2580.0,1.0,1,A100,1697548456788,1697548459368,120,89.0,20.0,"[6, 898, 66, 50, 62, 61, 49, 49, 61, 526, 73, 61, 48, 49, 49, 205, 47, 58, 49, 56, 57]","[1697548456794, 1697548457692, 1697548457758, 1697548457808, 1697548457870, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458689, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459206, 1697548459255, 1697548459311, 1697548459368]"
572,572,550,1,[],200,llama-7b,64,1,3096.0,1.0,1,A100,1697548441617,1697548444713,120,91.0,20.0,"[160, 983, 54, 60, 48, 46, 38, 417, 54, 55, 51, 40, 47, 680, 58, 46, 57, 55, 54, 47, 46]","[1697548441777, 1697548442760, 1697548442814, 1697548442874, 1697548442922, 1697548442968, 1697548443006, 1697548443423, 1697548443477, 1697548443532, 1697548443583, 1697548443623, 1697548443670, 1697548444350, 1697548444408, 1697548444454, 1697548444511, 1697548444566, 1697548444620, 1697548444667, 1697548444713]"
573,573,467,7,[],200,llama-7b,64,1,2233.0,1.0,1,A100,1697548456663,1697548458896,120,93.0,20.0,"[10, 404, 67, 62, 61, 47, 59, 54, 331, 50, 61, 62, 49, 49, 61, 526, 72, 62, 48, 48, 50]","[1697548456673, 1697548457077, 1697548457144, 1697548457206, 1697548457267, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458688, 1697548458750, 1697548458798, 1697548458846, 1697548458896]"
574,574,731,13,[],200,llama-7b,64,1,3154.0,1.0,1,A100,1697548459371,1697548462525,120,89.0,20.0,"[15, 1237, 88, 69, 52, 67, 63, 63, 48, 237, 49, 63, 61, 61, 61, 526, 175, 51, 57, 57, 53]","[1697548459386, 1697548460623, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461310, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462524]"
575,575,203,2,[],200,llama-7b,64,1,3268.0,1.0,1,A100,1697548444716,1697548447984,120,364.0,13.0,"[8, 1549, 79, 62, 61, 57, 46, 56, 358, 45, 45, 43, 57, 802]","[1697548444724, 1697548446273, 1697548446352, 1697548446414, 1697548446475, 1697548446532, 1697548446578, 1697548446634, 1697548446992, 1697548447037, 1697548447082, 1697548447125, 1697548447182, 1697548447984]"
576,576,891,4,[],200,llama-7b,64,1,1341.0,1.0,1,A100,1697548443008,1697548444349,120,52.0,2.0,"[6, 1126, 209]","[1697548443014, 1697548444140, 1697548444349]"
577,577,663,5,[],200,llama-7b,64,1,3756.0,1.0,1,A100,1697548444352,1697548448108,120,79.0,20.0,"[7, 1179, 310, 55, 41, 40, 369, 61, 60, 58, 47, 56, 358, 44, 44, 44, 57, 801, 63, 62]","[1697548444359, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446353, 1697548446414, 1697548446474, 1697548446532, 1697548446579, 1697548446635, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448108]"
578,578,71,2,[],200,llama-7b,64,1,2905.0,1.0,1,A100,1697548443674,1697548446579,120,364.0,11.0,"[6, 1608, 251, 309, 55, 41, 40, 368, 62, 61, 58, 46]","[1697548443680, 1697548445288, 1697548445539, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446579]"
579,579,767,3,[],200,llama-7b,64,1,349.0,1.0,1,A100,1697548446582,1697548446931,120,11.0,1.0,"[6, 343]","[1697548446588, 1697548446931]"
580,580,421,4,[],200,llama-7b,64,1,3506.0,1.0,1,A100,1697548446934,1697548450440,120,85.0,20.0,"[10, 531, 509, 62, 62, 59, 57, 56, 769, 67, 57, 46, 46, 55, 45, 308, 59, 45, 53, 53, 557]","[1697548446944, 1697548447475, 1697548447984, 1697548448046, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449116, 1697548449173, 1697548449219, 1697548449265, 1697548449320, 1697548449365, 1697548449673, 1697548449732, 1697548449777, 1697548449830, 1697548449883, 1697548450440]"
581,581,257,7,[],200,llama-7b,64,1,540.0,1.0,1,A100,1697548446936,1697548447476,120,14.0,1.0,"[22, 518]","[1697548446958, 1697548447476]"
582,582,840,8,[],200,llama-7b,64,1,1043.0,1.0,1,A100,1697548447479,1697548448522,120,17.0,1.0,"[26, 1017]","[1697548447505, 1697548448522]"
583,583,66,5,[],200,llama-7b,64,1,3802.0,1.0,1,A100,1697548446637,1697548450439,120,84.0,20.0,"[11, 827, 508, 63, 62, 58, 57, 57, 769, 66, 59, 45, 46, 56, 44, 307, 59, 45, 54, 53, 556]","[1697548446648, 1697548447475, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448280, 1697548449049, 1697548449115, 1697548449174, 1697548449219, 1697548449265, 1697548449321, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450439]"
584,584,615,9,[],200,llama-7b,64,1,3197.0,1.0,1,A100,1697548448525,1697548451722,120,93.0,20.0,"[15, 1064, 68, 59, 45, 54, 53, 558, 55, 52, 52, 405, 64, 61, 55, 42, 297, 49, 48, 53, 48]","[1697548448540, 1697548449604, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450441, 1697548450496, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722]"
585,585,920,2,[],200,llama-7b,64,1,1260.0,1.0,1,A100,1697548441614,1697548442874,120,96.0,4.0,"[21, 440, 33, 706, 60]","[1697548441635, 1697548442075, 1697548442108, 1697548442814, 1697548442874]"
586,586,582,3,[],200,llama-7b,64,1,478.0,1.0,1,A100,1697548442876,1697548443354,120,19.0,1.0,"[13, 465]","[1697548442889, 1697548443354]"
587,587,351,4,[],200,llama-7b,64,1,1210.0,1.0,1,A100,1697548443356,1697548444566,120,216.0,6.0,"[22, 762, 209, 59, 46, 56, 56]","[1697548443378, 1697548444140, 1697548444349, 1697548444408, 1697548444454, 1697548444510, 1697548444566]"
588,588,10,5,[],200,llama-7b,64,1,1906.0,1.0,1,A100,1697548444569,1697548446475,120,563.0,9.0,"[16, 703, 250, 310, 55, 41, 41, 368, 60, 62]","[1697548444585, 1697548445288, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445985, 1697548446353, 1697548446413, 1697548446475]"
589,589,637,5,[],200,llama-7b,64,1,3019.0,1.0,1,A100,1697548448112,1697548451131,120,96.0,20.0,"[18, 392, 528, 66, 58, 45, 45, 57, 44, 307, 59, 45, 54, 53, 557, 56, 52, 52, 405, 64, 61]","[1697548448130, 1697548448522, 1697548449050, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449321, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450440, 1697548450496, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130]"
590,590,708,6,[],200,llama-7b,64,1,453.0,1.0,1,A100,1697548446478,1697548446931,120,140.0,1.0,"[20, 433]","[1697548446498, 1697548446931]"
591,591,340,7,[],200,llama-7b,64,1,3507.0,1.0,1,A100,1697548446933,1697548450440,120,85.0,20.0,"[5, 537, 508, 63, 62, 59, 57, 56, 769, 66, 59, 45, 46, 55, 45, 308, 58, 46, 53, 53, 557]","[1697548446938, 1697548447475, 1697548447983, 1697548448046, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449115, 1697548449174, 1697548449219, 1697548449265, 1697548449320, 1697548449365, 1697548449673, 1697548449731, 1697548449777, 1697548449830, 1697548449883, 1697548450440]"
592,592,269,10,[],200,llama-7b,64,1,236.0,1.0,1,A100,1697548451726,1697548451962,120,11.0,1.0,"[10, 226]","[1697548451736, 1697548451962]"
593,593,16,11,[],200,llama-7b,64,1,746.0,1.0,1,A100,1697548451966,1697548452712,120,9.0,1.0,"[29, 717]","[1697548451995, 1697548452712]"
594,594,517,10,[],200,llama-7b,64,1,1223.0,1.0,1,A100,1697548461611,1697548462834,120,15.0,1.0,"[29, 1194]","[1697548461640, 1697548462834]"
595,595,603,12,[],200,llama-7b,64,1,1131.0,1.0,1,A100,1697548452715,1697548453846,120,9.0,1.0,"[26, 1105]","[1697548452741, 1697548453846]"
596,596,298,6,[],200,llama-7b,64,1,325.0,1.0,1,A100,1697548451134,1697548451459,120,17.0,1.0,"[15, 310]","[1697548451149, 1697548451459]"
597,597,287,11,[],200,llama-7b,64,1,739.0,1.0,1,A100,1697548462841,1697548463580,120,10.0,1.0,"[31, 708]","[1697548462872, 1697548463580]"
598,598,301,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438217,1697548441611,120,,,"[48, 1013, 406, 70, 62, 47, 61, 59, 723, 71, 52, 70, 64, 49, 62]","[1697548438265, 1697548439278, 1697548439684, 1697548439754, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440829, 1697548440899, 1697548440963, 1697548441012, 1697548441074]"
599,599,68,7,[],200,llama-7b,64,1,500.0,1.0,1,A100,1697548451462,1697548451962,120,12.0,1.0,"[23, 477]","[1697548451485, 1697548451962]"
600,600,652,8,[],200,llama-7b,64,1,747.0,1.0,1,A100,1697548451965,1697548452712,120,14.0,1.0,"[20, 727]","[1697548451985, 1697548452712]"
601,601,875,12,[],200,llama-7b,64,1,5517.0,1.0,1,A100,1697548463583,1697548469100,120,31.0,31.0,"[23, 949, 303, 460, 64, 50, 59, 46, 701, 72, 70, 53, 64, 63, 49, 691, 72, 71, 67, 68, 51, 62, 360, 72, 55, 70, 70, 64, 536, 65, 51, 66]","[1697548463606, 1697548464555, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465491, 1697548465537, 1697548466238, 1697548466310, 1697548466380, 1697548466433, 1697548466497, 1697548466560, 1697548466609, 1697548467300, 1697548467372, 1697548467443, 1697548467510, 1697548467578, 1697548467629, 1697548467691, 1697548468051, 1697548468123, 1697548468178, 1697548468248, 1697548468318, 1697548468382, 1697548468918, 1697548468983, 1697548469034, 1697548469100]"
602,602,429,9,[],200,llama-7b,64,1,7448.0,1.0,1,A100,1697548452715,1697548460163,120,244.0,50.0,"[21, 1110, 468, 66, 70, 54, 62, 59, 1117, 73, 76, 72, 71, 67, 53, 67, 248, 69, 52, 70, 65, 61, 358, 62, 60, 48, 59, 53, 332, 50, 61, 62, 49, 49, 61, 525, 73, 62, 48, 49, 49, 205, 47, 58, 48, 57, 57, 618, 65, 49, 63]","[1697548452736, 1697548453846, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455742, 1697548455815, 1697548455891, 1697548455963, 1697548456034, 1697548456101, 1697548456154, 1697548456221, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456786, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457426, 1697548457758, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459206, 1697548459254, 1697548459311, 1697548459368, 1697548459986, 1697548460051, 1697548460100, 1697548460163]"
603,603,374,13,[],200,llama-7b,64,1,3524.0,1.0,1,A100,1697548453849,1697548457373,120,85.0,20.0,"[7, 1204, 683, 73, 76, 71, 70, 68, 53, 67, 247, 69, 52, 71, 66, 60, 358, 62, 60, 48, 59]","[1697548453856, 1697548455060, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456660, 1697548456726, 1697548456786, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373]"
604,604,884,2,[],200,llama-7b,64,1,3049.0,1.0,1,A100,1697548441615,1697548444664,120,90.0,20.0,"[45, 415, 34, 705, 60, 48, 46, 37, 417, 55, 54, 51, 41, 47, 679, 58, 46, 57, 56, 53, 44]","[1697548441660, 1697548442075, 1697548442109, 1697548442814, 1697548442874, 1697548442922, 1697548442968, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443670, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444619, 1697548444663]"
605,605,644,13,[],200,llama-7b,64,1,478.0,1.0,1,A100,1697548469105,1697548469583,120,19.0,1.0,"[13, 465]","[1697548469118, 1697548469583]"
606,606,325,1,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438216,1697548441611,120,,,"[10, 1051, 408, 69, 62, 46, 62, 58, 725, 70, 53, 70, 62, 50, 62]","[1697548438226, 1697548439277, 1697548439685, 1697548439754, 1697548439816, 1697548439862, 1697548439924, 1697548439982, 1697548440707, 1697548440777, 1697548440830, 1697548440900, 1697548440962, 1697548441012, 1697548441074]"
607,607,524,16,[],200,llama-7b,64,1,3848.0,1.0,1,A100,1697548471268,1697548475116,120,100.0,30.0,"[13, 470, 78, 73, 74, 72, 67, 63, 49, 304, 71, 68, 68, 53, 66, 48, 529, 68, 49, 50, 65, 59, 346, 68, 66, 52, 66, 64, 63, 593, 73]","[1697548471281, 1697548471751, 1697548471829, 1697548471902, 1697548471976, 1697548472048, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472791, 1697548472857, 1697548472905, 1697548473434, 1697548473502, 1697548473551, 1697548473601, 1697548473666, 1697548473725, 1697548474071, 1697548474139, 1697548474205, 1697548474257, 1697548474323, 1697548474387, 1697548474450, 1697548475043, 1697548475116]"
608,608,778,7,[],200,llama-7b,64,1,7267.0,1.0,1,A100,1697548444514,1697548451781,120,16.0,50.0,"[6, 768, 250, 310, 55, 41, 40, 369, 60, 61, 58, 46, 55, 360, 44, 44, 44, 58, 800, 63, 62, 59, 57, 56, 769, 67, 58, 45, 45, 56, 45, 307, 59, 45, 54, 53, 556, 57, 52, 52, 405, 64, 61, 55, 42, 297, 49, 48, 53, 48, 59]","[1697548444520, 1697548445288, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446353, 1697548446413, 1697548446474, 1697548446532, 1697548446578, 1697548446633, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447183, 1697548447983, 1697548448046, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450439, 1697548450496, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451781]"
609,609,71,2,[],200,llama-7b,64,1,2008.0,1.0,1,A100,1697548441616,1697548443624,120,364.0,11.0,"[68, 1074, 57, 60, 47, 46, 37, 417, 55, 54, 51, 42]","[1697548441684, 1697548442758, 1697548442815, 1697548442875, 1697548442922, 1697548442968, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443624]"
610,610,654,3,[],200,llama-7b,64,1,828.0,1.0,1,A100,1697548443626,1697548444454,120,47.0,4.0,"[14, 501, 209, 58, 46]","[1697548443640, 1697548444141, 1697548444350, 1697548444408, 1697548444454]"
611,611,432,4,[],200,llama-7b,64,1,831.0,1.0,1,A100,1697548444457,1697548445288,120,13.0,1.0,"[13, 818]","[1697548444470, 1697548445288]"
612,612,243,8,[],200,llama-7b,64,1,1201.0,1.0,1,A100,1697548458899,1697548460100,120,67.0,4.0,"[18, 820, 250, 65, 48]","[1697548458917, 1697548459737, 1697548459987, 1697548460052, 1697548460100]"
613,613,85,5,[],200,llama-7b,64,1,3826.0,1.0,1,A100,1697548445291,1697548449117,120,88.0,20.0,"[12, 971, 79, 61, 61, 58, 45, 56, 359, 44, 44, 44, 57, 802, 62, 61, 59, 58, 55, 771, 67]","[1697548445303, 1697548446274, 1697548446353, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447984, 1697548448046, 1697548448107, 1697548448166, 1697548448224, 1697548448279, 1697548449050, 1697548449117]"
614,614,165,10,[],200,llama-7b,64,1,3610.0,1.0,1,A100,1697548464019,1697548467629,120,83.0,20.0,"[20, 517, 302, 460, 65, 53, 56, 45, 701, 72, 69, 54, 64, 64, 48, 689, 74, 71, 68, 67, 51]","[1697548464039, 1697548464556, 1697548464858, 1697548465318, 1697548465383, 1697548465436, 1697548465492, 1697548465537, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467443, 1697548467511, 1697548467578, 1697548467629]"
615,615,826,9,[],200,llama-7b,64,1,2421.0,1.0,1,A100,1697548460103,1697548462524,120,87.0,20.0,"[17, 503, 88, 69, 52, 67, 63, 63, 49, 236, 49, 63, 62, 60, 61, 526, 175, 51, 57, 57, 53]","[1697548460120, 1697548460623, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074, 1697548461310, 1697548461359, 1697548461422, 1697548461484, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462524]"
616,616,912,3,[],200,llama-7b,64,1,3143.0,1.0,1,A100,1697548447987,1697548451130,120,92.0,20.0,"[6, 529, 527, 67, 58, 45, 45, 56, 45, 307, 59, 45, 54, 53, 557, 56, 53, 51, 405, 64, 61]","[1697548447993, 1697548448522, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450440, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451130]"
617,617,836,4,[],200,llama-7b,64,1,1558.0,1.0,1,A100,1697548444716,1697548446274,120,11.0,1.0,"[9, 1548]","[1697548444725, 1697548446273]"
618,618,611,5,[],200,llama-7b,64,1,654.0,1.0,1,A100,1697548446277,1697548446931,120,14.0,1.0,"[11, 643]","[1697548446288, 1697548446931]"
619,619,596,2,[],200,llama-7b,64,1,4476.0,1.0,1,A100,1697548436423,1697548440899,120,87.0,20.0,"[18, 1466, 71, 53, 65, 64, 54, 387, 50, 40, 50, 943, 71, 60, 48, 61, 59, 723, 71, 53, 69]","[1697548436441, 1697548437907, 1697548437978, 1697548438031, 1697548438096, 1697548438160, 1697548438214, 1697548438601, 1697548438651, 1697548438691, 1697548438741, 1697548439684, 1697548439755, 1697548439815, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440777, 1697548440830, 1697548440899]"
620,620,264,6,[],200,llama-7b,64,1,3506.0,1.0,1,A100,1697548446934,1697548450440,120,86.0,20.0,"[9, 532, 509, 62, 62, 59, 57, 56, 769, 67, 57, 46, 46, 55, 45, 308, 59, 44, 54, 53, 557]","[1697548446943, 1697548447475, 1697548447984, 1697548448046, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449116, 1697548449173, 1697548449219, 1697548449265, 1697548449320, 1697548449365, 1697548449673, 1697548449732, 1697548449776, 1697548449830, 1697548449883, 1697548450440]"
621,621,897,8,[],200,llama-7b,64,1,578.0,1.0,1,A100,1697548446354,1697548446932,120,9.0,1.0,"[6, 572]","[1697548446360, 1697548446932]"
622,622,799,16,[],200,llama-7b,64,1,3204.0,1.0,1,A100,1697548469586,1697548472790,120,84.0,20.0,"[20, 1095, 420, 73, 71, 69, 67, 50, 65, 312, 74, 73, 72, 67, 64, 49, 304, 70, 69, 69, 51]","[1697548469606, 1697548470701, 1697548471121, 1697548471194, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472227, 1697548472531, 1697548472601, 1697548472670, 1697548472739, 1697548472790]"
623,623,433,10,[],200,llama-7b,64,1,416.0,1.0,1,A100,1697548456729,1697548457145,120,109.0,2.0,"[20, 396]","[1697548456749, 1697548457145]"
624,624,670,9,[],200,llama-7b,64,1,2895.0,1.0,1,A100,1697548446935,1697548449830,120,67.0,18.0,"[13, 528, 508, 63, 61, 59, 57, 56, 769, 67, 57, 46, 45, 56, 45, 308, 59, 45, 53]","[1697548446948, 1697548447476, 1697548447984, 1697548448047, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449116, 1697548449173, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449673, 1697548449732, 1697548449777, 1697548449830]"
625,625,179,11,[],200,llama-7b,64,1,723.0,1.0,1,A100,1697548457147,1697548457870,120,161.0,4.0,"[6, 540, 65, 50, 62]","[1697548457153, 1697548457693, 1697548457758, 1697548457808, 1697548457870]"
626,626,12,7,[],200,llama-7b,64,1,489.0,1.0,1,A100,1697548450446,1697548450935,120,11.0,1.0,"[26, 463]","[1697548450472, 1697548450935]"
627,627,166,10,[],200,llama-7b,64,1,760.0,1.0,1,A100,1697548449608,1697548450368,120,14.0,1.0,"[36, 723]","[1697548449644, 1697548450367]"
628,628,709,8,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548450944,1697548488023,120,,,"[21, 494, 66, 48, 49, 52, 48, 59, 245, 58, 45, 57, 54, 1018, 66, 52, 63, 875, 66, 70, 55, 61, 59, 1118, 73, 76, 71, 70, 68, 53, 67, 248, 68, 53, 70, 65, 60, 359, 62, 60, 48, 59, 53, 332, 50, 61, 62, 49, 49, 61, 525, 73, 62, 48, 48, 50, 205, 47, 57, 49, 57, 56, 619, 65, 49, 63, 60, 60, 427, 69, 52, 67, 64, 62, 49, 237, 49, 63, 61, 61, 61, 525, 176, 50, 57, 57, 54, 381, 57, 57, 53, 821, 62, 60, 58, 56, 729, 460, 64, 49, 60, 45, 702, 72, 69, 54, 64, 63, 49, 689, 73, 71, 68, 67, 51, 63, 359, 73, 54, 70, 70, 65, 535, 65, 52, 66, 64, 63, 59, 569, 69, 52, 67, 52, 51, 65, 65, 844, 73, 71, 70, 67, 49, 66, 312, 74, 73, 72, 67, 64, 48, 305, 70, 69, 68, 52, 66, 48, 530, 68, 49, 50, 65, 58, 346, 68, 67, 51, 66, 65, 62, 594, 72, 71, 70, 69, 68, 63, 861, 512, 288, 74, 57, 63, 618, 251, 54, 254, 70, 68, 62, 613, 255, 71, 55, 71, 67, 63, 912, 76, 74, 73, 71, 67, 67, 588, 269, 63, 74, 265, 68, 51, 68, 839, 75, 55, 73, 66, 66, 49, 338, 76, 79, 71, 70, 57, 70, 838, 314, 307, 77, 76, 71, 70, 324, 59, 59, 79, 306, 72, 70]","[1697548450965, 1697548451459, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451722, 1697548451781, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453258, 1697548453324, 1697548453376, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454505, 1697548454566, 1697548454625, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456469, 1697548456537, 1697548456590, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457426, 1697548457758, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458750, 1697548458798, 1697548458846, 1697548458896, 1697548459101, 1697548459148, 1697548459205, 1697548459254, 1697548459311, 1697548459367, 1697548459986, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460710, 1697548460779, 1697548460831, 1697548460898, 1697548460962, 1697548461024, 1697548461073, 1697548461310, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462130, 1697548462306, 1697548462356, 1697548462413, 1697548462470, 1697548462524, 1697548462905, 1697548462962, 1697548463019, 1697548463072, 1697548463893, 1697548463955, 1697548464015, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465431, 1697548465491, 1697548465536, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466560, 1697548466609, 1697548467298, 1697548467371, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468247, 1697548468317, 1697548468382, 1697548468917, 1697548468982, 1697548469034, 1697548469100, 1697548469164, 1697548469227, 1697548469286, 1697548469855, 1697548469924, 1697548469976, 1697548470043, 1697548470095, 1697548470146, 1697548470211, 1697548470276, 1697548471120, 1697548471193, 1697548471264, 1697548471334, 1697548471401, 1697548471450, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472226, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472904, 1697548473434, 1697548473502, 1697548473551, 1697548473601, 1697548473666, 1697548473724, 1697548474070, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474449, 1697548475043, 1697548475115, 1697548475186, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477191, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479556, 1697548479627, 1697548479682, 1697548479753, 1697548479820, 1697548479883, 1697548480795, 1697548480871, 1697548480945, 1697548481018, 1697548481089, 1697548481156, 1697548481223, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484385, 1697548484456, 1697548484526, 1697548484583, 1697548484653, 1697548485491, 1697548485805, 1697548486112, 1697548486189, 1697548486265, 1697548486336, 1697548486406, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487233, 1697548487305, 1697548487375]"
629,629,865,11,[],200,llama-7b,64,1,561.0,1.0,1,A100,1697548450374,1697548450935,120,9.0,1.0,"[22, 539]","[1697548450396, 1697548450935]"
630,630,276,14,[],200,llama-7b,64,1,2528.0,1.0,1,A100,1697548469587,1697548472115,120,732.0,13.0,"[29, 1505, 73, 71, 69, 68, 49, 66, 311, 74, 73, 72, 67]","[1697548469616, 1697548471121, 1697548471194, 1697548471265, 1697548471334, 1697548471402, 1697548471451, 1697548471517, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114]"
631,631,645,6,[],200,llama-7b,64,1,2936.0,1.0,1,A100,1697548450441,1697548453377,120,86.0,20.0,"[6, 488, 70, 64, 61, 55, 42, 297, 49, 48, 53, 49, 58, 244, 58, 45, 58, 54, 1017, 68, 52]","[1697548450447, 1697548450935, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451723, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453377]"
632,632,331,10,[],200,llama-7b,64,1,537.0,1.0,1,A100,1697548449831,1697548450368,120,26.0,1.0,"[9, 528]","[1697548449840, 1697548450368]"
633,633,762,12,[],200,llama-7b,64,1,2838.0,1.0,1,A100,1697548457873,1697548460711,120,92.0,20.0,"[6, 465, 272, 72, 62, 49, 48, 50, 204, 47, 60, 46, 57, 57, 619, 64, 50, 63, 59, 61, 427]","[1697548457879, 1697548458344, 1697548458616, 1697548458688, 1697548458750, 1697548458799, 1697548458847, 1697548458897, 1697548459101, 1697548459148, 1697548459208, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460101, 1697548460164, 1697548460223, 1697548460284, 1697548460711]"
634,634,99,11,[],200,llama-7b,64,1,563.0,1.0,1,A100,1697548450372,1697548450935,120,10.0,1.0,"[26, 537]","[1697548450398, 1697548450935]"
635,635,422,7,[],200,llama-7b,64,1,468.0,1.0,1,A100,1697548453379,1697548453847,120,26.0,1.0,"[11, 457]","[1697548453390, 1697548453847]"
636,636,689,12,[],200,llama-7b,64,1,520.0,1.0,1,A100,1697548450939,1697548451459,120,15.0,1.0,"[22, 498]","[1697548450961, 1697548451459]"
637,637,458,13,[],200,llama-7b,64,1,498.0,1.0,1,A100,1697548451464,1697548451962,120,11.0,1.0,"[38, 460]","[1697548451502, 1697548451962]"
638,638,114,14,[],200,llama-7b,64,1,4505.0,1.0,1,A100,1697548451964,1697548456469,120,88.0,20.0,"[11, 736, 546, 68, 52, 62, 874, 66, 72, 52, 63, 59, 1118, 73, 76, 72, 69, 68, 53, 68, 247]","[1697548451975, 1697548452711, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454313, 1697548454379, 1697548454451, 1697548454503, 1697548454566, 1697548454625, 1697548455743, 1697548455816, 1697548455892, 1697548455964, 1697548456033, 1697548456101, 1697548456154, 1697548456222, 1697548456469]"
639,639,74,8,[],200,llama-7b,64,1,3524.0,1.0,1,A100,1697548453850,1697548457374,120,88.0,20.0,"[15, 1195, 683, 73, 76, 71, 70, 68, 53, 67, 247, 69, 53, 70, 66, 59, 359, 62, 60, 48, 59]","[1697548453865, 1697548455060, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456590, 1697548456660, 1697548456726, 1697548456785, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373]"
640,640,820,15,[],200,llama-7b,64,1,1336.0,1.0,1,A100,1697548456472,1697548457808,120,161.0,9.0,"[6, 598, 68, 62, 60, 48, 59, 54, 331, 50]","[1697548456478, 1697548457076, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808]"
641,641,469,16,[],200,llama-7b,64,1,533.0,1.0,1,A100,1697548457811,1697548458344,120,17.0,1.0,"[6, 527]","[1697548457817, 1697548458344]"
642,642,215,17,[],200,llama-7b,64,1,690.0,1.0,1,A100,1697548458348,1697548459038,120,12.0,1.0,"[14, 676]","[1697548458362, 1697548459038]"
643,643,913,18,[],200,llama-7b,64,1,2566.0,1.0,1,A100,1697548459040,1697548461606,120,88.0,20.0,"[11, 685, 251, 64, 50, 62, 60, 61, 426, 69, 53, 67, 63, 62, 50, 236, 50, 62, 62, 61, 61]","[1697548459051, 1697548459736, 1697548459987, 1697548460051, 1697548460101, 1697548460163, 1697548460223, 1697548460284, 1697548460710, 1697548460779, 1697548460832, 1697548460899, 1697548460962, 1697548461024, 1697548461074, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461606]"
644,644,303,7,[],200,llama-7b,64,1,2603.0,1.0,1,A100,1697548449119,1697548451722,120,88.0,20.0,"[11, 474, 68, 60, 45, 53, 53, 558, 55, 53, 51, 405, 64, 62, 54, 42, 297, 49, 48, 52, 49]","[1697548449130, 1697548449604, 1697548449672, 1697548449732, 1697548449777, 1697548449830, 1697548449883, 1697548450441, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451131, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451673, 1697548451722]"
645,645,780,9,[],200,llama-7b,64,1,1992.0,1.0,1,A100,1697548457376,1697548459368,120,85.0,20.0,"[15, 301, 66, 50, 62, 62, 49, 48, 62, 525, 72, 62, 48, 49, 50, 204, 46, 58, 50, 56, 57]","[1697548457391, 1697548457692, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457981, 1697548458029, 1697548458091, 1697548458616, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458897, 1697548459101, 1697548459147, 1697548459205, 1697548459255, 1697548459311, 1697548459368]"
646,646,659,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548444681,1697548488026,120,,,"[20, 587, 251, 310, 54, 41, 41, 367, 61, 62, 57, 46, 56, 358, 45, 45, 43, 57, 802, 62, 63, 58, 57, 56, 769, 67, 58, 45, 45, 56, 45, 307, 59, 45, 54, 53, 556, 56, 53, 52, 405, 64, 61, 55, 41, 298, 49, 48, 53, 48, 58, 245, 59, 44, 58, 54, 1016, 68, 52, 62, 876, 65, 71, 54, 62, 59, 1117, 73, 76, 72, 69, 69, 53, 67, 247, 69, 52, 70, 66, 60, 359, 62, 60, 48, 59, 53, 331, 51, 61, 62, 49, 49, 61, 525, 73, 61, 49, 48, 50, 205, 46, 58, 49, 57, 56, 619, 65, 49, 62, 61, 60, 427, 69, 52, 67, 63, 63, 49, 237, 49, 62, 62, 61, 61, 526, 175, 50, 57, 57, 54, 381, 56, 58, 53, 821, 62, 61, 57, 56, 729, 459, 65, 49, 60, 45, 702, 72, 69, 54, 64, 63, 48, 690, 73, 71, 68, 67, 51, 63, 359, 73, 54, 70, 70, 65, 535, 65, 51, 67, 64, 62, 60, 569, 69, 52, 68, 50, 52, 66, 64, 844, 73, 71, 70, 67, 50, 65, 312, 73, 74, 72, 67, 64, 48, 305, 70, 68, 69, 52, 65, 49, 530, 67, 50, 50, 65, 58, 346, 68, 67, 51, 66, 65, 62, 594, 72, 71, 70, 69, 67, 64, 861, 512, 288, 74, 57, 63, 618, 251, 54, 254, 70, 68, 62, 613, 255, 71, 55, 71, 67, 63, 912, 76, 75, 72, 71, 67, 67, 588, 269, 63, 74, 265, 68, 51, 68, 839, 75, 55, 73, 66, 65, 50, 338, 76, 75, 75, 70, 57, 70, 838, 313, 308, 77, 76, 71, 70, 324, 59, 59, 79, 306, 72, 69]","[1697548444701, 1697548445288, 1697548445539, 1697548445849, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446413, 1697548446475, 1697548446532, 1697548446578, 1697548446634, 1697548446992, 1697548447037, 1697548447082, 1697548447125, 1697548447182, 1697548447984, 1697548448046, 1697548448109, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450439, 1697548450495, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451780, 1697548452025, 1697548452084, 1697548452128, 1697548452186, 1697548452240, 1697548453256, 1697548453324, 1697548453376, 1697548453438, 1697548454314, 1697548454379, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455742, 1697548455815, 1697548455891, 1697548455963, 1697548456032, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456659, 1697548456725, 1697548456785, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457426, 1697548457757, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458615, 1697548458688, 1697548458749, 1697548458798, 1697548458846, 1697548458896, 1697548459101, 1697548459147, 1697548459205, 1697548459254, 1697548459311, 1697548459367, 1697548459986, 1697548460051, 1697548460100, 1697548460162, 1697548460223, 1697548460283, 1697548460710, 1697548460779, 1697548460831, 1697548460898, 1697548460961, 1697548461024, 1697548461073, 1697548461310, 1697548461359, 1697548461421, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462356, 1697548462413, 1697548462470, 1697548462524, 1697548462905, 1697548462961, 1697548463019, 1697548463072, 1697548463893, 1697548463955, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465317, 1697548465382, 1697548465431, 1697548465491, 1697548465536, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466560, 1697548466608, 1697548467298, 1697548467371, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468247, 1697548468317, 1697548468382, 1697548468917, 1697548468982, 1697548469033, 1697548469100, 1697548469164, 1697548469226, 1697548469286, 1697548469855, 1697548469924, 1697548469976, 1697548470044, 1697548470094, 1697548470146, 1697548470212, 1697548470276, 1697548471120, 1697548471193, 1697548471264, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471901, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472226, 1697548472531, 1697548472601, 1697548472669, 1697548472738, 1697548472790, 1697548472855, 1697548472904, 1697548473434, 1697548473501, 1697548473551, 1697548473601, 1697548473666, 1697548473724, 1697548474070, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474449, 1697548475043, 1697548475115, 1697548475186, 1697548475256, 1697548475325, 1697548475392, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477191, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479556, 1697548479627, 1697548479682, 1697548479753, 1697548479820, 1697548479883, 1697548480795, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481156, 1697548481223, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711, 1697548483777, 1697548483842, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484526, 1697548484583, 1697548484653, 1697548485491, 1697548485804, 1697548486112, 1697548486189, 1697548486265, 1697548486336, 1697548486406, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487233, 1697548487305, 1697548487374]"
647,647,34,14,[],200,llama-7b,64,1,315.0,1.0,1,A100,1697548457378,1697548457693,120,12.0,1.0,"[29, 286]","[1697548457407, 1697548457693]"
648,648,542,13,[],200,llama-7b,64,1,527.0,1.0,1,A100,1697548460716,1697548461243,120,11.0,1.0,"[27, 500]","[1697548460743, 1697548461243]"
649,649,733,15,[],200,llama-7b,64,1,647.0,1.0,1,A100,1697548457697,1697548458344,120,31.0,1.0,"[25, 622]","[1697548457722, 1697548458344]"
650,650,392,16,[],200,llama-7b,64,1,689.0,1.0,1,A100,1697548458349,1697548459038,120,20.0,1.0,"[14, 675]","[1697548458363, 1697548459038]"
651,651,162,17,[],200,llama-7b,64,1,2566.0,1.0,1,A100,1697548459041,1697548461607,120,90.0,20.0,"[15, 681, 250, 64, 50, 62, 60, 61, 426, 69, 53, 67, 63, 62, 50, 236, 50, 62, 62, 61, 62]","[1697548459056, 1697548459737, 1697548459987, 1697548460051, 1697548460101, 1697548460163, 1697548460223, 1697548460284, 1697548460710, 1697548460779, 1697548460832, 1697548460899, 1697548460962, 1697548461024, 1697548461074, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461607]"
652,652,669,11,[],200,llama-7b,64,1,4074.0,1.0,1,A100,1697548462360,1697548466434,120,83.0,20.0,"[13, 461, 72, 56, 57, 54, 820, 63, 60, 57, 56, 730, 460, 64, 48, 60, 45, 703, 71, 70, 54]","[1697548462373, 1697548462834, 1697548462906, 1697548462962, 1697548463019, 1697548463073, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464859, 1697548465319, 1697548465383, 1697548465431, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466434]"
653,653,198,14,[],200,llama-7b,64,1,4191.0,1.0,1,A100,1697548461246,1697548465437,120,96.0,20.0,"[11, 678, 196, 175, 51, 57, 57, 54, 381, 56, 58, 52, 822, 62, 61, 56, 56, 729, 460, 64, 54]","[1697548461257, 1697548461935, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462525, 1697548462906, 1697548462962, 1697548463020, 1697548463072, 1697548463894, 1697548463956, 1697548464017, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465436]"
654,654,707,15,[],200,llama-7b,64,1,662.0,1.0,1,A100,1697548468920,1697548469582,120,8.0,1.0,"[10, 652]","[1697548468930, 1697548469582]"
655,655,360,16,[],200,llama-7b,64,1,1120.0,1.0,1,A100,1697548469585,1697548470705,120,16.0,1.0,"[21, 1098]","[1697548469606, 1697548470704]"
656,656,437,12,[],200,llama-7b,64,1,3839.0,1.0,1,A100,1697548466437,1697548470276,120,91.0,29.0,"[24, 555, 283, 74, 70, 67, 68, 51, 63, 359, 72, 54, 71, 70, 64, 536, 64, 51, 67, 64, 63, 59, 570, 69, 52, 68, 51, 52, 64, 63]","[1697548466461, 1697548467016, 1697548467299, 1697548467373, 1697548467443, 1697548467510, 1697548467578, 1697548467629, 1697548467692, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468318, 1697548468382, 1697548468918, 1697548468982, 1697548469033, 1697548469100, 1697548469164, 1697548469227, 1697548469286, 1697548469856, 1697548469925, 1697548469977, 1697548470045, 1697548470096, 1697548470148, 1697548470212, 1697548470275]"
657,657,136,17,[],200,llama-7b,64,1,1041.0,1.0,1,A100,1697548470710,1697548471751,120,31.0,1.0,"[14, 1027]","[1697548470724, 1697548471751]"
658,658,718,18,[],200,llama-7b,64,1,702.0,1.0,1,A100,1697548471754,1697548472456,120,13.0,1.0,"[24, 678]","[1697548471778, 1697548472456]"
659,659,498,19,[],200,llama-7b,64,1,728.0,1.0,1,A100,1697548472457,1697548473185,120,9.0,1.0,"[6, 722]","[1697548472463, 1697548473185]"
660,660,642,29,[],200,llama-7b,64,1,4205.0,1.0,1,A100,1697548514233,1697548518438,120,89.0,20.0,"[23, 602, 467, 71, 70, 64, 61, 975, 271, 69, 64, 65, 63, 294, 66, 66, 57, 53, 673, 66, 65]","[1697548514256, 1697548514858, 1697548515325, 1697548515396, 1697548515466, 1697548515530, 1697548515591, 1697548516566, 1697548516837, 1697548516906, 1697548516970, 1697548517035, 1697548517098, 1697548517392, 1697548517458, 1697548517524, 1697548517581, 1697548517634, 1697548518307, 1697548518373, 1697548518438]"
661,661,154,20,[],200,llama-7b,64,1,810.0,1.0,1,A100,1697548473187,1697548473997,120,13.0,1.0,"[6, 804]","[1697548473193, 1697548473997]"
662,662,506,11,[],200,llama-7b,64,1,603.0,1.0,1,A100,1697548456474,1697548457077,120,16.0,1.0,"[14, 589]","[1697548456488, 1697548457077]"
663,663,162,12,[],200,llama-7b,64,1,2288.0,1.0,1,A100,1697548457079,1697548459367,120,90.0,20.0,"[22, 591, 66, 50, 62, 62, 49, 48, 62, 524, 74, 61, 48, 49, 49, 205, 47, 57, 49, 57, 56]","[1697548457101, 1697548457692, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457981, 1697548458029, 1697548458091, 1697548458615, 1697548458689, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459205, 1697548459254, 1697548459311, 1697548459367]"
664,664,86,14,[],200,llama-7b,64,1,2958.0,1.0,1,A100,1697548472229,1697548475187,120,335.0,17.0,"[15, 941, 250, 67, 49, 51, 65, 58, 345, 69, 67, 51, 66, 65, 61, 595, 72, 71]","[1697548472244, 1697548473185, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473667, 1697548473725, 1697548474070, 1697548474139, 1697548474206, 1697548474257, 1697548474323, 1697548474388, 1697548474449, 1697548475044, 1697548475116, 1697548475187]"
665,665,852,21,[],200,llama-7b,64,1,4625.0,1.0,1,A100,1697548474002,1697548478627,120,100.0,20.0,"[19, 739, 283, 73, 71, 69, 70, 67, 63, 861, 512, 288, 74, 57, 64, 618, 251, 54, 253, 71, 68]","[1697548474021, 1697548474760, 1697548475043, 1697548475116, 1697548475187, 1697548475256, 1697548475326, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477191, 1697548477248, 1697548477312, 1697548477930, 1697548478181, 1697548478235, 1697548478488, 1697548478559, 1697548478627]"
666,666,328,12,[],200,llama-7b,64,1,1051.0,1.0,1,A100,1697548465447,1697548466498,120,109.0,6.0,"[15, 439, 338, 72, 69, 53, 65]","[1697548465462, 1697548465901, 1697548466239, 1697548466311, 1697548466380, 1697548466433, 1697548466498]"
667,667,676,15,[],200,llama-7b,64,1,646.0,1.0,1,A100,1697548475191,1697548475837,120,19.0,1.0,"[17, 629]","[1697548475208, 1697548475837]"
668,668,447,16,[],200,llama-7b,64,1,3912.0,1.0,1,A100,1697548475842,1697548479754,120,161.0,13.0,"[25, 2063, 251, 54, 253, 71, 68, 61, 613, 255, 71, 55, 72]","[1697548475867, 1697548477930, 1697548478181, 1697548478235, 1697548478488, 1697548478559, 1697548478627, 1697548478688, 1697548479301, 1697548479556, 1697548479627, 1697548479682, 1697548479754]"
669,669,172,5,[],200,llama-7b,64,1,466.0,1.0,1,A100,1697548453381,1697548453847,120,19.0,1.0,"[23, 442]","[1697548453404, 1697548453846]"
670,670,849,6,[],200,llama-7b,64,1,1208.0,1.0,1,A100,1697548453852,1697548455060,120,10.0,1.0,"[22, 1186]","[1697548453874, 1697548455060]"
671,671,917,13,[],200,llama-7b,64,1,799.0,1.0,1,A100,1697548466500,1697548467299,120,123.0,2.0,"[7, 509, 283]","[1697548466507, 1697548467016, 1697548467299]"
672,672,687,14,[],200,llama-7b,64,1,2846.0,1.0,1,A100,1697548467302,1697548470148,120,96.0,20.0,"[15, 654, 80, 73, 53, 71, 71, 64, 534, 65, 51, 67, 64, 63, 59, 570, 69, 53, 67, 51, 52]","[1697548467317, 1697548467971, 1697548468051, 1697548468124, 1697548468177, 1697548468248, 1697548468319, 1697548468383, 1697548468917, 1697548468982, 1697548469033, 1697548469100, 1697548469164, 1697548469227, 1697548469286, 1697548469856, 1697548469925, 1697548469978, 1697548470045, 1697548470096, 1697548470148]"
673,673,792,6,[],200,llama-7b,64,1,484.0,1.0,1,A100,1697548449121,1697548449605,120,11.0,1.0,"[19, 465]","[1697548449140, 1697548449605]"
674,674,446,7,[],200,llama-7b,64,1,759.0,1.0,1,A100,1697548449608,1697548450367,120,26.0,1.0,"[31, 728]","[1697548449639, 1697548450367]"
675,675,223,8,[],200,llama-7b,64,1,564.0,1.0,1,A100,1697548450371,1697548450935,120,16.0,1.0,"[15, 549]","[1697548450386, 1697548450935]"
676,676,807,9,[],200,llama-7b,64,1,3567.0,1.0,1,A100,1697548450938,1697548454505,120,90.0,20.0,"[18, 503, 66, 48, 49, 52, 48, 59, 245, 57, 46, 57, 54, 1018, 66, 54, 61, 875, 66, 70, 55]","[1697548450956, 1697548451459, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451722, 1697548451781, 1697548452026, 1697548452083, 1697548452129, 1697548452186, 1697548452240, 1697548453258, 1697548453324, 1697548453378, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454505]"
677,677,502,7,[],200,llama-7b,64,1,1330.0,1.0,1,A100,1697548455063,1697548456393,120,19.0,1.0,"[24, 1306]","[1697548455087, 1697548456393]"
678,678,865,11,[],200,llama-7b,64,1,339.0,1.0,1,A100,1697548467633,1697548467972,120,9.0,1.0,"[25, 314]","[1697548467658, 1697548467972]"
679,679,851,3,[],200,llama-7b,64,1,499.0,1.0,1,A100,1697548440904,1697548441403,120,23.0,1.0,"[22, 477]","[1697548440926, 1697548441403]"
680,680,562,2,[],200,llama-7b,64,1,6546.0,1.0,1,A100,1697548449675,1697548456221,120,67.0,39.0,"[15, 677, 73, 56, 53, 51, 405, 65, 61, 54, 42, 297, 49, 48, 53, 48, 59, 244, 58, 46, 57, 54, 1017, 68, 51, 63, 875, 66, 70, 54, 62, 59, 1118, 72, 77, 71, 70, 68, 53, 67]","[1697548449690, 1697548450367, 1697548450440, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451070, 1697548451131, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451781, 1697548452025, 1697548452083, 1697548452129, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453376, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455743, 1697548455815, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221]"
681,681,577,10,[],200,llama-7b,64,1,1714.0,1.0,1,A100,1697548454508,1697548456222,120,93.0,9.0,"[21, 531, 683, 74, 75, 72, 69, 69, 52, 68]","[1697548454529, 1697548455060, 1697548455743, 1697548455817, 1697548455892, 1697548455964, 1697548456033, 1697548456102, 1697548456154, 1697548456222]"
682,682,47,15,[],200,llama-7b,64,1,2271.0,1.0,1,A100,1697548472117,1697548474388,120,90.0,20.0,"[6, 333, 76, 70, 69, 68, 52, 65, 49, 530, 67, 49, 51, 65, 58, 345, 69, 67, 51, 66, 65]","[1697548472123, 1697548472456, 1697548472532, 1697548472602, 1697548472671, 1697548472739, 1697548472791, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473667, 1697548473725, 1697548474070, 1697548474139, 1697548474206, 1697548474257, 1697548474323, 1697548474388]"
683,683,309,3,[],200,llama-7b,64,1,2673.0,1.0,1,A100,1697548456224,1697548458897,120,52.0,20.0,"[7, 845, 68, 62, 60, 48, 59, 54, 332, 50, 61, 61, 50, 48, 61, 526, 72, 62, 48, 49, 50]","[1697548456231, 1697548457076, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457759, 1697548457809, 1697548457870, 1697548457931, 1697548457981, 1697548458029, 1697548458090, 1697548458616, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458897]"
684,684,816,17,[],200,llama-7b,64,1,1723.0,1.0,1,A100,1697548475396,1697548477119,120,182.0,4.0,"[13, 427, 482, 512, 288]","[1697548475409, 1697548475836, 1697548476318, 1697548476830, 1697548477118]"
685,685,469,18,[],200,llama-7b,64,1,557.0,1.0,1,A100,1697548477122,1697548477679,120,17.0,1.0,"[14, 543]","[1697548477136, 1697548477679]"
686,686,890,4,[],200,llama-7b,64,1,2706.0,1.0,1,A100,1697548458900,1697548461606,120,93.0,20.0,"[19, 818, 250, 65, 48, 63, 60, 60, 429, 68, 52, 68, 62, 63, 49, 236, 49, 63, 62, 60, 61]","[1697548458919, 1697548459737, 1697548459987, 1697548460052, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460712, 1697548460780, 1697548460832, 1697548460900, 1697548460962, 1697548461025, 1697548461074, 1697548461310, 1697548461359, 1697548461422, 1697548461484, 1697548461544, 1697548461605]"
687,687,152,20,[],200,llama-7b,64,1,3182.0,1.0,1,A100,1697548491713,1697548494895,120,87.0,20.0,"[24, 964, 360, 58, 49, 48, 39, 356, 49, 38, 37, 312, 52, 42, 46, 431, 60, 48, 59, 55, 55]","[1697548491737, 1697548492701, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493660, 1697548493698, 1697548493735, 1697548494047, 1697548494099, 1697548494141, 1697548494187, 1697548494618, 1697548494678, 1697548494726, 1697548494785, 1697548494840, 1697548494895]"
688,688,723,11,[],200,llama-7b,64,1,1617.0,1.0,1,A100,1697548453443,1697548455060,120,14.0,1.0,"[6, 1611]","[1697548453449, 1697548455060]"
689,689,567,6,[],200,llama-7b,64,1,3346.0,1.0,1,A100,1697548453380,1697548456726,120,90.0,20.0,"[18, 448, 469, 65, 71, 54, 62, 59, 1117, 73, 76, 71, 70, 68, 53, 67, 247, 69, 52, 71, 66]","[1697548453398, 1697548453846, 1697548454315, 1697548454380, 1697548454451, 1697548454505, 1697548454567, 1697548454626, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468, 1697548456537, 1697548456589, 1697548456660, 1697548456726]"
690,690,360,16,[],200,llama-7b,64,1,759.0,1.0,1,A100,1697548474001,1697548474760,120,16.0,1.0,"[14, 745]","[1697548474015, 1697548474760]"
691,691,132,17,[],200,llama-7b,64,1,5059.0,1.0,1,A100,1697548474763,1697548479822,120,100.0,20.0,"[10, 1063, 481, 512, 289, 74, 56, 63, 619, 250, 54, 254, 70, 70, 60, 614, 255, 71, 54, 71, 68]","[1697548474773, 1697548475836, 1697548476317, 1697548476829, 1697548477118, 1697548477192, 1697548477248, 1697548477311, 1697548477930, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478628, 1697548478688, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821]"
692,692,827,21,[],200,llama-7b,64,1,2179.0,1.0,1,A100,1697548494898,1697548497077,120,96.0,20.0,"[10, 271, 65, 49, 59, 58, 55, 51, 43, 232, 55, 53, 52, 51, 379, 40, 40, 51, 50, 445, 70]","[1697548494908, 1697548495179, 1697548495244, 1697548495293, 1697548495352, 1697548495410, 1697548495465, 1697548495516, 1697548495559, 1697548495791, 1697548495846, 1697548495899, 1697548495951, 1697548496002, 1697548496381, 1697548496421, 1697548496461, 1697548496512, 1697548496562, 1697548497007, 1697548497077]"
693,693,80,8,[],200,llama-7b,64,1,236.0,1.0,1,A100,1697548451726,1697548451962,120,13.0,1.0,"[15, 221]","[1697548451741, 1697548451962]"
694,694,660,9,[],200,llama-7b,64,1,4821.0,1.0,1,A100,1697548451965,1697548456786,120,732.0,25.0,"[29, 718, 545, 68, 52, 62, 875, 65, 72, 53, 63, 58, 1119, 72, 76, 72, 70, 68, 52, 68, 247, 69, 52, 70, 65, 61]","[1697548451994, 1697548452712, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454379, 1697548454451, 1697548454504, 1697548454567, 1697548454625, 1697548455744, 1697548455816, 1697548455892, 1697548455964, 1697548456034, 1697548456102, 1697548456154, 1697548456222, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456786]"
695,695,721,18,[],200,llama-7b,64,1,1195.0,1.0,1,A100,1697548479824,1697548481019,120,286.0,5.0,"[10, 468, 494, 76, 74, 72]","[1697548479834, 1697548480302, 1697548480796, 1697548480872, 1697548480946, 1697548481018]"
696,696,481,22,[],200,llama-7b,64,1,492.0,1.0,1,A100,1697548497080,1697548497572,120,10.0,1.0,"[21, 471]","[1697548497101, 1697548497572]"
697,697,258,23,[],200,llama-7b,64,1,7802.0,1.0,1,A100,1697548497576,1697548505378,120,244.0,50.0,"[28, 567, 66, 61, 52, 51, 51, 350, 60, 50, 50, 826, 61, 62, 59, 55, 51, 421, 65, 50, 50, 58, 54, 724, 63, 60, 58, 56, 54, 620, 66, 63, 62, 60, 55, 876, 62, 59, 57, 52, 42, 704, 62, 47, 60, 59, 58, 57, 352, 65, 51]","[1697548497604, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498452, 1697548498802, 1697548498862, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500497, 1697548500562, 1697548500612, 1697548500662, 1697548500720, 1697548500774, 1697548501498, 1697548501561, 1697548501621, 1697548501679, 1697548501735, 1697548501789, 1697548502409, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502715, 1697548503591, 1697548503653, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504853, 1697548504910, 1697548505262, 1697548505327, 1697548505378]"
698,698,491,19,[],200,llama-7b,64,1,501.0,1.0,1,A100,1697548481022,1697548481523,120,14.0,1.0,"[10, 491]","[1697548481032, 1697548481523]"
699,699,153,20,[],200,llama-7b,64,1,2115.0,1.0,1,A100,1697548481525,1697548483640,120,335.0,4.0,"[20, 1413, 550, 76, 55]","[1697548481545, 1697548482958, 1697548483508, 1697548483584, 1697548483639]"
700,700,852,21,[],200,llama-7b,64,1,3592.0,1.0,1,A100,1697548483642,1697548487234,120,100.0,20.0,"[9, 497, 82, 77, 78, 72, 70, 57, 70, 837, 314, 308, 76, 76, 72, 70, 323, 59, 59, 79, 306]","[1697548483651, 1697548484148, 1697548484230, 1697548484307, 1697548484385, 1697548484457, 1697548484527, 1697548484584, 1697548484654, 1697548485491, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487233]"
701,701,316,6,[],200,llama-7b,64,1,3020.0,1.0,1,A100,1697548448111,1697548451131,120,86.0,20.0,"[10, 401, 528, 66, 58, 45, 45, 57, 44, 307, 59, 45, 54, 53, 558, 54, 54, 51, 405, 64, 61]","[1697548448121, 1697548448522, 1697548449050, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449321, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450441, 1697548450495, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451130]"
702,702,337,7,[],200,llama-7b,64,1,349.0,1.0,1,A100,1697548456728,1697548457077,120,12.0,1.0,"[14, 335]","[1697548456742, 1697548457077]"
703,703,927,8,[],200,llama-7b,64,1,2288.0,1.0,1,A100,1697548457079,1697548459367,120,83.0,20.0,"[13, 600, 66, 50, 62, 62, 48, 49, 61, 526, 73, 61, 48, 49, 49, 205, 47, 59, 47, 57, 56]","[1697548457092, 1697548457692, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458689, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459207, 1697548459254, 1697548459311, 1697548459367]"
704,704,92,7,[],200,llama-7b,64,1,3372.0,1.0,1,A100,1697548451133,1697548454505,120,85.0,20.0,"[11, 315, 66, 48, 49, 52, 48, 59, 245, 58, 45, 57, 54, 1017, 68, 51, 63, 875, 66, 71, 54]","[1697548451144, 1697548451459, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451722, 1697548451781, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453376, 1697548453439, 1697548454314, 1697548454380, 1697548454451, 1697548454505]"
705,705,600,22,[],200,llama-7b,64,1,569.0,1.0,1,A100,1697548487239,1697548487808,120,23.0,1.0,"[18, 551]","[1697548487257, 1697548487808]"
706,706,842,24,[],200,llama-7b,64,1,2389.0,1.0,1,A100,1697548505385,1697548507774,120,161.0,16.0,"[17, 377, 63, 46, 54, 52, 678, 66, 58, 55, 44, 580, 65, 61, 60, 56, 57]","[1697548505402, 1697548505779, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507475, 1697548507540, 1697548507601, 1697548507661, 1697548507717, 1697548507774]"
707,707,255,23,[],200,llama-7b,64,1,14847.0,1.0,1,A100,1697548487813,1697548502660,120,216.0,119.0,"[18, 406, 651, 52, 35, 37, 285, 67, 50, 48, 41, 48, 40, 304, 52, 43, 42, 51, 49, 40, 41, 193, 374, 50, 49, 41, 554, 50, 49, 39, 49, 48, 329, 54, 52, 43, 51, 51, 49, 722, 58, 49, 48, 39, 357, 48, 38, 38, 311, 52, 42, 46, 432, 60, 47, 60, 55, 55, 51, 297, 49, 58, 58, 55, 53, 42, 232, 56, 53, 51, 51, 379, 41, 40, 50, 50, 446, 68, 64, 61, 63, 57, 55, 262, 60, 48, 46, 46, 400, 61, 52, 51, 50, 351, 59, 51, 49, 826, 62, 61, 59, 55, 51, 421, 65, 51, 49, 58, 54, 724, 63, 61, 58, 55, 55, 620, 66, 63, 62, 60]","[1697548487831, 1697548488237, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414, 1697548489462, 1697548489503, 1697548489551, 1697548489591, 1697548489895, 1697548489947, 1697548489990, 1697548490032, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490406, 1697548490780, 1697548490830, 1697548490879, 1697548490920, 1697548491474, 1697548491524, 1697548491573, 1697548491612, 1697548491661, 1697548491709, 1697548492038, 1697548492092, 1697548492144, 1697548492187, 1697548492238, 1697548492289, 1697548492338, 1697548493060, 1697548493118, 1697548493167, 1697548493215, 1697548493254, 1697548493611, 1697548493659, 1697548493697, 1697548493735, 1697548494046, 1697548494098, 1697548494140, 1697548494186, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494840, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495408, 1697548495463, 1697548495516, 1697548495558, 1697548495790, 1697548495846, 1697548495899, 1697548495950, 1697548496001, 1697548496380, 1697548496421, 1697548496461, 1697548496511, 1697548496561, 1697548497007, 1697548497075, 1697548497139, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497791, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499787, 1697548499849, 1697548499910, 1697548499969, 1697548500024, 1697548500075, 1697548500496, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500773, 1697548501497, 1697548501560, 1697548501621, 1697548501679, 1697548501734, 1697548501789, 1697548502409, 1697548502475, 1697548502538, 1697548502600, 1697548502660]"
708,708,184,13,[],200,llama-7b,64,1,3389.0,1.0,1,A100,1697548470278,1697548473667,120,87.0,20.0,"[13, 1460, 77, 74, 73, 72, 68, 63, 49, 304, 70, 69, 68, 52, 66, 48, 530, 68, 49, 50, 66]","[1697548470291, 1697548471751, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472904, 1697548473434, 1697548473502, 1697548473551, 1697548473601, 1697548473667]"
709,709,767,14,[],200,llama-7b,64,1,327.0,1.0,1,A100,1697548473671,1697548473998,120,11.0,1.0,"[22, 305]","[1697548473693, 1697548473998]"
710,710,542,15,[],200,llama-7b,64,1,759.0,1.0,1,A100,1697548474002,1697548474761,120,11.0,1.0,"[25, 733]","[1697548474027, 1697548474760]"
711,711,867,13,[],200,llama-7b,64,1,3155.0,1.0,1,A100,1697548459370,1697548462525,120,91.0,20.0,"[6, 1247, 88, 69, 52, 67, 63, 63, 48, 237, 50, 61, 63, 60, 61, 526, 175, 51, 57, 57, 53]","[1697548459376, 1697548460623, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461310, 1697548461360, 1697548461421, 1697548461484, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462524]"
712,712,195,16,[],200,llama-7b,64,1,12611.0,1.0,1,A100,1697548474764,1697548487375,120,286.0,64.0,"[19, 1053, 481, 512, 289, 74, 57, 62, 619, 250, 54, 254, 70, 68, 62, 614, 255, 71, 54, 71, 68, 63, 911, 77, 74, 72, 71, 68, 67, 587, 269, 63, 74, 265, 68, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 75, 75, 71, 57, 71, 837, 313, 308, 76, 77, 71, 70, 324, 58, 60, 79, 306, 71, 70]","[1697548474783, 1697548475836, 1697548476317, 1697548476829, 1697548477118, 1697548477192, 1697548477249, 1697548477311, 1697548477930, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480872, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484527, 1697548484584, 1697548484655, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486266, 1697548486337, 1697548486407, 1697548486731, 1697548486789, 1697548486849, 1697548486928, 1697548487234, 1697548487305, 1697548487375]"
713,713,695,9,[],200,llama-7b,64,1,3154.0,1.0,1,A100,1697548459371,1697548462525,120,92.0,20.0,"[25, 1227, 88, 69, 52, 67, 63, 63, 49, 237, 48, 63, 61, 61, 61, 526, 175, 51, 57, 57, 54]","[1697548459396, 1697548460623, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074, 1697548461311, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462525]"
714,714,715,7,[],200,llama-7b,64,1,450.0,1.0,1,A100,1697548451009,1697548451459,120,20.0,1.0,"[9, 441]","[1697548451018, 1697548451459]"
715,715,493,8,[],200,llama-7b,64,1,4502.0,1.0,1,A100,1697548451462,1697548455964,120,83.0,20.0,"[25, 475, 64, 58, 45, 57, 54, 1017, 68, 51, 63, 876, 65, 71, 54, 61, 59, 1118, 73, 76, 72]","[1697548451487, 1697548451962, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453376, 1697548453439, 1697548454315, 1697548454380, 1697548454451, 1697548454505, 1697548454566, 1697548454625, 1697548455743, 1697548455816, 1697548455892, 1697548455964]"
716,716,246,19,[],200,llama-7b,64,1,9166.0,1.0,1,A100,1697548477683,1697548486849,120,58.0,47.0,"[30, 1378, 211, 255, 71, 54, 71, 68, 63, 911, 76, 75, 73, 70, 68, 67, 588, 269, 63, 74, 264, 68, 51, 69, 838, 76, 55, 72, 66, 66, 49, 338, 76, 75, 75, 72, 56, 71, 837, 314, 308, 75, 77, 71, 70, 324, 59, 59]","[1697548477713, 1697548479091, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481019, 1697548481089, 1697548481157, 1697548481224, 1697548481812, 1697548482081, 1697548482144, 1697548482218, 1697548482482, 1697548482550, 1697548482601, 1697548482670, 1697548483508, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484528, 1697548484584, 1697548484655, 1697548485492, 1697548485806, 1697548486114, 1697548486189, 1697548486266, 1697548486337, 1697548486407, 1697548486731, 1697548486790, 1697548486849]"
717,717,495,14,[],200,llama-7b,64,1,1053.0,1.0,1,A100,1697548462528,1697548463581,120,13.0,1.0,"[27, 1026]","[1697548462555, 1697548463581]"
718,718,492,12,[],200,llama-7b,64,1,3029.0,1.0,1,A100,1697548455062,1697548458091,120,47.0,20.0,"[20, 1311, 76, 69, 52, 70, 65, 61, 359, 62, 60, 47, 60, 53, 331, 50, 62, 62, 49, 48, 62]","[1697548455082, 1697548456393, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456786, 1697548457145, 1697548457207, 1697548457267, 1697548457314, 1697548457374, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457981, 1697548458029, 1697548458091]"
719,719,433,10,[],200,llama-7b,64,1,1340.0,1.0,1,A100,1697548459371,1697548460711,120,109.0,2.0,"[23, 1317]","[1697548459394, 1697548460711]"
720,720,573,10,[],200,llama-7b,64,1,1367.0,1.0,1,A100,1697548462527,1697548463894,120,874.0,2.0,"[6, 1360]","[1697548462533, 1697548463893]"
721,721,204,11,[],200,llama-7b,64,1,831.0,1.0,1,A100,1697548460714,1697548461545,120,67.0,6.0,"[7, 521, 68, 50, 62, 62, 61]","[1697548460721, 1697548461242, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545]"
722,722,342,11,[],200,llama-7b,64,1,2713.0,1.0,1,A100,1697548463896,1697548466609,120,364.0,14.0,"[11, 648, 303, 460, 65, 49, 59, 46, 701, 72, 69, 54, 64, 63, 49]","[1697548463907, 1697548464555, 1697548464858, 1697548465318, 1697548465383, 1697548465432, 1697548465491, 1697548465537, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466560, 1697548466609]"
723,723,156,15,[],200,llama-7b,64,1,4046.0,1.0,1,A100,1697548463583,1697548467629,120,86.0,20.0,"[28, 944, 303, 460, 64, 50, 59, 46, 701, 72, 69, 54, 64, 63, 49, 691, 72, 71, 68, 67, 51]","[1697548463611, 1697548464555, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465491, 1697548465537, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466560, 1697548466609, 1697548467300, 1697548467372, 1697548467443, 1697548467511, 1697548467578, 1697548467629]"
724,724,854,16,[],200,llama-7b,64,1,3885.0,1.0,1,A100,1697548467632,1697548471517,120,67.0,29.0,"[16, 323, 81, 72, 53, 72, 70, 64, 534, 66, 51, 66, 65, 62, 60, 568, 70, 53, 67, 50, 51, 66, 64, 846, 72, 71, 69, 68, 49, 66]","[1697548467648, 1697548467971, 1697548468052, 1697548468124, 1697548468177, 1697548468249, 1697548468319, 1697548468383, 1697548468917, 1697548468983, 1697548469034, 1697548469100, 1697548469165, 1697548469227, 1697548469287, 1697548469855, 1697548469925, 1697548469978, 1697548470045, 1697548470095, 1697548470146, 1697548470212, 1697548470276, 1697548471122, 1697548471194, 1697548471265, 1697548471334, 1697548471402, 1697548471451, 1697548471517]"
725,725,881,12,[],200,llama-7b,64,1,924.0,1.0,1,A100,1697548461547,1697548462471,120,58.0,6.0,"[7, 381, 196, 175, 51, 57, 57]","[1697548461554, 1697548461935, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471]"
726,726,2,12,[],200,llama-7b,64,1,1707.0,1.0,1,A100,1697548466612,1697548468319,120,58.0,6.0,"[13, 1346, 80, 73, 53, 71, 70]","[1697548466625, 1697548467971, 1697548468051, 1697548468124, 1697548468177, 1697548468248, 1697548468318]"
727,727,701,13,[],200,llama-7b,64,1,5403.0,1.0,1,A100,1697548468322,1697548473725,120,58.0,43.0,"[15, 334, 246, 66, 51, 67, 65, 61, 60, 568, 69, 53, 67, 51, 52, 65, 64, 846, 72, 71, 70, 66, 50, 65, 313, 74, 72, 73, 67, 63, 49, 305, 70, 68, 68, 53, 65, 49, 530, 67, 49, 51, 64, 59]","[1697548468337, 1697548468671, 1697548468917, 1697548468983, 1697548469034, 1697548469101, 1697548469166, 1697548469227, 1697548469287, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471122, 1697548471194, 1697548471265, 1697548471335, 1697548471401, 1697548471451, 1697548471516, 1697548471829, 1697548471903, 1697548471975, 1697548472048, 1697548472115, 1697548472178, 1697548472227, 1697548472532, 1697548472602, 1697548472670, 1697548472738, 1697548472791, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473666, 1697548473725]"
728,728,538,13,[],200,llama-7b,64,1,3960.0,1.0,1,A100,1697548462474,1697548466434,120,89.0,20.0,"[18, 342, 72, 56, 58, 53, 820, 63, 60, 57, 56, 730, 460, 64, 48, 60, 45, 703, 71, 70, 54]","[1697548462492, 1697548462834, 1697548462906, 1697548462962, 1697548463020, 1697548463073, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464859, 1697548465319, 1697548465383, 1697548465431, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466434]"
729,729,195,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548479757,1697548488025,120,,,"[7, 538, 494, 76, 74, 72, 72, 67, 67, 587, 270, 62, 74, 265, 68, 51, 68, 839, 75, 55, 73, 66, 66, 49, 338, 76, 76, 74, 70, 57, 70, 839, 314, 308, 76, 76, 70, 71, 324, 59, 59, 79, 306, 71, 70]","[1697548479764, 1697548480302, 1697548480796, 1697548480872, 1697548480946, 1697548481018, 1697548481090, 1697548481157, 1697548481224, 1697548481811, 1697548482081, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484382, 1697548484456, 1697548484526, 1697548484583, 1697548484653, 1697548485492, 1697548485806, 1697548486114, 1697548486190, 1697548486266, 1697548486336, 1697548486407, 1697548486731, 1697548486790, 1697548486849, 1697548486928, 1697548487234, 1697548487305, 1697548487375]"
730,730,13,9,[],200,llama-7b,64,1,4401.0,1.0,1,A100,1697548444716,1697548449117,120,90.0,20.0,"[21, 1536, 79, 62, 61, 58, 45, 56, 358, 45, 44, 44, 57, 802, 62, 63, 58, 57, 56, 770, 66]","[1697548444737, 1697548446273, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634, 1697548446992, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447984, 1697548448046, 1697548448109, 1697548448167, 1697548448224, 1697548448280, 1697548449050, 1697548449116]"
731,731,169,5,[],200,llama-7b,64,1,493.0,1.0,1,A100,1697548450442,1697548450935,120,10.0,1.0,"[11, 482]","[1697548450453, 1697548450935]"
732,732,518,30,[],200,llama-7b,64,1,1001.0,1.0,1,A100,1697548499424,1697548500425,120,23.0,1.0,"[31, 970]","[1697548499455, 1697548500425]"
733,733,293,31,[],200,llama-7b,64,1,4139.0,1.0,1,A100,1697548500428,1697548504567,120,91.0,20.0,"[15, 760, 295, 63, 60, 58, 56, 54, 622, 65, 63, 62, 59, 55, 877, 62, 58, 57, 53, 41, 704]","[1697548500443, 1697548501203, 1697548501498, 1697548501561, 1697548501621, 1697548501679, 1697548501735, 1697548501789, 1697548502411, 1697548502476, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567]"
734,734,897,13,[],200,llama-7b,64,1,1223.0,1.0,1,A100,1697548461611,1697548462834,120,9.0,1.0,"[37, 1186]","[1697548461648, 1697548462834]"
735,735,348,10,[],200,llama-7b,64,1,4771.0,1.0,1,A100,1697548462529,1697548467300,120,91.0,20.0,"[24, 1027, 313, 63, 60, 57, 57, 728, 461, 63, 50, 59, 45, 704, 70, 70, 54, 64, 64, 47, 690]","[1697548462553, 1697548463580, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464130, 1697548464858, 1697548465319, 1697548465382, 1697548465432, 1697548465491, 1697548465536, 1697548466240, 1697548466310, 1697548466380, 1697548466434, 1697548466498, 1697548466562, 1697548466609, 1697548467299]"
736,736,510,17,[],200,llama-7b,64,1,1012.0,1.0,1,A100,1697548471519,1697548472531,120,79.0,2.0,"[6, 1006]","[1697548471525, 1697548472531]"
737,737,287,18,[],200,llama-7b,64,1,652.0,1.0,1,A100,1697548472534,1697548473186,120,10.0,1.0,"[7, 645]","[1697548472541, 1697548473186]"
738,738,125,11,[],200,llama-7b,64,1,667.0,1.0,1,A100,1697548467304,1697548467971,120,13.0,1.0,"[24, 643]","[1697548467328, 1697548467971]"
739,739,708,12,[],200,llama-7b,64,1,696.0,1.0,1,A100,1697548467975,1697548468671,120,140.0,1.0,"[22, 674]","[1697548467997, 1697548468671]"
740,740,482,13,[],200,llama-7b,64,1,3374.0,1.0,1,A100,1697548468674,1697548472048,120,91.0,20.0,"[14, 893, 275, 68, 53, 67, 51, 52, 65, 64, 844, 74, 72, 68, 67, 50, 65, 313, 74, 73, 72]","[1697548468688, 1697548469581, 1697548469856, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471194, 1697548471266, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471829, 1697548471903, 1697548471976, 1697548472048]"
741,741,432,8,[],200,llama-7b,64,1,928.0,1.0,1,A100,1697548451784,1697548452712,120,13.0,1.0,"[17, 911]","[1697548451801, 1697548452712]"
742,742,180,9,[],200,llama-7b,64,1,3318.0,1.0,1,A100,1697548452715,1697548456033,120,123.0,12.0,"[21, 1110, 468, 66, 71, 53, 62, 59, 1117, 73, 77, 71, 70]","[1697548452736, 1697548453846, 1697548454314, 1697548454380, 1697548454451, 1697548454504, 1697548454566, 1697548454625, 1697548455742, 1697548455815, 1697548455892, 1697548455963, 1697548456033]"
743,743,524,12,[],200,llama-7b,64,1,5284.0,1.0,1,A100,1697548450938,1697548456222,120,100.0,30.0,"[22, 499, 66, 48, 49, 52, 48, 59, 245, 58, 45, 57, 54, 1018, 66, 52, 63, 875, 66, 70, 54, 62, 59, 1118, 73, 76, 71, 70, 68, 53, 67]","[1697548450960, 1697548451459, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451722, 1697548451781, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453258, 1697548453324, 1697548453376, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221]"
744,744,880,10,[],200,llama-7b,64,1,433.0,1.0,1,A100,1697548456036,1697548456469,120,84.0,2.0,"[6, 351, 76]","[1697548456042, 1697548456393, 1697548456469]"
745,745,534,11,[],200,llama-7b,64,1,2425.0,1.0,1,A100,1697548456472,1697548458897,120,96.0,20.0,"[21, 584, 67, 62, 60, 48, 59, 54, 331, 50, 63, 60, 49, 49, 61, 526, 72, 62, 49, 48, 50]","[1697548456493, 1697548457077, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808, 1697548457871, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458688, 1697548458750, 1697548458799, 1697548458847, 1697548458897]"
746,746,519,12,[],200,llama-7b,64,1,6281.0,1.0,1,A100,1697548467975,1697548474256,120,58.0,47.0,"[11, 684, 247, 66, 51, 67, 64, 62, 60, 568, 69, 52, 68, 51, 51, 66, 64, 846, 72, 71, 70, 67, 49, 66, 312, 74, 72, 72, 68, 63, 49, 304, 71, 68, 68, 52, 66, 49, 530, 67, 49, 51, 64, 59, 346, 67, 67, 51]","[1697548467986, 1697548468670, 1697548468917, 1697548468983, 1697548469034, 1697548469101, 1697548469165, 1697548469227, 1697548469287, 1697548469855, 1697548469924, 1697548469976, 1697548470044, 1697548470095, 1697548470146, 1697548470212, 1697548470276, 1697548471122, 1697548471194, 1697548471265, 1697548471335, 1697548471402, 1697548471451, 1697548471517, 1697548471829, 1697548471903, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473666, 1697548473725, 1697548474071, 1697548474138, 1697548474205, 1697548474256]"
747,747,314,13,[],200,llama-7b,64,1,3508.0,1.0,1,A100,1697548475119,1697548478627,120,335.0,13.0,"[15, 703, 480, 512, 289, 74, 57, 63, 617, 251, 55, 253, 70, 68]","[1697548475134, 1697548475837, 1697548476317, 1697548476829, 1697548477118, 1697548477192, 1697548477249, 1697548477312, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478626]"
748,748,621,4,[],200,llama-7b,64,1,3256.0,1.0,1,A100,1697548441407,1697548444663,120,88.0,20.0,"[19, 249, 433, 706, 59, 48, 46, 38, 417, 54, 55, 51, 41, 47, 679, 58, 46, 57, 55, 54, 44]","[1697548441426, 1697548441675, 1697548442108, 1697548442814, 1697548442873, 1697548442921, 1697548442967, 1697548443005, 1697548443422, 1697548443476, 1697548443531, 1697548443582, 1697548443623, 1697548443670, 1697548444349, 1697548444407, 1697548444453, 1697548444510, 1697548444565, 1697548444619, 1697548444663]"
749,749,524,14,[],200,llama-7b,64,1,5719.0,1.0,1,A100,1697548462529,1697548468248,120,100.0,30.0,"[31, 1020, 313, 63, 60, 57, 57, 728, 461, 63, 50, 59, 46, 703, 70, 70, 52, 66, 64, 47, 690, 73, 70, 68, 67, 51, 63, 360, 72, 55, 70]","[1697548462560, 1697548463580, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464130, 1697548464858, 1697548465319, 1697548465382, 1697548465432, 1697548465491, 1697548465537, 1697548466240, 1697548466310, 1697548466380, 1697548466432, 1697548466498, 1697548466562, 1697548466609, 1697548467299, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468123, 1697548468178, 1697548468248]"
750,750,90,14,[],200,llama-7b,64,1,460.0,1.0,1,A100,1697548478631,1697548479091,120,19.0,1.0,"[33, 427]","[1697548478664, 1697548479091]"
751,751,673,15,[],200,llama-7b,64,1,4619.0,1.0,1,A100,1697548479092,1697548483711,120,93.0,20.0,"[14, 1196, 493, 76, 75, 72, 71, 68, 67, 587, 270, 63, 74, 265, 67, 52, 68, 838, 76, 55, 72]","[1697548479106, 1697548480302, 1697548480795, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482081, 1697548482144, 1697548482218, 1697548482483, 1697548482550, 1697548482602, 1697548482670, 1697548483508, 1697548483584, 1697548483639, 1697548483711]"
752,752,868,6,[],200,llama-7b,64,1,3567.0,1.0,1,A100,1697548450938,1697548454505,120,85.0,20.0,"[18, 502, 67, 48, 49, 52, 48, 59, 245, 58, 45, 57, 54, 1018, 67, 53, 61, 875, 66, 70, 54]","[1697548450956, 1697548451458, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451722, 1697548451781, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453258, 1697548453325, 1697548453378, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454504]"
753,753,527,7,[],200,llama-7b,64,1,6391.0,1.0,1,A100,1697548454508,1697548460899,120,732.0,50.0,"[26, 1209, 74, 75, 72, 69, 69, 52, 68, 247, 69, 52, 70, 65, 60, 359, 63, 60, 48, 59, 53, 331, 50, 62, 62, 48, 49, 61, 526, 72, 62, 48, 49, 49, 205, 47, 58, 48, 57, 57, 618, 65, 49, 63, 60, 60, 427, 70, 52, 67]","[1697548454534, 1697548455743, 1697548455817, 1697548455892, 1697548455964, 1697548456033, 1697548456102, 1697548456154, 1697548456222, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457207, 1697548457267, 1697548457315, 1697548457374, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459206, 1697548459254, 1697548459311, 1697548459368, 1697548459986, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460710, 1697548460780, 1697548460832, 1697548460899]"
754,754,448,16,[],200,llama-7b,64,1,2476.0,1.0,1,A100,1697548483714,1697548486190,120,335.0,12.0,"[11, 423, 83, 76, 78, 72, 70, 57, 70, 838, 313, 308, 76]","[1697548483725, 1697548484148, 1697548484231, 1697548484307, 1697548484385, 1697548484457, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189]"
755,755,285,9,[],200,llama-7b,64,1,4426.0,1.0,1,A100,1697548463957,1697548468383,120,100.0,27.0,"[11, 588, 302, 460, 64, 50, 60, 45, 701, 72, 69, 54, 64, 64, 48, 691, 72, 71, 68, 67, 51, 62, 360, 73, 54, 70, 70, 65]","[1697548463968, 1697548464556, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465492, 1697548465537, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466561, 1697548466609, 1697548467300, 1697548467372, 1697548467443, 1697548467511, 1697548467578, 1697548467629, 1697548467691, 1697548468051, 1697548468124, 1697548468178, 1697548468248, 1697548468318, 1697548468383]"
756,756,80,17,[],200,llama-7b,64,1,454.0,1.0,1,A100,1697548486194,1697548486648,120,13.0,1.0,"[7, 447]","[1697548486201, 1697548486648]"
757,757,297,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548474260,1697548488023,120,,,"[16, 485, 283, 72, 71, 70, 69, 67, 64, 860, 512, 288, 75, 56, 63, 619, 251, 54, 253, 71, 69, 60, 614, 255, 71, 54, 71, 68, 63, 911, 77, 74, 72, 71, 68, 67, 587, 269, 63, 74, 265, 68, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 75, 75, 71, 57, 70, 838, 313, 308, 76, 77, 71, 70, 324, 58, 60, 79, 306, 71, 70]","[1697548474276, 1697548474761, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475393, 1697548475457, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248, 1697548477311, 1697548477930, 1697548478181, 1697548478235, 1697548478488, 1697548478559, 1697548478628, 1697548478688, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480872, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486266, 1697548486337, 1697548486407, 1697548486731, 1697548486789, 1697548486849, 1697548486928, 1697548487234, 1697548487305, 1697548487375]"
758,758,778,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548486652,1697548488026,120,,,"[23, 1133]","[1697548486675, 1697548487808]"
759,759,345,15,[],200,llama-7b,64,1,2640.0,1.0,1,A100,1697548470151,1697548472791,120,39.0,20.0,"[18, 535, 417, 73, 71, 70, 67, 49, 66, 311, 74, 73, 72, 68, 63, 49, 304, 70, 69, 69, 51]","[1697548470169, 1697548470704, 1697548471121, 1697548471194, 1697548471265, 1697548471335, 1697548471402, 1697548471451, 1697548471517, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472601, 1697548472670, 1697548472739, 1697548472790]"
760,760,438,19,[],200,llama-7b,64,1,1208.0,1.0,1,A100,1697548488033,1697548489241,120,9.0,1.0,"[205, 1002]","[1697548488238, 1697548489240]"
761,761,207,20,[],200,llama-7b,64,1,587.0,1.0,1,A100,1697548489244,1697548489831,120,10.0,1.0,"[21, 565]","[1697548489265, 1697548489830]"
762,762,575,19,[],200,llama-7b,64,1,4825.0,1.0,1,A100,1697548461609,1697548466434,120,86.0,20.0,"[29, 1196, 72, 56, 57, 54, 820, 62, 62, 56, 56, 729, 460, 65, 55, 53, 45, 703, 71, 70, 54]","[1697548461638, 1697548462834, 1697548462906, 1697548462962, 1697548463019, 1697548463073, 1697548463893, 1697548463955, 1697548464017, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465383, 1697548465438, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466434]"
763,763,796,21,[],200,llama-7b,64,1,3228.0,1.0,1,A100,1697548489833,1697548493061,120,86.0,20.0,"[15, 518, 41, 374, 50, 48, 42, 553, 51, 48, 40, 49, 48, 328, 54, 53, 42, 52, 50, 49, 723]","[1697548489848, 1697548490366, 1697548490407, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491710, 1697548492038, 1697548492092, 1697548492145, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493061]"
764,764,566,22,[],200,llama-7b,64,1,4077.0,1.0,1,A100,1697548493063,1697548497140,120,109.0,36.0,"[16, 471, 62, 48, 38, 38, 310, 53, 41, 46, 433, 60, 47, 60, 55, 55, 51, 296, 49, 59, 58, 54, 53, 42, 233, 55, 53, 51, 52, 378, 41, 40, 51, 49, 446, 68, 65]","[1697548493079, 1697548493550, 1697548493612, 1697548493660, 1697548493698, 1697548493736, 1697548494046, 1697548494099, 1697548494140, 1697548494186, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494896, 1697548494947, 1697548495243, 1697548495292, 1697548495351, 1697548495409, 1697548495463, 1697548495516, 1697548495558, 1697548495791, 1697548495846, 1697548495899, 1697548495950, 1697548496002, 1697548496380, 1697548496421, 1697548496461, 1697548496512, 1697548496561, 1697548497007, 1697548497075, 1697548497140]"
765,765,56,10,[],200,llama-7b,64,1,3663.0,1.0,1,A100,1697548468385,1697548472048,120,86.0,20.0,"[6, 1191, 273, 69, 53, 67, 51, 52, 65, 64, 844, 74, 71, 69, 67, 50, 65, 313, 74, 72, 73]","[1697548468391, 1697548469582, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471194, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471829, 1697548471903, 1697548471975, 1697548472048]"
766,766,751,18,[],200,llama-7b,64,1,8600.0,1.0,1,A100,1697548461612,1697548470212,120,216.0,50.0,"[33, 1261, 56, 57, 54, 820, 62, 61, 57, 56, 730, 459, 65, 53, 55, 45, 703, 71, 70, 54, 64, 63, 48, 689, 74, 70, 68, 67, 51, 63, 360, 72, 55, 70, 70, 64, 535, 66, 51, 66, 65, 62, 59, 569, 69, 53, 67, 51, 52, 65]","[1697548461645, 1697548462906, 1697548462962, 1697548463019, 1697548463073, 1697548463893, 1697548463955, 1697548464016, 1697548464073, 1697548464129, 1697548464859, 1697548465318, 1697548465383, 1697548465436, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466434, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468123, 1697548468178, 1697548468248, 1697548468318, 1697548468382, 1697548468917, 1697548468983, 1697548469034, 1697548469100, 1697548469165, 1697548469227, 1697548469286, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212]"
767,767,408,10,[],200,llama-7b,64,1,903.0,1.0,1,A100,1697548456789,1697548457692,120,16.0,1.0,"[10, 893]","[1697548456799, 1697548457692]"
768,768,249,3,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548440902,1697548441612,120,,,"[8, 493]","[1697548440910, 1697548441403]"
769,769,851,12,[],200,llama-7b,64,1,1053.0,1.0,1,A100,1697548462528,1697548463581,120,23.0,1.0,"[20, 1032]","[1697548462548, 1697548463580]"
770,770,791,13,[],200,llama-7b,64,1,8623.0,1.0,1,A100,1697548465448,1697548474071,120,182.0,64.0,"[24, 767, 72, 69, 54, 64, 63, 48, 689, 74, 70, 68, 67, 51, 63, 359, 73, 54, 71, 69, 66, 535, 65, 52, 65, 65, 63, 59, 568, 69, 53, 67, 51, 53, 64, 64, 844, 73, 72, 69, 67, 50, 65, 312, 74, 73, 72, 68, 63, 49, 304, 70, 69, 68, 52, 65, 49, 531, 66, 50, 51, 64, 59, 345]","[1697548465472, 1697548466239, 1697548466311, 1697548466380, 1697548466434, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468248, 1697548468317, 1697548468383, 1697548468918, 1697548468983, 1697548469035, 1697548469100, 1697548469165, 1697548469228, 1697548469287, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470148, 1697548470212, 1697548470276, 1697548471120, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472855, 1697548472904, 1697548473435, 1697548473501, 1697548473551, 1697548473602, 1697548473666, 1697548473725, 1697548474070]"
771,771,511,13,[],200,llama-7b,64,1,9851.0,1.0,1,A100,1697548463584,1697548473435,120,364.0,64.0,"[36, 935, 303, 460, 64, 54, 55, 46, 701, 72, 69, 54, 64, 63, 49, 691, 72, 71, 68, 67, 51, 62, 360, 72, 55, 70, 70, 64, 536, 65, 51, 66, 65, 62, 60, 568, 69, 53, 67, 51, 52, 65, 64, 844, 73, 72, 69, 67, 50, 65, 312, 74, 73, 72, 67, 64, 49, 304, 70, 69, 68, 52, 66, 48, 531]","[1697548463620, 1697548464555, 1697548464858, 1697548465318, 1697548465382, 1697548465436, 1697548465491, 1697548465537, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466560, 1697548466609, 1697548467300, 1697548467372, 1697548467443, 1697548467511, 1697548467578, 1697548467629, 1697548467691, 1697548468051, 1697548468123, 1697548468178, 1697548468248, 1697548468318, 1697548468382, 1697548468918, 1697548468983, 1697548469034, 1697548469100, 1697548469165, 1697548469227, 1697548469287, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472227, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472904, 1697548473435]"
772,772,756,4,[],200,llama-7b,64,1,386.0,1.0,1,A100,1697548451073,1697548451459,120,19.0,1.0,"[14, 372]","[1697548451087, 1697548451459]"
773,773,146,9,[],200,llama-7b,64,1,2124.0,1.0,1,A100,1697548455967,1697548458091,120,96.0,20.0,"[9, 417, 76, 69, 52, 70, 66, 60, 359, 62, 59, 48, 59, 54, 331, 51, 61, 62, 49, 48, 62]","[1697548455976, 1697548456393, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456726, 1697548456786, 1697548457145, 1697548457207, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457809, 1697548457870, 1697548457932, 1697548457981, 1697548458029, 1697548458091]"
774,774,417,5,[],200,llama-7b,64,1,499.0,1.0,1,A100,1697548451463,1697548451962,120,17.0,1.0,"[32, 467]","[1697548451495, 1697548451962]"
775,775,112,8,[],200,llama-7b,64,1,563.0,1.0,1,A100,1697548450443,1697548451006,120,16.0,2.0,"[20, 471, 72]","[1697548450463, 1697548450934, 1697548451006]"
776,776,847,10,[],200,llama-7b,64,1,943.0,1.0,1,A100,1697548458095,1697548459038,120,10.0,1.0,"[23, 920]","[1697548458118, 1697548459038]"
777,777,695,9,[],200,llama-7b,64,1,3497.0,1.0,1,A100,1697548451008,1697548454505,120,92.0,20.0,"[7, 444, 66, 48, 49, 52, 48, 59, 245, 58, 45, 57, 54, 1018, 66, 52, 62, 876, 66, 70, 55]","[1697548451015, 1697548451459, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451722, 1697548451781, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453258, 1697548453324, 1697548453376, 1697548453438, 1697548454314, 1697548454380, 1697548454450, 1697548454505]"
778,778,286,9,[],200,llama-7b,64,1,2530.0,1.0,1,A100,1697548461364,1697548463894,120,161.0,12.0,"[11, 756, 175, 51, 57, 57, 54, 380, 57, 57, 53, 822]","[1697548461375, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462525, 1697548462905, 1697548462962, 1697548463019, 1697548463072, 1697548463894]"
779,779,55,10,[],200,llama-7b,64,1,661.0,1.0,1,A100,1697548463895,1697548464556,120,12.0,1.0,"[12, 648]","[1697548463907, 1697548464555]"
780,780,646,11,[],200,llama-7b,64,1,1342.0,1.0,1,A100,1697548464560,1697548465902,120,14.0,1.0,"[23, 1319]","[1697548464583, 1697548465902]"
781,781,281,5,[],200,llama-7b,64,1,611.0,1.0,1,A100,1697548444677,1697548445288,120,23.0,1.0,"[25, 586]","[1697548444702, 1697548445288]"
782,782,416,12,[],200,llama-7b,64,1,7000.0,1.0,1,A100,1697548465905,1697548472905,120,286.0,50.0,"[16, 1095, 282, 74, 71, 67, 67, 52, 62, 359, 73, 54, 72, 68, 66, 535, 65, 52, 65, 65, 63, 59, 568, 70, 52, 67, 52, 52, 64, 64, 845, 72, 72, 69, 67, 50, 65, 312, 74, 73, 72, 68, 63, 49, 304, 70, 69, 68, 52, 66, 49]","[1697548465921, 1697548467016, 1697548467298, 1697548467372, 1697548467443, 1697548467510, 1697548467577, 1697548467629, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468249, 1697548468317, 1697548468383, 1697548468918, 1697548468983, 1697548469035, 1697548469100, 1697548469165, 1697548469228, 1697548469287, 1697548469855, 1697548469925, 1697548469977, 1697548470044, 1697548470096, 1697548470148, 1697548470212, 1697548470276, 1697548471121, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472905]"
783,783,49,6,[],200,llama-7b,64,1,1124.0,1.0,1,A100,1697548445290,1697548446414,120,109.0,3.0,"[6, 978, 78, 62]","[1697548445296, 1697548446274, 1697548446352, 1697548446414]"
784,784,635,7,[],200,llama-7b,64,1,514.0,1.0,1,A100,1697548446418,1697548446932,120,23.0,1.0,"[6, 508]","[1697548446424, 1697548446932]"
785,785,405,8,[],200,llama-7b,64,1,3505.0,1.0,1,A100,1697548446935,1697548450440,120,87.0,20.0,"[14, 527, 508, 63, 61, 59, 57, 56, 769, 67, 57, 46, 45, 56, 45, 307, 60, 45, 53, 53, 557]","[1697548446949, 1697548447476, 1697548447984, 1697548448047, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449116, 1697548449173, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449732, 1697548449777, 1697548449830, 1697548449883, 1697548450440]"
786,786,48,13,[],200,llama-7b,64,1,1415.0,1.0,1,A100,1697548472908,1697548474323,120,6.0,6.0,"[12, 1150, 68, 68, 50, 66]","[1697548472920, 1697548474070, 1697548474138, 1697548474206, 1697548474256, 1697548474322]"
787,787,748,14,[],200,llama-7b,64,1,2987.0,1.0,1,A100,1697548474325,1697548477312,120,182.0,14.0,"[16, 703, 72, 71, 70, 69, 67, 64, 860, 512, 289, 74, 56, 63]","[1697548474341, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475393, 1697548475457, 1697548476317, 1697548476829, 1697548477118, 1697548477192, 1697548477248, 1697548477311]"
788,788,177,11,[],200,llama-7b,64,1,648.0,1.0,1,A100,1697548457697,1697548458345,120,14.0,1.0,"[31, 616]","[1697548457728, 1697548458344]"
789,789,611,25,[],200,llama-7b,64,1,903.0,1.0,1,A100,1697548507777,1697548508680,120,14.0,1.0,"[7, 895]","[1697548507784, 1697548508679]"
790,790,674,8,[],200,llama-7b,64,1,16820.0,1.0,1,A100,1697548454514,1697548471334,120,161.0,119.0,"[30, 516, 683, 74, 76, 71, 69, 69, 53, 67, 247, 69, 52, 70, 65, 60, 359, 63, 60, 48, 59, 53, 331, 50, 62, 62, 48, 49, 62, 525, 72, 62, 48, 49, 49, 205, 47, 57, 49, 57, 57, 619, 64, 49, 64, 59, 60, 427, 70, 52, 67, 63, 63, 48, 236, 50, 63, 61, 61, 61, 526, 175, 50, 58, 56, 54, 381, 57, 57, 53, 821, 63, 60, 57, 56, 729, 459, 65, 50, 59, 45, 702, 72, 69, 54, 64, 64, 48, 689, 73, 71, 68, 67, 51, 63, 359, 73, 54, 70, 70, 65, 535, 65, 52, 66, 64, 63, 59, 569, 69, 52, 68, 51, 52, 64, 65, 844, 73, 71, 70]","[1697548454544, 1697548455060, 1697548455743, 1697548455817, 1697548455893, 1697548455964, 1697548456033, 1697548456102, 1697548456155, 1697548456222, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457207, 1697548457267, 1697548457315, 1697548457374, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457980, 1697548458029, 1697548458091, 1697548458616, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459205, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460100, 1697548460164, 1697548460223, 1697548460283, 1697548460710, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461309, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462356, 1697548462414, 1697548462470, 1697548462524, 1697548462905, 1697548462962, 1697548463019, 1697548463072, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465317, 1697548465382, 1697548465432, 1697548465491, 1697548465536, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466561, 1697548466609, 1697548467298, 1697548467371, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468247, 1697548468317, 1697548468382, 1697548468917, 1697548468982, 1697548469034, 1697548469100, 1697548469164, 1697548469227, 1697548469286, 1697548469855, 1697548469924, 1697548469976, 1697548470044, 1697548470095, 1697548470147, 1697548470211, 1697548470276, 1697548471120, 1697548471193, 1697548471264, 1697548471334]"
791,791,268,26,[],200,llama-7b,64,1,757.0,1.0,1,A100,1697548508683,1697548509440,120,19.0,1.0,"[10, 746]","[1697548508693, 1697548509439]"
792,792,597,10,[],200,llama-7b,64,1,485.0,1.0,1,A100,1697548449120,1697548449605,120,39.0,1.0,"[15, 470]","[1697548449135, 1697548449605]"
793,793,762,12,[],200,llama-7b,64,1,2727.0,1.0,1,A100,1697548458347,1697548461074,120,92.0,20.0,"[10, 681, 63, 47, 61, 45, 57, 57, 619, 64, 49, 62, 61, 60, 428, 69, 52, 67, 63, 63, 49]","[1697548458357, 1697548459038, 1697548459101, 1697548459148, 1697548459209, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460100, 1697548460162, 1697548460223, 1697548460283, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074]"
794,794,345,11,[],200,llama-7b,64,1,2578.0,1.0,1,A100,1697548449608,1697548452186,120,39.0,20.0,"[36, 724, 72, 56, 53, 51, 405, 64, 62, 54, 42, 297, 49, 48, 53, 48, 59, 244, 58, 45, 58]","[1697548449644, 1697548450368, 1697548450440, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451131, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186]"
795,795,36,27,[],200,llama-7b,64,1,3677.0,1.0,1,A100,1697548509443,1697548513120,120,457.0,20.0,"[14, 870, 66, 60, 59, 46, 58, 57, 231, 55, 43, 42, 52, 51, 656, 55, 55, 53, 54, 749, 351]","[1697548509457, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510673, 1697548510904, 1697548510959, 1697548511002, 1697548511044, 1697548511096, 1697548511147, 1697548511803, 1697548511858, 1697548511913, 1697548511966, 1697548512020, 1697548512769, 1697548513120]"
796,796,419,30,[],200,llama-7b,64,1,2533.0,1.0,1,A100,1697548518441,1697548520974,120,88.0,20.0,"[28, 377, 69, 63, 50, 58, 56, 46, 606, 242, 69, 62, 62, 61, 60, 61, 336, 65, 63, 49, 50]","[1697548518469, 1697548518846, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519188, 1697548519794, 1697548520036, 1697548520105, 1697548520167, 1697548520229, 1697548520290, 1697548520350, 1697548520411, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520974]"
797,797,626,28,[],200,llama-7b,64,1,659.0,1.0,1,A100,1697548513124,1697548513783,120,10.0,1.0,"[20, 639]","[1697548513144, 1697548513783]"
798,798,396,29,[],200,llama-7b,64,1,4652.0,1.0,1,A100,1697548513785,1697548518437,120,89.0,20.0,"[7, 1066, 467, 71, 69, 64, 61, 977, 271, 68, 65, 64, 64, 293, 67, 64, 58, 53, 672, 66, 65]","[1697548513792, 1697548514858, 1697548515325, 1697548515396, 1697548515465, 1697548515529, 1697548515590, 1697548516567, 1697548516838, 1697548516906, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517459, 1697548517523, 1697548517581, 1697548517634, 1697548518306, 1697548518372, 1697548518437]"
799,799,136,14,[],200,llama-7b,64,1,405.0,1.0,1,A100,1697548472051,1697548472456,120,31.0,1.0,"[26, 379]","[1697548472077, 1697548472456]"
800,800,813,15,[],200,llama-7b,64,1,2935.0,1.0,1,A100,1697548472459,1697548475394,120,85.0,20.0,"[18, 708, 250, 67, 49, 51, 65, 58, 345, 68, 68, 51, 66, 64, 62, 595, 72, 71, 70, 69, 67]","[1697548472477, 1697548473185, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473667, 1697548473725, 1697548474070, 1697548474138, 1697548474206, 1697548474257, 1697548474323, 1697548474387, 1697548474449, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475393]"
801,801,543,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548461085,1697548488023,120,,,"[18, 832, 196, 175, 51, 57, 57, 53, 382, 56, 58, 52, 821, 63, 61, 56, 56, 729, 460, 64, 50, 59, 45, 703, 71, 70, 53, 65, 63, 48, 689, 74, 70, 68, 67, 51, 63, 360, 72, 54, 71, 70, 64, 535, 66, 51, 66, 65, 62, 59, 569, 69, 53, 67, 51, 52, 65, 64, 844, 73, 71, 70, 67, 50, 65, 312, 74, 73, 72, 67, 64, 48, 305, 70, 69, 68, 52, 65, 49, 531, 66, 50, 51, 64, 59, 345, 68, 67, 51, 66, 65, 63, 593, 72, 71, 70, 69, 68, 63, 861, 512, 288, 75, 56, 63, 618, 251, 54, 254, 70, 68, 62, 613, 255, 72, 54, 71, 68, 63, 911, 76, 75, 72, 71, 68, 67, 587, 269, 63, 74, 264, 69, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 75, 75, 70, 58, 70, 838, 313, 308, 76, 76, 72, 69, 324, 59, 59, 79, 307, 71, 70]","[1697548461103, 1697548461935, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462524, 1697548462906, 1697548462962, 1697548463020, 1697548463072, 1697548463893, 1697548463956, 1697548464017, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466433, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468318, 1697548468382, 1697548468917, 1697548468983, 1697548469034, 1697548469100, 1697548469165, 1697548469227, 1697548469286, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471193, 1697548471264, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472226, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472855, 1697548472904, 1697548473435, 1697548473501, 1697548473551, 1697548473602, 1697548473666, 1697548473725, 1697548474070, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474450, 1697548475043, 1697548475115, 1697548475186, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479556, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484526, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486406, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
802,802,186,6,[],200,llama-7b,64,1,4625.0,1.0,1,A100,1697548451965,1697548456590,120,123.0,22.0,"[15, 731, 546, 68, 52, 62, 875, 65, 72, 53, 63, 59, 1117, 73, 76, 72, 69, 68, 53, 68, 247, 69, 52]","[1697548451980, 1697548452711, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454379, 1697548454451, 1697548454504, 1697548454567, 1697548454626, 1697548455743, 1697548455816, 1697548455892, 1697548455964, 1697548456033, 1697548456101, 1697548456154, 1697548456222, 1697548456469, 1697548456538, 1697548456590]"
803,803,901,17,[],200,llama-7b,64,1,858.0,1.0,1,A100,1697548487379,1697548488237,120,17.0,1.0,"[12, 846]","[1697548487391, 1697548488237]"
804,804,209,11,[],200,llama-7b,64,1,852.0,1.0,1,A100,1697548456225,1697548457077,120,20.0,1.0,"[8, 843]","[1697548456233, 1697548457076]"
805,805,557,18,[],200,llama-7b,64,1,1001.0,1.0,1,A100,1697548488240,1697548489241,120,31.0,1.0,"[13, 988]","[1697548488253, 1697548489241]"
806,806,329,19,[],200,llama-7b,64,1,586.0,1.0,1,A100,1697548489245,1697548489831,120,15.0,1.0,"[34, 552]","[1697548489279, 1697548489831]"
807,807,905,12,[],200,llama-7b,64,1,611.0,1.0,1,A100,1697548457082,1697548457693,120,11.0,1.0,"[32, 579]","[1697548457114, 1697548457693]"
808,808,630,16,[],200,llama-7b,64,1,370.0,1.0,1,A100,1697548474391,1697548474761,120,6.0,1.0,"[19, 351]","[1697548474410, 1697548474761]"
809,809,562,13,[],200,llama-7b,64,1,5210.0,1.0,1,A100,1697548457696,1697548462906,120,67.0,39.0,"[11, 637, 272, 72, 62, 48, 49, 50, 204, 47, 57, 49, 57, 57, 619, 64, 49, 64, 59, 60, 428, 69, 52, 67, 63, 63, 49, 235, 50, 63, 61, 61, 61, 526, 175, 50, 58, 57, 53, 381]","[1697548457707, 1697548458344, 1697548458616, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458897, 1697548459101, 1697548459148, 1697548459205, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460100, 1697548460164, 1697548460223, 1697548460283, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074, 1697548461309, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462356, 1697548462414, 1697548462471, 1697548462524, 1697548462905]"
810,810,410,17,[],200,llama-7b,64,1,3795.0,1.0,1,A100,1697548474763,1697548478558,120,364.0,12.0,"[10, 1063, 481, 512, 289, 74, 57, 63, 618, 250, 54, 254, 70]","[1697548474773, 1697548475836, 1697548476317, 1697548476829, 1697548477118, 1697548477192, 1697548477249, 1697548477312, 1697548477930, 1697548478180, 1697548478234, 1697548478488, 1697548478558]"
811,811,919,20,[],200,llama-7b,64,1,533.0,1.0,1,A100,1697548489834,1697548490367,120,14.0,1.0,"[29, 504]","[1697548489863, 1697548490367]"
812,812,570,17,[],200,llama-7b,64,1,393.0,1.0,1,A100,1697548472793,1697548473186,120,18.0,1.0,"[6, 387]","[1697548472799, 1697548473186]"
813,813,272,10,[],200,llama-7b,64,1,2168.0,1.0,1,A100,1697548456728,1697548458896,120,86.0,20.0,"[16, 333, 68, 61, 61, 47, 59, 54, 331, 50, 62, 61, 49, 49, 61, 526, 72, 61, 49, 48, 50]","[1697548456744, 1697548457077, 1697548457145, 1697548457206, 1697548457267, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458688, 1697548458749, 1697548458798, 1697548458846, 1697548458896]"
814,814,60,9,[],200,llama-7b,64,1,6026.0,1.0,1,A100,1697548450443,1697548456469,120,93.0,36.0,"[19, 543, 65, 60, 55, 41, 298, 49, 48, 53, 49, 58, 244, 58, 45, 58, 54, 1017, 68, 52, 62, 875, 66, 70, 54, 62, 59, 1118, 73, 76, 71, 70, 68, 53, 67, 247]","[1697548450462, 1697548451005, 1697548451070, 1697548451130, 1697548451185, 1697548451226, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451723, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456468]"
815,815,702,4,[],200,llama-7b,64,1,3009.0,1.0,1,A100,1697548443626,1697548446635,120,89.0,20.0,"[11, 504, 209, 58, 46, 57, 55, 54, 43, 50, 825, 310, 55, 41, 40, 368, 62, 61, 58, 45, 57]","[1697548443637, 1697548444141, 1697548444350, 1697548444408, 1697548444454, 1697548444511, 1697548444566, 1697548444620, 1697548444663, 1697548444713, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446635]"
816,816,228,18,[],200,llama-7b,64,1,4061.0,1.0,1,A100,1697548473188,1697548477249,120,100.0,20.0,"[10, 799, 73, 68, 68, 50, 66, 65, 63, 594, 71, 72, 70, 68, 67, 65, 860, 513, 288, 74, 57]","[1697548473198, 1697548473997, 1697548474070, 1697548474138, 1697548474206, 1697548474256, 1697548474322, 1697548474387, 1697548474450, 1697548475044, 1697548475115, 1697548475187, 1697548475257, 1697548475325, 1697548475392, 1697548475457, 1697548476317, 1697548476830, 1697548477118, 1697548477192, 1697548477249]"
817,817,896,15,[],200,llama-7b,64,1,454.0,1.0,1,A100,1697548465448,1697548465902,120,15.0,1.0,"[19, 435]","[1697548465467, 1697548465902]"
818,818,557,16,[],200,llama-7b,64,1,1111.0,1.0,1,A100,1697548465906,1697548467017,120,31.0,1.0,"[20, 1090]","[1697548465926, 1697548467016]"
819,819,326,17,[],200,llama-7b,64,1,2145.0,1.0,1,A100,1697548467020,1697548469165,120,345.0,12.0,"[16, 935, 80, 73, 53, 71, 71, 64, 534, 65, 52, 66, 64]","[1697548467036, 1697548467971, 1697548468051, 1697548468124, 1697548468177, 1697548468248, 1697548468319, 1697548468383, 1697548468917, 1697548468982, 1697548469034, 1697548469100, 1697548469164]"
820,820,914,18,[],200,llama-7b,64,1,2881.0,1.0,1,A100,1697548469167,1697548472048,120,84.0,20.0,"[16, 399, 274, 68, 53, 67, 51, 52, 65, 64, 844, 75, 69, 70, 67, 50, 65, 312, 74, 73, 73]","[1697548469183, 1697548469582, 1697548469856, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471195, 1697548471264, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472048]"
821,821,234,2,[],200,llama-7b,64,1,4444.0,1.0,1,A100,1697548444672,1697548449116,120,457.0,25.0,"[19, 598, 249, 310, 55, 41, 41, 367, 61, 62, 57, 46, 56, 359, 45, 44, 43, 57, 801, 63, 62, 59, 57, 56, 769, 67]","[1697548444691, 1697548445289, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446413, 1697548446475, 1697548446532, 1697548446578, 1697548446634, 1697548446993, 1697548447038, 1697548447082, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448108, 1697548448167, 1697548448224, 1697548448280, 1697548449049, 1697548449116]"
822,822,564,4,[],200,llama-7b,64,1,3372.0,1.0,1,A100,1697548451133,1697548454505,120,84.0,20.0,"[6, 319, 67, 48, 49, 52, 48, 59, 245, 58, 45, 57, 54, 1017, 68, 51, 62, 876, 66, 70, 55]","[1697548451139, 1697548451458, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451722, 1697548451781, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453376, 1697548453438, 1697548454314, 1697548454380, 1697548454450, 1697548454505]"
823,823,445,5,[],200,llama-7b,64,1,766.0,1.0,1,A100,1697548449674,1697548450440,120,457.0,2.0,"[13, 680, 73]","[1697548449687, 1697548450367, 1697548450440]"
824,824,214,6,[],200,llama-7b,64,1,2935.0,1.0,1,A100,1697548450443,1697548453378,120,52.0,20.0,"[24, 467, 72, 64, 60, 55, 41, 299, 48, 49, 52, 49, 58, 244, 58, 45, 58, 54, 1017, 68, 52]","[1697548450467, 1697548450934, 1697548451006, 1697548451070, 1697548451130, 1697548451185, 1697548451226, 1697548451525, 1697548451573, 1697548451622, 1697548451674, 1697548451723, 1697548451781, 1697548452025, 1697548452083, 1697548452128, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453377]"
825,825,24,4,[],200,llama-7b,64,1,1863.0,1.0,1,A100,1697548441614,1697548443477,120,79.0,9.0,"[11, 483, 706, 60, 47, 46, 38, 417, 55]","[1697548441625, 1697548442108, 1697548442814, 1697548442874, 1697548442921, 1697548442967, 1697548443005, 1697548443422, 1697548443477]"
826,826,804,7,[],200,llama-7b,64,1,464.0,1.0,1,A100,1697548453383,1697548453847,120,20.0,1.0,"[26, 438]","[1697548453409, 1697548453847]"
827,827,795,1,[],200,llama-7b,64,1,381.0,1.0,1,A100,1697548438163,1697548438544,120,12.0,1.0,"[33, 348]","[1697548438196, 1697548438544]"
828,828,568,8,[],200,llama-7b,64,1,1210.0,1.0,1,A100,1697548453850,1697548455060,120,11.0,1.0,"[25, 1185]","[1697548453875, 1697548455060]"
829,829,48,13,[],200,llama-7b,64,1,966.0,1.0,1,A100,1697548458346,1697548459312,120,6.0,6.0,"[7, 748, 47, 61, 45, 57]","[1697548458353, 1697548459101, 1697548459148, 1697548459209, 1697548459254, 1697548459311]"
830,830,449,2,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.84 GiB. GPU 0 has a total capacty of 39.39 GiB of which 6.70 GiB is free. Process 1412106 has 32.69 GiB memory in use. Of the allocated memory 20.89 GiB is allocated by PyTorch, and 10.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548438547,1697548441612,120,,,"[16, 714, 408, 70, 61, 47, 61, 59, 723, 70, 54, 69, 64, 48, 63]","[1697548438563, 1697548439277, 1697548439685, 1697548439755, 1697548439816, 1697548439863, 1697548439924, 1697548439983, 1697548440706, 1697548440776, 1697548440830, 1697548440899, 1697548440963, 1697548441011, 1697548441074]"
831,831,606,5,[],200,llama-7b,64,1,661.0,1.0,1,A100,1697548443480,1697548444141,120,9.0,1.0,"[14, 647]","[1697548443494, 1697548444141]"
832,832,316,9,[],200,llama-7b,64,1,3030.0,1.0,1,A100,1697548455061,1697548458091,120,86.0,20.0,"[15, 1317, 76, 69, 52, 70, 65, 60, 359, 63, 60, 48, 59, 53, 331, 50, 62, 62, 49, 48, 62]","[1697548455076, 1697548456393, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457207, 1697548457267, 1697548457315, 1697548457374, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457981, 1697548458029, 1697548458091]"
833,833,745,14,[],200,llama-7b,64,1,424.0,1.0,1,A100,1697548459313,1697548459737,120,17.0,1.0,"[13, 411]","[1697548459326, 1697548459737]"
834,834,384,6,[],200,llama-7b,64,1,3964.0,1.0,1,A100,1697548444144,1697548448108,120,92.0,20.0,"[19, 1125, 250, 310, 55, 41, 40, 369, 61, 61, 57, 47, 56, 358, 44, 44, 44, 57, 801, 63, 62]","[1697548444163, 1697548445288, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445984, 1697548446353, 1697548446414, 1697548446475, 1697548446532, 1697548446579, 1697548446635, 1697548446993, 1697548447037, 1697548447081, 1697548447125, 1697548447182, 1697548447983, 1697548448046, 1697548448108]"
835,835,405,15,[],200,llama-7b,64,1,2784.0,1.0,1,A100,1697548459740,1697548462524,120,87.0,20.0,"[7, 876, 88, 69, 52, 67, 63, 63, 49, 236, 49, 63, 62, 60, 61, 526, 176, 50, 57, 57, 53]","[1697548459747, 1697548460623, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074, 1697548461310, 1697548461359, 1697548461422, 1697548461484, 1697548461544, 1697548461605, 1697548462131, 1697548462307, 1697548462357, 1697548462414, 1697548462471, 1697548462524]"
836,836,72,2,[],200,llama-7b,64,1,3096.0,1.0,1,A100,1697548441616,1697548444712,120,84.0,20.0,"[63, 1079, 57, 59, 48, 46, 37, 417, 55, 54, 51, 41, 48, 679, 57, 46, 57, 56, 53, 44, 49]","[1697548441679, 1697548442758, 1697548442815, 1697548442874, 1697548442922, 1697548442968, 1697548443005, 1697548443422, 1697548443477, 1697548443531, 1697548443582, 1697548443623, 1697548443671, 1697548444350, 1697548444407, 1697548444453, 1697548444510, 1697548444566, 1697548444619, 1697548444663, 1697548444712]"
837,837,273,8,[],200,llama-7b,64,1,680.0,1.0,1,A100,1697548456397,1697548457077,120,19.0,1.0,"[15, 664]","[1697548456412, 1697548457076]"
838,838,863,9,[],200,llama-7b,64,1,612.0,1.0,1,A100,1697548457080,1697548457692,120,10.0,1.0,"[26, 586]","[1697548457106, 1697548457692]"
839,839,294,13,[],200,llama-7b,64,1,919.0,1.0,1,A100,1697548456225,1697548457144,120,9.0,2.0,"[11, 908]","[1697548456236, 1697548457144]"
840,840,38,7,[],200,llama-7b,64,1,3019.0,1.0,1,A100,1697548448111,1697548451130,120,88.0,20.0,"[10, 401, 528, 66, 58, 45, 45, 57, 44, 307, 59, 45, 54, 53, 558, 55, 53, 51, 405, 64, 61]","[1697548448121, 1697548448522, 1697548449050, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449321, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450441, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451069, 1697548451130]"
841,841,883,14,[],200,llama-7b,64,1,547.0,1.0,1,A100,1697548457146,1697548457693,120,563.0,1.0,"[6, 541]","[1697548457152, 1697548457693]"
842,842,629,10,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548457701,1697548488027,120,,,"[26, 617, 272, 72, 62, 49, 48, 50, 204, 47, 60, 46, 57, 57, 619, 64, 50, 63, 59, 61, 427, 69, 52, 67, 63, 63, 49, 236, 49, 63, 62, 60, 61, 526, 175, 50, 58, 57, 53, 381, 57, 57, 53, 821, 63, 60, 57, 56, 729, 459, 65, 50, 59, 45, 702, 72, 69, 54, 64, 64, 48, 689, 73, 71, 68, 67, 51, 63, 359, 73, 54, 70, 70, 65, 535, 65, 52, 66, 64, 63, 59, 569, 69, 52, 68, 51, 52, 64, 65, 844, 73, 71, 70, 67, 50, 65, 312, 74, 73, 72, 67, 64, 48, 305, 70, 69, 68, 52, 65, 49, 531, 66, 50, 50, 65, 59, 345, 68, 67, 51, 66, 65, 62, 594, 72, 71, 70, 69, 68, 63, 861, 512, 288, 74, 57, 63, 618, 251, 54, 254, 70, 68, 62, 613, 255, 72, 54, 71, 68, 63, 911, 76, 75, 72, 71, 68, 66, 588, 269, 63, 74, 264, 69, 51, 68, 839, 75, 56, 72, 66, 65, 50, 338, 76, 75, 75, 71, 56, 71, 837, 314, 308, 76, 76, 71, 70, 324, 59, 59, 79, 307, 71, 70]","[1697548457727, 1697548458344, 1697548458616, 1697548458688, 1697548458750, 1697548458799, 1697548458847, 1697548458897, 1697548459101, 1697548459148, 1697548459208, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460101, 1697548460164, 1697548460223, 1697548460284, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074, 1697548461310, 1697548461359, 1697548461422, 1697548461484, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462356, 1697548462414, 1697548462471, 1697548462524, 1697548462905, 1697548462962, 1697548463019, 1697548463072, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465317, 1697548465382, 1697548465432, 1697548465491, 1697548465536, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466561, 1697548466609, 1697548467298, 1697548467371, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468247, 1697548468317, 1697548468382, 1697548468917, 1697548468982, 1697548469034, 1697548469100, 1697548469164, 1697548469227, 1697548469286, 1697548469855, 1697548469924, 1697548469976, 1697548470044, 1697548470095, 1697548470147, 1697548470211, 1697548470276, 1697548471120, 1697548471193, 1697548471264, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472226, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472855, 1697548472904, 1697548473435, 1697548473501, 1697548473551, 1697548473601, 1697548473666, 1697548473725, 1697548474070, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474449, 1697548475043, 1697548475115, 1697548475186, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477191, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479556, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481223, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483842, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484527, 1697548484583, 1697548484654, 1697548485491, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486336, 1697548486406, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
843,843,655,15,[],200,llama-7b,64,1,1558.0,1.0,1,A100,1697548457696,1697548459254,120,335.0,11.0,"[17, 631, 272, 72, 62, 48, 49, 50, 204, 46, 61, 46]","[1697548457713, 1697548458344, 1697548458616, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458897, 1697548459101, 1697548459147, 1697548459208, 1697548459254]"
844,844,689,21,[],200,llama-7b,64,1,356.0,1.0,1,A100,1697548490371,1697548490727,120,15.0,1.0,"[18, 338]","[1697548490389, 1697548490727]"
845,845,319,22,[],200,llama-7b,64,1,612.0,1.0,1,A100,1697548490733,1697548491345,120,31.0,1.0,"[36, 576]","[1697548490769, 1697548491345]"
846,846,89,23,[],200,llama-7b,64,1,2793.0,1.0,1,A100,1697548491348,1697548494141,120,52.0,20.0,"[20, 612, 58, 54, 53, 43, 51, 51, 49, 721, 59, 49, 48, 39, 357, 47, 38, 38, 312, 52, 42]","[1697548491368, 1697548491980, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492339, 1697548493060, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493612, 1697548493659, 1697548493697, 1697548493735, 1697548494047, 1697548494099, 1697548494141]"
847,847,678,24,[],200,llama-7b,64,1,1757.0,1.0,1,A100,1697548494143,1697548495900,120,244.0,18.0,"[11, 398, 67, 60, 47, 60, 55, 54, 52, 296, 49, 59, 58, 55, 53, 41, 234, 55, 52]","[1697548494154, 1697548494552, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494947, 1697548495243, 1697548495292, 1697548495351, 1697548495409, 1697548495464, 1697548495517, 1697548495558, 1697548495792, 1697548495847, 1697548495899]"
848,848,161,8,[],200,llama-7b,64,1,654.0,1.0,1,A100,1697548457376,1697548458030,120,109.0,7.0,"[11, 371, 50, 62, 62, 49, 48]","[1697548457387, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457981, 1697548458029]"
849,849,860,9,[],200,llama-7b,64,1,2679.0,1.0,1,A100,1697548458032,1697548460711,120,85.0,20.0,"[17, 295, 272, 72, 62, 49, 48, 50, 204, 47, 61, 45, 57, 57, 619, 64, 50, 63, 59, 61, 427]","[1697548458049, 1697548458344, 1697548458616, 1697548458688, 1697548458750, 1697548458799, 1697548458847, 1697548458897, 1697548459101, 1697548459148, 1697548459209, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460101, 1697548460164, 1697548460223, 1697548460284, 1697548460711]"
850,850,255,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548469291,1697548488027,120,,,"[8, 1402, 419, 73, 72, 69, 67, 50, 65, 312, 74, 73, 72, 67, 64, 49, 305, 70, 69, 68, 52, 65, 48, 532, 66, 50, 50, 65, 57, 347, 68, 66, 51, 67, 64, 63, 593, 73, 71, 69, 69, 68, 63, 861, 512, 288, 75, 56, 63, 618, 251, 55, 253, 70, 68, 62, 613, 256, 71, 54, 71, 68, 63, 911, 77, 74, 72, 71, 68, 67, 587, 269, 63, 74, 264, 69, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 75, 75, 71, 57, 70, 838, 313, 308, 76, 76, 72, 70, 323, 59, 59, 79, 307, 71, 70]","[1697548469299, 1697548470701, 1697548471120, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472227, 1697548472532, 1697548472602, 1697548472671, 1697548472739, 1697548472791, 1697548472856, 1697548472904, 1697548473436, 1697548473502, 1697548473552, 1697548473602, 1697548473667, 1697548473724, 1697548474071, 1697548474139, 1697548474205, 1697548474256, 1697548474323, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475187, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480872, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
851,851,123,16,[],200,llama-7b,64,1,391.0,1.0,1,A100,1697548472795,1697548473186,120,14.0,1.0,"[23, 368]","[1697548472818, 1697548473186]"
852,852,517,10,[],200,llama-7b,64,1,528.0,1.0,1,A100,1697548460715,1697548461243,120,15.0,1.0,"[15, 512]","[1697548460730, 1697548461242]"
853,853,289,11,[],200,llama-7b,64,1,4198.0,1.0,1,A100,1697548461245,1697548465443,120,89.0,20.0,"[7, 683, 196, 175, 51, 57, 57, 54, 381, 56, 58, 52, 822, 62, 61, 56, 56, 729, 460, 64, 55]","[1697548461252, 1697548461935, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462525, 1697548462906, 1697548462962, 1697548463020, 1697548463072, 1697548463894, 1697548463956, 1697548464017, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465437]"
854,854,346,20,[],200,llama-7b,64,1,2791.0,1.0,1,A100,1697548466436,1697548469227,120,85.0,20.0,"[15, 565, 283, 73, 71, 67, 68, 51, 63, 359, 72, 54, 71, 70, 64, 536, 65, 52, 66, 64, 61]","[1697548466451, 1697548467016, 1697548467299, 1697548467372, 1697548467443, 1697548467510, 1697548467578, 1697548467629, 1697548467692, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468318, 1697548468382, 1697548468918, 1697548468983, 1697548469035, 1697548469101, 1697548469165, 1697548469226]"
855,855,705,17,[],200,llama-7b,64,1,5438.0,1.0,1,A100,1697548473189,1697548478627,120,79.0,27.0,"[11, 870, 68, 68, 50, 66, 65, 63, 594, 71, 72, 70, 68, 67, 65, 860, 513, 288, 74, 57, 63, 617, 251, 55, 253, 71, 68]","[1697548473200, 1697548474070, 1697548474138, 1697548474206, 1697548474256, 1697548474322, 1697548474387, 1697548474450, 1697548475044, 1697548475115, 1697548475187, 1697548475257, 1697548475325, 1697548475392, 1697548475457, 1697548476317, 1697548476830, 1697548477118, 1697548477192, 1697548477249, 1697548477312, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478559, 1697548478627]"
856,856,875,32,[],200,llama-7b,64,1,4233.0,1.0,1,A100,1697548504571,1697548508804,120,31.0,31.0,"[17, 605, 69, 66, 51, 60, 60, 60, 283, 46, 54, 52, 679, 65, 58, 55, 43, 581, 65, 61, 59, 56, 57, 324, 65, 64, 60, 59, 55, 42, 305, 57]","[1697548504588, 1697548505193, 1697548505262, 1697548505328, 1697548505379, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506673, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507540, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508747, 1697548508804]"
857,857,869,19,[],200,llama-7b,64,1,2068.0,1.0,1,A100,1697548473189,1697548475257,120,244.0,12.0,"[23, 785, 73, 69, 66, 51, 66, 65, 63, 594, 71, 72, 70]","[1697548473212, 1697548473997, 1697548474070, 1697548474139, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474450, 1697548475044, 1697548475115, 1697548475187, 1697548475257]"
858,858,332,5,[],200,llama-7b,64,1,834.0,1.0,1,A100,1697548446641,1697548447475,120,39.0,1.0,"[22, 812]","[1697548446663, 1697548447475]"
859,859,617,20,[],200,llama-7b,64,1,4562.0,1.0,1,A100,1697548475260,1697548479822,120,87.0,20.0,"[6, 571, 481, 511, 289, 74, 57, 63, 617, 251, 55, 253, 70, 68, 62, 614, 255, 71, 54, 71, 69]","[1697548475266, 1697548475837, 1697548476318, 1697548476829, 1697548477118, 1697548477192, 1697548477249, 1697548477312, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479822]"
860,860,663,5,[],200,llama-7b,64,1,4825.0,1.0,1,A100,1697548461609,1697548466434,120,79.0,20.0,"[24, 1273, 56, 57, 54, 820, 62, 62, 56, 56, 729, 460, 65, 55, 53, 45, 703, 71, 70, 53]","[1697548461633, 1697548462906, 1697548462962, 1697548463019, 1697548463073, 1697548463893, 1697548463955, 1697548464017, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465383, 1697548465438, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466433]"
861,861,388,21,[],200,llama-7b,64,1,3887.0,1.0,1,A100,1697548479825,1697548483712,120,87.0,20.0,"[22, 455, 494, 76, 74, 73, 71, 67, 68, 586, 269, 63, 74, 266, 67, 51, 68, 839, 75, 55, 73]","[1697548479847, 1697548480302, 1697548480796, 1697548480872, 1697548480946, 1697548481019, 1697548481090, 1697548481157, 1697548481225, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482483, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711]"
862,862,102,6,[],200,llama-7b,64,1,3652.0,1.0,1,A100,1697548447478,1697548451130,120,84.0,20.0,"[18, 1025, 528, 67, 58, 45, 45, 56, 45, 307, 59, 45, 54, 53, 558, 55, 52, 52, 405, 64, 61]","[1697548447496, 1697548448521, 1697548449049, 1697548449116, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450441, 1697548450496, 1697548450548, 1697548450600, 1697548451005, 1697548451069, 1697548451130]"
863,863,691,7,[],200,llama-7b,64,1,326.0,1.0,1,A100,1697548451133,1697548451459,120,47.0,1.0,"[11, 315]","[1697548451144, 1697548451459]"
864,864,463,8,[],200,llama-7b,64,1,500.0,1.0,1,A100,1697548451462,1697548451962,120,39.0,1.0,"[20, 480]","[1697548451482, 1697548451962]"
865,865,116,9,[],200,llama-7b,64,1,747.0,1.0,1,A100,1697548451965,1697548452712,120,23.0,1.0,"[25, 722]","[1697548451990, 1697548452712]"
866,866,116,14,[],200,llama-7b,64,1,550.0,1.0,1,A100,1697548471906,1697548472456,120,23.0,1.0,"[6, 544]","[1697548471912, 1697548472456]"
867,867,822,10,[],200,llama-7b,64,1,4009.0,1.0,1,A100,1697548452717,1697548456726,120,88.0,20.0,"[24, 1105, 468, 66, 71, 53, 62, 59, 1117, 73, 76, 72, 69, 69, 53, 67, 249, 68, 52, 70, 66]","[1697548452741, 1697548453846, 1697548454314, 1697548454380, 1697548454451, 1697548454504, 1697548454566, 1697548454625, 1697548455742, 1697548455815, 1697548455891, 1697548455963, 1697548456032, 1697548456101, 1697548456154, 1697548456221, 1697548456470, 1697548456538, 1697548456590, 1697548456660, 1697548456726]"
868,868,700,15,[],200,llama-7b,64,1,6168.0,1.0,1,A100,1697548472459,1697548478627,120,140.0,33.0,"[9, 967, 67, 49, 51, 65, 58, 345, 68, 68, 51, 66, 65, 61, 595, 72, 71, 70, 69, 67, 64, 860, 513, 287, 75, 56, 64, 617, 251, 55, 253, 70, 69]","[1697548472468, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473667, 1697548473725, 1697548474070, 1697548474138, 1697548474206, 1697548474257, 1697548474323, 1697548474388, 1697548474449, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475393, 1697548475457, 1697548476317, 1697548476830, 1697548477117, 1697548477192, 1697548477248, 1697548477312, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478627]"
869,869,49,22,[],200,llama-7b,64,1,593.0,1.0,1,A100,1697548483714,1697548484307,120,109.0,3.0,"[6, 428, 83, 76]","[1697548483720, 1697548484148, 1697548484231, 1697548484307]"
870,870,748,23,[],200,llama-7b,64,1,2996.0,1.0,1,A100,1697548484310,1697548487306,120,182.0,14.0,"[13, 1170, 312, 308, 77, 76, 71, 70, 323, 59, 59, 79, 307, 71]","[1697548484323, 1697548485493, 1697548485805, 1697548486113, 1697548486190, 1697548486266, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234, 1697548487305]"
871,871,403,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548487308,1697548488027,120,,,"[6, 494]","[1697548487314, 1697548487808]"
872,872,180,25,[],200,llama-7b,64,1,1559.0,1.0,1,A100,1697548488032,1697548489591,120,123.0,12.0,"[129, 671, 57, 51, 35, 37, 286, 67, 50, 48, 41, 48, 39]","[1697548488161, 1697548488832, 1697548488889, 1697548488940, 1697548488975, 1697548489012, 1697548489298, 1697548489365, 1697548489415, 1697548489463, 1697548489504, 1697548489552, 1697548489591]"
873,873,763,26,[],200,llama-7b,64,1,772.0,1.0,1,A100,1697548489595,1697548490367,120,20.0,1.0,"[16, 755]","[1697548489611, 1697548490366]"
874,874,537,27,[],200,llama-7b,64,1,2751.0,1.0,1,A100,1697548490370,1697548493121,120,83.0,20.0,"[9, 347, 56, 49, 49, 41, 553, 51, 48, 40, 48, 49, 329, 54, 52, 43, 52, 50, 48, 724, 59]","[1697548490379, 1697548490726, 1697548490782, 1697548490831, 1697548490880, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492039, 1697548492093, 1697548492145, 1697548492188, 1697548492240, 1697548492290, 1697548492338, 1697548493062, 1697548493121]"
875,875,879,14,[],200,llama-7b,64,1,6754.0,1.0,1,A100,1697548488031,1697548494785,120,39.0,55.0,"[65, 737, 56, 51, 36, 36, 286, 66, 51, 48, 41, 48, 40, 303, 52, 43, 43, 50, 49, 40, 42, 192, 375, 49, 49, 42, 553, 50, 49, 40, 48, 48, 329, 54, 52, 43, 51, 51, 49, 722, 59, 48, 48, 40, 356, 48, 38, 38, 311, 52, 42, 46, 432, 60, 47, 60]","[1697548488096, 1697548488833, 1697548488889, 1697548488940, 1697548488976, 1697548489012, 1697548489298, 1697548489364, 1697548489415, 1697548489463, 1697548489504, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490214, 1697548490406, 1697548490781, 1697548490830, 1697548490879, 1697548490921, 1697548491474, 1697548491524, 1697548491573, 1697548491613, 1697548491661, 1697548491709, 1697548492038, 1697548492092, 1697548492144, 1697548492187, 1697548492238, 1697548492289, 1697548492338, 1697548493060, 1697548493119, 1697548493167, 1697548493215, 1697548493255, 1697548493611, 1697548493659, 1697548493697, 1697548493735, 1697548494046, 1697548494098, 1697548494140, 1697548494186, 1697548494618, 1697548494678, 1697548494725, 1697548494785]"
876,876,187,28,[],200,llama-7b,64,1,924.0,1.0,1,A100,1697548493123,1697548494047,120,161.0,6.0,"[22, 405, 62, 48, 38, 38, 311]","[1697548493145, 1697548493550, 1697548493612, 1697548493660, 1697548493698, 1697548493736, 1697548494047]"
877,877,477,11,[],200,llama-7b,64,1,5742.0,1.0,1,A100,1697548456729,1697548462471,120,244.0,50.0,"[25, 323, 68, 62, 60, 47, 59, 54, 331, 50, 62, 61, 49, 49, 61, 526, 73, 61, 48, 48, 50, 205, 47, 57, 49, 57, 57, 619, 64, 49, 64, 59, 60, 427, 70, 52, 67, 63, 63, 48, 236, 50, 63, 61, 61, 61, 526, 175, 50, 58, 57]","[1697548456754, 1697548457077, 1697548457145, 1697548457207, 1697548457267, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458689, 1697548458750, 1697548458798, 1697548458846, 1697548458896, 1697548459101, 1697548459148, 1697548459205, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460100, 1697548460164, 1697548460223, 1697548460283, 1697548460710, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461309, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462356, 1697548462414, 1697548462471]"
878,878,4,3,[],200,llama-7b,64,1,2603.0,1.0,1,A100,1697548449119,1697548451722,120,89.0,20.0,"[11, 475, 67, 60, 45, 53, 53, 558, 55, 53, 51, 405, 65, 60, 55, 42, 297, 49, 48, 53, 48]","[1697548449130, 1697548449605, 1697548449672, 1697548449732, 1697548449777, 1697548449830, 1697548449883, 1697548450441, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451070, 1697548451130, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722]"
879,879,737,10,[],200,llama-7b,64,1,673.0,1.0,1,A100,1697548456471,1697548457144,120,216.0,2.0,"[12, 594, 67]","[1697548456483, 1697548457077, 1697548457144]"
880,880,864,29,[],200,llama-7b,64,1,1953.0,1.0,1,A100,1697548494050,1697548496003,120,83.0,20.0,"[15, 487, 67, 60, 47, 60, 55, 54, 52, 296, 49, 58, 59, 54, 54, 41, 234, 55, 52, 52, 52]","[1697548494065, 1697548494552, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494947, 1697548495243, 1697548495292, 1697548495350, 1697548495409, 1697548495463, 1697548495517, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496003]"
881,881,778,18,[],200,llama-7b,64,1,6107.0,1.0,1,A100,1697548488033,1697548494140,120,16.0,50.0,"[148, 651, 56, 52, 35, 38, 284, 68, 49, 49, 41, 47, 40, 305, 51, 43, 43, 51, 48, 41, 42, 192, 374, 49, 49, 42, 553, 50, 49, 40, 48, 48, 329, 54, 52, 43, 52, 50, 49, 722, 59, 48, 49, 39, 356, 48, 38, 38, 311, 52, 42]","[1697548488181, 1697548488832, 1697548488888, 1697548488940, 1697548488975, 1697548489013, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489504, 1697548489551, 1697548489591, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490215, 1697548490407, 1697548490781, 1697548490830, 1697548490879, 1697548490921, 1697548491474, 1697548491524, 1697548491573, 1697548491613, 1697548491661, 1697548491709, 1697548492038, 1697548492092, 1697548492144, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493060, 1697548493119, 1697548493167, 1697548493216, 1697548493255, 1697548493611, 1697548493659, 1697548493697, 1697548493735, 1697548494046, 1697548494098, 1697548494140]"
882,882,311,12,[],200,llama-7b,64,1,2705.0,1.0,1,A100,1697548458901,1697548461606,120,93.0,20.0,"[33, 803, 250, 64, 49, 63, 60, 60, 427, 69, 52, 69, 61, 65, 48, 236, 50, 62, 62, 61, 61]","[1697548458934, 1697548459737, 1697548459987, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460710, 1697548460779, 1697548460831, 1697548460900, 1697548460961, 1697548461026, 1697548461074, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461606]"
883,883,220,3,[],200,llama-7b,64,1,1198.0,1.0,1,A100,1697548441617,1697548442815,120,67.0,2.0,"[123, 1019, 56]","[1697548441740, 1697548442759, 1697548442815]"
884,884,809,4,[],200,llama-7b,64,1,535.0,1.0,1,A100,1697548442819,1697548443354,120,16.0,1.0,"[8, 526]","[1697548442827, 1697548443353]"
885,885,580,5,[],200,llama-7b,64,1,3278.0,1.0,1,A100,1697548443356,1697548446634,120,88.0,20.0,"[27, 757, 209, 58, 47, 56, 56, 54, 43, 49, 826, 310, 55, 41, 41, 367, 62, 61, 58, 45, 56]","[1697548443383, 1697548444140, 1697548444349, 1697548444407, 1697548444454, 1697548444510, 1697548444566, 1697548444620, 1697548444663, 1697548444712, 1697548445538, 1697548445848, 1697548445903, 1697548445944, 1697548445985, 1697548446352, 1697548446414, 1697548446475, 1697548446533, 1697548446578, 1697548446634]"
886,886,893,13,[],200,llama-7b,64,1,2520.0,1.0,1,A100,1697548461609,1697548464129,120,335.0,10.0,"[19, 1205, 73, 56, 57, 54, 821, 61, 62, 56, 56]","[1697548461628, 1697548462833, 1697548462906, 1697548462962, 1697548463019, 1697548463073, 1697548463894, 1697548463955, 1697548464017, 1697548464073, 1697548464129]"
887,887,899,10,[],200,llama-7b,64,1,2980.0,1.0,1,A100,1697548458094,1697548461074,120,100.0,20.0,"[19, 924, 64, 47, 61, 45, 57, 57, 619, 64, 50, 63, 59, 60, 428, 69, 52, 67, 63, 63, 49]","[1697548458113, 1697548459037, 1697548459101, 1697548459148, 1697548459209, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460101, 1697548460164, 1697548460223, 1697548460283, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074]"
888,888,587,16,[],200,llama-7b,64,1,439.0,1.0,1,A100,1697548475398,1697548475837,120,13.0,1.0,"[24, 415]","[1697548475422, 1697548475837]"
889,889,670,11,[],200,llama-7b,64,1,4239.0,1.0,1,A100,1697548461079,1697548465318,120,67.0,18.0,"[16, 840, 196, 175, 50, 58, 57, 53, 382, 55, 59, 53, 820, 63, 60, 57, 56, 729, 460]","[1697548461095, 1697548461935, 1697548462131, 1697548462306, 1697548462356, 1697548462414, 1697548462471, 1697548462524, 1697548462906, 1697548462961, 1697548463020, 1697548463073, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465318]"
890,890,241,17,[],200,llama-7b,64,1,1839.0,1.0,1,A100,1697548475840,1697548477679,120,19.0,1.0,"[22, 1817]","[1697548475862, 1697548477679]"
891,891,654,15,[],200,llama-7b,64,1,564.0,1.0,1,A100,1697548494788,1697548495352,120,47.0,4.0,"[7, 384, 65, 49, 58]","[1697548494795, 1697548495179, 1697548495244, 1697548495293, 1697548495351]"
892,892,654,3,[],200,llama-7b,64,1,1759.0,1.0,1,A100,1697548444716,1697548446475,120,47.0,4.0,"[22, 1535, 79, 62, 61]","[1697548444738, 1697548446273, 1697548446352, 1697548446414, 1697548446475]"
893,893,18,18,[],200,llama-7b,64,1,1408.0,1.0,1,A100,1697548477683,1697548479091,120,15.0,1.0,"[30, 1378]","[1697548477713, 1697548479091]"
894,894,431,4,[],200,llama-7b,64,1,3299.0,1.0,1,A100,1697548446478,1697548449777,120,732.0,22.0,"[8, 446, 61, 45, 44, 44, 57, 800, 63, 62, 58, 57, 57, 770, 67, 57, 45, 45, 56, 45, 307, 59, 45]","[1697548446486, 1697548446932, 1697548446993, 1697548447038, 1697548447082, 1697548447126, 1697548447183, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448223, 1697548448280, 1697548449050, 1697548449117, 1697548449174, 1697548449219, 1697548449264, 1697548449320, 1697548449365, 1697548449672, 1697548449731, 1697548449776]"
895,895,304,16,[],200,llama-7b,64,1,2344.0,1.0,1,A100,1697548495353,1697548497697,120,86.0,20.0,"[15, 359, 65, 54, 53, 52, 51, 379, 40, 40, 51, 50, 445, 70, 63, 64, 59, 57, 55, 262, 60]","[1697548495368, 1697548495727, 1697548495792, 1697548495846, 1697548495899, 1697548495951, 1697548496002, 1697548496381, 1697548496421, 1697548496461, 1697548496512, 1697548496562, 1697548497007, 1697548497077, 1697548497140, 1697548497204, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697]"
896,896,599,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548479095,1697548488024,120,,,"[17, 1190, 493, 77, 74, 72, 71, 68, 67, 587, 270, 63, 73, 266, 67, 52, 67, 839, 76, 55, 72, 66, 66, 49, 338, 76, 75, 75, 70, 58, 69, 839, 314, 308, 76, 76, 71, 70, 324, 59, 59, 79, 306, 71, 70]","[1697548479112, 1697548480302, 1697548480795, 1697548480872, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482081, 1697548482144, 1697548482217, 1697548482483, 1697548482550, 1697548482602, 1697548482669, 1697548483508, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484526, 1697548484584, 1697548484653, 1697548485492, 1697548485806, 1697548486114, 1697548486190, 1697548486266, 1697548486337, 1697548486407, 1697548486731, 1697548486790, 1697548486849, 1697548486928, 1697548487234, 1697548487305, 1697548487375]"
897,897,503,11,[],200,llama-7b,64,1,2567.0,1.0,1,A100,1697548459039,1697548461606,120,109.0,20.0,"[7, 691, 250, 64, 50, 62, 60, 60, 427, 69, 53, 66, 63, 65, 48, 236, 50, 62, 62, 61, 61]","[1697548459046, 1697548459737, 1697548459987, 1697548460051, 1697548460101, 1697548460163, 1697548460223, 1697548460283, 1697548460710, 1697548460779, 1697548460832, 1697548460898, 1697548460961, 1697548461026, 1697548461074, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461606]"
898,898,296,8,[],200,llama-7b,64,1,341.0,1.0,1,A100,1697548460902,1697548461243,120,6.0,1.0,"[7, 334]","[1697548460909, 1697548461243]"
899,899,52,17,[],200,llama-7b,64,1,753.0,1.0,1,A100,1697548497699,1697548498452,120,58.0,6.0,"[14, 458, 66, 61, 52, 51, 50]","[1697548497713, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498451]"
900,900,886,9,[],200,llama-7b,64,1,688.0,1.0,1,A100,1697548461247,1697548461935,120,17.0,1.0,"[20, 668]","[1697548461267, 1697548461935]"
901,901,548,14,[],200,llama-7b,64,1,4463.0,1.0,1,A100,1697548462837,1697548467300,120,86.0,20.0,"[11, 733, 313, 62, 60, 58, 56, 728, 460, 64, 50, 59, 46, 703, 70, 70, 53, 65, 64, 47, 691]","[1697548462848, 1697548463581, 1697548463894, 1697548463956, 1697548464016, 1697548464074, 1697548464130, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465491, 1697548465537, 1697548466240, 1697548466310, 1697548466380, 1697548466433, 1697548466498, 1697548466562, 1697548466609, 1697548467300]"
902,902,660,10,[],200,llama-7b,64,1,5434.0,1.0,1,A100,1697548461938,1697548467372,120,732.0,25.0,"[10, 886, 72, 56, 57, 54, 820, 62, 61, 57, 56, 730, 459, 65, 53, 55, 45, 703, 71, 70, 54, 64, 63, 48, 690, 73]","[1697548461948, 1697548462834, 1697548462906, 1697548462962, 1697548463019, 1697548463073, 1697548463893, 1697548463955, 1697548464016, 1697548464073, 1697548464129, 1697548464859, 1697548465318, 1697548465383, 1697548465436, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466434, 1697548466498, 1697548466561, 1697548466609, 1697548467299, 1697548467372]"
903,903,154,13,[],200,llama-7b,64,1,942.0,1.0,1,A100,1697548458096,1697548459038,120,13.0,1.0,"[27, 915]","[1697548458123, 1697548459038]"
904,904,850,14,[],200,llama-7b,64,1,2564.0,1.0,1,A100,1697548459041,1697548461605,120,109.0,20.0,"[20, 676, 250, 64, 50, 62, 60, 61, 426, 69, 53, 67, 63, 63, 48, 237, 50, 62, 62, 61, 60]","[1697548459061, 1697548459737, 1697548459987, 1697548460051, 1697548460101, 1697548460163, 1697548460223, 1697548460284, 1697548460710, 1697548460779, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461605]"
905,905,318,15,[],200,llama-7b,64,1,1017.0,1.0,1,A100,1697548467302,1697548468319,120,6.0,6.0,"[23, 646, 81, 72, 53, 71, 71]","[1697548467325, 1697548467971, 1697548468052, 1697548468124, 1697548468177, 1697548468248, 1697548468319]"
906,906,908,16,[],200,llama-7b,64,1,6127.0,1.0,1,A100,1697548468323,1697548474450,120,6.0,50.0,"[19, 329, 246, 66, 51, 67, 65, 62, 59, 568, 69, 53, 67, 51, 52, 65, 64, 844, 74, 71, 70, 66, 50, 65, 313, 74, 72, 73, 67, 64, 47, 306, 70, 68, 69, 52, 65, 49, 530, 67, 49, 51, 64, 59, 346, 67, 67, 51, 66, 65, 63]","[1697548468342, 1697548468671, 1697548468917, 1697548468983, 1697548469034, 1697548469101, 1697548469166, 1697548469228, 1697548469287, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471194, 1697548471265, 1697548471335, 1697548471401, 1697548471451, 1697548471516, 1697548471829, 1697548471903, 1697548471975, 1697548472048, 1697548472115, 1697548472179, 1697548472226, 1697548472532, 1697548472602, 1697548472670, 1697548472739, 1697548472791, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473666, 1697548473725, 1697548474071, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474450]"
907,907,182,15,[],200,llama-7b,64,1,4377.0,1.0,1,A100,1697548458094,1697548462471,120,47.0,31.0,"[14, 930, 63, 47, 59, 47, 57, 57, 619, 64, 50, 63, 59, 60, 428, 69, 52, 67, 63, 63, 48, 237, 49, 63, 62, 60, 61, 526, 175, 51, 57, 57]","[1697548458108, 1697548459038, 1697548459101, 1697548459148, 1697548459207, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460101, 1697548460164, 1697548460223, 1697548460283, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461310, 1697548461359, 1697548461422, 1697548461484, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471]"
908,908,514,15,[],200,llama-7b,64,1,4825.0,1.0,1,A100,1697548461608,1697548466433,120,85.0,20.0,"[7, 1218, 72, 57, 57, 54, 821, 61, 62, 56, 56, 729, 460, 64, 54, 55, 45, 703, 71, 70, 53]","[1697548461615, 1697548462833, 1697548462905, 1697548462962, 1697548463019, 1697548463073, 1697548463894, 1697548463955, 1697548464017, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465436, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466433]"
909,909,684,19,[],200,llama-7b,64,1,2337.0,1.0,1,A100,1697548472051,1697548474388,120,100.0,20.0,"[24, 381, 76, 70, 68, 69, 52, 65, 49, 530, 67, 49, 51, 65, 58, 346, 68, 67, 51, 66, 65]","[1697548472075, 1697548472456, 1697548472532, 1697548472602, 1697548472670, 1697548472739, 1697548472791, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473667, 1697548473725, 1697548474071, 1697548474139, 1697548474206, 1697548474257, 1697548474323, 1697548474388]"
910,910,835,7,[],200,llama-7b,64,1,2182.0,1.0,1,A100,1697548488033,1697548490215,120,87.0,20.0,"[139, 660, 56, 52, 35, 37, 285, 68, 49, 49, 41, 47, 40, 305, 51, 43, 43, 51, 48, 41, 42]","[1697548488172, 1697548488832, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489504, 1697548489551, 1697548489591, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490215]"
911,911,676,17,[],200,llama-7b,64,1,1383.0,1.0,1,A100,1697548474454,1697548475837,120,19.0,1.0,"[6, 1376]","[1697548474460, 1697548475836]"
912,912,927,12,[],200,llama-7b,64,1,4280.0,1.0,1,A100,1697548452189,1697548456469,120,83.0,20.0,"[20, 503, 545, 68, 52, 62, 875, 66, 71, 53, 62, 59, 1119, 73, 75, 72, 70, 68, 52, 68, 247]","[1697548452209, 1697548452712, 1697548453257, 1697548453325, 1697548453377, 1697548453439, 1697548454314, 1697548454380, 1697548454451, 1697548454504, 1697548454566, 1697548454625, 1697548455744, 1697548455817, 1697548455892, 1697548455964, 1697548456034, 1697548456102, 1697548456154, 1697548456222, 1697548456469]"
913,913,317,6,[],200,llama-7b,64,1,6468.0,1.0,1,A100,1697548466437,1697548472905,120,244.0,50.0,"[19, 560, 283, 74, 70, 67, 68, 51, 63, 359, 72, 54, 71, 70, 64, 536, 65, 51, 66, 64, 62, 60, 570, 69, 52, 68, 51, 52, 64, 65, 845, 72, 71, 69, 68, 49, 65, 312, 74, 73, 72, 68, 63, 49, 304, 71, 68, 68, 52, 66, 49]","[1697548466456, 1697548467016, 1697548467299, 1697548467373, 1697548467443, 1697548467510, 1697548467578, 1697548467629, 1697548467692, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468318, 1697548468382, 1697548468918, 1697548468983, 1697548469034, 1697548469100, 1697548469164, 1697548469226, 1697548469286, 1697548469856, 1697548469925, 1697548469977, 1697548470045, 1697548470096, 1697548470148, 1697548470212, 1697548470277, 1697548471122, 1697548471194, 1697548471265, 1697548471334, 1697548471402, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472905]"
914,914,489,8,[],200,llama-7b,64,1,3921.0,1.0,1,A100,1697548490219,1697548494140,120,79.0,30.0,"[21, 541, 50, 48, 42, 554, 50, 48, 40, 48, 48, 329, 54, 53, 43, 51, 51, 48, 723, 59, 48, 48, 39, 356, 48, 38, 38, 311, 52, 42]","[1697548490240, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491709, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492338, 1697548493061, 1697548493120, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493659, 1697548493697, 1697548493735, 1697548494046, 1697548494098, 1697548494140]"
915,915,423,18,[],200,llama-7b,64,1,5250.0,1.0,1,A100,1697548475840,1697548481090,120,84.0,20.0,"[11, 1828, 250, 251, 55, 253, 71, 68, 61, 614, 254, 71, 55, 72, 68, 61, 913, 76, 74, 72, 71]","[1697548475851, 1697548477679, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478559, 1697548478627, 1697548478688, 1697548479302, 1697548479556, 1697548479627, 1697548479682, 1697548479754, 1697548479822, 1697548479883, 1697548480796, 1697548480872, 1697548480946, 1697548481018, 1697548481089]"
916,916,265,9,[],200,llama-7b,64,1,1860.0,1.0,1,A100,1697548494143,1697548496003,120,86.0,20.0,"[6, 402, 68, 60, 47, 60, 55, 54, 52, 296, 49, 58, 59, 54, 54, 41, 234, 55, 52, 52, 52]","[1697548494149, 1697548494551, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494947, 1697548495243, 1697548495292, 1697548495350, 1697548495409, 1697548495463, 1697548495517, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496003]"
917,917,846,10,[],200,llama-7b,64,1,1256.0,1.0,1,A100,1697548496007,1697548497263,120,140.0,6.0,"[38, 886, 76, 69, 64, 61, 62]","[1697548496045, 1697548496931, 1697548497007, 1697548497076, 1697548497140, 1697548497201, 1697548497263]"
918,918,284,16,[],200,llama-7b,64,1,4757.0,1.0,1,A100,1697548466437,1697548471194,120,90.0,31.0,"[18, 844, 73, 71, 67, 68, 51, 63, 359, 72, 54, 71, 70, 64, 536, 65, 52, 66, 63, 62, 60, 570, 69, 52, 68, 51, 52, 64, 65, 844, 73]","[1697548466455, 1697548467299, 1697548467372, 1697548467443, 1697548467510, 1697548467578, 1697548467629, 1697548467692, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468318, 1697548468382, 1697548468918, 1697548468983, 1697548469035, 1697548469101, 1697548469164, 1697548469226, 1697548469286, 1697548469856, 1697548469925, 1697548469977, 1697548470045, 1697548470096, 1697548470148, 1697548470212, 1697548470277, 1697548471121, 1697548471194]"
919,919,619,11,[],200,llama-7b,64,1,308.0,1.0,1,A100,1697548497265,1697548497573,120,10.0,1.0,"[17, 290]","[1697548497282, 1697548497572]"
920,920,702,13,[],200,llama-7b,64,1,2425.0,1.0,1,A100,1697548456472,1697548458897,120,89.0,20.0,"[8, 597, 67, 62, 60, 48, 59, 54, 331, 50, 62, 61, 49, 49, 61, 526, 72, 62, 49, 48, 50]","[1697548456480, 1697548457077, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458688, 1697548458750, 1697548458799, 1697548458847, 1697548458897]"
921,921,51,30,[],200,llama-7b,64,1,4730.0,1.0,1,A100,1697548518440,1697548523170,120,364.0,36.0,"[19, 387, 69, 63, 50, 58, 56, 45, 607, 242, 69, 61, 63, 61, 60, 61, 336, 65, 63, 49, 49, 53, 405, 62, 48, 60, 46, 54, 709, 70, 67, 67, 57, 66, 50, 316, 67]","[1697548518459, 1697548518846, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520105, 1697548520166, 1697548520229, 1697548520290, 1697548520350, 1697548520411, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520973, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103, 1697548523170]"
922,922,771,7,[],200,llama-7b,64,1,2303.0,1.0,1,A100,1697548456593,1697548458896,120,47.0,20.0,"[6, 478, 67, 62, 60, 48, 59, 54, 331, 50, 63, 60, 49, 49, 61, 526, 72, 62, 49, 47, 50]","[1697548456599, 1697548457077, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808, 1697548457871, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458688, 1697548458750, 1697548458799, 1697548458846, 1697548458896]"
923,923,467,10,[],200,llama-7b,64,1,2866.0,1.0,1,A100,1697548454508,1697548457374,120,93.0,20.0,"[26, 526, 683, 74, 76, 71, 69, 69, 52, 68, 246, 70, 52, 70, 65, 60, 359, 63, 60, 48, 59]","[1697548454534, 1697548455060, 1697548455743, 1697548455817, 1697548455893, 1697548455964, 1697548456033, 1697548456102, 1697548456154, 1697548456222, 1697548456468, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457207, 1697548457267, 1697548457315, 1697548457374]"
924,924,254,12,[],200,llama-7b,64,1,359.0,1.0,1,A100,1697548462475,1697548462834,120,58.0,1.0,"[22, 337]","[1697548462497, 1697548462834]"
925,925,551,8,[],200,llama-7b,64,1,2707.0,1.0,1,A100,1697548458899,1697548461606,120,90.0,20.0,"[11, 826, 251, 65, 48, 63, 60, 60, 429, 68, 52, 68, 62, 63, 49, 236, 49, 63, 62, 60, 61]","[1697548458910, 1697548459736, 1697548459987, 1697548460052, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460712, 1697548460780, 1697548460832, 1697548460900, 1697548460962, 1697548461025, 1697548461074, 1697548461310, 1697548461359, 1697548461422, 1697548461484, 1697548461544, 1697548461605]"
926,926,11,7,[],200,llama-7b,64,1,2700.0,1.0,1,A100,1697548454507,1697548457207,120,732.0,17.0,"[16, 537, 683, 73, 76, 72, 69, 69, 52, 67, 247, 70, 52, 70, 65, 60, 359, 63]","[1697548454523, 1697548455060, 1697548455743, 1697548455816, 1697548455892, 1697548455964, 1697548456033, 1697548456102, 1697548456154, 1697548456221, 1697548456468, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457207]"
927,927,871,12,[],200,llama-7b,64,1,1051.0,1.0,1,A100,1697548465447,1697548466498,120,123.0,6.0,"[10, 445, 337, 72, 69, 53, 65]","[1697548465457, 1697548465902, 1697548466239, 1697548466311, 1697548466380, 1697548466433, 1697548466498]"
928,928,332,14,[],200,llama-7b,64,1,672.0,1.0,1,A100,1697548462909,1697548463581,120,39.0,1.0,"[6, 666]","[1697548462915, 1697548463581]"
929,929,312,5,[],200,llama-7b,64,1,547.0,1.0,1,A100,1697548454513,1697548455060,120,23.0,1.0,"[26, 521]","[1697548454539, 1697548455060]"
930,930,894,6,[],200,llama-7b,64,1,1330.0,1.0,1,A100,1697548455063,1697548456393,120,14.0,1.0,"[17, 1313]","[1697548455080, 1697548456393]"
931,931,916,15,[],200,llama-7b,64,1,971.0,1.0,1,A100,1697548463584,1697548464555,120,8.0,1.0,"[32, 939]","[1697548463616, 1697548464555]"
932,932,77,19,[],200,llama-7b,64,1,3365.0,1.0,1,A100,1697548481092,1697548484457,120,92.0,20.0,"[14, 416, 290, 269, 62, 75, 264, 68, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 79, 71]","[1697548481106, 1697548481522, 1697548481812, 1697548482081, 1697548482143, 1697548482218, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484385, 1697548484456]"
933,933,693,16,[],200,llama-7b,64,1,1679.0,1.0,1,A100,1697548464560,1697548466239,120,67.0,2.0,"[26, 1316, 337]","[1697548464586, 1697548465902, 1697548466239]"
934,934,484,22,[],200,llama-7b,64,1,5954.0,1.0,1,A100,1697548478631,1697548484585,120,86.0,36.0,"[28, 643, 255, 71, 55, 71, 67, 63, 911, 76, 75, 72, 71, 68, 67, 587, 270, 63, 74, 265, 67, 52, 68, 838, 76, 55, 72, 66, 66, 49, 338, 76, 75, 75, 72, 56]","[1697548478659, 1697548479302, 1697548479557, 1697548479628, 1697548479683, 1697548479754, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482081, 1697548482144, 1697548482218, 1697548482483, 1697548482550, 1697548482602, 1697548482670, 1697548483508, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484528, 1697548484584]"
935,935,666,7,[],200,llama-7b,64,1,2501.0,1.0,1,A100,1697548456396,1697548458897,120,84.0,20.0,"[8, 672, 68, 62, 60, 48, 59, 54, 332, 49, 62, 61, 50, 48, 61, 526, 72, 62, 49, 48, 50]","[1697548456404, 1697548457076, 1697548457144, 1697548457206, 1697548457266, 1697548457314, 1697548457373, 1697548457427, 1697548457759, 1697548457808, 1697548457870, 1697548457931, 1697548457981, 1697548458029, 1697548458090, 1697548458616, 1697548458688, 1697548458750, 1697548458799, 1697548458847, 1697548458897]"
936,936,86,5,[],200,llama-7b,64,1,2246.0,1.0,1,A100,1697548449780,1697548452026,120,335.0,17.0,"[7, 580, 73, 56, 53, 51, 405, 65, 61, 54, 42, 297, 49, 48, 53, 48, 59, 244]","[1697548449787, 1697548450367, 1697548450440, 1697548450496, 1697548450549, 1697548450600, 1697548451005, 1697548451070, 1697548451131, 1697548451185, 1697548451227, 1697548451524, 1697548451573, 1697548451621, 1697548451674, 1697548451722, 1697548451781, 1697548452025]"
937,937,175,16,[],200,llama-7b,64,1,2792.0,1.0,1,A100,1697548462527,1697548465319,120,140.0,8.0,"[9, 1357, 63, 60, 57, 56, 730, 460]","[1697548462536, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464859, 1697548465319]"
938,938,444,14,[],200,llama-7b,64,1,1253.0,1.0,1,A100,1697548474073,1697548475326,120,457.0,6.0,"[7, 680, 283, 73, 71, 70, 69]","[1697548474080, 1697548474760, 1697548475043, 1697548475116, 1697548475187, 1697548475257, 1697548475326]"
939,939,273,12,[],200,llama-7b,64,1,1226.0,1.0,1,A100,1697548461608,1697548462834,120,19.0,1.0,"[15, 1211]","[1697548461623, 1697548462834]"
940,940,451,18,[],200,llama-7b,64,1,462.0,1.0,1,A100,1697548478629,1697548479091,120,286.0,1.0,"[15, 447]","[1697548478644, 1697548479091]"
941,941,221,19,[],200,llama-7b,64,1,7244.0,1.0,1,A100,1697548479094,1697548486338,120,364.0,36.0,"[20, 1187, 494, 76, 75, 72, 71, 68, 67, 587, 270, 62, 74, 266, 67, 52, 67, 839, 76, 55, 72, 66, 66, 49, 338, 76, 75, 75, 70, 58, 69, 839, 314, 308, 76, 76, 72]","[1697548479114, 1697548480301, 1697548480795, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482081, 1697548482143, 1697548482217, 1697548482483, 1697548482550, 1697548482602, 1697548482669, 1697548483508, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484526, 1697548484584, 1697548484653, 1697548485492, 1697548485806, 1697548486114, 1697548486190, 1697548486266, 1697548486338]"
942,942,315,11,[],200,llama-7b,64,1,1911.0,1.0,1,A100,1697548467376,1697548469287,120,335.0,14.0,"[6, 589, 81, 72, 53, 72, 70, 64, 534, 65, 52, 66, 65, 62, 59]","[1697548467382, 1697548467971, 1697548468052, 1697548468124, 1697548468177, 1697548468249, 1697548468319, 1697548468383, 1697548468917, 1697548468982, 1697548469034, 1697548469100, 1697548469165, 1697548469227, 1697548469286]"
943,943,810,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548486340,1697548488025,120,,,"[7, 301, 82, 59, 60, 79, 307, 71, 70]","[1697548486347, 1697548486648, 1697548486730, 1697548486789, 1697548486849, 1697548486928, 1697548487235, 1697548487306, 1697548487376]"
944,944,0,21,[],200,llama-7b,64,1,6227.0,1.0,1,A100,1697548469230,1697548475457,120,244.0,50.0,"[6, 346, 274, 68, 53, 67, 51, 52, 65, 64, 844, 73, 72, 69, 67, 50, 65, 312, 74, 73, 73, 66, 64, 48, 306, 70, 68, 69, 52, 65, 48, 531, 67, 50, 50, 65, 57, 347, 68, 66, 51, 66, 65, 63, 593, 73, 71, 69, 69, 68, 63]","[1697548469236, 1697548469582, 1697548469856, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472048, 1697548472114, 1697548472178, 1697548472226, 1697548472532, 1697548472602, 1697548472670, 1697548472739, 1697548472791, 1697548472856, 1697548472904, 1697548473435, 1697548473502, 1697548473552, 1697548473602, 1697548473667, 1697548473724, 1697548474071, 1697548474139, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475187, 1697548475256, 1697548475325, 1697548475393, 1697548475456]"
945,945,639,11,[],200,llama-7b,64,1,742.0,1.0,1,A100,1697548472049,1697548472791,120,100.0,6.0,"[18, 390, 75, 70, 68, 69, 52]","[1697548472067, 1697548472457, 1697548472532, 1697548472602, 1697548472670, 1697548472739, 1697548472791]"
946,946,506,11,[],200,llama-7b,64,1,546.0,1.0,1,A100,1697548457147,1697548457693,120,16.0,1.0,"[10, 536]","[1697548457157, 1697548457693]"
947,947,419,12,[],200,llama-7b,64,1,2599.0,1.0,1,A100,1697548472794,1697548475393,120,88.0,20.0,"[19, 373, 249, 67, 50, 50, 65, 58, 345, 68, 68, 50, 67, 64, 62, 595, 72, 71, 70, 69, 66]","[1697548472813, 1697548473186, 1697548473435, 1697548473502, 1697548473552, 1697548473602, 1697548473667, 1697548473725, 1697548474070, 1697548474138, 1697548474206, 1697548474256, 1697548474323, 1697548474387, 1697548474449, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475392]"
948,948,167,12,[],200,llama-7b,64,1,3015.0,1.0,1,A100,1697548457696,1697548460711,120,88.0,20.0,"[27, 621, 272, 72, 62, 49, 48, 50, 204, 47, 58, 48, 57, 57, 619, 64, 50, 63, 59, 61, 427]","[1697548457723, 1697548458344, 1697548458616, 1697548458688, 1697548458750, 1697548458799, 1697548458847, 1697548458897, 1697548459101, 1697548459148, 1697548459206, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460101, 1697548460164, 1697548460223, 1697548460284, 1697548460711]"
949,949,71,13,[],200,llama-7b,64,1,3093.0,1.0,1,A100,1697548475395,1697548478488,120,364.0,11.0,"[17, 425, 481, 512, 288, 74, 57, 63, 617, 251, 55, 253]","[1697548475412, 1697548475837, 1697548476318, 1697548476830, 1697548477118, 1697548477192, 1697548477249, 1697548477312, 1697548477929, 1697548478180, 1697548478235, 1697548478488]"
950,950,748,14,[],200,llama-7b,64,1,2666.0,1.0,1,A100,1697548478491,1697548481157,120,182.0,14.0,"[15, 796, 255, 71, 54, 71, 68, 63, 911, 76, 75, 73, 70, 68]","[1697548478506, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481019, 1697548481089, 1697548481157]"
951,951,273,12,[],200,llama-7b,64,1,594.0,1.0,1,A100,1697548497577,1697548498171,120,19.0,1.0,"[26, 568]","[1697548497603, 1697548498171]"
952,952,44,13,[],200,llama-7b,64,1,564.0,1.0,1,A100,1697548498175,1697548498739,120,12.0,1.0,"[19, 545]","[1697548498194, 1697548498739]"
953,953,519,30,[],200,llama-7b,64,1,7585.0,1.0,1,A100,1697548496006,1697548503591,120,58.0,47.0,"[18, 907, 78, 67, 63, 61, 63, 57, 55, 262, 61, 47, 47, 45, 400, 61, 52, 51, 50, 351, 59, 51, 49, 826, 62, 62, 59, 54, 51, 422, 64, 51, 49, 58, 55, 723, 64, 60, 58, 55, 55, 620, 66, 63, 62, 60, 55, 876]","[1697548496024, 1697548496931, 1697548497009, 1697548497076, 1697548497139, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497745, 1697548497792, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499787, 1697548499849, 1697548499911, 1697548499970, 1697548500024, 1697548500075, 1697548500497, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501497, 1697548501561, 1697548501621, 1697548501679, 1697548501734, 1697548501789, 1697548502409, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502715, 1697548503591]"
954,954,517,15,[],200,llama-7b,64,1,363.0,1.0,1,A100,1697548481160,1697548481523,120,15.0,1.0,"[7, 356]","[1697548481167, 1697548481523]"
955,955,605,14,[],200,llama-7b,64,1,677.0,1.0,1,A100,1697548498743,1697548499420,120,8.0,1.0,"[28, 648]","[1697548498771, 1697548499419]"
956,956,173,16,[],200,llama-7b,64,1,4739.0,1.0,1,A100,1697548481526,1697548486265,120,96.0,20.0,"[24, 1409, 549, 76, 55, 72, 66, 66, 49, 338, 77, 78, 72, 69, 58, 70, 837, 313, 311, 75, 75]","[1697548481550, 1697548482959, 1697548483508, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484307, 1697548484385, 1697548484457, 1697548484526, 1697548484584, 1697548484654, 1697548485491, 1697548485804, 1697548486115, 1697548486190, 1697548486265]"
957,957,374,15,[],200,llama-7b,64,1,4170.0,1.0,1,A100,1697548499422,1697548503592,120,85.0,20.0,"[20, 983, 72, 65, 50, 49, 59, 54, 723, 64, 62, 57, 54, 56, 620, 65, 64, 62, 59, 55, 876]","[1697548499442, 1697548500425, 1697548500497, 1697548500562, 1697548500612, 1697548500661, 1697548500720, 1697548500774, 1697548501497, 1697548501561, 1697548501623, 1697548501680, 1697548501734, 1697548501790, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503591]"
958,958,346,17,[],200,llama-7b,64,1,2986.0,1.0,1,A100,1697548466242,1697548469228,120,85.0,20.0,"[7, 767, 282, 74, 71, 67, 68, 51, 62, 359, 73, 54, 72, 68, 66, 535, 65, 52, 65, 65, 63]","[1697548466249, 1697548467016, 1697548467298, 1697548467372, 1697548467443, 1697548467510, 1697548467578, 1697548467629, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468249, 1697548468317, 1697548468383, 1697548468918, 1697548468983, 1697548469035, 1697548469100, 1697548469165, 1697548469228]"
959,959,669,14,[],200,llama-7b,64,1,4186.0,1.0,1,A100,1697548464132,1697548468318,120,83.0,20.0,"[12, 1757, 337, 73, 68, 54, 64, 64, 48, 689, 74, 70, 68, 67, 51, 63, 360, 73, 54, 70, 70]","[1697548464144, 1697548465901, 1697548466238, 1697548466311, 1697548466379, 1697548466433, 1697548466497, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468124, 1697548468178, 1697548468248, 1697548468318]"
960,960,777,20,[],200,llama-7b,64,1,482.0,1.0,1,A100,1697548484458,1697548484940,120,9.0,1.0,"[10, 472]","[1697548484468, 1697548484940]"
961,961,438,21,[],200,llama-7b,64,1,1702.0,1.0,1,A100,1697548484945,1697548486647,120,9.0,1.0,"[9, 1693]","[1697548484954, 1697548486647]"
962,962,240,6,[],200,llama-7b,64,1,3803.0,1.0,1,A100,1697548446637,1697548450440,120,83.0,20.0,"[20, 818, 508, 63, 62, 58, 58, 56, 769, 66, 59, 45, 46, 56, 44, 308, 58, 45, 54, 53, 557]","[1697548446657, 1697548447475, 1697548447983, 1697548448046, 1697548448108, 1697548448166, 1697548448224, 1697548448280, 1697548449049, 1697548449115, 1697548449174, 1697548449219, 1697548449265, 1697548449321, 1697548449365, 1697548449673, 1697548449731, 1697548449776, 1697548449830, 1697548449883, 1697548450440]"
963,963,212,22,[],200,llama-7b,64,1,1157.0,1.0,1,A100,1697548486651,1697548487808,120,31.0,1.0,"[18, 1139]","[1697548486669, 1697548487808]"
964,964,801,23,[],200,llama-7b,64,1,2402.0,1.0,1,A100,1697548487811,1697548490213,120,47.0,20.0,"[18, 408, 651, 52, 35, 37, 285, 67, 50, 49, 40, 48, 40, 304, 52, 43, 42, 51, 49, 40, 41]","[1697548487829, 1697548488237, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489895, 1697548489947, 1697548489990, 1697548490032, 1697548490083, 1697548490132, 1697548490172, 1697548490213]"
965,965,878,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548486268,1697548488024,120,,,"[6, 374, 82, 59, 60, 79, 306, 72, 70]","[1697548486274, 1697548486648, 1697548486730, 1697548486789, 1697548486849, 1697548486928, 1697548487234, 1697548487306, 1697548487376]"
966,966,318,15,[],200,llama-7b,64,1,845.0,1.0,1,A100,1697548468321,1697548469166,120,6.0,6.0,"[6, 344, 246, 66, 51, 67, 65]","[1697548468327, 1697548468671, 1697548468917, 1697548468983, 1697548469034, 1697548469101, 1697548469166]"
967,967,8,7,[],200,llama-7b,64,1,627.0,1.0,1,A100,1697548450443,1697548451070,120,39.0,3.0,"[14, 477, 72, 64]","[1697548450457, 1697548450934, 1697548451006, 1697548451070]"
968,968,532,18,[],200,llama-7b,64,1,2185.0,1.0,1,A100,1697548488029,1697548490214,120,92.0,20.0,"[27, 776, 56, 52, 35, 37, 285, 67, 50, 49, 40, 48, 40, 304, 52, 43, 43, 50, 49, 40, 42]","[1697548488056, 1697548488832, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490214]"
969,969,569,24,[],200,llama-7b,64,1,507.0,1.0,1,A100,1697548490219,1697548490726,120,16.0,1.0,"[25, 482]","[1697548490244, 1697548490726]"
970,970,230,25,[],200,llama-7b,64,1,885.0,1.0,1,A100,1697548490728,1697548491613,120,86.0,5.0,"[7, 609, 130, 51, 48, 40]","[1697548490735, 1697548491344, 1697548491474, 1697548491525, 1697548491573, 1697548491613]"
971,971,124,18,[],200,llama-7b,64,1,627.0,1.0,1,A100,1697548469230,1697548469857,120,83.0,2.0,"[11, 616]","[1697548469241, 1697548469857]"
972,972,0,26,[],200,llama-7b,64,1,5647.0,1.0,1,A100,1697548491616,1697548497263,120,244.0,50.0,"[7, 357, 59, 54, 52, 44, 51, 50, 49, 722, 58, 49, 48, 39, 356, 49, 38, 37, 312, 52, 42, 45, 432, 60, 47, 60, 55, 55, 51, 297, 49, 58, 59, 54, 53, 42, 232, 56, 53, 51, 51, 379, 41, 40, 51, 49, 446, 69, 63, 61, 63]","[1697548491623, 1697548491980, 1697548492039, 1697548492093, 1697548492145, 1697548492189, 1697548492240, 1697548492290, 1697548492339, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493660, 1697548493698, 1697548493735, 1697548494047, 1697548494099, 1697548494141, 1697548494186, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494840, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495409, 1697548495463, 1697548495516, 1697548495558, 1697548495790, 1697548495846, 1697548495899, 1697548495950, 1697548496001, 1697548496380, 1697548496421, 1697548496461, 1697548496512, 1697548496561, 1697548497007, 1697548497076, 1697548497139, 1697548497200, 1697548497263]"
973,973,704,19,[],200,llama-7b,64,1,846.0,1.0,1,A100,1697548469860,1697548470706,120,14.0,1.0,"[6, 839]","[1697548469866, 1697548470705]"
974,974,452,20,[],200,llama-7b,64,1,1261.0,1.0,1,A100,1697548470715,1697548471976,120,216.0,4.0,"[14, 1022, 77, 74, 74]","[1697548470729, 1697548471751, 1697548471828, 1697548471902, 1697548471976]"
975,975,94,16,[],200,llama-7b,64,1,2879.0,1.0,1,A100,1697548469169,1697548472048,120,86.0,20.0,"[19, 394, 274, 68, 53, 67, 51, 52, 65, 64, 844, 73, 72, 69, 67, 50, 65, 312, 74, 73, 73]","[1697548469188, 1697548469582, 1697548469856, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472048]"
976,976,788,6,[],200,llama-7b,64,1,684.0,1.0,1,A100,1697548452028,1697548452712,120,31.0,1.0,"[6, 678]","[1697548452034, 1697548452712]"
977,977,225,21,[],200,llama-7b,64,1,478.0,1.0,1,A100,1697548471979,1697548472457,120,23.0,1.0,"[12, 465]","[1697548471991, 1697548472456]"
978,978,449,7,[],200,llama-7b,64,1,4010.0,1.0,1,A100,1697548452716,1697548456726,120,86.0,20.0,"[30, 1100, 468, 66, 71, 53, 63, 58, 1118, 73, 76, 71, 70, 68, 53, 67, 248, 69, 52, 70, 66]","[1697548452746, 1697548453846, 1697548454314, 1697548454380, 1697548454451, 1697548454504, 1697548454567, 1697548454625, 1697548455743, 1697548455816, 1697548455892, 1697548455963, 1697548456033, 1697548456101, 1697548456154, 1697548456221, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456726]"
979,979,809,22,[],200,llama-7b,64,1,726.0,1.0,1,A100,1697548472460,1697548473186,120,16.0,1.0,"[22, 704]","[1697548472482, 1697548473186]"
980,980,19,13,[],200,llama-7b,64,1,10764.0,1.0,1,A100,1697548462838,1697548473602,120,563.0,72.0,"[24, 718, 314, 62, 60, 58, 55, 729, 460, 64, 50, 59, 46, 701, 72, 70, 53, 65, 64, 46, 692, 72, 70, 68, 67, 51, 63, 360, 72, 55, 70, 70, 64, 535, 66, 51, 66, 65, 62, 59, 569, 69, 53, 67, 51, 52, 65, 64, 844, 73, 72, 69, 67, 50, 65, 312, 74, 73, 72, 67, 64, 48, 305, 70, 69, 68, 52, 65, 49, 531, 66, 50, 51]","[1697548462862, 1697548463580, 1697548463894, 1697548463956, 1697548464016, 1697548464074, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465491, 1697548465537, 1697548466238, 1697548466310, 1697548466380, 1697548466433, 1697548466498, 1697548466562, 1697548466608, 1697548467300, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468123, 1697548468178, 1697548468248, 1697548468318, 1697548468382, 1697548468917, 1697548468983, 1697548469034, 1697548469100, 1697548469165, 1697548469227, 1697548469286, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472226, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472855, 1697548472904, 1697548473435, 1697548473501, 1697548473551, 1697548473602]"
981,981,586,23,[],200,llama-7b,64,1,4060.0,1.0,1,A100,1697548473189,1697548477249,120,85.0,20.0,"[18, 790, 73, 69, 66, 51, 66, 65, 63, 594, 71, 72, 70, 68, 67, 64, 862, 512, 288, 74, 57]","[1697548473207, 1697548473997, 1697548474070, 1697548474139, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474450, 1697548475044, 1697548475115, 1697548475187, 1697548475257, 1697548475325, 1697548475392, 1697548475456, 1697548476318, 1697548476830, 1697548477118, 1697548477192, 1697548477249]"
982,982,219,8,[],200,llama-7b,64,1,2169.0,1.0,1,A100,1697548456727,1697548458896,120,90.0,20.0,"[9, 340, 68, 62, 61, 47, 59, 54, 331, 50, 61, 62, 49, 49, 61, 526, 72, 61, 49, 48, 50]","[1697548456736, 1697548457076, 1697548457144, 1697548457206, 1697548457267, 1697548457314, 1697548457373, 1697548457427, 1697548457758, 1697548457808, 1697548457869, 1697548457931, 1697548457980, 1697548458029, 1697548458090, 1697548458616, 1697548458688, 1697548458749, 1697548458798, 1697548458846, 1697548458896]"
983,983,679,17,[],200,llama-7b,64,1,407.0,1.0,1,A100,1697548472050,1697548472457,120,15.0,1.0,"[13, 393]","[1697548472063, 1697548472456]"
984,984,448,18,[],200,llama-7b,64,1,1865.0,1.0,1,A100,1697548472458,1697548474323,120,335.0,12.0,"[14, 713, 250, 67, 49, 51, 65, 58, 345, 68, 68, 51, 66]","[1697548472472, 1697548473185, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473667, 1697548473725, 1697548474070, 1697548474138, 1697548474206, 1697548474257, 1697548474323]"
985,985,807,9,[],200,llama-7b,64,1,2707.0,1.0,1,A100,1697548458899,1697548461606,120,90.0,20.0,"[25, 812, 250, 65, 49, 63, 60, 60, 427, 70, 52, 68, 63, 62, 49, 236, 50, 62, 62, 61, 61]","[1697548458924, 1697548459736, 1697548459986, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460710, 1697548460780, 1697548460832, 1697548460900, 1697548460963, 1697548461025, 1697548461074, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461606]"
986,986,300,15,[],200,llama-7b,64,1,420.0,1.0,1,A100,1697548468251,1697548468671,120,9.0,1.0,"[13, 407]","[1697548468264, 1697548468671]"
987,987,583,27,[],200,llama-7b,64,1,2761.0,1.0,1,A100,1697548497264,1697548500025,120,96.0,20.0,"[10, 298, 65, 60, 48, 47, 45, 400, 61, 52, 52, 50, 350, 59, 51, 49, 827, 61, 62, 59, 55]","[1697548497274, 1697548497572, 1697548497637, 1697548497697, 1697548497745, 1697548497792, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498402, 1697548498452, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025]"
988,988,882,16,[],200,llama-7b,64,1,2521.0,1.0,1,A100,1697548468674,1697548471195,120,345.0,11.0,"[9, 899, 273, 69, 53, 67, 51, 52, 65, 64, 844, 74]","[1697548468683, 1697548469582, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471194]"
989,989,120,11,[],200,llama-7b,64,1,316.0,1.0,1,A100,1697548457377,1697548457693,120,17.0,1.0,"[25, 291]","[1697548457402, 1697548457693]"
990,990,825,12,[],200,llama-7b,64,1,3015.0,1.0,1,A100,1697548457696,1697548460711,120,96.0,20.0,"[6, 642, 272, 72, 62, 48, 49, 50, 204, 46, 58, 49, 57, 57, 619, 64, 49, 64, 59, 60, 428]","[1697548457702, 1697548458344, 1697548458616, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458897, 1697548459101, 1697548459147, 1697548459205, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460100, 1697548460164, 1697548460223, 1697548460283, 1697548460711]"
991,991,205,9,[],200,llama-7b,64,1,4824.0,1.0,1,A100,1697548461610,1697548466434,120,87.0,20.0,"[33, 1191, 72, 56, 57, 54, 820, 62, 61, 57, 56, 729, 460, 65, 53, 55, 45, 703, 71, 70, 54]","[1697548461643, 1697548462834, 1697548462906, 1697548462962, 1697548463019, 1697548463073, 1697548463893, 1697548463955, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465383, 1697548465436, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466434]"
992,992,4,19,[],200,llama-7b,64,1,3839.0,1.0,1,A100,1697548477251,1697548481090,120,89.0,20.0,"[23, 405, 250, 252, 54, 254, 70, 68, 61, 613, 255, 72, 54, 72, 67, 62, 913, 76, 75, 71, 72]","[1697548477274, 1697548477679, 1697548477929, 1697548478181, 1697548478235, 1697548478489, 1697548478559, 1697548478627, 1697548478688, 1697548479301, 1697548479556, 1697548479628, 1697548479682, 1697548479754, 1697548479821, 1697548479883, 1697548480796, 1697548480872, 1697548480947, 1697548481018, 1697548481090]"
993,993,478,13,[],200,llama-7b,64,1,9498.0,1.0,1,A100,1697548460714,1697548470212,120,161.0,62.0,"[19, 509, 68, 50, 62, 62, 61, 61, 525, 175, 50, 58, 56, 54, 382, 56, 58, 53, 820, 63, 60, 57, 56, 729, 460, 64, 50, 59, 45, 703, 71, 69, 54, 65, 63, 48, 689, 74, 70, 68, 67, 51, 63, 360, 72, 54, 71, 70, 64, 535, 66, 51, 66, 64, 63, 59, 569, 69, 53, 67, 51, 52, 65]","[1697548460733, 1697548461242, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461606, 1697548462131, 1697548462306, 1697548462356, 1697548462414, 1697548462470, 1697548462524, 1697548462906, 1697548462962, 1697548463020, 1697548463073, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466379, 1697548466433, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468318, 1697548468382, 1697548468917, 1697548468983, 1697548469034, 1697548469100, 1697548469164, 1697548469227, 1697548469286, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212]"
994,994,69,31,[],200,llama-7b,64,1,2445.0,1.0,1,A100,1697548520977,1697548523422,120,85.0,20.0,"[29, 358, 67, 63, 47, 60, 46, 54, 710, 70, 67, 66, 57, 66, 51, 316, 66, 64, 64, 62, 62]","[1697548521006, 1697548521364, 1697548521431, 1697548521494, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522481, 1697548522548, 1697548522614, 1697548522671, 1697548522737, 1697548522788, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422]"
995,995,582,21,[],200,llama-7b,64,1,799.0,1.0,1,A100,1697548488034,1697548488833,120,19.0,1.0,"[157, 642]","[1697548488191, 1697548488833]"
996,996,648,13,[],200,llama-7b,64,1,2726.0,1.0,1,A100,1697548466501,1697548469227,120,84.0,20.0,"[6, 510, 282, 74, 70, 68, 67, 51, 63, 359, 72, 54, 71, 70, 65, 535, 64, 51, 67, 64, 63]","[1697548466507, 1697548467017, 1697548467299, 1697548467373, 1697548467443, 1697548467511, 1697548467578, 1697548467629, 1697548467692, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468318, 1697548468383, 1697548468918, 1697548468982, 1697548469033, 1697548469100, 1697548469164, 1697548469227]"
997,997,444,16,[],200,llama-7b,64,1,1125.0,1.0,1,A100,1697548478629,1697548479754,120,457.0,6.0,"[13, 449, 211, 255, 71, 54, 71]","[1697548478642, 1697548479091, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753]"
998,998,214,17,[],200,llama-7b,64,1,3956.0,1.0,1,A100,1697548479755,1697548483711,120,52.0,20.0,"[17, 530, 494, 76, 74, 72, 72, 67, 68, 587, 269, 62, 74, 265, 68, 51, 68, 839, 75, 55, 73]","[1697548479772, 1697548480302, 1697548480796, 1697548480872, 1697548480946, 1697548481018, 1697548481090, 1697548481157, 1697548481225, 1697548481812, 1697548482081, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711]"
999,999,231,22,[],200,llama-7b,64,1,404.0,1.0,1,A100,1697548488838,1697548489242,120,13.0,1.0,"[55, 348]","[1697548488893, 1697548489241]"
1000,1000,9,23,[],200,llama-7b,64,1,2465.0,1.0,1,A100,1697548489245,1697548491710,120,85.0,20.0,"[40, 546, 65, 51, 44, 42, 51, 48, 41, 41, 193, 373, 51, 49, 41, 553, 51, 48, 40, 48, 49]","[1697548489285, 1697548489831, 1697548489896, 1697548489947, 1697548489991, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490214, 1697548490407, 1697548490780, 1697548490831, 1697548490880, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710]"
1001,1001,255,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548470216,1697548488024,120,,,"[6, 481, 418, 73, 71, 70, 67, 49, 66, 311, 74, 73, 72, 68, 63, 49, 304, 70, 69, 68, 52, 66, 48, 532, 66, 50, 50, 65, 58, 346, 68, 66, 52, 66, 64, 63, 593, 73, 71, 69, 69, 68, 63, 861, 513, 287, 75, 56, 64, 617, 251, 55, 253, 70, 69, 61, 614, 255, 71, 54, 71, 68, 63, 911, 77, 74, 72, 71, 68, 67, 587, 269, 63, 74, 265, 68, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 75, 75, 71, 57, 70, 838, 313, 308, 76, 76, 72, 70, 323, 59, 60, 79, 306, 71, 70]","[1697548470222, 1697548470703, 1697548471121, 1697548471194, 1697548471265, 1697548471335, 1697548471402, 1697548471451, 1697548471517, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472904, 1697548473436, 1697548473502, 1697548473552, 1697548473602, 1697548473667, 1697548473725, 1697548474071, 1697548474139, 1697548474205, 1697548474257, 1697548474323, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475187, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476317, 1697548476830, 1697548477117, 1697548477192, 1697548477248, 1697548477312, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478627, 1697548478688, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480872, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486849, 1697548486928, 1697548487234, 1697548487305, 1697548487375]"
1002,1002,802,18,[],200,llama-7b,64,1,433.0,1.0,1,A100,1697548483716,1697548484149,120,9.0,1.0,"[14, 418]","[1697548483730, 1697548484148]"
1003,1003,572,19,[],200,llama-7b,64,1,788.0,1.0,1,A100,1697548484152,1697548484940,120,16.0,1.0,"[16, 772]","[1697548484168, 1697548484940]"
1004,1004,232,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484943,1697548488024,120,,,"[13, 1692, 82, 59, 60, 78, 307, 72, 70]","[1697548484956, 1697548486648, 1697548486730, 1697548486789, 1697548486849, 1697548486927, 1697548487234, 1697548487306, 1697548487376]"
1005,1005,4,21,[],200,llama-7b,64,1,2183.0,1.0,1,A100,1697548488031,1697548490214,120,89.0,20.0,"[23, 777, 57, 52, 35, 37, 285, 67, 50, 49, 40, 48, 40, 304, 52, 43, 43, 50, 49, 40, 41]","[1697548488054, 1697548488831, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490213]"
1006,1006,591,24,[],200,llama-7b,64,1,5985.0,1.0,1,A100,1697548491713,1697548497698,120,874.0,47.0,"[29, 959, 360, 58, 49, 48, 39, 356, 49, 38, 37, 312, 52, 42, 46, 431, 60, 48, 59, 55, 55, 51, 297, 49, 58, 59, 54, 53, 42, 232, 56, 53, 51, 51, 379, 41, 40, 51, 49, 446, 69, 63, 61, 63, 57, 55, 262, 61]","[1697548491742, 1697548492701, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493660, 1697548493698, 1697548493735, 1697548494047, 1697548494099, 1697548494141, 1697548494187, 1697548494618, 1697548494678, 1697548494726, 1697548494785, 1697548494840, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495409, 1697548495463, 1697548495516, 1697548495558, 1697548495790, 1697548495846, 1697548495899, 1697548495950, 1697548496001, 1697548496380, 1697548496421, 1697548496461, 1697548496512, 1697548496561, 1697548497007, 1697548497076, 1697548497139, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497698]"
1007,1007,428,20,[],200,llama-7b,64,1,1928.0,1.0,1,A100,1697548474390,1697548476318,120,31.0,9.0,"[7, 363, 284, 72, 71, 70, 69, 67, 64, 860]","[1697548474397, 1697548474760, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475393, 1697548475457, 1697548476317]"
1008,1008,84,21,[],200,llama-7b,64,1,1357.0,1.0,1,A100,1697548476322,1697548477679,120,26.0,1.0,"[12, 1345]","[1697548476334, 1697548477679]"
1009,1009,301,14,[],200,llama-7b,64,1,4206.0,1.0,1,A100,1697548469230,1697548473436,120,109.0,31.0,"[7, 345, 274, 69, 52, 68, 50, 52, 65, 65, 843, 73, 72, 69, 67, 50, 65, 312, 74, 73, 72, 67, 64, 48, 306, 70, 68, 69, 52, 65, 48, 532]","[1697548469237, 1697548469582, 1697548469856, 1697548469925, 1697548469977, 1697548470045, 1697548470095, 1697548470147, 1697548470212, 1697548470277, 1697548471120, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472226, 1697548472532, 1697548472602, 1697548472670, 1697548472739, 1697548472791, 1697548472856, 1697548472904, 1697548473436]"
1010,1010,782,22,[],200,llama-7b,64,1,4801.0,1.0,1,A100,1697548477682,1697548482483,120,90.0,20.0,"[21, 1387, 212, 255, 71, 55, 70, 68, 63, 911, 78, 73, 73, 70, 67, 68, 588, 269, 63, 74, 264]","[1697548477703, 1697548479090, 1697548479302, 1697548479557, 1697548479628, 1697548479683, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480873, 1697548480946, 1697548481019, 1697548481089, 1697548481156, 1697548481224, 1697548481812, 1697548482081, 1697548482144, 1697548482218, 1697548482482]"
1011,1011,88,7,[],200,llama-7b,64,1,4341.0,1.0,1,A100,1697548472908,1697548477249,120,58.0,20.0,"[18, 1071, 73, 68, 68, 50, 66, 65, 63, 594, 73, 70, 70, 68, 67, 65, 860, 513, 287, 75, 56]","[1697548472926, 1697548473997, 1697548474070, 1697548474138, 1697548474206, 1697548474256, 1697548474322, 1697548474387, 1697548474450, 1697548475044, 1697548475117, 1697548475187, 1697548475257, 1697548475325, 1697548475392, 1697548475457, 1697548476317, 1697548476830, 1697548477117, 1697548477192, 1697548477248]"
1012,1012,443,23,[],200,llama-7b,64,1,473.0,1.0,1,A100,1697548482486,1697548482959,120,19.0,1.0,"[25, 447]","[1697548482511, 1697548482958]"
1013,1013,586,22,[],200,llama-7b,64,1,2901.0,1.0,1,A100,1697548490219,1697548493120,120,85.0,20.0,"[26, 481, 55, 50, 48, 42, 554, 50, 48, 40, 48, 48, 329, 55, 52, 43, 51, 51, 48, 723, 59]","[1697548490245, 1697548490726, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491709, 1697548492038, 1697548492093, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492338, 1697548493061, 1697548493120]"
1014,1014,210,24,[],200,llama-7b,64,1,1269.0,1.0,1,A100,1697548482961,1697548484230,120,140.0,2.0,"[15, 1172, 82]","[1697548482976, 1697548484148, 1697548484230]"
1015,1015,366,23,[],200,llama-7b,64,1,925.0,1.0,1,A100,1697548493122,1697548494047,120,85.0,6.0,"[11, 417, 62, 48, 38, 38, 311]","[1697548493133, 1697548493550, 1697548493612, 1697548493660, 1697548493698, 1697548493736, 1697548494047]"
1016,1016,47,15,[],200,llama-7b,64,1,3811.0,1.0,1,A100,1697548473438,1697548477249,120,90.0,20.0,"[9, 551, 73, 68, 66, 51, 67, 64, 63, 594, 71, 72, 69, 69, 68, 63, 862, 512, 288, 74, 57]","[1697548473447, 1697548473998, 1697548474071, 1697548474139, 1697548474205, 1697548474256, 1697548474323, 1697548474387, 1697548474450, 1697548475044, 1697548475115, 1697548475187, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476318, 1697548476830, 1697548477118, 1697548477192, 1697548477249]"
1017,1017,19,24,[],200,llama-7b,64,1,9604.0,1.0,1,A100,1697548494050,1697548503654,120,563.0,72.0,"[10, 492, 67, 60, 47, 60, 55, 54, 52, 296, 49, 59, 58, 54, 54, 41, 234, 55, 52, 52, 52, 377, 41, 40, 51, 49, 446, 69, 64, 60, 63, 57, 55, 262, 60, 49, 45, 46, 400, 61, 52, 52, 49, 351, 59, 51, 49, 827, 61, 61, 60, 54, 51, 421, 65, 51, 49, 58, 55, 723, 63, 61, 58, 55, 55, 620, 66, 63, 62, 60, 54, 876, 64]","[1697548494060, 1697548494552, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494947, 1697548495243, 1697548495292, 1697548495351, 1697548495409, 1697548495463, 1697548495517, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496003, 1697548496380, 1697548496421, 1697548496461, 1697548496512, 1697548496561, 1697548497007, 1697548497076, 1697548497140, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497746, 1697548497791, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499788, 1697548499849, 1697548499910, 1697548499970, 1697548500024, 1697548500075, 1697548500496, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501497, 1697548501560, 1697548501621, 1697548501679, 1697548501734, 1697548501789, 1697548502409, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502714, 1697548503590, 1697548503654]"
1018,1018,800,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484232,1697548488027,120,,,"[7, 1254, 312, 308, 77, 76, 71, 70, 323, 59, 59, 79, 307, 71, 70]","[1697548484239, 1697548485493, 1697548485805, 1697548486113, 1697548486190, 1697548486266, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
1019,1019,359,14,[],200,llama-7b,64,1,835.0,1.0,1,A100,1697548458902,1697548459737,120,10.0,1.0,"[40, 795]","[1697548458942, 1697548459737]"
1020,1020,136,15,[],200,llama-7b,64,1,882.0,1.0,1,A100,1697548459742,1697548460624,120,31.0,1.0,"[16, 865]","[1697548459758, 1697548460623]"
1021,1021,721,16,[],200,llama-7b,64,1,858.0,1.0,1,A100,1697548460626,1697548461484,120,286.0,5.0,"[7, 609, 68, 50, 62, 62]","[1697548460633, 1697548461242, 1697548461310, 1697548461360, 1697548461422, 1697548461484]"
1022,1022,490,17,[],200,llama-7b,64,1,929.0,1.0,1,A100,1697548461486,1697548462415,120,11.0,5.0,"[18, 431, 196, 175, 51, 57]","[1697548461504, 1697548461935, 1697548462131, 1697548462306, 1697548462357, 1697548462414]"
1023,1023,151,18,[],200,llama-7b,64,1,416.0,1.0,1,A100,1697548462418,1697548462834,120,39.0,1.0,"[13, 403]","[1697548462431, 1697548462834]"
1024,1024,849,19,[],200,llama-7b,64,1,743.0,1.0,1,A100,1697548462838,1697548463581,120,10.0,1.0,"[15, 728]","[1697548462853, 1697548463581]"
1025,1025,595,20,[],200,llama-7b,64,1,970.0,1.0,1,A100,1697548463585,1697548464555,120,8.0,1.0,"[36, 934]","[1697548463621, 1697548464555]"
1026,1026,245,21,[],200,llama-7b,64,1,3761.0,1.0,1,A100,1697548464557,1697548468318,120,100.0,20.0,"[15, 1330, 336, 73, 69, 53, 64, 64, 48, 689, 74, 70, 68, 67, 51, 63, 360, 72, 55, 70, 70]","[1697548464572, 1697548465902, 1697548466238, 1697548466311, 1697548466380, 1697548466433, 1697548466497, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468123, 1697548468178, 1697548468248, 1697548468318]"
1027,1027,678,8,[],200,llama-7b,64,1,3696.0,1.0,1,A100,1697548477251,1697548480947,120,244.0,18.0,"[46, 382, 251, 251, 54, 254, 70, 68, 62, 613, 255, 71, 54, 71, 68, 63, 912, 76, 75]","[1697548477297, 1697548477679, 1697548477930, 1697548478181, 1697548478235, 1697548478489, 1697548478559, 1697548478627, 1697548478689, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480796, 1697548480872, 1697548480947]"
1028,1028,677,4,[],200,llama-7b,64,1,237.0,1.0,1,A100,1697548451725,1697548451962,120,9.0,1.0,"[6, 231]","[1697548451731, 1697548451962]"
1029,1029,332,5,[],200,llama-7b,64,1,746.0,1.0,1,A100,1697548451966,1697548452712,120,39.0,1.0,"[33, 713]","[1697548451999, 1697548452712]"
1030,1030,447,9,[],200,llama-7b,64,1,2761.0,1.0,1,A100,1697548480950,1697548483711,120,161.0,13.0,"[8, 854, 268, 63, 74, 265, 68, 51, 68, 839, 75, 56, 72]","[1697548480958, 1697548481812, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711]"
1031,1031,22,22,[],200,llama-7b,64,1,349.0,1.0,1,A100,1697548468322,1697548468671,120,16.0,1.0,"[21, 328]","[1697548468343, 1697548468671]"
1032,1032,102,6,[],200,llama-7b,64,1,4010.0,1.0,1,A100,1697548452715,1697548456725,120,84.0,20.0,"[16, 1115, 468, 66, 70, 54, 62, 59, 1117, 73, 76, 73, 70, 68, 53, 66, 248, 69, 52, 70, 65]","[1697548452731, 1697548453846, 1697548454314, 1697548454380, 1697548454450, 1697548454504, 1697548454566, 1697548454625, 1697548455742, 1697548455815, 1697548455891, 1697548455964, 1697548456034, 1697548456102, 1697548456155, 1697548456221, 1697548456469, 1697548456538, 1697548456590, 1697548456660, 1697548456725]"
1033,1033,253,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484587,1697548488023,120,,,"[7, 346, 553, 312, 309, 76, 76, 72, 69, 323, 59, 60, 78, 307, 71, 70]","[1697548484594, 1697548484940, 1697548485493, 1697548485805, 1697548486114, 1697548486190, 1697548486266, 1697548486338, 1697548486407, 1697548486730, 1697548486789, 1697548486849, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
1034,1034,572,8,[],200,llama-7b,64,1,386.0,1.0,1,A100,1697548451073,1697548451459,120,16.0,1.0,"[20, 366]","[1697548451093, 1697548451459]"
1035,1035,606,23,[],200,llama-7b,64,1,908.0,1.0,1,A100,1697548468674,1697548469582,120,9.0,1.0,"[26, 882]","[1697548468700, 1697548469582]"
1036,1036,342,9,[],200,llama-7b,64,1,3043.0,1.0,1,A100,1697548451462,1697548454505,120,364.0,14.0,"[18, 482, 64, 58, 45, 57, 54, 1017, 68, 51, 63, 875, 66, 71, 54]","[1697548451480, 1697548451962, 1697548452026, 1697548452084, 1697548452129, 1697548452186, 1697548452240, 1697548453257, 1697548453325, 1697548453376, 1697548453439, 1697548454314, 1697548454380, 1697548454451, 1697548454505]"
1037,1037,376,24,[],200,llama-7b,64,1,3205.0,1.0,1,A100,1697548469585,1697548472790,120,87.0,20.0,"[16, 1100, 420, 72, 72, 69, 67, 50, 65, 312, 74, 73, 72, 67, 64, 49, 305, 69, 69, 69, 51]","[1697548469601, 1697548470701, 1697548471121, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472227, 1697548472532, 1697548472601, 1697548472670, 1697548472739, 1697548472790]"
1038,1038,106,10,[],200,llama-7b,64,1,3520.0,1.0,1,A100,1697548483714,1697548487234,120,161.0,20.0,"[19, 415, 82, 77, 78, 72, 70, 57, 70, 838, 313, 308, 76, 76, 72, 70, 323, 59, 59, 79, 307]","[1697548483733, 1697548484148, 1697548484230, 1697548484307, 1697548484385, 1697548484457, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234]"
1039,1039,842,24,[],200,llama-7b,64,1,2002.0,1.0,1,A100,1697548488031,1697548490033,120,161.0,16.0,"[40, 761, 56, 52, 35, 38, 284, 67, 50, 49, 40, 49, 40, 303, 52, 43, 43]","[1697548488071, 1697548488832, 1697548488888, 1697548488940, 1697548488975, 1697548489013, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033]"
1040,1040,926,10,[],200,llama-7b,64,1,4180.0,1.0,1,A100,1697548454508,1697548458688,120,563.0,30.0,"[31, 521, 683, 74, 76, 71, 69, 69, 52, 68, 246, 70, 52, 70, 65, 60, 359, 63, 60, 48, 59, 53, 331, 50, 62, 62, 48, 49, 62, 525, 72]","[1697548454539, 1697548455060, 1697548455743, 1697548455817, 1697548455893, 1697548455964, 1697548456033, 1697548456102, 1697548456154, 1697548456222, 1697548456468, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457207, 1697548457267, 1697548457315, 1697548457374, 1697548457427, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457980, 1697548458029, 1697548458091, 1697548458616, 1697548458688]"
1041,1041,691,7,[],200,llama-7b,64,1,349.0,1.0,1,A100,1697548456728,1697548457077,120,47.0,1.0,"[26, 323]","[1697548456754, 1697548457077]"
1042,1042,611,25,[],200,llama-7b,64,1,331.0,1.0,1,A100,1697548490036,1697548490367,120,14.0,1.0,"[16, 315]","[1697548490052, 1697548490367]"
1043,1043,805,11,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548487237,1697548488027,120,,,"[15, 556]","[1697548487252, 1697548487808]"
1044,1044,461,8,[],200,llama-7b,64,1,3819.0,1.0,1,A100,1697548457080,1697548460899,120,216.0,30.0,"[19, 659, 50, 62, 62, 48, 49, 62, 525, 73, 61, 48, 49, 49, 205, 47, 57, 49, 57, 56, 620, 64, 49, 64, 59, 60, 427, 70, 52, 67]","[1697548457099, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457980, 1697548458029, 1697548458091, 1697548458616, 1697548458689, 1697548458750, 1697548458798, 1697548458847, 1697548458896, 1697548459101, 1697548459148, 1697548459205, 1697548459254, 1697548459311, 1697548459367, 1697548459987, 1697548460051, 1697548460100, 1697548460164, 1697548460223, 1697548460283, 1697548460710, 1697548460780, 1697548460832, 1697548460899]"
1045,1045,268,26,[],200,llama-7b,64,1,357.0,1.0,1,A100,1697548490370,1697548490727,120,19.0,1.0,"[15, 342]","[1697548490385, 1697548490727]"
1046,1046,37,25,[],200,llama-7b,64,1,392.0,1.0,1,A100,1697548472794,1697548473186,120,20.0,1.0,"[10, 382]","[1697548472804, 1697548473186]"
1047,1047,40,27,[],200,llama-7b,64,1,2882.0,1.0,1,A100,1697548490730,1697548493612,120,86.0,20.0,"[19, 595, 131, 50, 48, 40, 48, 49, 329, 54, 52, 42, 52, 50, 49, 724, 59, 47, 48, 39, 357]","[1697548490749, 1697548491344, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492039, 1697548492093, 1697548492145, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493062, 1697548493121, 1697548493168, 1697548493216, 1697548493255, 1697548493612]"
1048,1048,733,26,[],200,llama-7b,64,1,808.0,1.0,1,A100,1697548473189,1697548473997,120,31.0,1.0,"[13, 795]","[1697548473202, 1697548473997]"
1049,1049,327,8,[],200,llama-7b,64,1,1931.0,1.0,1,A100,1697548458901,1697548460832,120,563.0,10.0,"[36, 800, 250, 64, 49, 63, 60, 61, 426, 69, 52]","[1697548458937, 1697548459737, 1697548459987, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460284, 1697548460710, 1697548460779, 1697548460831]"
1050,1050,836,13,[],200,llama-7b,64,1,743.0,1.0,1,A100,1697548462838,1697548463581,120,11.0,1.0,"[19, 724]","[1697548462857, 1697548463581]"
1051,1051,63,18,[],200,llama-7b,64,1,529.0,1.0,1,A100,1697548478563,1697548479092,120,39.0,1.0,"[6, 522]","[1697548478569, 1697548479091]"
1052,1052,608,14,[],200,llama-7b,64,1,4045.0,1.0,1,A100,1697548463584,1697548467629,120,96.0,20.0,"[31, 940, 303, 460, 64, 50, 59, 46, 701, 72, 69, 54, 64, 63, 49, 691, 72, 71, 68, 67, 51]","[1697548463615, 1697548464555, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465491, 1697548465537, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466560, 1697548466609, 1697548467300, 1697548467372, 1697548467443, 1697548467511, 1697548467578, 1697548467629]"
1053,1053,96,9,[],200,llama-7b,64,1,408.0,1.0,1,A100,1697548460835,1697548461243,120,31.0,1.0,"[6, 402]","[1697548460841, 1697548461243]"
1054,1054,120,9,[],200,llama-7b,64,1,340.0,1.0,1,A100,1697548460903,1697548461243,120,17.0,1.0,"[11, 329]","[1697548460914, 1697548461243]"
1055,1055,684,10,[],200,llama-7b,64,1,4192.0,1.0,1,A100,1697548461246,1697548465438,120,100.0,20.0,"[11, 678, 196, 175, 51, 57, 57, 54, 381, 56, 58, 53, 821, 61, 62, 56, 56, 729, 460, 64, 56]","[1697548461257, 1697548461935, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462525, 1697548462906, 1697548462962, 1697548463020, 1697548463073, 1697548463894, 1697548463955, 1697548464017, 1697548464073, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465438]"
1056,1056,659,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548471204,1697548488025,120,,,"[6, 541, 78, 73, 74, 72, 67, 63, 49, 304, 71, 68, 68, 52, 67, 48, 529, 68, 49, 50, 65, 59, 346, 68, 66, 51, 67, 64, 63, 593, 73, 71, 69, 70, 67, 63, 861, 513, 287, 75, 56, 64, 617, 251, 55, 253, 70, 68, 62, 614, 255, 71, 54, 71, 68, 63, 911, 77, 74, 72, 71, 68, 67, 587, 269, 63, 74, 265, 68, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 75, 75, 71, 57, 70, 838, 313, 308, 76, 76, 72, 70, 324, 58, 60, 79, 306, 71, 70]","[1697548471210, 1697548471751, 1697548471829, 1697548471902, 1697548471976, 1697548472048, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472790, 1697548472857, 1697548472905, 1697548473434, 1697548473502, 1697548473551, 1697548473601, 1697548473666, 1697548473725, 1697548474071, 1697548474139, 1697548474205, 1697548474256, 1697548474323, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475187, 1697548475256, 1697548475326, 1697548475393, 1697548475456, 1697548476317, 1697548476830, 1697548477117, 1697548477192, 1697548477248, 1697548477312, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480872, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486731, 1697548486789, 1697548486849, 1697548486928, 1697548487234, 1697548487305, 1697548487375]"
1057,1057,301,17,[],200,llama-7b,64,1,7026.0,1.0,1,A100,1697548475118,1697548482144,120,109.0,31.0,"[10, 709, 480, 512, 289, 74, 57, 63, 617, 251, 54, 254, 70, 68, 62, 614, 255, 71, 54, 71, 69, 62, 911, 77, 74, 72, 71, 68, 67, 587, 269, 64]","[1697548475128, 1697548475837, 1697548476317, 1697548476829, 1697548477118, 1697548477192, 1697548477249, 1697548477312, 1697548477929, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479822, 1697548479884, 1697548480795, 1697548480872, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482144]"
1058,1058,401,15,[],200,llama-7b,64,1,5168.0,1.0,1,A100,1697548477314,1697548482482,120,84.0,20.0,"[7, 1769, 212, 255, 71, 55, 70, 68, 63, 912, 76, 75, 71, 71, 67, 67, 589, 268, 64, 74, 264]","[1697548477321, 1697548479090, 1697548479302, 1697548479557, 1697548479628, 1697548479683, 1697548479753, 1697548479821, 1697548479884, 1697548480796, 1697548480872, 1697548480947, 1697548481018, 1697548481089, 1697548481156, 1697548481223, 1697548481812, 1697548482080, 1697548482144, 1697548482218, 1697548482482]"
1059,1059,830,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548486851,1697548488026,120,,,"[6, 951]","[1697548486857, 1697548487808]"
1060,1060,600,21,[],200,llama-7b,64,1,1208.0,1.0,1,A100,1697548488033,1697548489241,120,23.0,1.0,"[205, 1003]","[1697548488238, 1697548489241]"
1061,1061,264,22,[],200,llama-7b,64,1,2466.0,1.0,1,A100,1697548489244,1697548491710,120,86.0,20.0,"[15, 571, 65, 52, 43, 43, 50, 49, 41, 41, 193, 374, 50, 50, 40, 553, 51, 48, 40, 48, 49]","[1697548489259, 1697548489830, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490173, 1697548490214, 1697548490407, 1697548490781, 1697548490831, 1697548490881, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710]"
1062,1062,770,32,[],200,llama-7b,64,1,928.0,1.0,1,A100,1697548523425,1697548524353,120,13.0,1.0,"[11, 916]","[1697548523436, 1697548524352]"
1063,1063,430,33,[],200,llama-7b,64,1,893.0,1.0,1,A100,1697548524356,1697548525249,120,15.0,1.0,"[7, 886]","[1697548524363, 1697548525249]"
1064,1064,33,23,[],200,llama-7b,64,1,1899.0,1.0,1,A100,1697548491712,1697548493611,120,140.0,7.0,"[15, 974, 360, 58, 49, 48, 39, 356]","[1697548491727, 1697548492701, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493611]"
1065,1065,622,24,[],200,llama-7b,64,1,374.0,1.0,1,A100,1697548493614,1697548493988,120,20.0,1.0,"[21, 353]","[1697548493635, 1697548493988]"
1066,1066,392,25,[],200,llama-7b,64,1,559.0,1.0,1,A100,1697548493993,1697548494552,120,20.0,1.0,"[21, 538]","[1697548494014, 1697548494552]"
1067,1067,570,26,[],200,llama-7b,64,1,799.0,1.0,1,A100,1697548488033,1697548488832,120,18.0,1.0,"[113, 686]","[1697548488146, 1697548488832]"
1068,1068,229,27,[],200,llama-7b,64,1,404.0,1.0,1,A100,1697548488837,1697548489241,120,15.0,1.0,"[40, 364]","[1697548488877, 1697548489241]"
1069,1069,6,28,[],200,llama-7b,64,1,3875.0,1.0,1,A100,1697548489244,1697548493119,120,100.0,29.0,"[26, 626, 51, 43, 43, 51, 48, 41, 41, 193, 374, 50, 49, 41, 553, 51, 48, 40, 48, 49, 328, 54, 53, 42, 52, 50, 49, 722, 59]","[1697548489270, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490214, 1697548490407, 1697548490781, 1697548490831, 1697548490880, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492038, 1697548492092, 1697548492145, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493060, 1697548493119]"
1070,1070,549,19,[],200,llama-7b,64,1,1860.0,1.0,1,A100,1697548494144,1697548496004,120,93.0,20.0,"[20, 388, 67, 60, 47, 60, 55, 54, 52, 296, 49, 59, 58, 55, 53, 41, 235, 53, 54, 51, 52]","[1697548494164, 1697548494552, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494947, 1697548495243, 1697548495292, 1697548495351, 1697548495409, 1697548495464, 1697548495517, 1697548495558, 1697548495793, 1697548495846, 1697548495900, 1697548495951, 1697548496003]"
1071,1071,868,17,[],200,llama-7b,64,1,2469.0,1.0,1,A100,1697548471197,1697548473666,120,85.0,20.0,"[5, 549, 78, 73, 73, 73, 67, 63, 49, 304, 71, 68, 68, 52, 66, 49, 529, 68, 49, 50, 65]","[1697548471202, 1697548471751, 1697548471829, 1697548471902, 1697548471975, 1697548472048, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472905, 1697548473434, 1697548473502, 1697548473551, 1697548473601, 1697548473666]"
1072,1072,289,31,[],200,llama-7b,64,1,3143.0,1.0,1,A100,1697548503594,1697548506737,120,89.0,20.0,"[11, 707, 255, 62, 48, 60, 58, 59, 57, 351, 65, 51, 61, 60, 60, 282, 46, 55, 51, 680, 64]","[1697548503605, 1697548504312, 1697548504567, 1697548504629, 1697548504677, 1697548504737, 1697548504795, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505841, 1697548505887, 1697548505942, 1697548505993, 1697548506673, 1697548506737]"
1073,1073,878,32,[],200,llama-7b,64,1,2250.0,1.0,1,A100,1697548506739,1697548508989,120,83.0,20.0,"[10, 534, 193, 63, 62, 59, 56, 57, 324, 65, 64, 60, 59, 55, 42, 306, 56, 55, 49, 41, 40]","[1697548506749, 1697548507283, 1697548507476, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989]"
1074,1074,210,20,[],200,llama-7b,64,1,1001.0,1.0,1,A100,1697548496007,1697548497008,120,140.0,2.0,"[43, 881, 77]","[1697548496050, 1697548496931, 1697548497008]"
1075,1075,908,21,[],200,llama-7b,64,1,7784.0,1.0,1,A100,1697548497011,1697548504795,120,6.0,50.0,"[7, 554, 65, 61, 48, 46, 45, 400, 62, 51, 52, 49, 351, 60, 50, 49, 827, 61, 62, 59, 55, 51, 421, 64, 51, 49, 58, 55, 723, 64, 60, 58, 56, 54, 620, 66, 63, 62, 60, 55, 875, 63, 58, 58, 52, 41, 704, 63, 47, 60, 59]","[1697548497018, 1697548497572, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498862, 1697548498912, 1697548498961, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500497, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501497, 1697548501561, 1697548501621, 1697548501679, 1697548501735, 1697548501789, 1697548502409, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502715, 1697548503590, 1697548503653, 1697548503711, 1697548503769, 1697548503821, 1697548503862, 1697548504566, 1697548504629, 1697548504676, 1697548504736, 1697548504795]"
1076,1076,304,16,[],200,llama-7b,64,1,2350.0,1.0,1,A100,1697548459255,1697548461605,120,86.0,20.0,"[17, 465, 250, 64, 50, 62, 60, 61, 426, 70, 52, 67, 63, 63, 48, 237, 50, 62, 62, 60, 61]","[1697548459272, 1697548459737, 1697548459987, 1697548460051, 1697548460101, 1697548460163, 1697548460223, 1697548460284, 1697548460710, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461544, 1697548461605]"
1077,1077,647,33,[],200,llama-7b,64,1,4128.0,1.0,1,A100,1697548508992,1697548513120,120,83.0,20.0,"[10, 1325, 66, 60, 59, 46, 58, 57, 231, 55, 43, 44, 51, 50, 656, 55, 55, 52, 55, 749, 351]","[1697548509002, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510673, 1697548510904, 1697548510959, 1697548511002, 1697548511046, 1697548511097, 1697548511147, 1697548511803, 1697548511858, 1697548511913, 1697548511965, 1697548512020, 1697548512769, 1697548513120]"
1078,1078,91,12,[],200,llama-7b,64,1,1414.0,1.0,1,A100,1697548469290,1697548470704,120,23.0,1.0,"[12, 1401]","[1697548469302, 1697548470703]"
1079,1079,200,34,[],200,llama-7b,64,1,1828.0,1.0,1,A100,1697548525252,1697548527080,120,6.0,9.0,"[17, 760, 66, 48, 48, 60, 59, 655, 64, 50]","[1697548525269, 1697548526029, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526965, 1697548527029, 1697548527079]"
1080,1080,672,13,[],200,llama-7b,64,1,2958.0,1.0,1,A100,1697548470708,1697548473666,120,93.0,20.0,"[11, 1032, 77, 74, 73, 72, 68, 63, 49, 304, 71, 68, 68, 52, 66, 49, 529, 68, 49, 50, 65]","[1697548470719, 1697548471751, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472905, 1697548473434, 1697548473502, 1697548473551, 1697548473601, 1697548473666]"
1081,1081,332,28,[],200,llama-7b,64,1,396.0,1.0,1,A100,1697548500030,1697548500426,120,39.0,1.0,"[18, 378]","[1697548500048, 1697548500426]"
1082,1082,910,29,[],200,llama-7b,64,1,774.0,1.0,1,A100,1697548500429,1697548501203,120,8.0,1.0,"[19, 755]","[1697548500448, 1697548501203]"
1083,1083,523,19,[],200,llama-7b,64,1,1900.0,1.0,1,A100,1697548470215,1697548472115,120,345.0,13.0,"[7, 482, 417, 73, 71, 70, 67, 49, 66, 311, 74, 73, 72, 68]","[1697548470222, 1697548470704, 1697548471121, 1697548471194, 1697548471265, 1697548471335, 1697548471402, 1697548471451, 1697548471517, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115]"
1084,1084,588,29,[],200,llama-7b,64,1,428.0,1.0,1,A100,1697548493122,1697548493550,120,11.0,1.0,"[6, 422]","[1697548493128, 1697548493550]"
1085,1085,685,30,[],200,llama-7b,64,1,1204.0,1.0,1,A100,1697548501206,1697548502410,120,364.0,2.0,"[11, 939, 254]","[1697548501217, 1697548502156, 1697548502410]"
1086,1086,759,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548527082,1697548527817,120,,,"[6, 489]","[1697548527088, 1697548527577]"
1087,1087,334,30,[],200,llama-7b,64,1,426.0,1.0,1,A100,1697548493562,1697548493988,120,15.0,1.0,"[35, 391]","[1697548493597, 1697548493988]"
1088,1088,917,31,[],200,llama-7b,64,1,627.0,1.0,1,A100,1697548493992,1697548494619,120,123.0,2.0,"[26, 534, 67]","[1697548494018, 1697548494552, 1697548494619]"
1089,1089,172,20,[],200,llama-7b,64,1,337.0,1.0,1,A100,1697548472119,1697548472456,120,19.0,1.0,"[6, 331]","[1697548472125, 1697548472456]"
1090,1090,528,36,[],200,llama-7b,64,1,2623.0,1.0,1,A100,1697548527826,1697548530449,120,52.0,20.0,"[115, 954, 60, 51, 51, 48, 47, 39, 365, 52, 49, 41, 41, 41, 49, 347, 55, 49, 59, 59, 51]","[1697548527941, 1697548528895, 1697548528955, 1697548529006, 1697548529057, 1697548529105, 1697548529152, 1697548529191, 1697548529556, 1697548529608, 1697548529657, 1697548529698, 1697548529739, 1697548529780, 1697548529829, 1697548530176, 1697548530231, 1697548530280, 1697548530339, 1697548530398, 1697548530449]"
1091,1091,849,21,[],200,llama-7b,64,1,725.0,1.0,1,A100,1697548472461,1697548473186,120,10.0,1.0,"[12, 713]","[1697548472473, 1697548473186]"
1092,1092,617,22,[],200,llama-7b,64,1,4061.0,1.0,1,A100,1697548473188,1697548477249,120,87.0,20.0,"[7, 802, 73, 68, 68, 50, 66, 65, 63, 594, 73, 70, 70, 68, 67, 65, 860, 513, 288, 74, 56]","[1697548473195, 1697548473997, 1697548474070, 1697548474138, 1697548474206, 1697548474256, 1697548474322, 1697548474387, 1697548474450, 1697548475044, 1697548475117, 1697548475187, 1697548475257, 1697548475325, 1697548475392, 1697548475457, 1697548476317, 1697548476830, 1697548477118, 1697548477192, 1697548477248]"
1093,1093,277,23,[],200,llama-7b,64,1,418.0,1.0,1,A100,1697548477262,1697548477680,120,18.0,1.0,"[32, 385]","[1697548477294, 1697548477679]"
1094,1094,47,24,[],200,llama-7b,64,1,4801.0,1.0,1,A100,1697548477682,1697548482483,120,90.0,20.0,"[21, 1387, 212, 255, 71, 54, 71, 68, 63, 911, 76, 75, 73, 70, 68, 67, 588, 269, 63, 74, 264]","[1697548477703, 1697548479090, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481019, 1697548481089, 1697548481157, 1697548481224, 1697548481812, 1697548482081, 1697548482144, 1697548482218, 1697548482482]"
1095,1095,631,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548482486,1697548488026,120,,,"[15, 1007, 76, 56, 71, 66, 66, 49, 338, 77, 77, 73, 69, 58, 70, 837, 314, 308, 77, 75, 71, 70, 325, 59, 59, 79, 307, 70, 70]","[1697548482501, 1697548483508, 1697548483584, 1697548483640, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484307, 1697548484384, 1697548484457, 1697548484526, 1697548484584, 1697548484654, 1697548485491, 1697548485805, 1697548486113, 1697548486190, 1697548486265, 1697548486336, 1697548486406, 1697548486731, 1697548486790, 1697548486849, 1697548486928, 1697548487235, 1697548487305, 1697548487375]"
1096,1096,694,32,[],200,llama-7b,64,1,1379.0,1.0,1,A100,1697548494623,1697548496002,120,161.0,13.0,"[10, 611, 49, 58, 59, 54, 53, 41, 233, 55, 53, 51, 52]","[1697548494633, 1697548495244, 1697548495293, 1697548495351, 1697548495410, 1697548495464, 1697548495517, 1697548495558, 1697548495791, 1697548495846, 1697548495899, 1697548495950, 1697548496002]"
1097,1097,189,37,[],200,llama-7b,64,1,3230.0,1.0,1,A100,1697548530454,1697548533684,120,88.0,20.0,"[49, 865, 79, 64, 64, 61, 60, 51, 736, 53, 58, 64, 51, 54, 578, 63, 64, 49, 67, 54, 46]","[1697548530503, 1697548531368, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531696, 1697548531747, 1697548532483, 1697548532536, 1697548532594, 1697548532658, 1697548532709, 1697548532763, 1697548533341, 1697548533404, 1697548533468, 1697548533517, 1697548533584, 1697548533638, 1697548533684]"
1098,1098,695,25,[],200,llama-7b,64,1,3081.0,1.0,1,A100,1697548503656,1697548506737,120,92.0,20.0,"[6, 651, 255, 61, 48, 60, 59, 58, 57, 351, 65, 51, 61, 60, 60, 283, 45, 55, 52, 678, 65]","[1697548503662, 1697548504313, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505887, 1697548505942, 1697548505994, 1697548506672, 1697548506737]"
1099,1099,349,33,[],200,llama-7b,64,1,2854.0,1.0,1,A100,1697548496007,1697548498861,120,88.0,20.0,"[22, 902, 78, 67, 64, 60, 63, 57, 55, 262, 61, 47, 47, 45, 400, 62, 51, 51, 50, 351, 59]","[1697548496029, 1697548496931, 1697548497009, 1697548497076, 1697548497140, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497745, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861]"
1100,1100,331,12,[],200,llama-7b,64,1,580.0,1.0,1,A100,1697548465322,1697548465902,120,26.0,1.0,"[8, 572]","[1697548465330, 1697548465902]"
1101,1101,451,9,[],200,llama-7b,64,1,416.0,1.0,1,A100,1697548471336,1697548471752,120,286.0,1.0,"[7, 408]","[1697548471343, 1697548471751]"
1102,1102,394,27,[],200,llama-7b,64,1,758.0,1.0,1,A100,1697548474002,1697548474760,120,11.0,1.0,"[14, 744]","[1697548474016, 1697548474760]"
1103,1103,409,26,[],200,llama-7b,64,1,3628.0,1.0,1,A100,1697548488033,1697548491661,120,109.0,30.0,"[134, 665, 57, 51, 35, 37, 285, 68, 50, 48, 41, 47, 40, 305, 51, 43, 43, 51, 48, 41, 41, 193, 374, 49, 49, 41, 554, 50, 49, 40, 48]","[1697548488167, 1697548488832, 1697548488889, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489365, 1697548489415, 1697548489463, 1697548489504, 1697548489551, 1697548489591, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490214, 1697548490407, 1697548490781, 1697548490830, 1697548490879, 1697548490920, 1697548491474, 1697548491524, 1697548491573, 1697548491613, 1697548491661]"
1104,1104,622,28,[],200,llama-7b,64,1,372.0,1.0,1,A100,1697548493615,1697548493987,120,20.0,1.0,"[41, 331]","[1697548493656, 1697548493987]"
1105,1105,399,29,[],200,llama-7b,64,1,2014.0,1.0,1,A100,1697548493988,1697548496002,120,87.0,20.0,"[6, 557, 67, 60, 47, 60, 55, 55, 51, 298, 49, 58, 59, 54, 52, 42, 234, 54, 53, 51, 52]","[1697548493994, 1697548494551, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494840, 1697548494895, 1697548494946, 1697548495244, 1697548495293, 1697548495351, 1697548495410, 1697548495464, 1697548495516, 1697548495558, 1697548495792, 1697548495846, 1697548495899, 1697548495950, 1697548496002]"
1106,1106,50,30,[],200,llama-7b,64,1,1133.0,1.0,1,A100,1697548496007,1697548497140,120,90.0,4.0,"[23, 901, 78, 67, 64]","[1697548496030, 1697548496931, 1697548497009, 1697548497076, 1697548497140]"
1107,1107,305,19,[],200,llama-7b,64,1,2901.0,1.0,1,A100,1697548490219,1697548493120,120,86.0,20.0,"[31, 476, 55, 50, 48, 42, 554, 50, 48, 40, 48, 48, 329, 55, 52, 43, 51, 51, 48, 723, 59]","[1697548490250, 1697548490726, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491709, 1697548492038, 1697548492093, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492338, 1697548493061, 1697548493120]"
1108,1108,727,31,[],200,llama-7b,64,1,650.0,1.0,1,A100,1697548497142,1697548497792,120,58.0,5.0,"[7, 423, 66, 60, 48, 45]","[1697548497149, 1697548497572, 1697548497638, 1697548497698, 1697548497746, 1697548497791]"
1109,1109,497,32,[],200,llama-7b,64,1,445.0,1.0,1,A100,1697548497792,1697548498237,120,67.0,2.0,"[6, 373, 66]","[1697548497798, 1697548498171, 1697548498237]"
1110,1110,153,33,[],200,llama-7b,64,1,672.0,1.0,1,A100,1697548498240,1697548498912,120,335.0,4.0,"[6, 492, 65, 58, 51]","[1697548498246, 1697548498738, 1697548498803, 1697548498861, 1697548498912]"
1111,1111,82,10,[],200,llama-7b,64,1,2358.0,1.0,1,A100,1697548460166,1697548462524,120,67.0,20.0,"[6, 451, 88, 69, 52, 67, 63, 63, 49, 236, 50, 62, 62, 60, 61, 526, 175, 51, 57, 57, 53]","[1697548460172, 1697548460623, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461074, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461544, 1697548461605, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462524]"
1112,1112,349,26,[],200,llama-7b,64,1,2249.0,1.0,1,A100,1697548506740,1697548508989,120,88.0,20.0,"[15, 527, 194, 64, 61, 59, 56, 57, 324, 65, 64, 60, 59, 55, 42, 306, 56, 55, 49, 41, 40]","[1697548506755, 1697548507282, 1697548507476, 1697548507540, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989]"
1113,1113,768,19,[],200,llama-7b,64,1,1996.0,1.0,1,A100,1697548479094,1697548481090,120,47.0,6.0,"[23, 1184, 495, 75, 75, 72, 72]","[1697548479117, 1697548480301, 1697548480796, 1697548480871, 1697548480946, 1697548481018, 1697548481090]"
1114,1114,788,11,[],200,llama-7b,64,1,1052.0,1.0,1,A100,1697548462528,1697548463580,120,31.0,1.0,"[10, 1042]","[1697548462538, 1697548463580]"
1115,1115,421,20,[],200,llama-7b,64,1,3364.0,1.0,1,A100,1697548481093,1697548484457,120,85.0,20.0,"[20, 410, 289, 268, 64, 74, 264, 68, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 78, 73]","[1697548481113, 1697548481523, 1697548481812, 1697548482080, 1697548482144, 1697548482218, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484384, 1697548484457]"
1116,1116,728,31,[],200,llama-7b,64,1,493.0,1.0,1,A100,1697548523173,1697548523666,120,20.0,1.0,"[14, 479]","[1697548523187, 1697548523666]"
1117,1117,240,24,[],200,llama-7b,64,1,3838.0,1.0,1,A100,1697548477252,1697548481090,120,83.0,20.0,"[35, 392, 251, 251, 54, 253, 71, 68, 62, 612, 255, 72, 54, 71, 68, 62, 913, 76, 75, 71, 72]","[1697548477287, 1697548477679, 1697548477930, 1697548478181, 1697548478235, 1697548478488, 1697548478559, 1697548478627, 1697548478689, 1697548479301, 1697548479556, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479883, 1697548480796, 1697548480872, 1697548480947, 1697548481018, 1697548481090]"
1118,1118,381,32,[],200,llama-7b,64,1,922.0,1.0,1,A100,1697548523668,1697548524590,120,140.0,2.0,"[11, 674, 237]","[1697548523679, 1697548524353, 1697548524590]"
1119,1119,436,12,[],200,llama-7b,64,1,4047.0,1.0,1,A100,1697548463582,1697548467629,120,86.0,20.0,"[15, 958, 303, 460, 64, 50, 59, 46, 701, 72, 70, 53, 66, 61, 49, 691, 72, 70, 68, 68, 51]","[1697548463597, 1697548464555, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465491, 1697548465537, 1697548466238, 1697548466310, 1697548466380, 1697548466433, 1697548466499, 1697548466560, 1697548466609, 1697548467300, 1697548467372, 1697548467442, 1697548467510, 1697548467578, 1697548467629]"
1120,1120,161,33,[],200,llama-7b,64,1,1195.0,1.0,1,A100,1697548524593,1697548525788,120,109.0,7.0,"[6, 890, 68, 65, 51, 50, 65]","[1697548524599, 1697548525489, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788]"
1121,1121,193,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484458,1697548488028,120,,,"[15, 1020, 312, 309, 76, 76, 72, 69, 323, 59, 60, 78, 307, 71, 70]","[1697548484473, 1697548485493, 1697548485805, 1697548486114, 1697548486190, 1697548486266, 1697548486338, 1697548486407, 1697548486730, 1697548486789, 1697548486849, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
1122,1122,742,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548525791,1697548527818,120,,,"[15, 223, 66, 48, 48, 60, 59, 653, 66, 50, 64, 65, 63]","[1697548525806, 1697548526029, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526963, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527271]"
1123,1123,567,22,[],200,llama-7b,64,1,2863.0,1.0,1,A100,1697548504798,1697548507661,120,90.0,20.0,"[8, 387, 70, 65, 51, 60, 61, 59, 283, 46, 54, 52, 678, 66, 58, 55, 43, 581, 64, 62, 60]","[1697548504806, 1697548505193, 1697548505263, 1697548505328, 1697548505379, 1697548505439, 1697548505500, 1697548505559, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507661]"
1124,1124,356,15,[],200,llama-7b,64,1,421.0,1.0,1,A100,1697548467631,1697548468052,120,874.0,2.0,"[7, 334, 80]","[1697548467638, 1697548467972, 1697548468052]"
1125,1125,782,22,[],200,llama-7b,64,1,2180.0,1.0,1,A100,1697548488033,1697548490213,120,90.0,20.0,"[202, 598, 56, 51, 35, 38, 284, 68, 49, 49, 40, 48, 40, 305, 51, 43, 42, 52, 48, 40, 41]","[1697548488235, 1697548488833, 1697548488889, 1697548488940, 1697548488975, 1697548489013, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489896, 1697548489947, 1697548489990, 1697548490032, 1697548490084, 1697548490132, 1697548490172, 1697548490213]"
1126,1126,864,13,[],200,llama-7b,64,1,3303.0,1.0,1,A100,1697548460714,1697548464017,120,83.0,20.0,"[7, 521, 68, 50, 62, 62, 61, 61, 526, 174, 50, 58, 56, 54, 382, 56, 58, 53, 820, 63, 60]","[1697548460721, 1697548461242, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461606, 1697548462132, 1697548462306, 1697548462356, 1697548462414, 1697548462470, 1697548462524, 1697548462906, 1697548462962, 1697548463020, 1697548463073, 1697548463893, 1697548463956, 1697548464016]"
1127,1127,47,26,[],200,llama-7b,64,1,2522.0,1.0,1,A100,1697548494554,1697548497076,120,90.0,20.0,"[18, 607, 64, 50, 58, 58, 55, 53, 41, 233, 55, 54, 51, 52, 378, 40, 40, 51, 50, 445, 69]","[1697548494572, 1697548495179, 1697548495243, 1697548495293, 1697548495351, 1697548495409, 1697548495464, 1697548495517, 1697548495558, 1697548495791, 1697548495846, 1697548495900, 1697548495951, 1697548496003, 1697548496381, 1697548496421, 1697548496461, 1697548496512, 1697548496562, 1697548497007, 1697548497076]"
1128,1128,359,14,[],200,llama-7b,64,1,1033.0,1.0,1,A100,1697548473727,1697548474760,120,10.0,1.0,"[7, 1026]","[1697548473734, 1697548474760]"
1129,1129,528,14,[],200,llama-7b,64,1,3610.0,1.0,1,A100,1697548464019,1697548467629,120,52.0,20.0,"[13, 524, 303, 459, 65, 54, 55, 45, 701, 72, 69, 54, 64, 64, 48, 689, 74, 71, 68, 67, 51]","[1697548464032, 1697548464556, 1697548464859, 1697548465318, 1697548465383, 1697548465437, 1697548465492, 1697548465537, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467443, 1697548467511, 1697548467578, 1697548467629]"
1130,1130,136,15,[],200,llama-7b,64,1,1073.0,1.0,1,A100,1697548474764,1697548475837,120,31.0,1.0,"[23, 1050]","[1697548474787, 1697548475837]"
1131,1131,552,23,[],200,llama-7b,64,1,2901.0,1.0,1,A100,1697548490219,1697548493120,120,87.0,20.0,"[51, 457, 54, 50, 48, 42, 554, 50, 48, 40, 48, 49, 328, 55, 52, 43, 51, 51, 49, 722, 59]","[1697548490270, 1697548490727, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492038, 1697548492093, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492339, 1697548493061, 1697548493120]"
1132,1132,883,18,[],200,llama-7b,64,1,812.0,1.0,1,A100,1697548482147,1697548482959,120,563.0,1.0,"[12, 800]","[1697548482159, 1697548482959]"
1133,1133,298,15,[],200,llama-7b,64,1,340.0,1.0,1,A100,1697548467632,1697548467972,120,17.0,1.0,"[21, 319]","[1697548467653, 1697548467972]"
1134,1134,882,16,[],200,llama-7b,64,1,2003.0,1.0,1,A100,1697548467974,1697548469977,120,345.0,11.0,"[18, 679, 246, 66, 51, 67, 64, 62, 60, 568, 69, 53]","[1697548467992, 1697548468671, 1697548468917, 1697548468983, 1697548469034, 1697548469101, 1697548469165, 1697548469227, 1697548469287, 1697548469855, 1697548469924, 1697548469977]"
1135,1135,723,27,[],200,llama-7b,64,1,493.0,1.0,1,A100,1697548497080,1697548497573,120,14.0,1.0,"[20, 472]","[1697548497100, 1697548497572]"
1136,1136,493,28,[],200,llama-7b,64,1,3086.0,1.0,1,A100,1697548497576,1697548500662,120,83.0,20.0,"[22, 573, 66, 61, 52, 51, 51, 350, 60, 50, 49, 827, 61, 62, 59, 55, 51, 421, 65, 50, 50]","[1697548497598, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498452, 1697548498802, 1697548498862, 1697548498912, 1697548498961, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500497, 1697548500562, 1697548500612, 1697548500662]"
1137,1137,719,16,[],200,llama-7b,64,1,2719.0,1.0,1,A100,1697548475840,1697548478559,120,182.0,6.0,"[6, 2083, 251, 55, 253, 71]","[1697548475846, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478559]"
1138,1138,658,19,[],200,llama-7b,64,1,1186.0,1.0,1,A100,1697548482963,1697548484149,120,11.0,1.0,"[18, 1167]","[1697548482981, 1697548484148]"
1139,1139,315,20,[],200,llama-7b,64,1,3154.0,1.0,1,A100,1697548484152,1697548487306,120,335.0,14.0,"[11, 777, 552, 313, 308, 76, 77, 71, 70, 323, 59, 59, 79, 307, 72]","[1697548484163, 1697548484940, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486266, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234, 1697548487306]"
1140,1140,63,21,[],200,llama-7b,64,1,497.0,1.0,1,A100,1697548487311,1697548487808,120,39.0,1.0,"[8, 489]","[1697548487319, 1697548487808]"
1141,1141,647,22,[],200,llama-7b,64,1,2402.0,1.0,1,A100,1697548487811,1697548490213,120,83.0,20.0,"[12, 414, 651, 52, 35, 37, 285, 66, 51, 48, 41, 48, 40, 304, 52, 43, 42, 51, 48, 41, 41]","[1697548487823, 1697548488237, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489363, 1697548489414, 1697548489462, 1697548489503, 1697548489551, 1697548489591, 1697548489895, 1697548489947, 1697548489990, 1697548490032, 1697548490083, 1697548490131, 1697548490172, 1697548490213]"
1142,1142,148,29,[],200,llama-7b,64,1,539.0,1.0,1,A100,1697548500665,1697548501204,120,16.0,1.0,"[18, 521]","[1697548500683, 1697548501204]"
1143,1143,847,30,[],200,llama-7b,64,1,950.0,1.0,1,A100,1697548501207,1697548502157,120,10.0,1.0,"[20, 929]","[1697548501227, 1697548502156]"
1144,1144,508,31,[],200,llama-7b,64,1,3399.0,1.0,1,A100,1697548502160,1697548505559,120,86.0,20.0,"[11, 958, 462, 63, 58, 57, 52, 42, 705, 61, 48, 60, 59, 58, 57, 351, 65, 51, 61, 60, 60]","[1697548502171, 1697548503129, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559]"
1145,1145,493,17,[],200,llama-7b,64,1,3921.0,1.0,1,A100,1697548478562,1697548482483,120,83.0,20.0,"[18, 511, 211, 255, 71, 54, 71, 68, 63, 911, 76, 75, 73, 70, 68, 67, 588, 269, 63, 74, 265]","[1697548478580, 1697548479091, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481019, 1697548481089, 1697548481157, 1697548481224, 1697548481812, 1697548482081, 1697548482144, 1697548482218, 1697548482483]"
1146,1146,280,32,[],200,llama-7b,64,1,3186.0,1.0,1,A100,1697548505562,1697548508748,120,91.0,20.0,"[20, 818, 272, 66, 58, 55, 44, 580, 65, 62, 59, 56, 57, 323, 65, 65, 60, 58, 55, 42, 306]","[1697548505582, 1697548506400, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507475, 1697548507540, 1697548507602, 1697548507661, 1697548507717, 1697548507774, 1697548508097, 1697548508162, 1697548508227, 1697548508287, 1697548508345, 1697548508400, 1697548508442, 1697548508748]"
1147,1147,418,23,[],200,llama-7b,64,1,612.0,1.0,1,A100,1697548490219,1697548490831,120,286.0,3.0,"[15, 492, 55, 50]","[1697548490234, 1697548490726, 1697548490781, 1697548490831]"
1148,1148,733,8,[],200,llama-7b,64,1,325.0,1.0,1,A100,1697548451134,1697548451459,120,31.0,1.0,"[15, 310]","[1697548451149, 1697548451459]"
1149,1149,147,18,[],200,llama-7b,64,1,474.0,1.0,1,A100,1697548482485,1697548482959,120,182.0,1.0,"[6, 468]","[1697548482491, 1697548482959]"
1150,1150,824,19,[],200,llama-7b,64,1,1423.0,1.0,1,A100,1697548482962,1697548484385,120,58.0,4.0,"[20, 1166, 82, 77, 78]","[1697548482982, 1697548484148, 1697548484230, 1697548484307, 1697548484385]"
1151,1151,78,24,[],200,llama-7b,64,1,2778.0,1.0,1,A100,1697548490834,1697548493612,120,84.0,20.0,"[15, 496, 130, 50, 48, 40, 49, 48, 329, 54, 52, 42, 52, 50, 49, 724, 59, 47, 48, 39, 357]","[1697548490849, 1697548491345, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491710, 1697548492039, 1697548492093, 1697548492145, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493062, 1697548493121, 1697548493168, 1697548493216, 1697548493255, 1697548493612]"
1152,1152,481,9,[],200,llama-7b,64,1,499.0,1.0,1,A100,1697548451463,1697548451962,120,10.0,1.0,"[34, 465]","[1697548451497, 1697548451962]"
1153,1153,480,20,[],200,llama-7b,64,1,550.0,1.0,1,A100,1697548484390,1697548484940,120,26.0,1.0,"[11, 539]","[1697548484401, 1697548484940]"
1154,1154,133,10,[],200,llama-7b,64,1,747.0,1.0,1,A100,1697548451965,1697548452712,120,15.0,1.0,"[24, 722]","[1697548451989, 1697548452711]"
1155,1155,280,14,[],200,llama-7b,64,1,3811.0,1.0,1,A100,1697548473438,1697548477249,120,91.0,20.0,"[7, 552, 74, 68, 66, 51, 67, 64, 63, 594, 71, 71, 71, 68, 68, 63, 862, 512, 288, 74, 57]","[1697548473445, 1697548473997, 1697548474071, 1697548474139, 1697548474205, 1697548474256, 1697548474323, 1697548474387, 1697548474450, 1697548475044, 1697548475115, 1697548475186, 1697548475257, 1697548475325, 1697548475393, 1697548475456, 1697548476318, 1697548476830, 1697548477118, 1697548477192, 1697548477249]"
1156,1156,839,11,[],200,llama-7b,64,1,1789.0,1.0,1,A100,1697548452715,1697548454504,120,58.0,5.0,"[6, 1125, 468, 66, 71, 53]","[1697548452721, 1697548453846, 1697548454314, 1697548454380, 1697548454451, 1697548454504]"
1157,1157,107,10,[],200,llama-7b,64,1,778.0,1.0,1,A100,1697548471754,1697548472532,120,216.0,2.0,"[19, 759]","[1697548471773, 1697548472532]"
1158,1158,245,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484944,1697548488024,120,,,"[19, 1685, 82, 59, 60, 78, 307, 72, 70]","[1697548484963, 1697548486648, 1697548486730, 1697548486789, 1697548486849, 1697548486927, 1697548487234, 1697548487306, 1697548487376]"
1159,1159,806,11,[],200,llama-7b,64,1,2859.0,1.0,1,A100,1697548472535,1697548475394,120,89.0,20.0,"[9, 642, 249, 67, 49, 51, 65, 58, 345, 68, 68, 51, 66, 64, 62, 595, 72, 71, 70, 69, 67]","[1697548472544, 1697548473186, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473667, 1697548473725, 1697548474070, 1697548474138, 1697548474206, 1697548474257, 1697548474323, 1697548474387, 1697548474449, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475393]"
1160,1160,835,22,[],200,llama-7b,64,1,2183.0,1.0,1,A100,1697548488030,1697548490213,120,87.0,20.0,"[19, 782, 57, 52, 35, 37, 285, 67, 50, 49, 40, 48, 40, 304, 52, 43, 42, 51, 49, 40, 41]","[1697548488049, 1697548488831, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489895, 1697548489947, 1697548489990, 1697548490032, 1697548490083, 1697548490132, 1697548490172, 1697548490213]"
1161,1161,27,15,[],200,llama-7b,64,1,9082.0,1.0,1,A100,1697548477255,1697548486337,120,15.0,50.0,"[24, 400, 251, 251, 54, 254, 70, 68, 62, 612, 255, 72, 54, 72, 67, 62, 913, 76, 75, 71, 72, 67, 67, 588, 269, 63, 74, 264, 68, 51, 69, 838, 76, 55, 72, 66, 66, 49, 338, 76, 76, 74, 71, 57, 71, 837, 314, 307, 76, 77, 71]","[1697548477279, 1697548477679, 1697548477930, 1697548478181, 1697548478235, 1697548478489, 1697548478559, 1697548478627, 1697548478689, 1697548479301, 1697548479556, 1697548479628, 1697548479682, 1697548479754, 1697548479821, 1697548479883, 1697548480796, 1697548480872, 1697548480947, 1697548481018, 1697548481090, 1697548481157, 1697548481224, 1697548481812, 1697548482081, 1697548482144, 1697548482218, 1697548482482, 1697548482550, 1697548482601, 1697548482670, 1697548483508, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484382, 1697548484456, 1697548484527, 1697548484584, 1697548484655, 1697548485492, 1697548485806, 1697548486113, 1697548486189, 1697548486266, 1697548486337]"
1162,1162,440,12,[],200,llama-7b,64,1,4424.0,1.0,1,A100,1697548475398,1697548479822,120,84.0,20.0,"[19, 420, 481, 511, 289, 74, 57, 63, 617, 251, 55, 253, 70, 69, 61, 614, 254, 71, 55, 71, 69]","[1697548475417, 1697548475837, 1697548476318, 1697548476829, 1697548477118, 1697548477192, 1697548477249, 1697548477312, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478627, 1697548478688, 1697548479302, 1697548479556, 1697548479627, 1697548479682, 1697548479753, 1697548479822]"
1163,1163,114,34,[],200,llama-7b,64,1,3546.0,1.0,1,A100,1697548498864,1697548502410,120,88.0,20.0,"[14, 541, 369, 62, 61, 59, 55, 51, 421, 64, 51, 49, 58, 55, 724, 64, 60, 58, 55, 55, 620]","[1697548498878, 1697548499419, 1697548499788, 1697548499850, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500497, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501498, 1697548501562, 1697548501622, 1697548501680, 1697548501735, 1697548501790, 1697548502410]"
1164,1164,603,23,[],200,llama-7b,64,1,507.0,1.0,1,A100,1697548490219,1697548490726,120,9.0,1.0,"[21, 486]","[1697548490240, 1697548490726]"
1165,1165,263,24,[],200,llama-7b,64,1,615.0,1.0,1,A100,1697548490730,1697548491345,120,15.0,1.0,"[24, 591]","[1697548490754, 1697548491345]"
1166,1166,33,25,[],200,llama-7b,64,1,942.0,1.0,1,A100,1697548491348,1697548492290,120,140.0,7.0,"[24, 608, 58, 54, 53, 43, 51, 51]","[1697548491372, 1697548491980, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492290]"
1167,1167,617,26,[],200,llama-7b,64,1,2603.0,1.0,1,A100,1697548492293,1697548494896,120,87.0,20.0,"[19, 390, 359, 59, 48, 48, 39, 357, 47, 38, 39, 310, 52, 42, 46, 432, 61, 47, 60, 55, 54]","[1697548492312, 1697548492702, 1697548493061, 1697548493120, 1697548493168, 1697548493216, 1697548493255, 1697548493612, 1697548493659, 1697548493697, 1697548493736, 1697548494046, 1697548494098, 1697548494140, 1697548494186, 1697548494618, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895]"
1168,1168,395,27,[],200,llama-7b,64,1,2179.0,1.0,1,A100,1697548494898,1697548497077,120,88.0,20.0,"[19, 262, 65, 49, 59, 58, 55, 51, 42, 233, 55, 53, 52, 51, 379, 40, 40, 51, 50, 445, 70]","[1697548494917, 1697548495179, 1697548495244, 1697548495293, 1697548495352, 1697548495410, 1697548495465, 1697548495516, 1697548495558, 1697548495791, 1697548495846, 1697548495899, 1697548495951, 1697548496002, 1697548496381, 1697548496421, 1697548496461, 1697548496512, 1697548496562, 1697548497007, 1697548497077]"
1169,1169,51,28,[],200,llama-7b,64,1,5459.0,1.0,1,A100,1697548497080,1697548502539,120,364.0,36.0,"[16, 476, 66, 60, 48, 45, 46, 400, 61, 52, 52, 49, 351, 59, 51, 50, 826, 61, 62, 59, 55, 51, 421, 65, 50, 49, 59, 54, 723, 64, 60, 58, 56, 54, 620, 66, 63]","[1697548497096, 1697548497572, 1697548497638, 1697548497698, 1697548497746, 1697548497791, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500497, 1697548500562, 1697548500612, 1697548500661, 1697548500720, 1697548500774, 1697548501497, 1697548501561, 1697548501621, 1697548501679, 1697548501735, 1697548501789, 1697548502409, 1697548502475, 1697548502538]"
1170,1170,556,12,[],200,llama-7b,64,1,800.0,1.0,1,A100,1697548488033,1697548488833,120,9.0,1.0,"[108, 692]","[1697548488141, 1697548488833]"
1171,1171,212,13,[],200,llama-7b,64,1,404.0,1.0,1,A100,1697548488837,1697548489241,120,31.0,1.0,"[45, 359]","[1697548488882, 1697548489241]"
1172,1172,366,25,[],200,llama-7b,64,1,752.0,1.0,1,A100,1697548497700,1697548498452,120,85.0,6.0,"[18, 453, 66, 61, 52, 52, 49]","[1697548497718, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498402, 1697548498451]"
1173,1173,210,13,[],200,llama-7b,64,1,972.0,1.0,1,A100,1697548479824,1697548480796,120,140.0,2.0,"[18, 460, 494]","[1697548479842, 1697548480302, 1697548480796]"
1174,1174,910,14,[],200,llama-7b,64,1,586.0,1.0,1,A100,1697548489245,1697548489831,120,8.0,1.0,"[29, 557]","[1697548489274, 1697548489831]"
1175,1175,570,15,[],200,llama-7b,64,1,533.0,1.0,1,A100,1697548489834,1697548490367,120,18.0,1.0,"[29, 504]","[1697548489863, 1697548490367]"
1176,1176,339,16,[],200,llama-7b,64,1,2752.0,1.0,1,A100,1697548490368,1697548493120,120,87.0,20.0,"[6, 352, 55, 50, 48, 42, 553, 51, 48, 40, 48, 49, 329, 54, 52, 43, 51, 51, 48, 724, 58]","[1697548490374, 1697548490726, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492039, 1697548492093, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492338, 1697548493062, 1697548493120]"
1177,1177,798,14,[],200,llama-7b,64,1,1684.0,1.0,1,A100,1697548480798,1697548482482,120,79.0,6.0,"[15, 999, 268, 63, 74, 265]","[1697548480813, 1697548481812, 1697548482080, 1697548482143, 1697548482217, 1697548482482]"
1178,1178,567,15,[],200,llama-7b,64,1,3780.0,1.0,1,A100,1697548482485,1697548486265,120,90.0,20.0,"[16, 457, 550, 76, 55, 72, 66, 66, 49, 338, 77, 78, 72, 69, 58, 70, 837, 313, 311, 75, 75]","[1697548482501, 1697548482958, 1697548483508, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484307, 1697548484385, 1697548484457, 1697548484526, 1697548484584, 1697548484654, 1697548485491, 1697548485804, 1697548486115, 1697548486190, 1697548486265]"
1179,1179,167,28,[],200,llama-7b,64,1,5059.0,1.0,1,A100,1697548474762,1697548479821,120,88.0,20.0,"[6, 1068, 481, 512, 289, 74, 57, 63, 618, 250, 54, 254, 70, 70, 60, 614, 255, 71, 54, 71, 68]","[1697548474768, 1697548475836, 1697548476317, 1697548476829, 1697548477118, 1697548477192, 1697548477249, 1697548477312, 1697548477930, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478628, 1697548478688, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821]"
1180,1180,752,29,[],200,llama-7b,64,1,1049.0,1.0,1,A100,1697548479823,1697548480872,120,39.0,3.0,"[6, 473, 494, 76]","[1697548479829, 1697548480302, 1697548480796, 1697548480872]"
1181,1181,499,30,[],200,llama-7b,64,1,3582.0,1.0,1,A100,1697548480875,1697548484457,120,88.0,20.0,"[14, 633, 290, 268, 63, 74, 265, 68, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 77, 73]","[1697548480889, 1697548481522, 1697548481812, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484383, 1697548484456]"
1182,1182,229,16,[],200,llama-7b,64,1,375.0,1.0,1,A100,1697548486273,1697548486648,120,15.0,1.0,"[19, 356]","[1697548486292, 1697548486648]"
1183,1183,928,17,[],200,llama-7b,64,1,1155.0,1.0,1,A100,1697548486653,1697548487808,120,20.0,1.0,"[26, 1129]","[1697548486679, 1697548487808]"
1184,1184,582,18,[],200,llama-7b,64,1,424.0,1.0,1,A100,1697548487813,1697548488237,120,19.0,1.0,"[21, 403]","[1697548487834, 1697548488237]"
1185,1185,355,19,[],200,llama-7b,64,1,2641.0,1.0,1,A100,1697548488239,1697548490880,120,90.0,20.0,"[9, 992, 57, 68, 49, 49, 40, 49, 40, 304, 51, 43, 43, 50, 49, 40, 41, 194, 374, 49, 50]","[1697548488248, 1697548489240, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489503, 1697548489552, 1697548489592, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490407, 1697548490781, 1697548490830, 1697548490880]"
1186,1186,728,29,[],200,llama-7b,64,1,588.0,1.0,1,A100,1697548502542,1697548503130,120,20.0,1.0,"[18, 570]","[1697548502560, 1697548503130]"
1187,1187,496,30,[],200,llama-7b,64,1,2245.0,1.0,1,A100,1697548503133,1697548505378,120,335.0,11.0,"[6, 1174, 254, 62, 48, 59, 59, 59, 57, 350, 66, 51]","[1697548503139, 1697548504313, 1697548504567, 1697548504629, 1697548504677, 1697548504736, 1697548504795, 1697548504854, 1697548504911, 1697548505261, 1697548505327, 1697548505378]"
1188,1188,156,31,[],200,llama-7b,64,1,2907.0,1.0,1,A100,1697548505380,1697548508287,120,86.0,20.0,"[15, 384, 63, 46, 54, 52, 678, 66, 58, 55, 43, 581, 65, 61, 60, 56, 56, 324, 65, 64, 61]","[1697548505395, 1697548505779, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507540, 1697548507601, 1697548507661, 1697548507717, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508287]"
1189,1189,307,34,[],200,llama-7b,64,1,660.0,1.0,1,A100,1697548513123,1697548513783,120,26.0,1.0,"[11, 649]","[1697548513134, 1697548513783]"
1190,1190,7,20,[],200,llama-7b,64,1,1305.0,1.0,1,A100,1697548490883,1697548492188,120,345.0,11.0,"[18, 444, 130, 50, 48, 40, 49, 48, 328, 54, 53, 43]","[1697548490901, 1697548491345, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491710, 1697548492038, 1697548492092, 1697548492145, 1697548492188]"
1191,1191,856,32,[],200,llama-7b,64,1,11878.0,1.0,1,A100,1697548508289,1697548520167,120,286.0,72.0,"[15, 375, 69, 56, 55, 50, 40, 40, 579, 57, 54, 52, 54, 49, 560, 59, 59, 46, 58, 56, 231, 56, 43, 43, 51, 51, 654, 57, 54, 53, 51, 753, 350, 59, 55, 43, 54, 833, 66, 60, 59, 57, 919, 72, 69, 65, 60, 976, 271, 67, 66, 65, 63, 294, 66, 64, 59, 53, 672, 66, 64, 59, 58, 361, 63, 50, 58, 57, 45, 607, 242, 68, 62]","[1697548508304, 1697548508679, 1697548508748, 1697548508804, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509568, 1697548509625, 1697548509679, 1697548509731, 1697548509785, 1697548509834, 1697548510394, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510672, 1697548510903, 1697548510959, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511801, 1697548511858, 1697548511912, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514163, 1697548514229, 1697548514289, 1697548514348, 1697548514405, 1697548515324, 1697548515396, 1697548515465, 1697548515530, 1697548515590, 1697548516566, 1697548516837, 1697548516904, 1697548516970, 1697548517035, 1697548517098, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517634, 1697548518306, 1697548518372, 1697548518436, 1697548518495, 1697548518553, 1697548518914, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166]"
1192,1192,684,21,[],200,llama-7b,64,1,2705.0,1.0,1,A100,1697548492191,1697548494896,120,100.0,20.0,"[9, 501, 360, 58, 49, 48, 39, 357, 48, 38, 37, 312, 51, 42, 47, 431, 61, 47, 59, 56, 54]","[1697548492200, 1697548492701, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493612, 1697548493660, 1697548493698, 1697548493735, 1697548494047, 1697548494098, 1697548494140, 1697548494187, 1697548494618, 1697548494679, 1697548494726, 1697548494785, 1697548494841, 1697548494895]"
1193,1193,453,22,[],200,llama-7b,64,1,281.0,1.0,1,A100,1697548494899,1697548495180,120,26.0,1.0,"[14, 266]","[1697548494913, 1697548495179]"
1194,1194,153,31,[],200,llama-7b,64,1,1657.0,1.0,1,A100,1697548484457,1697548486114,120,335.0,4.0,"[6, 477, 553, 312, 309]","[1697548484463, 1697548484940, 1697548485493, 1697548485805, 1697548486114]"
1195,1195,109,23,[],200,llama-7b,64,1,2515.0,1.0,1,A100,1697548495182,1697548497697,120,90.0,20.0,"[11, 534, 64, 55, 53, 52, 51, 379, 40, 40, 51, 50, 445, 70, 63, 61, 62, 57, 55, 262, 60]","[1697548495193, 1697548495727, 1697548495791, 1697548495846, 1697548495899, 1697548495951, 1697548496002, 1697548496381, 1697548496421, 1697548496461, 1697548496512, 1697548496562, 1697548497007, 1697548497077, 1697548497140, 1697548497201, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697]"
1196,1196,857,32,[],200,llama-7b,64,1,530.0,1.0,1,A100,1697548486118,1697548486648,120,18.0,1.0,"[18, 512]","[1697548486136, 1697548486648]"
1197,1197,65,27,[],200,llama-7b,64,1,3628.0,1.0,1,A100,1697548491664,1697548495292,120,39.0,30.0,"[13, 303, 59, 54, 52, 44, 51, 50, 49, 722, 58, 49, 48, 39, 356, 49, 38, 38, 311, 52, 42, 45, 432, 60, 48, 59, 55, 55, 51, 297, 49]","[1697548491677, 1697548491980, 1697548492039, 1697548492093, 1697548492145, 1697548492189, 1697548492240, 1697548492290, 1697548492339, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493660, 1697548493698, 1697548493736, 1697548494047, 1697548494099, 1697548494141, 1697548494186, 1697548494618, 1697548494678, 1697548494726, 1697548494785, 1697548494840, 1697548494895, 1697548494946, 1697548495243, 1697548495292]"
1198,1198,510,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548486652,1697548488026,120,,,[21],[1697548486673]
1199,1199,287,34,[],200,llama-7b,64,1,800.0,1.0,1,A100,1697548488033,1697548488833,120,10.0,1.0,"[153, 647]","[1697548488186, 1697548488833]"
1200,1200,868,35,[],200,llama-7b,64,1,2044.0,1.0,1,A100,1697548488836,1697548490880,120,85.0,20.0,"[32, 373, 57, 67, 50, 48, 40, 49, 40, 303, 52, 43, 43, 50, 49, 40, 41, 194, 374, 50, 49]","[1697548488868, 1697548489241, 1697548489298, 1697548489365, 1697548489415, 1697548489463, 1697548489503, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490407, 1697548490781, 1697548490831, 1697548490880]"
1201,1201,814,24,[],200,llama-7b,64,1,2961.0,1.0,1,A100,1697548497702,1697548500663,120,89.0,20.0,"[26, 443, 66, 61, 52, 51, 50, 351, 59, 51, 50, 826, 61, 61, 60, 54, 52, 421, 65, 51, 49]","[1697548497728, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499910, 1697548499970, 1697548500024, 1697548500076, 1697548500497, 1697548500562, 1697548500613, 1697548500662]"
1202,1202,576,10,[],200,llama-7b,64,1,1225.0,1.0,1,A100,1697548461609,1697548462834,120,14.0,1.0,"[21, 1204]","[1697548461630, 1697548462834]"
1203,1203,324,11,[],200,llama-7b,64,1,8428.0,1.0,1,A100,1697548462837,1697548471265,120,17.0,50.0,"[30, 713, 314, 62, 60, 58, 55, 729, 460, 64, 50, 59, 46, 701, 72, 70, 53, 65, 62, 49, 691, 72, 70, 68, 68, 50, 63, 360, 72, 55, 70, 70, 64, 536, 65, 51, 66, 65, 62, 59, 569, 69, 53, 67, 51, 52, 65, 64, 844, 73, 72]","[1697548462867, 1697548463580, 1697548463894, 1697548463956, 1697548464016, 1697548464074, 1697548464129, 1697548464858, 1697548465318, 1697548465382, 1697548465432, 1697548465491, 1697548465537, 1697548466238, 1697548466310, 1697548466380, 1697548466433, 1697548466498, 1697548466560, 1697548466609, 1697548467300, 1697548467372, 1697548467442, 1697548467510, 1697548467578, 1697548467628, 1697548467691, 1697548468051, 1697548468123, 1697548468178, 1697548468248, 1697548468318, 1697548468382, 1697548468918, 1697548468983, 1697548469034, 1697548469100, 1697548469165, 1697548469227, 1697548469286, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471193, 1697548471265]"
1204,1204,597,8,[],200,llama-7b,64,1,483.0,1.0,1,A100,1697548457210,1697548457693,120,39.0,1.0,"[8, 475]","[1697548457218, 1697548457693]"
1205,1205,120,27,[],200,llama-7b,64,1,1334.0,1.0,1,A100,1697548508993,1697548510327,120,17.0,1.0,"[21, 1313]","[1697548509014, 1697548510327]"
1206,1206,710,28,[],200,llama-7b,64,1,508.0,1.0,1,A100,1697548510331,1697548510839,120,14.0,1.0,"[23, 485]","[1697548510354, 1697548510839]"
1207,1207,41,11,[],200,llama-7b,64,1,7410.0,1.0,1,A100,1697548458900,1697548466310,120,39.0,43.0,"[27, 810, 249, 65, 49, 63, 60, 60, 427, 70, 51, 69, 63, 63, 48, 236, 50, 62, 62, 61, 61, 525, 175, 51, 57, 57, 53, 381, 57, 57, 54, 820, 63, 60, 57, 56, 729, 459, 65, 50, 59, 45, 702, 72]","[1697548458927, 1697548459737, 1697548459986, 1697548460051, 1697548460100, 1697548460163, 1697548460223, 1697548460283, 1697548460710, 1697548460780, 1697548460831, 1697548460900, 1697548460963, 1697548461026, 1697548461074, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461545, 1697548461606, 1697548462131, 1697548462306, 1697548462357, 1697548462414, 1697548462471, 1697548462524, 1697548462905, 1697548462962, 1697548463019, 1697548463073, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464858, 1697548465317, 1697548465382, 1697548465432, 1697548465491, 1697548465536, 1697548466238, 1697548466310]"
1208,1208,480,29,[],200,llama-7b,64,1,716.0,1.0,1,A100,1697548510843,1697548511559,120,26.0,1.0,"[27, 689]","[1697548510870, 1697548511559]"
1209,1209,136,30,[],200,llama-7b,64,1,897.0,1.0,1,A100,1697548511562,1697548512459,120,31.0,1.0,"[10, 887]","[1697548511572, 1697548512459]"
1210,1210,652,33,[],200,llama-7b,64,1,633.0,1.0,1,A100,1697548508807,1697548509440,120,14.0,1.0,"[6, 627]","[1697548508813, 1697548509440]"
1211,1211,308,34,[],200,llama-7b,64,1,3678.0,1.0,1,A100,1697548509442,1697548513120,120,87.0,20.0,"[20, 865, 66, 60, 59, 46, 58, 57, 230, 56, 43, 42, 52, 51, 656, 55, 55, 53, 54, 749, 351]","[1697548509462, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510673, 1697548510903, 1697548510959, 1697548511002, 1697548511044, 1697548511096, 1697548511147, 1697548511803, 1697548511858, 1697548511913, 1697548511966, 1697548512020, 1697548512769, 1697548513120]"
1212,1212,833,31,[],200,llama-7b,64,1,2935.0,1.0,1,A100,1697548512461,1697548515396,120,563.0,8.0,"[15, 1306, 382, 66, 60, 58, 58, 918, 71]","[1697548512476, 1697548513782, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406, 1697548515324, 1697548515395]"
1213,1213,15,25,[],200,llama-7b,64,1,3364.0,1.0,1,A100,1697548481093,1697548484457,120,100.0,20.0,"[19, 410, 290, 268, 63, 74, 265, 68, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 79, 72]","[1697548481112, 1697548481522, 1697548481812, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484385, 1697548484457]"
1214,1214,494,32,[],200,llama-7b,64,1,2124.0,1.0,1,A100,1697548515399,1697548517523,120,6.0,10.0,"[17, 653, 498, 270, 68, 66, 64, 64, 293, 66, 64]","[1697548515416, 1697548516069, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517522]"
1215,1215,450,11,[],200,llama-7b,64,1,2869.0,1.0,1,A100,1697548465448,1697548468317,120,91.0,20.0,"[24, 430, 337, 72, 69, 54, 64, 63, 48, 689, 74, 70, 68, 67, 51, 63, 359, 73, 54, 72, 68]","[1697548465472, 1697548465902, 1697548466239, 1697548466311, 1697548466380, 1697548466434, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468249, 1697548468317]"
1216,1216,110,12,[],200,llama-7b,64,1,713.0,1.0,1,A100,1697548468321,1697548469034,120,96.0,4.0,"[8, 342, 246, 66, 51]","[1697548468329, 1697548468671, 1697548468917, 1697548468983, 1697548469034]"
1217,1217,811,13,[],200,llama-7b,64,1,11836.0,1.0,1,A100,1697548469036,1697548480872,120,457.0,72.0,"[9, 537, 274, 68, 53, 67, 51, 52, 65, 64, 844, 74, 70, 70, 67, 50, 65, 313, 72, 75, 72, 67, 64, 47, 306, 70, 68, 69, 52, 65, 49, 530, 67, 50, 50, 64, 59, 346, 68, 66, 51, 66, 65, 63, 593, 73, 71, 69, 69, 68, 63, 861, 512, 288, 75, 56, 63, 618, 251, 55, 253, 70, 68, 62, 613, 256, 71, 54, 71, 68, 63, 911, 77]","[1697548469045, 1697548469582, 1697548469856, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471194, 1697548471264, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471829, 1697548471901, 1697548471976, 1697548472048, 1697548472115, 1697548472179, 1697548472226, 1697548472532, 1697548472602, 1697548472670, 1697548472739, 1697548472791, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473552, 1697548473602, 1697548473666, 1697548473725, 1697548474071, 1697548474139, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475187, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480872]"
1218,1218,264,33,[],200,llama-7b,64,1,2885.0,1.0,1,A100,1697548517526,1697548520411,120,86.0,20.0,"[20, 519, 242, 66, 64, 59, 58, 361, 62, 50, 58, 57, 45, 607, 242, 68, 62, 63, 61, 60, 61]","[1697548517546, 1697548518065, 1697548518307, 1697548518373, 1697548518437, 1697548518496, 1697548518554, 1697548518915, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520229, 1697548520290, 1697548520350, 1697548520411]"
1219,1219,464,14,[],200,llama-7b,64,1,647.0,1.0,1,A100,1697548480876,1697548481523,120,12.0,1.0,"[7, 640]","[1697548480883, 1697548481523]"
1220,1220,597,26,[],200,llama-7b,64,1,480.0,1.0,1,A100,1697548484460,1697548484940,120,39.0,1.0,"[13, 467]","[1697548484473, 1697548484940]"
1221,1221,213,15,[],200,llama-7b,64,1,2253.0,1.0,1,A100,1697548481525,1697548483778,120,123.0,6.0,"[14, 1419, 551, 75, 55, 72, 67]","[1697548481539, 1697548482958, 1697548483509, 1697548483584, 1697548483639, 1697548483711, 1697548483778]"
1222,1222,374,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484943,1697548488024,120,,,"[6, 1698, 83, 59, 60, 78, 307, 72, 70]","[1697548484949, 1697548486647, 1697548486730, 1697548486789, 1697548486849, 1697548486927, 1697548487234, 1697548487306, 1697548487376]"
1223,1223,81,35,[],200,llama-7b,64,1,3715.0,1.0,1,A100,1697548513123,1697548516838,120,732.0,13.0,"[16, 644, 381, 66, 60, 58, 58, 918, 71, 70, 64, 61, 977, 271]","[1697548513139, 1697548513783, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406, 1697548515324, 1697548515395, 1697548515465, 1697548515529, 1697548515590, 1697548516567, 1697548516838]"
1224,1224,848,34,[],200,llama-7b,64,1,949.0,1.0,1,A100,1697548520414,1697548521363,120,47.0,1.0,"[15, 934]","[1697548520429, 1697548521363]"
1225,1225,911,16,[],200,llama-7b,64,1,2334.0,1.0,1,A100,1697548483780,1697548486114,120,335.0,11.0,"[6, 362, 82, 77, 78, 72, 70, 58, 69, 838, 313, 308]","[1697548483786, 1697548484148, 1697548484230, 1697548484307, 1697548484385, 1697548484457, 1697548484527, 1697548484585, 1697548484654, 1697548485492, 1697548485805, 1697548486113]"
1226,1226,746,16,[],200,llama-7b,64,1,3695.0,1.0,1,A100,1697548477252,1697548480947,120,345.0,18.0,"[14, 413, 251, 251, 54, 253, 71, 68, 61, 613, 255, 72, 54, 72, 68, 61, 913, 76, 75]","[1697548477266, 1697548477679, 1697548477930, 1697548478181, 1697548478235, 1697548478488, 1697548478559, 1697548478627, 1697548478688, 1697548479301, 1697548479556, 1697548479628, 1697548479682, 1697548479754, 1697548479822, 1697548479883, 1697548480796, 1697548480872, 1697548480947]"
1227,1227,596,35,[],200,llama-7b,64,1,2670.0,1.0,1,A100,1697548521366,1697548524036,120,87.0,20.0,"[20, 759, 266, 70, 66, 68, 56, 66, 50, 317, 66, 64, 64, 62, 61, 317, 52, 67, 51, 64, 64]","[1697548521386, 1697548522145, 1697548522411, 1697548522481, 1697548522547, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524036]"
1228,1228,410,17,[],200,llama-7b,64,1,2690.0,1.0,1,A100,1697548480949,1697548483639,120,364.0,12.0,"[7, 567, 289, 268, 63, 74, 265, 68, 51, 68, 839, 75, 56]","[1697548480956, 1697548481523, 1697548481812, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639]"
1229,1229,671,36,[],200,llama-7b,64,1,481.0,1.0,1,A100,1697548516841,1697548517322,120,12.0,1.0,"[16, 465]","[1697548516857, 1697548517322]"
1230,1230,182,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548483642,1697548488026,120,,,"[15, 491, 82, 77, 78, 72, 70, 57, 70, 838, 313, 308, 76, 76, 72, 70, 323, 59, 59, 79, 306, 73, 69]","[1697548483657, 1697548484148, 1697548484230, 1697548484307, 1697548484385, 1697548484457, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487233, 1697548487306, 1697548487375]"
1231,1231,25,28,[],200,llama-7b,64,1,802.0,1.0,1,A100,1697548488031,1697548488833,120,12.0,1.0,"[63, 738]","[1697548488094, 1697548488832]"
1232,1232,438,37,[],200,llama-7b,64,1,737.0,1.0,1,A100,1697548517327,1697548518064,120,9.0,1.0,"[19, 718]","[1697548517346, 1697548518064]"
1233,1233,70,38,[],200,llama-7b,64,1,777.0,1.0,1,A100,1697548518069,1697548518846,120,39.0,1.0,"[29, 748]","[1697548518098, 1697548518846]"
1234,1234,764,19,[],200,llama-7b,64,1,803.0,1.0,1,A100,1697548488030,1697548488833,120,39.0,1.0,"[56, 746]","[1697548488086, 1697548488832]"
1235,1235,541,20,[],200,llama-7b,64,1,2045.0,1.0,1,A100,1697548488835,1697548490880,120,90.0,20.0,"[38, 368, 57, 67, 50, 48, 41, 48, 40, 303, 52, 43, 43, 50, 49, 40, 41, 194, 374, 50, 49]","[1697548488873, 1697548489241, 1697548489298, 1697548489365, 1697548489415, 1697548489463, 1697548489504, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490407, 1697548490781, 1697548490831, 1697548490880]"
1236,1236,365,36,[],200,llama-7b,64,1,1196.0,1.0,1,A100,1697548524053,1697548525249,120,23.0,1.0,"[20, 1176]","[1697548524073, 1697548525249]"
1237,1237,27,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548525253,1697548527817,120,,,"[25, 750, 67, 48, 48, 60, 59, 655, 64, 50, 64, 65, 63]","[1697548525278, 1697548526028, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526965, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527271]"
1238,1238,339,31,[],200,llama-7b,64,1,3147.0,1.0,1,A100,1697548502412,1697548505559,120,87.0,20.0,"[7, 711, 461, 63, 58, 57, 52, 42, 705, 61, 48, 60, 59, 58, 57, 351, 66, 50, 61, 60, 59]","[1697548502419, 1697548503130, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505328, 1697548505378, 1697548505439, 1697548505499, 1697548505558]"
1239,1239,616,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548473673,1697548488023,120,,,"[22, 303, 73, 68, 67, 51, 66, 64, 63, 593, 73, 71, 69, 70, 67, 63, 862, 511, 288, 74, 57, 64, 617, 252, 54, 253, 71, 68, 61, 614, 255, 71, 54, 71, 68, 63, 911, 77, 74, 72, 71, 68, 67, 587, 269, 63, 74, 265, 68, 51, 68, 839, 76, 55, 72, 66, 66, 49, 338, 76, 75, 75, 71, 57, 70, 838, 313, 308, 76, 77, 71, 70, 324, 58, 60, 79, 306, 71, 70]","[1697548473695, 1697548473998, 1697548474071, 1697548474139, 1697548474206, 1697548474257, 1697548474323, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475187, 1697548475256, 1697548475326, 1697548475393, 1697548475456, 1697548476318, 1697548476829, 1697548477117, 1697548477191, 1697548477248, 1697548477312, 1697548477929, 1697548478181, 1697548478235, 1697548478488, 1697548478559, 1697548478627, 1697548478688, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480872, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486266, 1697548486337, 1697548486407, 1697548486731, 1697548486789, 1697548486849, 1697548486928, 1697548487234, 1697548487305, 1697548487375]"
1240,1240,893,38,[],200,llama-7b,64,1,2048.0,1.0,1,A100,1697548533686,1697548535734,120,335.0,10.0,"[16, 862, 67, 46, 59, 58, 57, 707, 59, 59, 58]","[1697548533702, 1697548534564, 1697548534631, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734]"
1241,1241,193,21,[],200,llama-7b,64,1,2729.0,1.0,1,A100,1697548490883,1697548493612,120,79.0,20.0,"[10, 582, 50, 48, 40, 49, 48, 328, 54, 53, 43, 51, 50, 49, 724, 57, 49, 48, 39, 357]","[1697548490893, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491710, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492289, 1697548492338, 1697548493062, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493612]"
1242,1242,492,12,[],200,llama-7b,64,1,2867.0,1.0,1,A100,1697548454507,1697548457374,120,47.0,20.0,"[17, 536, 683, 73, 76, 72, 69, 69, 52, 68, 246, 70, 52, 70, 65, 60, 359, 63, 60, 48, 59]","[1697548454524, 1697548455060, 1697548455743, 1697548455816, 1697548455892, 1697548455964, 1697548456033, 1697548456102, 1697548456154, 1697548456222, 1697548456468, 1697548456538, 1697548456590, 1697548456660, 1697548456725, 1697548456785, 1697548457144, 1697548457207, 1697548457267, 1697548457315, 1697548457374]"
1243,1243,862,33,[],200,llama-7b,64,1,818.0,1.0,1,A100,1697548508750,1697548509568,120,216.0,2.0,"[23, 795]","[1697548508773, 1697548509568]"
1244,1244,547,39,[],200,llama-7b,64,1,536.0,1.0,1,A100,1697548535737,1697548536273,120,12.0,1.0,"[8, 528]","[1697548535745, 1697548536273]"
1245,1245,323,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536277,1697548537862,120,,,"[25, 564, 72, 51, 61, 66, 63, 60, 47]","[1697548536302, 1697548536866, 1697548536938, 1697548536989, 1697548537050, 1697548537116, 1697548537179, 1697548537239, 1697548537286]"
1246,1246,264,13,[],200,llama-7b,64,1,1992.0,1.0,1,A100,1697548457376,1697548459368,120,86.0,20.0,"[18, 298, 66, 50, 62, 62, 49, 49, 61, 525, 72, 62, 48, 49, 50, 204, 46, 58, 49, 57, 57]","[1697548457394, 1697548457692, 1697548457758, 1697548457808, 1697548457870, 1697548457932, 1697548457981, 1697548458030, 1697548458091, 1697548458616, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458897, 1697548459101, 1697548459147, 1697548459205, 1697548459254, 1697548459311, 1697548459368]"
1247,1247,638,34,[],200,llama-7b,64,1,3550.0,1.0,1,A100,1697548509570,1697548513120,120,88.0,20.0,"[6, 751, 66, 60, 59, 46, 59, 56, 230, 56, 43, 43, 51, 51, 656, 55, 55, 53, 54, 750, 350]","[1697548509576, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510617, 1697548510673, 1697548510903, 1697548510959, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511803, 1697548511858, 1697548511913, 1697548511966, 1697548512020, 1697548512770, 1697548513120]"
1248,1248,906,41,[],200,llama-7b,64,1,2904.0,1.0,1,A100,1697548537876,1697548540780,120,86.0,20.0,"[183, 773, 52, 55, 41, 47, 45, 678, 62, 55, 54, 44, 52, 41, 401, 49, 60, 47, 56, 58, 51]","[1697548538059, 1697548538832, 1697548538884, 1697548538939, 1697548538980, 1697548539027, 1697548539072, 1697548539750, 1697548539812, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540459, 1697548540508, 1697548540568, 1697548540615, 1697548540671, 1697548540729, 1697548540780]"
1249,1249,25,24,[],200,llama-7b,64,1,468.0,1.0,1,A100,1697548502662,1697548503130,120,12.0,1.0,"[6, 462]","[1697548502668, 1697548503130]"
1250,1250,614,25,[],200,llama-7b,64,1,1180.0,1.0,1,A100,1697548503133,1697548504313,120,15.0,1.0,"[11, 1169]","[1697548503144, 1697548504313]"
1251,1251,80,35,[],200,llama-7b,64,1,1070.0,1.0,1,A100,1697548513788,1697548514858,120,13.0,1.0,"[17, 1053]","[1697548513805, 1697548514858]"
1252,1252,665,36,[],200,llama-7b,64,1,4167.0,1.0,1,A100,1697548514861,1697548519028,120,90.0,20.0,"[18, 1190, 498, 270, 68, 66, 64, 64, 293, 66, 64, 59, 53, 673, 66, 65, 58, 57, 362, 63, 50]","[1697548514879, 1697548516069, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517634, 1697548518307, 1697548518373, 1697548518438, 1697548518496, 1697548518553, 1697548518915, 1697548518978, 1697548519028]"
1253,1253,928,17,[],200,llama-7b,64,1,427.0,1.0,1,A100,1697548493124,1697548493551,120,20.0,1.0,"[24, 403]","[1697548493148, 1697548493551]"
1254,1254,698,18,[],200,llama-7b,64,1,1062.0,1.0,1,A100,1697548493557,1697548494619,120,182.0,6.0,"[23, 407, 60, 52, 41, 47, 432]","[1697548493580, 1697548493987, 1697548494047, 1697548494099, 1697548494140, 1697548494187, 1697548494619]"
1255,1255,681,42,[],200,llama-7b,64,1,805.0,1.0,1,A100,1697548540786,1697548541591,120,23.0,1.0,"[63, 742]","[1697548540849, 1697548541591]"
1256,1256,335,43,[],200,llama-7b,64,1,9607.0,1.0,1,A100,1697548541597,1697548551204,120,58.0,62.0,"[39, 866, 245, 62, 62, 56, 45, 55, 692, 59, 47, 57, 55, 52, 788, 55, 54, 46, 951, 71, 66, 64, 62, 61, 48, 62, 759, 66, 64, 59, 49, 57, 46, 256, 54, 47, 370, 51, 47, 810, 61, 51, 59, 47, 58, 55, 395, 61, 47, 47, 48, 59, 56, 312, 64, 55, 49, 53, 371, 53, 52, 50, 49]","[1697548541636, 1697548542502, 1697548542747, 1697548542809, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544886, 1697548544932, 1697548545883, 1697548545954, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547076, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547371, 1697548547417, 1697548547673, 1697548547727, 1697548547774, 1697548548144, 1697548548195, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549778, 1697548549839, 1697548549886, 1697548549933, 1697548549981, 1697548550040, 1697548550096, 1697548550408, 1697548550472, 1697548550527, 1697548550576, 1697548550629, 1697548551000, 1697548551053, 1697548551105, 1697548551155, 1697548551204]"
1257,1257,610,16,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548486340,1697548488025,120,,,"[7, 301, 82, 60, 59, 79, 307, 71, 70]","[1697548486347, 1697548486648, 1697548486730, 1697548486790, 1697548486849, 1697548486928, 1697548487235, 1697548487306, 1697548487376]"
1258,1258,350,19,[],200,llama-7b,64,1,557.0,1.0,1,A100,1697548494622,1697548495179,120,216.0,1.0,"[6, 551]","[1697548494628, 1697548495179]"
1259,1259,127,20,[],200,llama-7b,64,1,769.0,1.0,1,A100,1697548495182,1697548495951,120,100.0,5.0,"[6, 539, 64, 55, 53, 52]","[1697548495188, 1697548495727, 1697548495791, 1697548495846, 1697548495899, 1697548495951]"
1260,1260,383,26,[],200,llama-7b,64,1,877.0,1.0,1,A100,1697548504316,1697548505193,120,15.0,1.0,"[19, 858]","[1697548504335, 1697548505193]"
1261,1261,39,27,[],200,llama-7b,64,1,585.0,1.0,1,A100,1697548505195,1697548505780,120,8.0,1.0,"[11, 574]","[1697548505206, 1697548505780]"
1262,1262,862,34,[],200,llama-7b,64,1,873.0,1.0,1,A100,1697548498915,1697548499788,120,216.0,2.0,"[6, 867]","[1697548498921, 1697548499788]"
1263,1263,515,35,[],200,llama-7b,64,1,634.0,1.0,1,A100,1697548499792,1697548500426,120,11.0,1.0,"[7, 626]","[1697548499799, 1697548500425]"
1264,1264,377,17,[],200,llama-7b,64,1,801.0,1.0,1,A100,1697548488032,1697548488833,120,13.0,1.0,"[104, 697]","[1697548488136, 1697548488833]"
1265,1265,38,18,[],200,llama-7b,64,1,2045.0,1.0,1,A100,1697548488836,1697548490881,120,88.0,20.0,"[42, 363, 57, 67, 50, 48, 41, 48, 40, 303, 52, 43, 43, 50, 49, 40, 41, 193, 375, 50, 49]","[1697548488878, 1697548489241, 1697548489298, 1697548489365, 1697548489415, 1697548489463, 1697548489504, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490406, 1697548490781, 1697548490831, 1697548490880]"
1266,1266,290,36,[],200,llama-7b,64,1,775.0,1.0,1,A100,1697548500429,1697548501204,120,14.0,1.0,"[29, 745]","[1697548500458, 1697548501203]"
1267,1267,442,37,[],200,llama-7b,64,1,3381.0,1.0,1,A100,1697548519030,1697548522411,120,39.0,22.0,"[19, 561, 185, 242, 67, 63, 62, 61, 60, 60, 337, 65, 63, 48, 49, 54, 405, 63, 47, 61, 46, 54, 708]","[1697548519049, 1697548519610, 1697548519795, 1697548520037, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520350, 1697548520410, 1697548520747, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521026, 1697548521431, 1697548521494, 1697548521541, 1697548521602, 1697548521648, 1697548521702, 1697548522410]"
1268,1268,872,37,[],200,llama-7b,64,1,3704.0,1.0,1,A100,1697548501207,1697548504911,120,91.0,20.0,"[13, 936, 254, 65, 64, 62, 59, 55, 875, 64, 59, 57, 52, 41, 705, 61, 48, 59, 60, 58, 57]","[1697548501220, 1697548502156, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503590, 1697548503654, 1697548503713, 1697548503770, 1697548503822, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504736, 1697548504796, 1697548504854, 1697548504911]"
1269,1269,638,36,[],200,llama-7b,64,1,2730.0,1.0,1,A100,1697548490882,1697548493612,120,88.0,20.0,"[14, 448, 131, 50, 48, 40, 49, 48, 328, 54, 53, 43, 51, 50, 49, 722, 59, 49, 48, 39, 357]","[1697548490896, 1697548491344, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491710, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492289, 1697548492338, 1697548493060, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493612]"
1270,1270,737,19,[],200,llama-7b,64,1,593.0,1.0,1,A100,1697548490883,1697548491476,120,216.0,2.0,"[28, 434, 131]","[1697548490911, 1697548491345, 1697548491476]"
1271,1271,395,20,[],200,llama-7b,64,1,2663.0,1.0,1,A100,1697548491478,1697548494141,120,88.0,20.0,"[14, 489, 58, 54, 52, 44, 51, 50, 49, 722, 58, 48, 49, 39, 356, 49, 38, 38, 311, 52, 42]","[1697548491492, 1697548491981, 1697548492039, 1697548492093, 1697548492145, 1697548492189, 1697548492240, 1697548492290, 1697548492339, 1697548493061, 1697548493119, 1697548493167, 1697548493216, 1697548493255, 1697548493611, 1697548493660, 1697548493698, 1697548493736, 1697548494047, 1697548494099, 1697548494141]"
1272,1272,216,15,[],200,llama-7b,64,1,4493.0,1.0,1,A100,1697548475329,1697548479822,120,91.0,20.0,"[7, 500, 482, 511, 289, 74, 57, 63, 617, 251, 55, 253, 70, 69, 61, 614, 254, 72, 54, 71, 69]","[1697548475336, 1697548475836, 1697548476318, 1697548476829, 1697548477118, 1697548477192, 1697548477249, 1697548477312, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478627, 1697548478688, 1697548479302, 1697548479556, 1697548479628, 1697548479682, 1697548479753, 1697548479822]"
1273,1273,333,23,[],200,llama-7b,64,1,1195.0,1.0,1,A100,1697548507664,1697548508859,120,563.0,11.0,"[19, 343, 72, 65, 64, 60, 59, 54, 42, 306, 57, 54]","[1697548507683, 1697548508026, 1697548508098, 1697548508163, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748, 1697548508805, 1697548508859]"
1274,1274,284,7,[],200,llama-7b,64,1,3681.0,1.0,1,A100,1697548488029,1697548491710,120,90.0,31.0,"[8, 851, 52, 35, 37, 285, 67, 50, 49, 40, 48, 40, 304, 52, 43, 42, 51, 49, 40, 41, 193, 374, 50, 49, 42, 553, 50, 49, 39, 49, 48]","[1697548488037, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489895, 1697548489947, 1697548489990, 1697548490032, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490406, 1697548490780, 1697548490830, 1697548490879, 1697548490921, 1697548491474, 1697548491524, 1697548491573, 1697548491612, 1697548491661, 1697548491709]"
1275,1275,179,16,[],200,llama-7b,64,1,1155.0,1.0,1,A100,1697548482485,1697548483640,120,161.0,4.0,"[11, 463, 549, 76, 56]","[1697548482496, 1697548482959, 1697548483508, 1697548483584, 1697548483640]"
1276,1276,768,39,[],200,llama-7b,64,1,1380.0,1.0,1,A100,1697548518849,1697548520229,120,47.0,6.0,"[16, 745, 184, 243, 67, 63, 62]","[1697548518865, 1697548519610, 1697548519794, 1697548520037, 1697548520104, 1697548520167, 1697548520229]"
1277,1277,570,17,[],200,llama-7b,64,1,530.0,1.0,1,A100,1697548486118,1697548486648,120,18.0,1.0,"[20, 510]","[1697548486138, 1697548486648]"
1278,1278,759,17,[],200,llama-7b,64,1,3591.0,1.0,1,A100,1697548483643,1697548487234,120,92.0,20.0,"[20, 485, 83, 76, 78, 72, 70, 57, 70, 838, 313, 308, 76, 76, 72, 70, 323, 59, 59, 79, 306]","[1697548483663, 1697548484148, 1697548484231, 1697548484307, 1697548484385, 1697548484457, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487233]"
1279,1279,343,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548486653,1697548488026,120,,,"[25, 1130]","[1697548486678, 1697548487808]"
1280,1280,4,19,[],200,llama-7b,64,1,2182.0,1.0,1,A100,1697548488033,1697548490215,120,89.0,20.0,"[149, 651, 55, 52, 35, 38, 284, 68, 49, 49, 40, 48, 40, 305, 51, 43, 43, 51, 48, 41, 42]","[1697548488182, 1697548488833, 1697548488888, 1697548488940, 1697548488975, 1697548489013, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490215]"
1281,1281,796,5,[],200,llama-7b,64,1,2185.0,1.0,1,A100,1697548488029,1697548490214,120,86.0,20.0,"[40, 763, 56, 52, 35, 38, 284, 67, 50, 49, 40, 49, 40, 303, 52, 43, 43, 50, 49, 40, 42]","[1697548488069, 1697548488832, 1697548488888, 1697548488940, 1697548488975, 1697548489013, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490214]"
1282,1282,54,8,[],200,llama-7b,64,1,3182.0,1.0,1,A100,1697548491713,1697548494895,120,87.0,20.0,"[6, 982, 360, 58, 49, 48, 39, 356, 49, 38, 38, 311, 52, 42, 46, 431, 60, 48, 59, 55, 55]","[1697548491719, 1697548492701, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493660, 1697548493698, 1697548493736, 1697548494047, 1697548494099, 1697548494141, 1697548494187, 1697548494618, 1697548494678, 1697548494726, 1697548494785, 1697548494840, 1697548494895]"
1283,1283,724,29,[],200,llama-7b,64,1,405.0,1.0,1,A100,1697548488836,1697548489241,120,11.0,1.0,"[26, 379]","[1697548488862, 1697548489241]"
1284,1284,356,30,[],200,llama-7b,64,1,652.0,1.0,1,A100,1697548489244,1697548489896,120,874.0,2.0,"[16, 570, 66]","[1697548489260, 1697548489830, 1697548489896]"
1285,1285,424,40,[],200,llama-7b,64,1,2555.0,1.0,1,A100,1697548520232,1697548522787,120,88.0,20.0,"[6, 437, 72, 65, 63, 49, 49, 53, 405, 62, 48, 60, 46, 54, 710, 69, 67, 67, 57, 66, 50]","[1697548520238, 1697548520675, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520973, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787]"
1286,1286,124,31,[],200,llama-7b,64,1,510.0,1.0,1,A100,1697548489898,1697548490408,120,83.0,2.0,"[6, 504]","[1697548489904, 1697548490408]"
1287,1287,714,32,[],200,llama-7b,64,1,3200.0,1.0,1,A100,1697548490411,1697548493611,120,83.0,20.0,"[6, 927, 130, 51, 48, 40, 48, 49, 329, 54, 52, 43, 51, 50, 49, 724, 59, 47, 48, 39, 356]","[1697548490417, 1697548491344, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492039, 1697548492093, 1697548492145, 1697548492188, 1697548492239, 1697548492289, 1697548492338, 1697548493062, 1697548493121, 1697548493168, 1697548493216, 1697548493255, 1697548493611]"
1288,1288,536,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548487237,1697548488026,120,,,"[15, 556]","[1697548487252, 1697548487808]"
1289,1289,186,19,[],200,llama-7b,64,1,3441.0,1.0,1,A100,1697548488033,1697548491474,120,123.0,22.0,"[210, 998, 56, 68, 49, 49, 40, 49, 39, 305, 51, 43, 42, 52, 48, 40, 41, 194, 374, 50, 48, 42, 553]","[1697548488243, 1697548489241, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489503, 1697548489552, 1697548489591, 1697548489896, 1697548489947, 1697548489990, 1697548490032, 1697548490084, 1697548490132, 1697548490172, 1697548490213, 1697548490407, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491474]"
1290,1290,419,14,[],200,llama-7b,64,1,3580.0,1.0,1,A100,1697548473669,1697548477249,120,88.0,20.0,"[6, 323, 73, 68, 67, 51, 66, 64, 63, 593, 73, 71, 69, 70, 67, 63, 862, 511, 288, 75, 57]","[1697548473675, 1697548473998, 1697548474071, 1697548474139, 1697548474206, 1697548474257, 1697548474323, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475187, 1697548475256, 1697548475326, 1697548475393, 1697548475456, 1697548476318, 1697548476829, 1697548477117, 1697548477192, 1697548477249]"
1291,1291,643,9,[],200,llama-7b,64,1,283.0,1.0,1,A100,1697548494897,1697548495180,120,18.0,1.0,"[6, 276]","[1697548494903, 1697548495179]"
1292,1292,483,33,[],200,llama-7b,64,1,2178.0,1.0,1,A100,1697548493614,1697548495792,120,84.0,20.0,"[21, 352, 60, 52, 42, 46, 431, 60, 47, 61, 54, 56, 50, 298, 49, 57, 60, 54, 52, 42, 234]","[1697548493635, 1697548493987, 1697548494047, 1697548494099, 1697548494141, 1697548494187, 1697548494618, 1697548494678, 1697548494725, 1697548494786, 1697548494840, 1697548494896, 1697548494946, 1697548495244, 1697548495293, 1697548495350, 1697548495410, 1697548495464, 1697548495516, 1697548495558, 1697548495792]"
1293,1293,412,10,[],200,llama-7b,64,1,1280.0,1.0,1,A100,1697548495182,1697548496462,120,244.0,9.0,"[7, 538, 64, 55, 53, 52, 51, 379, 40, 41]","[1697548495189, 1697548495727, 1697548495791, 1697548495846, 1697548495899, 1697548495951, 1697548496002, 1697548496381, 1697548496421, 1697548496462]"
1294,1294,776,25,[],200,llama-7b,64,1,433.0,1.0,1,A100,1697548493615,1697548494048,120,67.0,2.0,"[41, 331, 61]","[1697548493656, 1697548493987, 1697548494048]"
1295,1295,43,11,[],200,llama-7b,64,1,910.0,1.0,1,A100,1697548496465,1697548497375,120,732.0,8.0,"[6, 460, 77, 69, 63, 64, 59, 57, 55]","[1697548496471, 1697548496931, 1697548497008, 1697548497077, 1697548497140, 1697548497204, 1697548497263, 1697548497320, 1697548497375]"
1296,1296,436,26,[],200,llama-7b,64,1,1952.0,1.0,1,A100,1697548494050,1697548496002,120,86.0,20.0,"[10, 492, 67, 59, 48, 60, 55, 54, 51, 297, 49, 59, 59, 53, 53, 42, 234, 55, 52, 52, 51]","[1697548494060, 1697548494552, 1697548494619, 1697548494678, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495351, 1697548495410, 1697548495463, 1697548495516, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496002]"
1297,1297,201,27,[],200,llama-7b,64,1,2855.0,1.0,1,A100,1697548496006,1697548498861,120,67.0,20.0,"[20, 905, 78, 67, 63, 61, 63, 57, 55, 262, 60, 49, 46, 45, 400, 61, 52, 51, 50, 351, 59]","[1697548496026, 1697548496931, 1697548497009, 1697548497076, 1697548497139, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861]"
1298,1298,742,12,[],200,llama-7b,64,1,3285.0,1.0,1,A100,1697548497377,1697548500662,120,89.0,20.0,"[7, 787, 66, 61, 52, 52, 50, 350, 60, 50, 50, 826, 61, 62, 59, 55, 51, 421, 65, 50, 49]","[1697548497384, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498402, 1697548498452, 1697548498802, 1697548498862, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500497, 1697548500562, 1697548500612, 1697548500661]"
1299,1299,19,26,[],200,llama-7b,64,1,10533.0,1.0,1,A100,1697548498456,1697548508989,120,563.0,72.0,"[11, 953, 368, 62, 61, 59, 55, 51, 420, 65, 51, 51, 57, 54, 724, 63, 61, 57, 56, 54, 620, 66, 63, 63, 59, 55, 876, 63, 58, 57, 52, 42, 704, 62, 47, 60, 59, 58, 57, 351, 66, 51, 61, 60, 59, 284, 45, 55, 51, 679, 65, 59, 55, 43, 581, 64, 62, 59, 56, 57, 324, 65, 63, 61, 59, 54, 43, 305, 57, 55, 49, 40, 41]","[1697548498467, 1697548499420, 1697548499788, 1697548499850, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500496, 1697548500561, 1697548500612, 1697548500663, 1697548500720, 1697548500774, 1697548501498, 1697548501561, 1697548501622, 1697548501679, 1697548501735, 1697548501789, 1697548502409, 1697548502475, 1697548502538, 1697548502601, 1697548502660, 1697548502715, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504853, 1697548504910, 1697548505261, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505558, 1697548505842, 1697548505887, 1697548505942, 1697548505993, 1697548506672, 1697548506737, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508225, 1697548508286, 1697548508345, 1697548508399, 1697548508442, 1697548508747, 1697548508804, 1697548508859, 1697548508908, 1697548508948, 1697548508989]"
1300,1300,752,18,[],200,llama-7b,64,1,1396.0,1.0,1,A100,1697548498454,1697548499850,120,39.0,3.0,"[7, 958, 369, 62]","[1697548498461, 1697548499419, 1697548499788, 1697548499850]"
1301,1301,405,19,[],200,llama-7b,64,1,3741.0,1.0,1,A100,1697548499851,1697548503592,120,87.0,20.0,"[10, 564, 72, 65, 51, 48, 59, 54, 723, 64, 62, 57, 54, 56, 620, 65, 64, 62, 59, 55, 876]","[1697548499861, 1697548500425, 1697548500497, 1697548500562, 1697548500613, 1697548500661, 1697548500720, 1697548500774, 1697548501497, 1697548501561, 1697548501623, 1697548501680, 1697548501734, 1697548501790, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503591]"
1302,1302,791,28,[],200,llama-7b,64,1,9482.0,1.0,1,A100,1697548498863,1697548508345,120,182.0,64.0,"[9, 916, 62, 61, 59, 55, 51, 420, 65, 51, 49, 58, 55, 724, 63, 61, 58, 55, 55, 620, 65, 64, 62, 59, 55, 876, 63, 58, 57, 52, 42, 704, 62, 47, 60, 59, 58, 57, 352, 65, 51, 61, 60, 60, 283, 45, 55, 51, 679, 65, 59, 55, 43, 581, 64, 62, 59, 56, 57, 324, 65, 64, 60, 59]","[1697548498872, 1697548499788, 1697548499850, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500496, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501498, 1697548501561, 1697548501622, 1697548501680, 1697548501735, 1697548501790, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504853, 1697548504910, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505887, 1697548505942, 1697548505993, 1697548506672, 1697548506737, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345]"
1303,1303,183,20,[],200,llama-7b,64,1,6857.0,1.0,1,A100,1697548503595,1697548510452,120,17.0,50.0,"[26, 692, 255, 61, 48, 60, 59, 58, 57, 351, 65, 51, 61, 60, 60, 283, 45, 55, 52, 678, 65, 59, 55, 43, 581, 64, 62, 59, 56, 57, 324, 65, 64, 60, 59, 54, 43, 305, 57, 55, 49, 41, 40, 579, 57, 54, 52, 53, 50, 559, 59]","[1697548503621, 1697548504313, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505887, 1697548505942, 1697548505994, 1697548506672, 1697548506737, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508399, 1697548508442, 1697548508747, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989, 1697548509568, 1697548509625, 1697548509679, 1697548509731, 1697548509784, 1697548509834, 1697548510393, 1697548510452]"
1304,1304,362,9,[],200,llama-7b,64,1,802.0,1.0,1,A100,1697548488030,1697548488832,120,14.0,1.0,"[16, 786]","[1697548488046, 1697548488832]"
1305,1305,139,10,[],200,llama-7b,64,1,2086.0,1.0,1,A100,1697548488835,1697548490921,120,39.0,21.0,"[23, 383, 56, 67, 50, 49, 40, 49, 40, 303, 52, 43, 43, 50, 49, 40, 41, 194, 374, 50, 49, 41]","[1697548488858, 1697548489241, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490407, 1697548490781, 1697548490831, 1697548490880, 1697548490921]"
1306,1306,95,38,[],200,llama-7b,64,1,615.0,1.0,1,A100,1697548522415,1697548523030,120,12.0,1.0,"[6, 609]","[1697548522421, 1697548523030]"
1307,1307,769,28,[],200,llama-7b,64,1,2403.0,1.0,1,A100,1697548495295,1697548497698,120,47.0,20.0,"[10, 422, 64, 55, 53, 52, 51, 379, 40, 40, 51, 50, 445, 70, 63, 61, 62, 57, 55, 262, 60]","[1697548495305, 1697548495727, 1697548495791, 1697548495846, 1697548495899, 1697548495951, 1697548496002, 1697548496381, 1697548496421, 1697548496461, 1697548496512, 1697548496562, 1697548497007, 1697548497077, 1697548497140, 1697548497201, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697]"
1308,1308,771,39,[],200,llama-7b,64,1,2822.0,1.0,1,A100,1697548523033,1697548525855,120,47.0,20.0,"[16, 616, 73, 52, 67, 52, 63, 64, 553, 64, 66, 65, 59, 58, 589, 67, 65, 51, 50, 65, 66]","[1697548523049, 1697548523665, 1697548523738, 1697548523790, 1697548523857, 1697548523909, 1697548523972, 1697548524036, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525490, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525854]"
1309,1309,468,25,[],200,llama-7b,64,1,3902.0,1.0,1,A100,1697548500665,1697548504567,120,31.0,20.0,"[23, 516, 294, 63, 61, 58, 55, 55, 621, 64, 63, 62, 60, 55, 877, 62, 58, 57, 53, 41, 704]","[1697548500688, 1697548501204, 1697548501498, 1697548501561, 1697548501622, 1697548501680, 1697548501735, 1697548501790, 1697548502411, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502715, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567]"
1310,1310,519,35,[],200,llama-7b,64,1,6437.0,1.0,1,A100,1697548527826,1697548534263,120,58.0,47.0,"[123, 947, 59, 51, 51, 48, 47, 39, 365, 52, 49, 41, 41, 41, 49, 347, 55, 49, 59, 59, 51, 486, 39, 472, 64, 64, 62, 59, 51, 737, 53, 58, 64, 51, 55, 576, 64, 63, 50, 63, 58, 46, 311, 60, 45, 46, 59, 57]","[1697548527949, 1697548528896, 1697548528955, 1697548529006, 1697548529057, 1697548529105, 1697548529152, 1697548529191, 1697548529556, 1697548529608, 1697548529657, 1697548529698, 1697548529739, 1697548529780, 1697548529829, 1697548530176, 1697548530231, 1697548530280, 1697548530339, 1697548530398, 1697548530449, 1697548530935, 1697548530974, 1697548531446, 1697548531510, 1697548531574, 1697548531636, 1697548531695, 1697548531746, 1697548532483, 1697548532536, 1697548532594, 1697548532658, 1697548532709, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533580, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534262]"
1311,1311,562,29,[],200,llama-7b,64,1,5939.0,1.0,1,A100,1697548508351,1697548514290,120,67.0,39.0,"[9, 319, 69, 57, 54, 50, 40, 41, 578, 57, 54, 53, 53, 49, 560, 59, 59, 46, 58, 57, 230, 57, 42, 43, 51, 51, 654, 57, 54, 53, 51, 753, 350, 59, 55, 43, 54, 833, 66, 60]","[1697548508360, 1697548508679, 1697548508748, 1697548508805, 1697548508859, 1697548508909, 1697548508949, 1697548508990, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509834, 1697548510394, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510673, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511801, 1697548511858, 1697548511912, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514163, 1697548514229, 1697548514289]"
1312,1312,908,12,[],200,llama-7b,64,1,8290.0,1.0,1,A100,1697548471267,1697548479557,120,6.0,50.0,"[14, 470, 78, 73, 74, 72, 67, 63, 49, 304, 71, 68, 68, 53, 65, 49, 530, 67, 49, 50, 65, 59, 346, 68, 67, 51, 66, 64, 63, 593, 73, 71, 70, 69, 67, 64, 860, 513, 287, 75, 56, 64, 617, 251, 55, 253, 71, 68, 61, 614, 255]","[1697548471281, 1697548471751, 1697548471829, 1697548471902, 1697548471976, 1697548472048, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472791, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473601, 1697548473666, 1697548473725, 1697548474071, 1697548474139, 1697548474206, 1697548474257, 1697548474323, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475393, 1697548475457, 1697548476317, 1697548476830, 1697548477117, 1697548477192, 1697548477248, 1697548477312, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478559, 1697548478627, 1697548478688, 1697548479302, 1697548479557]"
1313,1313,165,21,[],200,llama-7b,64,1,1860.0,1.0,1,A100,1697548494143,1697548496003,120,83.0,20.0,"[16, 393, 67, 60, 47, 60, 55, 54, 52, 296, 49, 59, 58, 55, 53, 41, 234, 55, 52, 52, 52]","[1697548494159, 1697548494552, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494947, 1697548495243, 1697548495292, 1697548495351, 1697548495409, 1697548495464, 1697548495517, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496003]"
1314,1314,585,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548481093,1697548488026,120,,,"[24, 406, 289, 268, 64, 74, 264, 68, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 79, 72, 69, 57, 70, 839, 314, 308, 76, 76, 70, 70, 325, 59, 59, 79, 307, 70, 69]","[1697548481117, 1697548481523, 1697548481812, 1697548482080, 1697548482144, 1697548482218, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484385, 1697548484457, 1697548484526, 1697548484583, 1697548484653, 1697548485492, 1697548485806, 1697548486114, 1697548486190, 1697548486266, 1697548486336, 1697548486406, 1697548486731, 1697548486790, 1697548486849, 1697548486928, 1697548487235, 1697548487305, 1697548487374]"
1315,1315,184,13,[],200,llama-7b,64,1,2515.0,1.0,1,A100,1697548467632,1697548470147,120,87.0,20.0,"[11, 328, 81, 72, 53, 72, 70, 64, 534, 65, 52, 66, 65, 62, 59, 569, 70, 53, 67, 49, 52]","[1697548467643, 1697548467971, 1697548468052, 1697548468124, 1697548468177, 1697548468249, 1697548468319, 1697548468383, 1697548468917, 1697548468982, 1697548469034, 1697548469100, 1697548469165, 1697548469227, 1697548469286, 1697548469855, 1697548469925, 1697548469978, 1697548470045, 1697548470094, 1697548470146]"
1316,1316,216,30,[],200,llama-7b,64,1,4147.0,1.0,1,A100,1697548514291,1697548518438,120,91.0,20.0,"[7, 560, 467, 71, 70, 64, 61, 975, 271, 69, 64, 65, 63, 294, 66, 65, 59, 52, 673, 66, 65]","[1697548514298, 1697548514858, 1697548515325, 1697548515396, 1697548515466, 1697548515530, 1697548515591, 1697548516566, 1697548516837, 1697548516906, 1697548516970, 1697548517035, 1697548517098, 1697548517392, 1697548517458, 1697548517523, 1697548517582, 1697548517634, 1697548518307, 1697548518373, 1697548518438]"
1317,1317,173,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548534266,1697548537856,120,,,"[22, 888, 382, 59, 60, 58, 55, 668, 62, 49, 54, 315, 52, 60, 66, 63, 60, 46]","[1697548534288, 1697548535176, 1697548535558, 1697548535617, 1697548535677, 1697548535735, 1697548535790, 1697548536458, 1697548536520, 1697548536569, 1697548536623, 1697548536938, 1697548536990, 1697548537050, 1697548537116, 1697548537179, 1697548537239, 1697548537285]"
1318,1318,362,21,[],200,llama-7b,64,1,800.0,1.0,1,A100,1697548488032,1697548488832,120,14.0,1.0,"[115, 685]","[1697548488147, 1697548488832]"
1319,1319,217,23,[],200,llama-7b,64,1,2882.0,1.0,1,A100,1697548497143,1697548500025,120,85.0,20.0,"[6, 423, 65, 61, 48, 46, 45, 400, 61, 52, 52, 50, 350, 59, 51, 49, 826, 62, 62, 59, 55]","[1697548497149, 1697548497572, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498402, 1697548498452, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499787, 1697548499849, 1697548499911, 1697548499970, 1697548500025]"
1320,1320,766,14,[],200,llama-7b,64,1,555.0,1.0,1,A100,1697548470150,1697548470705,120,11.0,1.0,"[13, 542]","[1697548470163, 1697548470705]"
1321,1321,537,15,[],200,llama-7b,64,1,2957.0,1.0,1,A100,1697548470709,1697548473666,120,83.0,20.0,"[15, 1027, 77, 74, 73, 72, 68, 63, 49, 304, 71, 68, 68, 52, 67, 48, 529, 68, 49, 50, 65]","[1697548470724, 1697548471751, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472790, 1697548472857, 1697548472905, 1697548473434, 1697548473502, 1697548473551, 1697548473601, 1697548473666]"
1322,1322,15,22,[],200,llama-7b,64,1,2045.0,1.0,1,A100,1697548488835,1697548490880,120,100.0,20.0,"[18, 388, 56, 67, 50, 49, 40, 49, 40, 303, 52, 43, 43, 50, 49, 40, 41, 194, 374, 50, 49]","[1697548488853, 1697548489241, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490407, 1697548490781, 1697548490831, 1697548490880]"
1323,1323,716,12,[],200,llama-7b,64,1,4808.0,1.0,1,A100,1697548466313,1697548471121,120,79.0,30.0,"[14, 971, 74, 71, 67, 68, 51, 63, 359, 72, 54, 71, 69, 65, 536, 65, 52, 66, 64, 63, 58, 570, 69, 52, 67, 52, 52, 64, 64, 845]","[1697548466327, 1697548467298, 1697548467372, 1697548467443, 1697548467510, 1697548467578, 1697548467629, 1697548467692, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468317, 1697548468382, 1697548468918, 1697548468983, 1697548469035, 1697548469101, 1697548469165, 1697548469228, 1697548469286, 1697548469856, 1697548469925, 1697548469977, 1697548470044, 1697548470096, 1697548470148, 1697548470212, 1697548470276, 1697548471121]"
1324,1324,199,16,[],200,llama-7b,64,1,327.0,1.0,1,A100,1697548473671,1697548473998,120,13.0,1.0,"[19, 308]","[1697548473690, 1697548473998]"
1325,1325,897,17,[],200,llama-7b,64,1,760.0,1.0,1,A100,1697548474001,1697548474761,120,9.0,1.0,"[20, 739]","[1697548474021, 1697548474760]"
1326,1326,556,18,[],200,llama-7b,64,1,1074.0,1.0,1,A100,1697548474763,1697548475837,120,9.0,1.0,"[15, 1058]","[1697548474778, 1697548475836]"
1327,1327,329,19,[],200,llama-7b,64,1,1838.0,1.0,1,A100,1697548475841,1697548477679,120,15.0,1.0,"[15, 1823]","[1697548475856, 1697548477679]"
1328,1328,919,20,[],200,llama-7b,64,1,1408.0,1.0,1,A100,1697548477683,1697548479091,120,14.0,1.0,"[25, 1383]","[1697548477708, 1697548479091]"
1329,1329,898,22,[],200,llama-7b,64,1,434.0,1.0,1,A100,1697548493614,1697548494048,120,79.0,2.0,"[36, 398]","[1697548493650, 1697548494048]"
1330,1330,552,23,[],200,llama-7b,64,1,1953.0,1.0,1,A100,1697548494050,1697548496003,120,87.0,20.0,"[15, 487, 67, 60, 47, 60, 55, 54, 52, 296, 49, 59, 58, 54, 54, 41, 234, 55, 52, 52, 52]","[1697548494065, 1697548494552, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494947, 1697548495243, 1697548495292, 1697548495351, 1697548495409, 1697548495463, 1697548495517, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496003]"
1331,1331,115,32,[],200,llama-7b,64,1,838.0,1.0,1,A100,1697548505562,1697548506400,120,13.0,1.0,"[35, 803]","[1697548505597, 1697548506400]"
1332,1332,700,33,[],200,llama-7b,64,1,4499.0,1.0,1,A100,1697548506403,1697548510902,120,140.0,33.0,"[14, 1059, 63, 62, 59, 56, 57, 325, 65, 64, 61, 58, 54, 42, 306, 56, 55, 49, 41, 40, 580, 56, 54, 52, 54, 49, 559, 59, 59, 47, 58, 56, 230]","[1697548506417, 1697548507476, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508098, 1697548508163, 1697548508227, 1697548508288, 1697548508346, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989, 1697548509569, 1697548509625, 1697548509679, 1697548509731, 1697548509785, 1697548509834, 1697548510393, 1697548510452, 1697548510511, 1697548510558, 1697548510616, 1697548510672, 1697548510902]"
1333,1333,690,21,[],200,llama-7b,64,1,1207.0,1.0,1,A100,1697548479095,1697548480302,120,39.0,1.0,"[29, 1178]","[1697548479124, 1697548480302]"
1334,1334,344,22,[],200,llama-7b,64,1,1218.0,1.0,1,A100,1697548480304,1697548481522,120,13.0,1.0,"[6, 1212]","[1697548480310, 1697548481522]"
1335,1335,92,23,[],200,llama-7b,64,1,4740.0,1.0,1,A100,1697548481525,1697548486265,120,85.0,20.0,"[14, 1419, 551, 75, 55, 72, 66, 66, 49, 338, 77, 78, 72, 69, 57, 71, 838, 314, 309, 75, 75]","[1697548481539, 1697548482958, 1697548483509, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484307, 1697548484385, 1697548484457, 1697548484526, 1697548484583, 1697548484654, 1697548485492, 1697548485806, 1697548486115, 1697548486190, 1697548486265]"
1336,1336,320,24,[],200,llama-7b,64,1,5554.0,1.0,1,A100,1697548496007,1697548501561,120,109.0,36.0,"[47, 954, 68, 64, 61, 62, 57, 55, 262, 61, 48, 46, 45, 400, 62, 51, 52, 49, 351, 59, 51, 49, 826, 62, 62, 59, 54, 51, 422, 64, 51, 49, 58, 55, 723, 64]","[1697548496054, 1697548497008, 1697548497076, 1697548497140, 1697548497201, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499787, 1697548499849, 1697548499911, 1697548499970, 1697548500024, 1697548500075, 1697548500497, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501497, 1697548501561]"
1337,1337,434,6,[],200,llama-7b,64,1,2182.0,1.0,1,A100,1697548488033,1697548490215,120,85.0,20.0,"[143, 656, 56, 52, 35, 38, 284, 68, 49, 49, 41, 47, 40, 305, 51, 43, 43, 51, 48, 41, 42]","[1697548488176, 1697548488832, 1697548488888, 1697548488940, 1697548488975, 1697548489013, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489504, 1697548489551, 1697548489591, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490215]"
1338,1338,87,7,[],200,llama-7b,64,1,2844.0,1.0,1,A100,1697548490218,1697548493062,120,335.0,19.0,"[37, 471, 55, 50, 48, 42, 554, 50, 48, 40, 48, 49, 328, 55, 52, 43, 51, 51, 48, 723]","[1697548490255, 1697548490726, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492038, 1697548492093, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492338, 1697548493061]"
1339,1339,292,35,[],200,llama-7b,64,1,660.0,1.0,1,A100,1697548513123,1697548513783,120,286.0,1.0,"[6, 654]","[1697548513129, 1697548513783]"
1340,1340,469,34,[],200,llama-7b,64,1,654.0,1.0,1,A100,1697548510906,1697548511560,120,17.0,1.0,"[15, 639]","[1697548510921, 1697548511560]"
1341,1341,130,35,[],200,llama-7b,64,1,896.0,1.0,1,A100,1697548511563,1697548512459,120,14.0,1.0,"[32, 864]","[1697548511595, 1697548512459]"
1342,1342,760,17,[],200,llama-7b,64,1,8747.0,1.0,1,A100,1697548465323,1697548474070,120,335.0,64.0,"[16, 563, 337, 72, 69, 53, 65, 63, 48, 689, 74, 70, 68, 67, 51, 63, 360, 73, 54, 70, 71, 64, 535, 65, 51, 66, 65, 62, 60, 568, 69, 53, 67, 51, 52, 65, 64, 844, 73, 72, 69, 67, 50, 65, 312, 74, 73, 72, 67, 64, 49, 304, 70, 69, 68, 52, 65, 49, 531, 66, 50, 51, 64, 59, 345]","[1697548465339, 1697548465902, 1697548466239, 1697548466311, 1697548466380, 1697548466433, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468124, 1697548468178, 1697548468248, 1697548468319, 1697548468383, 1697548468918, 1697548468983, 1697548469034, 1697548469100, 1697548469165, 1697548469227, 1697548469287, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472227, 1697548472531, 1697548472601, 1697548472670, 1697548472738, 1697548472790, 1697548472855, 1697548472904, 1697548473435, 1697548473501, 1697548473551, 1697548473602, 1697548473666, 1697548473725, 1697548474070]"
1343,1343,831,36,[],200,llama-7b,64,1,1322.0,1.0,1,A100,1697548512460,1697548513782,120,11.0,1.0,"[26, 1296]","[1697548512486, 1697548513782]"
1344,1344,579,37,[],200,llama-7b,64,1,1074.0,1.0,1,A100,1697548513784,1697548514858,120,19.0,1.0,"[6, 1068]","[1697548513790, 1697548514858]"
1345,1345,794,8,[],200,llama-7b,64,1,485.0,1.0,1,A100,1697548493065,1697548493550,120,11.0,1.0,"[24, 461]","[1697548493089, 1697548493550]"
1346,1346,232,38,[],200,llama-7b,64,1,4168.0,1.0,1,A100,1697548514860,1697548519028,120,93.0,20.0,"[7, 1201, 499, 270, 68, 65, 65, 64, 293, 66, 64, 59, 53, 673, 66, 65, 58, 57, 362, 62, 50]","[1697548514867, 1697548516068, 1697548516567, 1697548516837, 1697548516905, 1697548516970, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517634, 1697548518307, 1697548518373, 1697548518438, 1697548518496, 1697548518553, 1697548518915, 1697548518977, 1697548519027]"
1347,1347,449,9,[],200,llama-7b,64,1,2236.0,1.0,1,A100,1697548493555,1697548495791,120,86.0,20.0,"[15, 416, 61, 52, 41, 46, 433, 60, 47, 60, 55, 55, 51, 297, 48, 59, 59, 53, 53, 42, 233]","[1697548493570, 1697548493986, 1697548494047, 1697548494099, 1697548494140, 1697548494186, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494896, 1697548494947, 1697548495244, 1697548495292, 1697548495351, 1697548495410, 1697548495463, 1697548495516, 1697548495558, 1697548495791]"
1348,1348,880,25,[],200,llama-7b,64,1,846.0,1.0,1,A100,1697548501564,1697548502410,120,84.0,2.0,"[11, 582, 253]","[1697548501575, 1697548502157, 1697548502410]"
1349,1349,886,20,[],200,llama-7b,64,1,428.0,1.0,1,A100,1697548493123,1697548493551,120,17.0,1.0,"[27, 401]","[1697548493150, 1697548493551]"
1350,1350,650,26,[],200,llama-7b,64,1,718.0,1.0,1,A100,1697548502412,1697548503130,120,13.0,1.0,"[20, 698]","[1697548502432, 1697548503130]"
1351,1351,663,21,[],200,llama-7b,64,1,2234.0,1.0,1,A100,1697548493557,1697548495791,120,79.0,20.0,"[30, 460, 52, 42, 46, 432, 59, 47, 61, 54, 56, 51, 297, 49, 58, 59, 54, 52, 42, 233]","[1697548493587, 1697548494047, 1697548494099, 1697548494141, 1697548494187, 1697548494619, 1697548494678, 1697548494725, 1697548494786, 1697548494840, 1697548494896, 1697548494947, 1697548495244, 1697548495293, 1697548495351, 1697548495410, 1697548495464, 1697548495516, 1697548495558, 1697548495791]"
1352,1352,309,27,[],200,llama-7b,64,1,3604.0,1.0,1,A100,1697548503133,1697548506737,120,52.0,20.0,"[14, 1166, 254, 62, 47, 61, 58, 59, 57, 350, 66, 51, 61, 60, 60, 283, 45, 55, 52, 678, 65]","[1697548503147, 1697548504313, 1697548504567, 1697548504629, 1697548504676, 1697548504737, 1697548504795, 1697548504854, 1697548504911, 1697548505261, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505887, 1697548505942, 1697548505994, 1697548506672, 1697548506737]"
1353,1353,709,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548495960,1697548527820,120,,,"[6, 359, 56, 40, 41, 51, 49, 447, 67, 63, 61, 63, 57, 55, 262, 60, 48, 47, 45, 400, 62, 51, 51, 50, 351, 59, 51, 49, 826, 62, 61, 60, 54, 51, 421, 65, 51, 49, 58, 55, 723, 63, 61, 58, 55, 55, 620, 66, 63, 62, 60, 54, 876, 64, 57, 57, 53, 41, 704, 63, 47, 60, 59, 58, 57, 351, 66, 51, 61, 60, 59, 284, 45, 55, 51, 679, 65, 59, 55, 43, 581, 64, 62, 59, 56, 57, 324, 65, 64, 60, 59, 54, 43, 305, 57, 55, 49, 40, 40, 580, 57, 53, 53, 53, 50, 559, 59, 59, 46, 59, 56, 230, 57, 42, 43, 52, 50, 655, 57, 54, 53, 51, 753, 350, 58, 56, 43, 53, 834, 66, 60, 58, 58, 919, 71, 70, 64, 61, 976, 271, 68, 65, 65, 63, 293, 67, 64, 59, 53, 672, 66, 64, 59, 57, 362, 63, 50, 58, 57, 44, 608, 242, 68, 62, 62, 62, 60, 60, 336, 66, 63, 48, 49, 53, 405, 63, 47, 61, 46, 54, 709, 70, 67, 67, 57, 66, 49, 317, 67, 64, 64, 62, 61, 317, 51, 68, 51, 63, 64, 553, 65, 65, 65, 60, 57, 588, 68, 65, 51, 50, 65, 66, 241, 49, 47, 60, 59, 654, 66, 49, 65, 64, 64]","[1697548495966, 1697548496325, 1697548496381, 1697548496421, 1697548496462, 1697548496513, 1697548496562, 1697548497009, 1697548497076, 1697548497139, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499787, 1697548499849, 1697548499910, 1697548499970, 1697548500024, 1697548500075, 1697548500496, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501497, 1697548501560, 1697548501621, 1697548501679, 1697548501734, 1697548501789, 1697548502409, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502714, 1697548503590, 1697548503654, 1697548503711, 1697548503768, 1697548503821, 1697548503862, 1697548504566, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504853, 1697548504910, 1697548505261, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505558, 1697548505842, 1697548505887, 1697548505942, 1697548505993, 1697548506672, 1697548506737, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508399, 1697548508442, 1697548508747, 1697548508804, 1697548508859, 1697548508908, 1697548508948, 1697548508988, 1697548509568, 1697548509625, 1697548509678, 1697548509731, 1697548509784, 1697548509834, 1697548510393, 1697548510452, 1697548510511, 1697548510557, 1697548510616, 1697548510672, 1697548510902, 1697548510959, 1697548511001, 1697548511044, 1697548511096, 1697548511146, 1697548511801, 1697548511858, 1697548511912, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513177, 1697548513233, 1697548513276, 1697548513329, 1697548514163, 1697548514229, 1697548514289, 1697548514347, 1697548514405, 1697548515324, 1697548515395, 1697548515465, 1697548515529, 1697548515590, 1697548516566, 1697548516837, 1697548516905, 1697548516970, 1697548517035, 1697548517098, 1697548517391, 1697548517458, 1697548517522, 1697548517581, 1697548517634, 1697548518306, 1697548518372, 1697548518436, 1697548518495, 1697548518552, 1697548518914, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519186, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521025, 1697548521430, 1697548521493, 1697548521540, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522786, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523789, 1697548523857, 1697548523908, 1697548523971, 1697548524035, 1697548524588, 1697548524653, 1697548524718, 1697548524783, 1697548524843, 1697548524900, 1697548525488, 1697548525556, 1697548525621, 1697548525672, 1697548525722, 1697548525787, 1697548525853, 1697548526094, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526963, 1697548527029, 1697548527078, 1697548527143, 1697548527207, 1697548527271]"
1354,1354,737,28,[],200,llama-7b,64,1,891.0,1.0,1,A100,1697548505782,1697548506673,120,216.0,2.0,"[14, 604, 273]","[1697548505796, 1697548506400, 1697548506673]"
1355,1355,3,39,[],200,llama-7b,64,1,2618.0,1.0,1,A100,1697548519030,1697548521648,120,89.0,20.0,"[12, 568, 185, 242, 67, 63, 62, 61, 60, 60, 337, 65, 63, 48, 49, 54, 405, 63, 47, 61, 46]","[1697548519042, 1697548519610, 1697548519795, 1697548520037, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520350, 1697548520410, 1697548520747, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521026, 1697548521431, 1697548521494, 1697548521541, 1697548521602, 1697548521648]"
1356,1356,393,29,[],200,llama-7b,64,1,2949.0,1.0,1,A100,1697548506676,1697548509625,120,182.0,22.0,"[6, 600, 194, 63, 62, 59, 56, 57, 324, 65, 64, 60, 59, 55, 42, 306, 56, 55, 49, 41, 40, 580, 56]","[1697548506682, 1697548507282, 1697548507476, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989, 1697548509569, 1697548509625]"
1357,1357,170,30,[],200,llama-7b,64,1,2232.0,1.0,1,A100,1697548509627,1697548511859,120,335.0,15.0,"[13, 687, 66, 60, 59, 46, 58, 57, 230, 56, 43, 43, 51, 51, 656, 56]","[1697548509640, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510673, 1697548510903, 1697548510959, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511803, 1697548511859]"
1358,1358,753,31,[],200,llama-7b,64,1,5044.0,1.0,1,A100,1697548511862,1697548516906,120,83.0,20.0,"[6, 591, 311, 350, 58, 55, 44, 53, 833, 67, 60, 58, 57, 920, 71, 71, 63, 61, 976, 270, 68]","[1697548511868, 1697548512459, 1697548512770, 1697548513120, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514163, 1697548514230, 1697548514290, 1697548514348, 1697548514405, 1697548515325, 1697548515396, 1697548515467, 1697548515530, 1697548515591, 1697548516567, 1697548516837, 1697548516905]"
1359,1359,78,28,[],200,llama-7b,64,1,2250.0,1.0,1,A100,1697548506740,1697548508990,120,84.0,20.0,"[20, 522, 194, 64, 61, 59, 56, 57, 324, 65, 64, 60, 59, 55, 42, 305, 57, 55, 50, 40, 40]","[1697548506760, 1697548507282, 1697548507476, 1697548507540, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508747, 1697548508804, 1697548508859, 1697548508909, 1697548508949, 1697548508989]"
1360,1360,500,32,[],200,llama-7b,64,1,1645.0,1.0,1,A100,1697548516908,1697548518553,120,335.0,11.0,"[14, 399, 72, 66, 64, 58, 54, 671, 66, 65, 58, 58]","[1697548516922, 1697548517321, 1697548517393, 1697548517459, 1697548517523, 1697548517581, 1697548517635, 1697548518306, 1697548518372, 1697548518437, 1697548518495, 1697548518553]"
1361,1361,218,10,[],200,llama-7b,64,1,1215.0,1.0,1,A100,1697548495794,1697548497009,120,109.0,7.0,"[6, 580, 41, 41, 50, 50, 446]","[1697548495800, 1697548496380, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497008]"
1362,1362,808,11,[],200,llama-7b,64,1,624.0,1.0,1,A100,1697548497014,1697548497638,120,286.0,2.0,"[7, 551, 66]","[1697548497021, 1697548497572, 1697548497638]"
1363,1363,81,19,[],200,llama-7b,64,1,2923.0,1.0,1,A100,1697548474326,1697548477249,120,732.0,13.0,"[20, 414, 284, 72, 71, 70, 69, 67, 64, 860, 512, 288, 75, 56]","[1697548474346, 1697548474760, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475393, 1697548475457, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248]"
1364,1364,668,29,[],200,llama-7b,64,1,1623.0,1.0,1,A100,1697548508993,1697548510616,120,109.0,6.0,"[14, 1320, 66, 60, 59, 46, 58]","[1697548509007, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616]"
1365,1365,581,12,[],200,llama-7b,64,1,3022.0,1.0,1,A100,1697548497640,1697548500662,120,47.0,20.0,"[5, 526, 66, 61, 52, 52, 49, 351, 59, 51, 50, 826, 61, 62, 59, 55, 51, 421, 65, 50, 50]","[1697548497645, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500497, 1697548500562, 1697548500612, 1697548500662]"
1366,1366,440,30,[],200,llama-7b,64,1,3612.0,1.0,1,A100,1697548510619,1697548514231,120,84.0,20.0,"[7, 213, 65, 56, 42, 44, 51, 51, 653, 58, 53, 53, 54, 750, 350, 59, 56, 43, 53, 834, 66]","[1697548510626, 1697548510839, 1697548510904, 1697548510960, 1697548511002, 1697548511046, 1697548511097, 1697548511148, 1697548511801, 1697548511859, 1697548511912, 1697548511965, 1697548512019, 1697548512769, 1697548513119, 1697548513178, 1697548513234, 1697548513277, 1697548513330, 1697548514164, 1697548514230]"
1367,1367,777,20,[],200,llama-7b,64,1,425.0,1.0,1,A100,1697548477254,1697548477679,120,9.0,1.0,"[28, 397]","[1697548477282, 1697548477679]"
1368,1368,152,33,[],200,llama-7b,64,1,3091.0,1.0,1,A100,1697548518556,1697548521647,120,87.0,20.0,"[6, 1048, 184, 243, 67, 63, 62, 61, 61, 59, 336, 66, 63, 48, 51, 52, 405, 62, 48, 60, 46]","[1697548518562, 1697548519610, 1697548519794, 1697548520037, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520351, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520974, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647]"
1369,1369,438,21,[],200,llama-7b,64,1,1407.0,1.0,1,A100,1697548477684,1697548479091,120,9.0,1.0,"[34, 1373]","[1697548477718, 1697548479091]"
1370,1370,211,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548479101,1697548488025,120,,,"[26, 1175, 494, 76, 74, 72, 72, 67, 67, 587, 270, 62, 74, 265, 68, 52, 67, 839, 75, 57, 71, 66, 66, 49, 338, 76, 79, 71, 70, 59, 68, 839, 314, 308, 76, 76, 72, 69, 324, 59, 59, 79, 306, 71, 70]","[1697548479127, 1697548480302, 1697548480796, 1697548480872, 1697548480946, 1697548481018, 1697548481090, 1697548481157, 1697548481224, 1697548481811, 1697548482081, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482602, 1697548482669, 1697548483508, 1697548483583, 1697548483640, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484385, 1697548484456, 1697548484526, 1697548484585, 1697548484653, 1697548485492, 1697548485806, 1697548486114, 1697548486190, 1697548486266, 1697548486338, 1697548486407, 1697548486731, 1697548486790, 1697548486849, 1697548486928, 1697548487234, 1697548487305, 1697548487375]"
1371,1371,298,37,[],200,llama-7b,64,1,372.0,1.0,1,A100,1697548493616,1697548493988,120,17.0,1.0,"[45, 326]","[1697548493661, 1697548493987]"
1372,1372,861,34,[],200,llama-7b,64,1,496.0,1.0,1,A100,1697548521649,1697548522145,120,10.0,1.0,"[15, 481]","[1697548521664, 1697548522145]"
1373,1373,211,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548500669,1697548527819,120,,,"[25, 509, 295, 63, 61, 57, 56, 55, 620, 65, 63, 62, 60, 55, 877, 62, 58, 57, 53, 41, 705, 61, 48, 59, 59, 59, 56, 352, 65, 51, 60, 61, 59, 283, 46, 55, 51, 679, 65, 59, 55, 43, 581, 64, 62, 59, 56, 57, 324, 65, 64, 60, 59, 55, 42, 305, 57, 55, 49, 40, 41, 579, 56, 55, 52, 53, 50, 559, 59, 59, 46, 59, 56, 230, 57, 43, 42, 52, 50, 655, 57, 54, 53, 51, 753, 350, 58, 56, 43, 53, 834, 66, 60, 58, 58, 919, 71, 70, 64, 61, 976, 270, 69, 65, 65, 63, 294, 66, 64, 59, 53, 672, 66, 64, 59, 58, 362, 62, 50, 58, 57, 45, 607, 242, 68, 62, 62, 62, 60, 60, 336, 66, 63, 48, 49, 53, 405, 63, 47, 61, 46, 54, 709, 70, 67, 67, 57, 66, 49, 317, 67, 64, 64, 62, 61, 317, 52, 67, 51, 63, 64, 554, 64, 66, 65, 58, 58, 588, 68, 65, 51, 50, 65, 66, 242, 48, 47, 60, 59, 654, 66, 49, 65, 65, 63]","[1697548500694, 1697548501203, 1697548501498, 1697548501561, 1697548501622, 1697548501679, 1697548501735, 1697548501790, 1697548502410, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502715, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504736, 1697548504795, 1697548504854, 1697548504910, 1697548505262, 1697548505327, 1697548505378, 1697548505438, 1697548505499, 1697548505558, 1697548505841, 1697548505887, 1697548505942, 1697548505993, 1697548506672, 1697548506737, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508747, 1697548508804, 1697548508859, 1697548508908, 1697548508948, 1697548508989, 1697548509568, 1697548509624, 1697548509679, 1697548509731, 1697548509784, 1697548509834, 1697548510393, 1697548510452, 1697548510511, 1697548510557, 1697548510616, 1697548510672, 1697548510902, 1697548510959, 1697548511002, 1697548511044, 1697548511096, 1697548511146, 1697548511801, 1697548511858, 1697548511912, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513177, 1697548513233, 1697548513276, 1697548513329, 1697548514163, 1697548514229, 1697548514289, 1697548514347, 1697548514405, 1697548515324, 1697548515395, 1697548515465, 1697548515529, 1697548515590, 1697548516566, 1697548516836, 1697548516905, 1697548516970, 1697548517035, 1697548517098, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517634, 1697548518306, 1697548518372, 1697548518436, 1697548518495, 1697548518553, 1697548518915, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521025, 1697548521430, 1697548521493, 1697548521540, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522786, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523971, 1697548524035, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524842, 1697548524900, 1697548525488, 1697548525556, 1697548525621, 1697548525672, 1697548525722, 1697548525787, 1697548525853, 1697548526095, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526963, 1697548527029, 1697548527078, 1697548527143, 1697548527208, 1697548527271]"
1374,1374,515,35,[],200,llama-7b,64,1,882.0,1.0,1,A100,1697548522148,1697548523030,120,11.0,1.0,"[13, 869]","[1697548522161, 1697548523030]"
1375,1375,286,36,[],200,llama-7b,64,1,1808.0,1.0,1,A100,1697548523035,1697548524843,120,161.0,12.0,"[19, 685, 51, 67, 52, 63, 64, 553, 64, 66, 65, 59]","[1697548523054, 1697548523739, 1697548523790, 1697548523857, 1697548523909, 1697548523972, 1697548524036, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843]"
1376,1376,540,40,[],200,llama-7b,64,1,1286.0,1.0,1,A100,1697548525857,1697548527143,120,140.0,5.0,"[24, 829, 254, 65, 50, 64]","[1697548525881, 1697548526710, 1697548526964, 1697548527029, 1697548527079, 1697548527143]"
1377,1377,200,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548527146,1697548527818,120,,,"[6, 425]","[1697548527152, 1697548527577]"
1378,1378,876,37,[],200,llama-7b,64,1,404.0,1.0,1,A100,1697548524846,1697548525250,120,11.0,1.0,"[7, 396]","[1697548524853, 1697548525249]"
1379,1379,68,38,[],200,llama-7b,64,1,561.0,1.0,1,A100,1697548493991,1697548494552,120,12.0,1.0,"[18, 543]","[1697548494009, 1697548494552]"
1380,1380,646,38,[],200,llama-7b,64,1,775.0,1.0,1,A100,1697548525254,1697548526029,120,14.0,1.0,"[25, 750]","[1697548525279, 1697548526029]"
1381,1381,9,16,[],200,llama-7b,64,1,3280.0,1.0,1,A100,1697548468055,1697548471335,120,85.0,20.0,"[9, 607, 246, 66, 51, 67, 64, 62, 60, 568, 69, 53, 67, 51, 52, 64, 65, 846, 72, 71, 70]","[1697548468064, 1697548468671, 1697548468917, 1697548468983, 1697548469034, 1697548469101, 1697548469165, 1697548469227, 1697548469287, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470211, 1697548470276, 1697548471122, 1697548471194, 1697548471265, 1697548471335]"
1382,1382,304,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526030,1697548527819,120,,,"[6, 674, 254, 65, 50, 64, 65, 64]","[1697548526036, 1697548526710, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
1383,1383,899,42,[],200,llama-7b,64,1,2572.0,1.0,1,A100,1697548527826,1697548530398,120,100.0,20.0,"[33, 406, 34, 655, 52, 50, 48, 47, 40, 365, 51, 49, 41, 42, 40, 49, 347, 56, 48, 59, 59]","[1697548527859, 1697548528265, 1697548528299, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529556, 1697548529607, 1697548529656, 1697548529697, 1697548529739, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530397]"
1384,1384,755,22,[],200,llama-7b,64,1,3904.0,1.0,1,A100,1697548496007,1697548499911,120,286.0,25.0,"[42, 882, 76, 69, 64, 61, 62, 57, 55, 262, 61, 48, 46, 45, 400, 62, 51, 51, 50, 351, 59, 51, 49, 826, 62, 62]","[1697548496049, 1697548496931, 1697548497007, 1697548497076, 1697548497140, 1697548497201, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499787, 1697548499849, 1697548499911]"
1385,1385,743,39,[],200,llama-7b,64,1,910.0,1.0,1,A100,1697548494554,1697548495464,120,123.0,6.0,"[19, 606, 64, 50, 58, 58, 55]","[1697548494573, 1697548495179, 1697548495243, 1697548495293, 1697548495351, 1697548495409, 1697548495464]"
1386,1386,527,23,[],200,llama-7b,64,1,7688.0,1.0,1,A100,1697548499913,1697548507601,120,732.0,50.0,"[14, 570, 65, 51, 49, 58, 54, 723, 64, 60, 58, 56, 54, 622, 65, 63, 62, 59, 55, 876, 63, 58, 57, 53, 41, 704, 62, 47, 60, 59, 59, 56, 352, 65, 51, 61, 60, 60, 282, 46, 55, 51, 679, 65, 59, 55, 43, 581, 64, 62]","[1697548499927, 1697548500497, 1697548500562, 1697548500613, 1697548500662, 1697548500720, 1697548500774, 1697548501497, 1697548501561, 1697548501621, 1697548501679, 1697548501735, 1697548501789, 1697548502411, 1697548502476, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504854, 1697548504910, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505841, 1697548505887, 1697548505942, 1697548505993, 1697548506672, 1697548506737, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601]"
1387,1387,74,40,[],200,llama-7b,64,1,2572.0,1.0,1,A100,1697548527826,1697548530398,120,88.0,20.0,"[41, 398, 34, 656, 51, 50, 48, 47, 40, 365, 51, 49, 41, 42, 40, 49, 347, 56, 48, 59, 59]","[1697548527867, 1697548528265, 1697548528299, 1697548528955, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529556, 1697548529607, 1697548529656, 1697548529697, 1697548529739, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530397]"
1388,1388,873,37,[],200,llama-7b,64,1,310.0,1.0,1,A100,1697548537864,1697548538174,120,6.0,1.0,"[28, 282]","[1697548537892, 1697548538174]"
1389,1389,534,38,[],200,llama-7b,64,1,2602.0,1.0,1,A100,1697548538177,1697548540779,120,96.0,20.0,"[10, 644, 53, 55, 42, 46, 46, 677, 61, 56, 53, 45, 51, 42, 401, 49, 59, 47, 58, 55, 52]","[1697548538187, 1697548538831, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539073, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539965, 1697548540016, 1697548540058, 1697548540459, 1697548540508, 1697548540567, 1697548540614, 1697548540672, 1697548540727, 1697548540779]"
1390,1390,396,40,[],200,llama-7b,64,1,2231.0,1.0,1,A100,1697548495467,1697548497698,120,89.0,20.0,"[7, 253, 65, 55, 53, 51, 52, 378, 40, 41, 50, 49, 447, 69, 63, 64, 59, 57, 55, 262, 60]","[1697548495474, 1697548495727, 1697548495792, 1697548495847, 1697548495900, 1697548495951, 1697548496003, 1697548496381, 1697548496421, 1697548496462, 1697548496512, 1697548496561, 1697548497008, 1697548497077, 1697548497140, 1697548497204, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697]"
1391,1391,715,17,[],200,llama-7b,64,1,413.0,1.0,1,A100,1697548471338,1697548471751,120,20.0,1.0,"[17, 396]","[1697548471355, 1697548471751]"
1392,1392,363,18,[],200,llama-7b,64,1,3292.0,1.0,1,A100,1697548471752,1697548475044,120,286.0,22.0,"[14, 690, 76, 70, 68, 68, 53, 65, 49, 530, 67, 49, 51, 65, 58, 346, 68, 67, 51, 66, 64, 63, 594]","[1697548471766, 1697548472456, 1697548472532, 1697548472602, 1697548472670, 1697548472738, 1697548472791, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473667, 1697548473725, 1697548474071, 1697548474139, 1697548474206, 1697548474257, 1697548474323, 1697548474387, 1697548474450, 1697548475044]"
1393,1393,923,24,[],200,llama-7b,64,1,924.0,1.0,1,A100,1697548508862,1697548509786,120,140.0,6.0,"[8, 570, 128, 57, 54, 53, 54]","[1697548508870, 1697548509440, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509786]"
1394,1394,140,19,[],200,llama-7b,64,1,4775.0,1.0,1,A100,1697548475047,1697548479822,120,96.0,20.0,"[6, 784, 480, 512, 289, 74, 57, 63, 617, 251, 54, 254, 70, 68, 62, 614, 255, 71, 54, 71, 69]","[1697548475053, 1697548475837, 1697548476317, 1697548476829, 1697548477118, 1697548477192, 1697548477249, 1697548477312, 1697548477929, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479822]"
1395,1395,173,41,[],200,llama-7b,64,1,2961.0,1.0,1,A100,1697548497702,1697548500663,120,96.0,20.0,"[31, 438, 66, 61, 52, 52, 49, 351, 59, 51, 50, 826, 61, 61, 60, 54, 52, 421, 65, 51, 50]","[1697548497733, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499910, 1697548499970, 1697548500024, 1697548500076, 1697548500497, 1697548500562, 1697548500613, 1697548500663]"
1396,1396,694,25,[],200,llama-7b,64,1,1357.0,1.0,1,A100,1697548509790,1697548511147,120,161.0,13.0,"[18, 585, 60, 59, 46, 59, 56, 230, 57, 43, 42, 51, 51]","[1697548509808, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510617, 1697548510673, 1697548510903, 1697548510960, 1697548511003, 1697548511045, 1697548511096, 1697548511147]"
1397,1397,296,24,[],200,llama-7b,64,1,427.0,1.0,1,A100,1697548493123,1697548493550,120,6.0,1.0,"[17, 410]","[1697548493140, 1697548493550]"
1398,1398,303,39,[],200,llama-7b,64,1,4103.0,1.0,1,A100,1697548540783,1697548544886,120,88.0,20.0,"[31, 777, 335, 61, 59, 51, 651, 61, 61, 58, 43, 55, 694, 58, 47, 58, 54, 52, 788, 55, 54]","[1697548540814, 1697548541591, 1697548541926, 1697548541987, 1697548542046, 1697548542097, 1697548542748, 1697548542809, 1697548542870, 1697548542928, 1697548542971, 1697548543026, 1697548543720, 1697548543778, 1697548543825, 1697548543883, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544886]"
1399,1399,704,20,[],200,llama-7b,64,1,506.0,1.0,1,A100,1697548490221,1697548490727,120,14.0,1.0,"[43, 463]","[1697548490264, 1697548490727]"
1400,1400,201,41,[],200,llama-7b,64,1,3064.0,1.0,1,A100,1697548522790,1697548525854,120,67.0,20.0,"[21, 854, 73, 52, 67, 51, 64, 64, 553, 65, 65, 65, 60, 56, 590, 66, 66, 51, 50, 65, 66]","[1697548522811, 1697548523665, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524036, 1697548524589, 1697548524654, 1697548524719, 1697548524784, 1697548524844, 1697548524900, 1697548525490, 1697548525556, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525854]"
1401,1401,100,13,[],200,llama-7b,64,1,2478.0,1.0,1,A100,1697548465905,1697548468383,120,732.0,14.0,"[14, 1097, 282, 74, 70, 68, 67, 51, 63, 359, 73, 54, 71, 69, 66]","[1697548465919, 1697548467016, 1697548467298, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468248, 1697548468317, 1697548468383]"
1402,1402,358,21,[],200,llama-7b,64,1,794.0,1.0,1,A100,1697548490731,1697548491525,120,216.0,3.0,"[28, 586, 130, 50]","[1697548490759, 1697548491345, 1697548491475, 1697548491525]"
1403,1403,869,6,[],200,llama-7b,64,1,1561.0,1.0,1,A100,1697548488031,1697548489592,120,244.0,12.0,"[53, 748, 57, 51, 36, 36, 286, 66, 51, 48, 41, 48, 40]","[1697548488084, 1697548488832, 1697548488889, 1697548488940, 1697548488976, 1697548489012, 1697548489298, 1697548489364, 1697548489415, 1697548489463, 1697548489504, 1697548489552, 1697548489592]"
1404,1404,690,14,[],200,llama-7b,64,1,1196.0,1.0,1,A100,1697548468386,1697548469582,120,39.0,1.0,"[6, 1190]","[1697548468392, 1697548469582]"
1405,1405,139,34,[],200,llama-7b,64,1,2555.0,1.0,1,A100,1697548495795,1697548498350,120,39.0,21.0,"[20, 510, 55, 41, 41, 50, 50, 447, 68, 64, 59, 64, 56, 55, 262, 60, 48, 46, 46, 400, 61, 52]","[1697548495815, 1697548496325, 1697548496380, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497009, 1697548497077, 1697548497141, 1697548497200, 1697548497264, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497791, 1697548497837, 1697548498237, 1697548498298, 1697548498350]"
1406,1406,135,22,[],200,llama-7b,64,1,512.0,1.0,1,A100,1697548491527,1697548492039,120,52.0,2.0,"[7, 447, 58]","[1697548491534, 1697548491981, 1697548492039]"
1407,1407,848,35,[],200,llama-7b,64,1,386.0,1.0,1,A100,1697548498353,1697548498739,120,47.0,1.0,"[14, 372]","[1697548498367, 1697548498739]"
1408,1408,502,36,[],200,llama-7b,64,1,678.0,1.0,1,A100,1697548498742,1697548499420,120,19.0,1.0,"[23, 654]","[1697548498765, 1697548499419]"
1409,1409,278,37,[],200,llama-7b,64,1,1003.0,1.0,1,A100,1697548499422,1697548500425,120,13.0,1.0,"[10, 993]","[1697548499432, 1697548500425]"
1410,1410,859,38,[],200,llama-7b,64,1,775.0,1.0,1,A100,1697548500429,1697548501204,120,23.0,1.0,"[24, 750]","[1697548500453, 1697548501203]"
1411,1411,607,39,[],200,llama-7b,64,1,2507.0,1.0,1,A100,1697548501206,1697548503713,120,6.0,10.0,"[9, 941, 254, 65, 64, 62, 59, 55, 875, 64, 59]","[1697548501215, 1697548502156, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503590, 1697548503654, 1697548503713]"
1412,1412,703,11,[],200,llama-7b,64,1,345.0,1.0,1,A100,1697548458693,1697548459038,120,12.0,1.0,"[6, 339]","[1697548458699, 1697548459038]"
1413,1413,356,12,[],200,llama-7b,64,1,946.0,1.0,1,A100,1697548459041,1697548459987,120,874.0,2.0,"[16, 680, 250]","[1697548459057, 1697548459737, 1697548459987]"
1414,1414,131,13,[],200,llama-7b,64,1,633.0,1.0,1,A100,1697548459991,1697548460624,120,8.0,1.0,"[6, 626]","[1697548459997, 1697548460623]"
1415,1415,376,40,[],200,llama-7b,64,1,3023.0,1.0,1,A100,1697548503715,1697548506738,120,87.0,20.0,"[12, 586, 255, 61, 48, 60, 59, 58, 57, 351, 65, 51, 61, 60, 60, 283, 46, 54, 52, 679, 64]","[1697548503727, 1697548504313, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506673, 1697548506737]"
1416,1416,714,14,[],200,llama-7b,64,1,3389.0,1.0,1,A100,1697548460627,1697548464016,120,83.0,20.0,"[16, 599, 68, 50, 62, 62, 60, 62, 526, 175, 49, 58, 56, 54, 382, 56, 58, 53, 820, 63, 60]","[1697548460643, 1697548461242, 1697548461310, 1697548461360, 1697548461422, 1697548461484, 1697548461544, 1697548461606, 1697548462132, 1697548462307, 1697548462356, 1697548462414, 1697548462470, 1697548462524, 1697548462906, 1697548462962, 1697548463020, 1697548463073, 1697548463893, 1697548463956, 1697548464016]"
1417,1417,32,41,[],200,llama-7b,64,1,975.0,1.0,1,A100,1697548506742,1697548507717,120,140.0,6.0,"[31, 510, 193, 63, 62, 59, 57]","[1697548506773, 1697548507283, 1697548507476, 1697548507539, 1697548507601, 1697548507660, 1697548507717]"
1418,1418,734,42,[],200,llama-7b,64,1,627.0,1.0,1,A100,1697548507719,1697548508346,120,100.0,6.0,"[6, 301, 72, 65, 64, 60, 59]","[1697548507725, 1697548508026, 1697548508098, 1697548508163, 1697548508227, 1697548508287, 1697548508346]"
1419,1419,375,20,[],200,llama-7b,64,1,2054.0,1.0,1,A100,1697548488029,1697548490083,120,874.0,17.0,"[30, 829, 52, 35, 37, 285, 66, 51, 49, 40, 48, 40, 304, 52, 43, 43, 50]","[1697548488059, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489363, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083]"
1420,1420,388,43,[],200,llama-7b,64,1,2555.0,1.0,1,A100,1697548508348,1697548510903,120,87.0,20.0,"[7, 324, 69, 56, 55, 50, 40, 40, 579, 57, 54, 52, 54, 49, 559, 60, 59, 46, 58, 56, 231]","[1697548508355, 1697548508679, 1697548508748, 1697548508804, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509568, 1697548509625, 1697548509679, 1697548509731, 1697548509785, 1697548509834, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510672, 1697548510903]"
1421,1421,162,44,[],200,llama-7b,64,1,4560.0,1.0,1,A100,1697548510906,1697548515466,120,90.0,20.0,"[11, 642, 243, 56, 55, 52, 55, 749, 350, 59, 55, 43, 54, 834, 67, 59, 58, 58, 919, 71, 70]","[1697548510917, 1697548511559, 1697548511802, 1697548511858, 1697548511913, 1697548511965, 1697548512020, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514164, 1697548514231, 1697548514290, 1697548514348, 1697548514406, 1697548515325, 1697548515396, 1697548515466]"
1422,1422,423,29,[],200,llama-7b,64,1,2960.0,1.0,1,A100,1697548497702,1697548500662,120,84.0,20.0,"[26, 443, 66, 61, 52, 51, 50, 351, 59, 51, 50, 826, 61, 62, 59, 54, 52, 421, 65, 50, 50]","[1697548497728, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500024, 1697548500076, 1697548500497, 1697548500562, 1697548500612, 1697548500662]"
1423,1423,532,18,[],200,llama-7b,64,1,4556.0,1.0,1,A100,1697548474072,1697548478628,120,92.0,20.0,"[14, 675, 283, 72, 71, 70, 69, 67, 63, 861, 512, 288, 75, 56, 64, 618, 251, 54, 253, 71, 68]","[1697548474086, 1697548474761, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248, 1697548477312, 1697548477930, 1697548478181, 1697548478235, 1697548478488, 1697548478559, 1697548478627]"
1424,1424,30,21,[],200,llama-7b,64,1,2975.0,1.0,1,A100,1697548490086,1697548493061,120,93.0,20.0,"[11, 270, 41, 373, 49, 49, 42, 553, 51, 48, 40, 49, 49, 327, 54, 53, 43, 51, 50, 49, 723]","[1697548490097, 1697548490367, 1697548490408, 1697548490781, 1697548490830, 1697548490879, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491711, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492289, 1697548492338, 1697548493061]"
1425,1425,186,19,[],200,llama-7b,64,1,3972.0,1.0,1,A100,1697548478630,1697548482602,120,123.0,22.0,"[19, 442, 211, 255, 71, 54, 71, 68, 63, 911, 76, 75, 72, 71, 68, 67, 588, 269, 63, 74, 264, 68, 52]","[1697548478649, 1697548479091, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481812, 1697548482081, 1697548482144, 1697548482218, 1697548482482, 1697548482550, 1697548482602]"
1426,1426,729,22,[],200,llama-7b,64,1,549.0,1.0,1,A100,1697548493063,1697548493612,120,874.0,2.0,"[7, 542]","[1697548493070, 1697548493612]"
1427,1427,880,10,[],200,llama-7b,64,1,864.0,1.0,1,A100,1697548466435,1697548467299,120,84.0,2.0,"[7, 574, 283]","[1697548466442, 1697548467016, 1697548467299]"
1428,1428,391,23,[],200,llama-7b,64,1,2177.0,1.0,1,A100,1697548493615,1697548495792,120,79.0,20.0,"[35, 398, 51, 42, 46, 431, 60, 47, 60, 55, 55, 51, 298, 49, 58, 59, 54, 52, 42, 234]","[1697548493650, 1697548494048, 1697548494099, 1697548494141, 1697548494187, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494840, 1697548494895, 1697548494946, 1697548495244, 1697548495293, 1697548495351, 1697548495410, 1697548495464, 1697548495516, 1697548495558, 1697548495792]"
1429,1429,892,20,[],200,llama-7b,64,1,3660.0,1.0,1,A100,1697548482605,1697548486265,120,87.0,20.0,"[6, 347, 550, 76, 56, 71, 66, 66, 49, 338, 77, 78, 72, 70, 57, 70, 837, 313, 309, 76, 76]","[1697548482611, 1697548482958, 1697548483508, 1697548483584, 1697548483640, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484307, 1697548484385, 1697548484457, 1697548484527, 1697548484584, 1697548484654, 1697548485491, 1697548485804, 1697548486113, 1697548486189, 1697548486265]"
1430,1430,565,6,[],200,llama-7b,64,1,2899.0,1.0,1,A100,1697548490221,1697548493120,120,91.0,20.0,"[54, 452, 54, 50, 48, 42, 554, 51, 47, 40, 48, 49, 328, 55, 52, 43, 51, 51, 48, 723, 59]","[1697548490275, 1697548490727, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491475, 1697548491526, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492038, 1697548492093, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492338, 1697548493061, 1697548493120]"
1431,1431,877,25,[],200,llama-7b,64,1,2235.0,1.0,1,A100,1697548493556,1697548495791,120,85.0,20.0,"[29, 402, 60, 52, 42, 46, 432, 59, 47, 61, 55, 55, 51, 297, 48, 58, 60, 53, 53, 42, 233]","[1697548493585, 1697548493987, 1697548494047, 1697548494099, 1697548494141, 1697548494187, 1697548494619, 1697548494678, 1697548494725, 1697548494786, 1697548494841, 1697548494896, 1697548494947, 1697548495244, 1697548495292, 1697548495350, 1697548495410, 1697548495463, 1697548495516, 1697548495558, 1697548495791]"
1432,1432,722,20,[],200,llama-7b,64,1,476.0,1.0,1,A100,1697548479826,1697548480302,120,39.0,1.0,"[20, 456]","[1697548479846, 1697548480302]"
1433,1433,494,21,[],200,llama-7b,64,1,3206.0,1.0,1,A100,1697548480303,1697548483509,120,6.0,10.0,"[6, 1213, 290, 268, 63, 74, 266, 67, 51, 68, 839]","[1697548480309, 1697548481522, 1697548481812, 1697548482080, 1697548482143, 1697548482217, 1697548482483, 1697548482550, 1697548482601, 1697548482669, 1697548483508]"
1434,1434,73,15,[],200,llama-7b,64,1,414.0,1.0,1,A100,1697548477266,1697548477680,120,9.0,1.0,"[36, 377]","[1697548477302, 1697548477679]"
1435,1435,774,16,[],200,llama-7b,64,1,1408.0,1.0,1,A100,1697548477683,1697548479091,120,8.0,1.0,"[34, 1374]","[1697548477717, 1697548479091]"
1436,1436,853,14,[],200,llama-7b,64,1,3591.0,1.0,1,A100,1697548459371,1697548462962,120,364.0,22.0,"[28, 1224, 88, 69, 52, 67, 63, 63, 48, 238, 48, 63, 61, 61, 61, 526, 176, 50, 57, 57, 54, 381, 56]","[1697548459399, 1697548460623, 1697548460711, 1697548460780, 1697548460832, 1697548460899, 1697548460962, 1697548461025, 1697548461073, 1697548461311, 1697548461359, 1697548461422, 1697548461483, 1697548461544, 1697548461605, 1697548462131, 1697548462307, 1697548462357, 1697548462414, 1697548462471, 1697548462525, 1697548462906, 1697548462962]"
1437,1437,155,22,[],200,llama-7b,64,1,3721.0,1.0,1,A100,1697548483512,1697548487233,120,90.0,20.0,"[13, 623, 82, 77, 78, 72, 70, 57, 70, 838, 313, 308, 76, 76, 72, 70, 323, 59, 59, 79, 306]","[1697548483525, 1697548484148, 1697548484230, 1697548484307, 1697548484385, 1697548484457, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487233]"
1438,1438,430,17,[],200,llama-7b,64,1,1207.0,1.0,1,A100,1697548479095,1697548480302,120,15.0,1.0,"[27, 1180]","[1697548479122, 1697548480302]"
1439,1439,819,10,[],200,llama-7b,64,1,689.0,1.0,1,A100,1697548461246,1697548461935,120,13.0,1.0,"[21, 668]","[1697548461267, 1697548461935]"
1440,1440,200,18,[],200,llama-7b,64,1,2364.0,1.0,1,A100,1697548480305,1697548482669,120,6.0,9.0,"[9, 1208, 290, 268, 63, 74, 266, 67, 51, 68]","[1697548480314, 1697548481522, 1697548481812, 1697548482080, 1697548482143, 1697548482217, 1697548482483, 1697548482550, 1697548482601, 1697548482669]"
1441,1441,96,31,[],200,llama-7b,64,1,623.0,1.0,1,A100,1697548514235,1697548514858,120,31.0,1.0,"[26, 597]","[1697548514261, 1697548514858]"
1442,1442,483,11,[],200,llama-7b,64,1,4496.0,1.0,1,A100,1697548461938,1697548466434,120,84.0,20.0,"[9, 887, 72, 56, 57, 54, 820, 63, 60, 57, 56, 730, 459, 65, 54, 54, 45, 703, 71, 70, 54]","[1697548461947, 1697548462834, 1697548462906, 1697548462962, 1697548463019, 1697548463073, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464859, 1697548465318, 1697548465383, 1697548465437, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466434]"
1443,1443,803,32,[],200,llama-7b,64,1,1206.0,1.0,1,A100,1697548514863,1697548516069,120,20.0,1.0,"[9, 1197]","[1697548514872, 1697548516069]"
1444,1444,456,33,[],200,llama-7b,64,1,4034.0,1.0,1,A100,1697548516070,1697548520104,120,90.0,20.0,"[6, 1245, 72, 65, 65, 58, 54, 672, 65, 65, 58, 58, 362, 63, 50, 58, 56, 45, 607, 242, 68]","[1697548516076, 1697548517321, 1697548517393, 1697548517458, 1697548517523, 1697548517581, 1697548517635, 1697548518307, 1697548518372, 1697548518437, 1697548518495, 1697548518553, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104]"
1445,1445,450,25,[],200,llama-7b,64,1,2397.0,1.0,1,A100,1697548495902,1697548498299,120,91.0,20.0,"[15, 408, 56, 40, 41, 50, 50, 447, 67, 63, 61, 64, 56, 55, 262, 60, 48, 47, 45, 400, 61]","[1697548495917, 1697548496325, 1697548496381, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497009, 1697548497076, 1697548497139, 1697548497200, 1697548497264, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497792, 1697548497837, 1697548498237, 1697548498298]"
1446,1446,886,16,[],200,llama-7b,64,1,359.0,1.0,1,A100,1697548462475,1697548462834,120,17.0,1.0,"[14, 345]","[1697548462489, 1697548462834]"
1447,1447,540,17,[],200,llama-7b,64,1,1237.0,1.0,1,A100,1697548462837,1697548464074,120,140.0,5.0,"[6, 738, 313, 62, 60, 58]","[1697548462843, 1697548463581, 1697548463894, 1697548463956, 1697548464016, 1697548464074]"
1448,1448,202,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548520108,1697548527818,120,,,"[10, 557, 72, 65, 63, 49, 48, 54, 405, 62, 48, 60, 46, 54, 710, 69, 67, 67, 57, 66, 50, 317, 66, 64, 63, 63, 61, 317, 52, 67, 51, 64, 64, 553, 64, 66, 65, 59, 58, 588, 67, 66, 51, 50, 65, 65, 242, 48, 48, 59, 59, 654, 66, 50, 64, 65, 63]","[1697548520118, 1697548520675, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520972, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523297, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524036, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526250, 1697548526309, 1697548526963, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527271]"
1449,1449,287,18,[],200,llama-7b,64,1,480.0,1.0,1,A100,1697548464076,1697548464556,120,10.0,1.0,"[5, 475]","[1697548464081, 1697548464556]"
1450,1450,59,19,[],200,llama-7b,64,1,3761.0,1.0,1,A100,1697548464558,1697548468319,120,91.0,20.0,"[18, 1325, 337, 73, 69, 53, 64, 64, 48, 689, 75, 69, 68, 67, 51, 63, 360, 73, 54, 70, 71]","[1697548464576, 1697548465901, 1697548466238, 1697548466311, 1697548466380, 1697548466433, 1697548466497, 1697548466561, 1697548466609, 1697548467298, 1697548467373, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468124, 1697548468178, 1697548468248, 1697548468319]"
1451,1451,82,44,[],200,llama-7b,64,1,4168.0,1.0,1,A100,1697548551207,1697548555375,120,67.0,20.0,"[11, 1068, 374, 54, 66, 50, 50, 64, 64, 275, 48, 47, 61, 59, 349, 60, 45, 59, 860, 251, 253]","[1697548551218, 1697548552286, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552944, 1697548553008, 1697548553283, 1697548553331, 1697548553378, 1697548553439, 1697548553498, 1697548553847, 1697548553907, 1697548553952, 1697548554011, 1697548554871, 1697548555122, 1697548555375]"
1452,1452,533,11,[],200,llama-7b,64,1,751.0,1.0,1,A100,1697548467301,1697548468052,120,216.0,2.0,"[19, 731]","[1697548467320, 1697548468051]"
1453,1453,716,23,[],200,llama-7b,64,1,3843.0,1.0,1,A100,1697548490883,1697548494726,120,79.0,30.0,"[23, 569, 50, 48, 40, 49, 49, 327, 54, 53, 43, 51, 50, 49, 722, 59, 49, 48, 38, 358, 47, 38, 38, 312, 52, 41, 46, 432, 60, 47]","[1697548490906, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491711, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492289, 1697548492338, 1697548493060, 1697548493119, 1697548493168, 1697548493216, 1697548493254, 1697548493612, 1697548493659, 1697548493697, 1697548493735, 1697548494047, 1697548494099, 1697548494140, 1697548494186, 1697548494618, 1697548494678, 1697548494725]"
1454,1454,313,7,[],200,llama-7b,64,1,427.0,1.0,1,A100,1697548493124,1697548493551,120,20.0,1.0,"[19, 407]","[1697548493143, 1697548493550]"
1455,1455,894,8,[],200,llama-7b,64,1,432.0,1.0,1,A100,1697548493555,1697548493987,120,14.0,1.0,"[22, 410]","[1697548493577, 1697548493987]"
1456,1456,747,41,[],200,llama-7b,64,1,5275.0,1.0,1,A100,1697548530401,1697548535676,120,140.0,36.0,"[28, 456, 51, 39, 471, 66, 62, 63, 60, 51, 736, 52, 59, 64, 51, 54, 576, 64, 63, 50, 68, 53, 46, 311, 60, 45, 46, 59, 57, 368, 47, 58, 58, 57, 708, 59, 59]","[1697548530429, 1697548530885, 1697548530936, 1697548530975, 1697548531446, 1697548531512, 1697548531574, 1697548531637, 1697548531697, 1697548531748, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533585, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534262, 1697548534630, 1697548534677, 1697548534735, 1697548534793, 1697548534850, 1697548535558, 1697548535617, 1697548535676]"
1457,1457,674,9,[],200,llama-7b,64,1,15636.0,1.0,1,A100,1697548493989,1697548509625,120,161.0,119.0,"[14, 548, 67, 60, 47, 60, 56, 54, 51, 298, 49, 58, 59, 54, 52, 42, 234, 54, 53, 52, 51, 378, 41, 40, 51, 49, 446, 69, 64, 60, 63, 57, 55, 262, 60, 49, 46, 45, 399, 62, 52, 51, 50, 351, 59, 51, 49, 827, 61, 61, 60, 54, 51, 421, 65, 51, 49, 58, 55, 723, 63, 61, 58, 55, 55, 620, 66, 63, 62, 60, 54, 876, 63, 58, 57, 53, 41, 704, 63, 47, 60, 59, 58, 57, 351, 66, 51, 61, 59, 60, 283, 46, 55, 51, 679, 65, 59, 55, 43, 581, 64, 62, 59, 56, 57, 324, 65, 64, 60, 59, 54, 43, 305, 57, 54, 50, 40, 40, 580, 57]","[1697548494003, 1697548494551, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494841, 1697548494895, 1697548494946, 1697548495244, 1697548495293, 1697548495351, 1697548495410, 1697548495464, 1697548495516, 1697548495558, 1697548495792, 1697548495846, 1697548495899, 1697548495951, 1697548496002, 1697548496380, 1697548496421, 1697548496461, 1697548496512, 1697548496561, 1697548497007, 1697548497076, 1697548497140, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497746, 1697548497792, 1697548497837, 1697548498236, 1697548498298, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499788, 1697548499849, 1697548499910, 1697548499970, 1697548500024, 1697548500075, 1697548500496, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501497, 1697548501560, 1697548501621, 1697548501679, 1697548501734, 1697548501789, 1697548502409, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502714, 1697548503590, 1697548503653, 1697548503711, 1697548503768, 1697548503821, 1697548503862, 1697548504566, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504853, 1697548504910, 1697548505261, 1697548505327, 1697548505378, 1697548505439, 1697548505498, 1697548505558, 1697548505841, 1697548505887, 1697548505942, 1697548505993, 1697548506672, 1697548506737, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508399, 1697548508442, 1697548508747, 1697548508804, 1697548508858, 1697548508908, 1697548508948, 1697548508988, 1697548509568, 1697548509625]"
1458,1458,726,38,[],200,llama-7b,64,1,6379.0,1.0,1,A100,1697548527826,1697548534205,120,67.0,47.0,"[23, 416, 34, 655, 52, 50, 48, 47, 40, 364, 52, 49, 41, 41, 41, 49, 347, 56, 48, 59, 59, 51, 487, 39, 472, 64, 64, 62, 59, 51, 737, 52, 59, 64, 50, 55, 576, 65, 63, 50, 63, 58, 45, 312, 60, 45, 46, 59]","[1697548527849, 1697548528265, 1697548528299, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529656, 1697548529697, 1697548529738, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530397, 1697548530448, 1697548530935, 1697548530974, 1697548531446, 1697548531510, 1697548531574, 1697548531636, 1697548531695, 1697548531746, 1697548532483, 1697548532535, 1697548532594, 1697548532658, 1697548532708, 1697548532763, 1697548533339, 1697548533404, 1697548533467, 1697548533517, 1697548533580, 1697548533638, 1697548533683, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205]"
1459,1459,387,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548534206,1697548537855,120,,,"[16, 343, 65, 47, 59, 58, 57, 708, 58, 59, 59, 55, 668, 62, 49, 54, 315, 52, 60, 65, 64, 60, 46]","[1697548534222, 1697548534565, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535559, 1697548535617, 1697548535676, 1697548535735, 1697548535790, 1697548536458, 1697548536520, 1697548536569, 1697548536623, 1697548536938, 1697548536990, 1697548537050, 1697548537115, 1697548537179, 1697548537239, 1697548537285]"
1460,1460,41,36,[],200,llama-7b,64,1,7645.0,1.0,1,A100,1697548513786,1697548521431,120,39.0,43.0,"[16, 1056, 467, 71, 69, 65, 60, 977, 270, 69, 65, 64, 63, 295, 66, 64, 58, 53, 672, 66, 65, 59, 57, 362, 62, 50, 58, 57, 45, 607, 243, 67, 62, 62, 62, 60, 60, 336, 66, 63, 48, 49, 54, 405]","[1697548513802, 1697548514858, 1697548515325, 1697548515396, 1697548515465, 1697548515530, 1697548515590, 1697548516567, 1697548516837, 1697548516906, 1697548516971, 1697548517035, 1697548517098, 1697548517393, 1697548517459, 1697548517523, 1697548517581, 1697548517634, 1697548518306, 1697548518372, 1697548518437, 1697548518496, 1697548518553, 1697548518915, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520037, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521026, 1697548521431]"
1461,1461,183,24,[],200,llama-7b,64,1,7794.0,1.0,1,A100,1697548507602,1697548515396,120,17.0,50.0,"[6, 418, 72, 64, 64, 61, 59, 54, 43, 305, 56, 55, 49, 41, 40, 579, 57, 54, 53, 53, 50, 558, 59, 59, 47, 58, 56, 230, 57, 42, 44, 51, 51, 654, 57, 54, 53, 51, 753, 350, 59, 55, 43, 54, 833, 66, 60, 58, 58, 919, 72]","[1697548507608, 1697548508026, 1697548508098, 1697548508162, 1697548508226, 1697548508287, 1697548508346, 1697548508400, 1697548508443, 1697548508748, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835, 1697548510393, 1697548510452, 1697548510511, 1697548510558, 1697548510616, 1697548510672, 1697548510902, 1697548510959, 1697548511001, 1697548511045, 1697548511096, 1697548511147, 1697548511801, 1697548511858, 1697548511912, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514163, 1697548514229, 1697548514289, 1697548514347, 1697548514405, 1697548515324, 1697548515396]"
1462,1462,33,16,[],200,llama-7b,64,1,1260.0,1.0,1,A100,1697548503594,1697548504854,120,140.0,7.0,"[17, 701, 256, 61, 48, 60, 58, 59]","[1697548503611, 1697548504312, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504795, 1697548504854]"
1463,1463,732,17,[],200,llama-7b,64,1,1816.0,1.0,1,A100,1697548504857,1697548506673,120,345.0,12.0,"[16, 320, 70, 65, 51, 60, 60, 60, 283, 46, 54, 52, 678]","[1697548504873, 1697548505193, 1697548505263, 1697548505328, 1697548505379, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506672]"
1464,1464,779,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548525857,1697548527818,120,,,"[14, 839, 254, 65, 50, 64, 65, 64]","[1697548525871, 1697548526710, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
1465,1465,393,13,[],200,llama-7b,64,1,4012.0,1.0,1,A100,1697548500665,1697548504677,120,182.0,22.0,"[14, 525, 294, 63, 61, 57, 56, 55, 621, 65, 63, 62, 59, 55, 877, 62, 58, 57, 53, 41, 704, 62, 47]","[1697548500679, 1697548501204, 1697548501498, 1697548501561, 1697548501622, 1697548501679, 1697548501735, 1697548501790, 1697548502411, 1697548502476, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504676]"
1466,1466,555,43,[],200,llama-7b,64,1,440.0,1.0,1,A100,1697548527826,1697548528266,120,11.0,1.0,"[53, 387]","[1697548527879, 1697548528266]"
1467,1467,624,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548521436,1697548527819,120,,,"[9, 700, 265, 70, 67, 67, 57, 66, 50, 317, 67, 63, 64, 62, 61, 318, 51, 68, 51, 63, 65, 552, 64, 66, 65, 59, 58, 588, 67, 66, 51, 50, 65, 65, 242, 48, 48, 59, 60, 653, 66, 50, 64, 65, 63]","[1697548521445, 1697548522145, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523171, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523739, 1697548523790, 1697548523858, 1697548523909, 1697548523972, 1697548524037, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526250, 1697548526310, 1697548526963, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527271]"
1468,1468,208,44,[],200,llama-7b,64,1,2177.0,1.0,1,A100,1697548528272,1697548530449,120,96.0,20.0,"[11, 613, 59, 51, 51, 48, 47, 39, 365, 51, 50, 41, 41, 40, 50, 347, 55, 48, 59, 59, 52]","[1697548528283, 1697548528896, 1697548528955, 1697548529006, 1697548529057, 1697548529105, 1697548529152, 1697548529191, 1697548529556, 1697548529607, 1697548529657, 1697548529698, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530279, 1697548530338, 1697548530397, 1697548530449]"
1469,1469,839,15,[],200,llama-7b,64,1,984.0,1.0,1,A100,1697548488029,1697548489013,120,58.0,5.0,"[45, 758, 56, 52, 35, 38]","[1697548488074, 1697548488832, 1697548488888, 1697548488940, 1697548488975, 1697548489013]"
1470,1470,860,25,[],200,llama-7b,64,1,3629.0,1.0,1,A100,1697548515399,1697548519028,120,85.0,20.0,"[10, 660, 498, 270, 68, 66, 64, 64, 293, 66, 64, 59, 54, 672, 65, 66, 58, 57, 362, 63, 50]","[1697548515409, 1697548516069, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517635, 1697548518307, 1697548518372, 1697548518438, 1697548518496, 1697548518553, 1697548518915, 1697548518978, 1697548519028]"
1471,1471,316,22,[],200,llama-7b,64,1,2503.0,1.0,1,A100,1697548495795,1697548498298,120,86.0,20.0,"[20, 510, 56, 40, 41, 50, 50, 447, 68, 64, 64, 59, 56, 55, 262, 61, 47, 47, 45, 400, 61]","[1697548495815, 1697548496325, 1697548496381, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497009, 1697548497077, 1697548497141, 1697548497205, 1697548497264, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497745, 1697548497792, 1697548497837, 1697548498237, 1697548498298]"
1472,1472,609,16,[],200,llama-7b,64,1,2695.0,1.0,1,A100,1697548489015,1697548491710,120,88.0,20.0,"[7, 808, 65, 52, 43, 43, 50, 49, 40, 41, 194, 374, 50, 49, 41, 553, 51, 48, 40, 48, 49]","[1697548489022, 1697548489830, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490407, 1697548490781, 1697548490831, 1697548490880, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710]"
1473,1473,522,7,[],200,llama-7b,64,1,772.0,1.0,1,A100,1697548489595,1697548490367,120,20.0,1.0,"[7, 765]","[1697548489602, 1697548490367]"
1474,1474,299,8,[],200,llama-7b,64,1,357.0,1.0,1,A100,1697548490370,1697548490727,120,14.0,1.0,"[14, 342]","[1697548490384, 1697548490726]"
1475,1475,601,14,[],200,llama-7b,64,1,3645.0,1.0,1,A100,1697548473604,1697548477249,120,83.0,20.0,"[16, 378, 73, 68, 67, 50, 67, 64, 63, 594, 71, 72, 69, 69, 68, 63, 862, 512, 288, 74, 57]","[1697548473620, 1697548473998, 1697548474071, 1697548474139, 1697548474206, 1697548474256, 1697548474323, 1697548474387, 1697548474450, 1697548475044, 1697548475115, 1697548475187, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476318, 1697548476830, 1697548477118, 1697548477192, 1697548477249]"
1476,1476,914,45,[],200,llama-7b,64,1,3231.0,1.0,1,A100,1697548530454,1697548533685,120,84.0,20.0,"[86, 827, 80, 64, 64, 61, 60, 51, 736, 53, 58, 64, 51, 55, 577, 63, 64, 49, 66, 55, 46]","[1697548530540, 1697548531367, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531696, 1697548531747, 1697548532483, 1697548532536, 1697548532594, 1697548532658, 1697548532709, 1697548532764, 1697548533341, 1697548533404, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533684]"
1477,1477,360,17,[],200,llama-7b,64,1,988.0,1.0,1,A100,1697548491713,1697548492701,120,16.0,1.0,"[16, 972]","[1697548491729, 1697548492701]"
1478,1478,12,18,[],200,llama-7b,64,1,845.0,1.0,1,A100,1697548492705,1697548493550,120,11.0,1.0,"[23, 822]","[1697548492728, 1697548493550]"
1479,1479,718,19,[],200,llama-7b,64,1,430.0,1.0,1,A100,1697548493557,1697548493987,120,13.0,1.0,"[35, 395]","[1697548493592, 1697548493987]"
1480,1480,880,9,[],200,llama-7b,64,1,746.0,1.0,1,A100,1697548490729,1697548491475,120,84.0,2.0,"[11, 604, 130]","[1697548490740, 1697548491344, 1697548491474]"
1481,1481,660,10,[],200,llama-7b,64,1,3310.0,1.0,1,A100,1697548491476,1697548494786,120,732.0,25.0,"[6, 498, 58, 55, 52, 43, 52, 50, 49, 722, 58, 49, 47, 40, 358, 47, 37, 38, 312, 52, 42, 45, 432, 60, 47, 60]","[1697548491482, 1697548491980, 1697548492038, 1697548492093, 1697548492145, 1697548492188, 1697548492240, 1697548492290, 1697548492339, 1697548493061, 1697548493119, 1697548493168, 1697548493215, 1697548493255, 1697548493613, 1697548493660, 1697548493697, 1697548493735, 1697548494047, 1697548494099, 1697548494141, 1697548494186, 1697548494618, 1697548494678, 1697548494725, 1697548494785]"
1482,1482,512,26,[],200,llama-7b,64,1,580.0,1.0,1,A100,1697548519030,1697548519610,120,11.0,1.0,"[24, 556]","[1697548519054, 1697548519610]"
1483,1483,371,20,[],200,llama-7b,64,1,561.0,1.0,1,A100,1697548493991,1697548494552,120,13.0,1.0,"[22, 539]","[1697548494013, 1697548494552]"
1484,1484,288,27,[],200,llama-7b,64,1,3175.0,1.0,1,A100,1697548519612,1697548522787,120,93.0,20.0,"[14, 1049, 72, 65, 63, 49, 48, 54, 405, 63, 47, 60, 46, 54, 710, 69, 67, 67, 57, 66, 50]","[1697548519626, 1697548520675, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520972, 1697548521026, 1697548521431, 1697548521494, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787]"
1485,1485,143,21,[],200,llama-7b,64,1,1395.0,1.0,1,A100,1697548494555,1697548495950,120,6.0,12.0,"[22, 602, 64, 50, 58, 58, 55, 53, 41, 233, 55, 53, 51]","[1697548494577, 1697548495179, 1697548495243, 1697548495293, 1697548495351, 1697548495409, 1697548495464, 1697548495517, 1697548495558, 1697548495791, 1697548495846, 1697548495899, 1697548495950]"
1486,1486,788,19,[],200,llama-7b,64,1,1476.0,1.0,1,A100,1697548482673,1697548484149,120,31.0,1.0,"[6, 1469]","[1697548482679, 1697548484148]"
1487,1487,558,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484152,1697548488026,120,,,"[12, 776, 552, 313, 308, 77, 76, 71, 70, 323, 59, 59, 79, 307, 72, 69]","[1697548484164, 1697548484940, 1697548485492, 1697548485805, 1697548486113, 1697548486190, 1697548486266, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234, 1697548487306, 1697548487375]"
1488,1488,744,45,[],200,llama-7b,64,1,1566.0,1.0,1,A100,1697548515470,1697548517036,120,161.0,6.0,"[14, 585, 498, 270, 68, 66, 65]","[1697548515484, 1697548516069, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517036]"
1489,1489,522,46,[],200,llama-7b,64,1,282.0,1.0,1,A100,1697548517040,1697548517322,120,20.0,1.0,"[12, 270]","[1697548517052, 1697548517322]"
1490,1490,176,47,[],200,llama-7b,64,1,983.0,1.0,1,A100,1697548517324,1697548518307,120,216.0,2.0,"[11, 972]","[1697548517335, 1697548518307]"
1491,1491,679,13,[],200,llama-7b,64,1,741.0,1.0,1,A100,1697548479561,1697548480302,120,15.0,1.0,"[14, 727]","[1697548479575, 1697548480302]"
1492,1492,876,48,[],200,llama-7b,64,1,536.0,1.0,1,A100,1697548518310,1697548518846,120,11.0,1.0,"[7, 529]","[1697548518317, 1697548518846]"
1493,1493,551,43,[],200,llama-7b,64,1,3184.0,1.0,1,A100,1697548530400,1697548533584,120,90.0,20.0,"[24, 461, 51, 39, 471, 66, 62, 63, 60, 51, 736, 52, 59, 64, 51, 54, 576, 64, 63, 50, 66]","[1697548530424, 1697548530885, 1697548530936, 1697548530975, 1697548531446, 1697548531512, 1697548531574, 1697548531637, 1697548531697, 1697548531748, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533583]"
1494,1494,218,21,[],200,llama-7b,64,1,1334.0,1.0,1,A100,1697548488030,1697548489364,120,109.0,7.0,"[6, 852, 51, 36, 37, 285, 67]","[1697548488036, 1697548488888, 1697548488939, 1697548488975, 1697548489012, 1697548489297, 1697548489364]"
1495,1495,511,49,[],200,llama-7b,64,1,8422.0,1.0,1,A100,1697548518850,1697548527272,120,364.0,64.0,"[30, 730, 184, 243, 67, 63, 62, 61, 61, 59, 336, 66, 63, 48, 49, 54, 405, 62, 48, 61, 45, 54, 709, 70, 67, 67, 57, 66, 50, 316, 67, 64, 64, 62, 61, 317, 52, 67, 51, 64, 63, 554, 64, 66, 65, 59, 58, 588, 67, 66, 51, 50, 65, 65, 241, 49, 47, 60, 59, 654, 66, 50, 64, 65, 63]","[1697548518880, 1697548519610, 1697548519794, 1697548520037, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520351, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521602, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524035, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526094, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526963, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527271]"
1496,1496,919,22,[],200,llama-7b,64,1,465.0,1.0,1,A100,1697548489366,1697548489831,120,14.0,1.0,"[13, 451]","[1697548489379, 1697548489830]"
1497,1497,335,14,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548480306,1697548488026,120,,,"[9, 1207, 290, 268, 63, 74, 265, 68, 51, 68, 839, 75, 55, 73, 66, 66, 49, 338, 76, 79, 71, 70, 58, 70, 838, 314, 308, 76, 76, 70, 71, 324, 59, 59, 79, 307, 70, 70]","[1697548480315, 1697548481522, 1697548481812, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484385, 1697548484456, 1697548484526, 1697548484584, 1697548484654, 1697548485492, 1697548485806, 1697548486114, 1697548486190, 1697548486266, 1697548486336, 1697548486407, 1697548486731, 1697548486790, 1697548486849, 1697548486928, 1697548487235, 1697548487305, 1697548487375]"
1498,1498,572,23,[],200,llama-7b,64,1,533.0,1.0,1,A100,1697548489834,1697548490367,120,16.0,1.0,"[24, 509]","[1697548489858, 1697548490367]"
1499,1499,348,26,[],200,llama-7b,64,1,5755.0,1.0,1,A100,1697548511150,1697548516905,120,91.0,20.0,"[6, 1303, 310, 350, 59, 55, 43, 54, 833, 68, 59, 58, 57, 920, 71, 71, 63, 60, 977, 270, 68]","[1697548511156, 1697548512459, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514163, 1697548514231, 1697548514290, 1697548514348, 1697548514405, 1697548515325, 1697548515396, 1697548515467, 1697548515530, 1697548515590, 1697548516567, 1697548516837, 1697548516905]"
1500,1500,323,24,[],200,llama-7b,64,1,2752.0,1.0,1,A100,1697548490369,1697548493121,120,84.0,20.0,"[6, 351, 55, 50, 49, 41, 553, 51, 48, 40, 48, 49, 329, 54, 52, 43, 51, 51, 48, 724, 58]","[1697548490375, 1697548490726, 1697548490781, 1697548490831, 1697548490880, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492039, 1697548492093, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492338, 1697548493062, 1697548493120]"
1501,1501,544,21,[],200,llama-7b,64,1,375.0,1.0,1,A100,1697548486273,1697548486648,120,26.0,1.0,"[11, 363]","[1697548486284, 1697548486647]"
1502,1502,291,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548486649,1697548488025,120,,,[14],[1697548486663]
1503,1503,50,40,[],200,llama-7b,64,1,1131.0,1.0,1,A100,1697548544889,1697548546020,120,90.0,4.0,"[28, 598, 369, 71, 65]","[1697548544917, 1697548545515, 1697548545884, 1697548545955, 1697548546020]"
1504,1504,874,23,[],200,llama-7b,64,1,6107.0,1.0,1,A100,1697548488033,1697548494140,120,140.0,50.0,"[154, 646, 55, 52, 35, 38, 284, 68, 49, 49, 40, 48, 40, 305, 51, 43, 43, 51, 47, 41, 43, 192, 374, 49, 49, 42, 553, 50, 49, 40, 48, 49, 328, 54, 52, 43, 52, 50, 49, 722, 59, 48, 49, 39, 356, 48, 38, 38, 311, 52, 42]","[1697548488187, 1697548488833, 1697548488888, 1697548488940, 1697548488975, 1697548489013, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490131, 1697548490172, 1697548490215, 1697548490407, 1697548490781, 1697548490830, 1697548490879, 1697548490921, 1697548491474, 1697548491524, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492038, 1697548492092, 1697548492144, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493060, 1697548493119, 1697548493167, 1697548493216, 1697548493255, 1697548493611, 1697548493659, 1697548493697, 1697548493735, 1697548494046, 1697548494098, 1697548494140]"
1505,1505,373,13,[],200,llama-7b,64,1,628.0,1.0,1,A100,1697548471123,1697548471751,120,15.0,1.0,"[17, 611]","[1697548471140, 1697548471751]"
1506,1506,150,14,[],200,llama-7b,64,1,780.0,1.0,1,A100,1697548471752,1697548472532,120,216.0,2.0,"[16, 763]","[1697548471768, 1697548472531]"
1507,1507,734,15,[],200,llama-7b,64,1,1134.0,1.0,1,A100,1697548472533,1697548473667,120,100.0,6.0,"[6, 647, 249, 67, 49, 51, 65]","[1697548472539, 1697548473186, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473667]"
1508,1508,94,25,[],200,llama-7b,64,1,2287.0,1.0,1,A100,1697548493123,1697548495410,120,86.0,20.0,"[15, 412, 62, 48, 38, 38, 311, 51, 42, 46, 433, 60, 47, 60, 55, 55, 51, 296, 49, 58, 59]","[1697548493138, 1697548493550, 1697548493612, 1697548493660, 1697548493698, 1697548493736, 1697548494047, 1697548494098, 1697548494140, 1697548494186, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494896, 1697548494947, 1697548495243, 1697548495292, 1697548495350, 1697548495409]"
1509,1509,628,41,[],200,llama-7b,64,1,1703.0,1.0,1,A100,1697548546024,1697548547727,120,732.0,10.0,"[20, 1033, 67, 64, 58, 48, 58, 46, 256, 53]","[1697548546044, 1697548547077, 1697548547144, 1697548547208, 1697548547266, 1697548547314, 1697548547372, 1697548547418, 1697548547674, 1697548547727]"
1510,1510,464,24,[],200,llama-7b,64,1,451.0,1.0,1,A100,1697548494729,1697548495180,120,12.0,1.0,"[15, 435]","[1697548494744, 1697548495179]"
1511,1511,113,25,[],200,llama-7b,64,1,543.0,1.0,1,A100,1697548495184,1697548495727,120,13.0,1.0,"[9, 534]","[1697548495193, 1697548495727]"
1512,1512,159,40,[],200,llama-7b,64,1,312.0,1.0,1,A100,1697548537862,1697548538174,120,31.0,1.0,"[27, 285]","[1697548537889, 1697548538174]"
1513,1513,819,26,[],200,llama-7b,64,1,594.0,1.0,1,A100,1697548495731,1697548496325,120,13.0,1.0,"[19, 575]","[1697548495750, 1697548496325]"
1514,1514,52,17,[],200,llama-7b,64,1,2286.0,1.0,1,A100,1697548461608,1697548463894,120,58.0,6.0,"[12, 1213, 73, 56, 57, 54, 821]","[1697548461620, 1697548462833, 1697548462906, 1697548462962, 1697548463019, 1697548463073, 1697548463894]"
1515,1515,741,41,[],200,llama-7b,64,1,1690.0,1.0,1,A100,1697548538177,1697548539867,120,364.0,9.0,"[16, 639, 52, 55, 42, 46, 46, 677, 61, 56]","[1697548538193, 1697548538832, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539073, 1697548539750, 1697548539811, 1697548539867]"
1516,1516,471,27,[],200,llama-7b,64,1,2534.0,1.0,1,A100,1697548496328,1697548498862,120,86.0,20.0,"[14, 590, 76, 68, 64, 64, 59, 57, 55, 262, 61, 48, 46, 45, 400, 62, 51, 52, 49, 351, 60]","[1697548496342, 1697548496932, 1697548497008, 1697548497076, 1697548497140, 1697548497204, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498862]"
1517,1517,623,15,[],200,llama-7b,64,1,991.0,1.0,1,A100,1697548462965,1697548463956,120,140.0,3.0,"[15, 914, 62]","[1697548462980, 1697548463894, 1697548463956]"
1518,1518,247,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548498871,1697548527818,120,,,"[25, 524, 368, 62, 61, 60, 54, 51, 421, 65, 50, 49, 58, 55, 723, 63, 62, 58, 54, 56, 620, 65, 64, 62, 59, 55, 876, 63, 58, 57, 53, 41, 704, 62, 47, 60, 59, 59, 57, 351, 65, 51, 61, 60, 59, 284, 45, 55, 51, 679, 65, 59, 55, 43, 581, 64, 62, 59, 56, 57, 324, 65, 64, 60, 59, 55, 42, 305, 57, 55, 49, 40, 41, 579, 56, 54, 53, 53, 50, 559, 59, 59, 46, 59, 56, 230, 57, 42, 43, 52, 50, 655, 57, 54, 53, 51, 753, 350, 58, 56, 43, 53, 834, 66, 60, 58, 58, 919, 71, 70, 64, 61, 976, 270, 69, 65, 65, 63, 294, 66, 64, 59, 53, 672, 66, 64, 59, 58, 362, 62, 50, 58, 57, 45, 607, 242, 68, 62, 62, 62, 60, 60, 336, 66, 63, 48, 49, 53, 405, 63, 48, 60, 46, 54, 709, 70, 67, 67, 57, 66, 49, 317, 67, 64, 64, 62, 61, 317, 52, 67, 51, 63, 64, 553, 65, 66, 65, 58, 58, 588, 68, 65, 51, 50, 65, 66, 242, 48, 47, 60, 59, 654, 66, 49, 65, 64, 64]","[1697548498896, 1697548499420, 1697548499788, 1697548499850, 1697548499911, 1697548499971, 1697548500025, 1697548500076, 1697548500497, 1697548500562, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501497, 1697548501560, 1697548501622, 1697548501680, 1697548501734, 1697548501790, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505558, 1697548505842, 1697548505887, 1697548505942, 1697548505993, 1697548506672, 1697548506737, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508747, 1697548508804, 1697548508859, 1697548508908, 1697548508948, 1697548508989, 1697548509568, 1697548509624, 1697548509678, 1697548509731, 1697548509784, 1697548509834, 1697548510393, 1697548510452, 1697548510511, 1697548510557, 1697548510616, 1697548510672, 1697548510902, 1697548510959, 1697548511001, 1697548511044, 1697548511096, 1697548511146, 1697548511801, 1697548511858, 1697548511912, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513177, 1697548513233, 1697548513276, 1697548513329, 1697548514163, 1697548514229, 1697548514289, 1697548514347, 1697548514405, 1697548515324, 1697548515395, 1697548515465, 1697548515529, 1697548515590, 1697548516566, 1697548516836, 1697548516905, 1697548516970, 1697548517035, 1697548517098, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517634, 1697548518306, 1697548518372, 1697548518436, 1697548518495, 1697548518553, 1697548518915, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521025, 1697548521430, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522786, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523971, 1697548524035, 1697548524588, 1697548524653, 1697548524719, 1697548524784, 1697548524842, 1697548524900, 1697548525488, 1697548525556, 1697548525621, 1697548525672, 1697548525722, 1697548525787, 1697548525853, 1697548526095, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526963, 1697548527029, 1697548527078, 1697548527143, 1697548527207, 1697548527271]"
1519,1519,518,42,[],200,llama-7b,64,1,515.0,1.0,1,A100,1697548539871,1697548540386,120,23.0,1.0,"[14, 500]","[1697548539885, 1697548540385]"
1520,1520,169,43,[],200,llama-7b,64,1,687.0,1.0,1,A100,1697548540388,1697548541075,120,10.0,1.0,"[22, 665]","[1697548540410, 1697548541075]"
1521,1521,287,16,[],200,llama-7b,64,1,596.0,1.0,1,A100,1697548463960,1697548464556,120,10.0,1.0,"[6, 590]","[1697548463966, 1697548464556]"
1522,1522,847,44,[],200,llama-7b,64,1,510.0,1.0,1,A100,1697548541081,1697548541591,120,10.0,1.0,"[17, 493]","[1697548541098, 1697548541591]"
1523,1523,496,45,[],200,llama-7b,64,1,2285.0,1.0,1,A100,1697548541597,1697548543882,120,335.0,11.0,"[19, 886, 245, 62, 61, 58, 44, 55, 692, 59, 47, 57]","[1697548541616, 1697548542502, 1697548542747, 1697548542809, 1697548542870, 1697548542928, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882]"
1524,1524,267,46,[],200,llama-7b,64,1,3533.0,1.0,1,A100,1697548543885,1697548547418,120,83.0,20.0,"[16, 621, 255, 55, 54, 46, 951, 71, 65, 66, 61, 62, 47, 63, 760, 65, 64, 59, 48, 58, 46]","[1697548543901, 1697548544522, 1697548544777, 1697548544832, 1697548544886, 1697548544932, 1697548545883, 1697548545954, 1697548546019, 1697548546085, 1697548546146, 1697548546208, 1697548546255, 1697548546318, 1697548547078, 1697548547143, 1697548547207, 1697548547266, 1697548547314, 1697548547372, 1697548547418]"
1525,1525,55,17,[],200,llama-7b,64,1,1343.0,1.0,1,A100,1697548464559,1697548465902,120,12.0,1.0,"[19, 1323]","[1697548464578, 1697548465901]"
1526,1526,616,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548465907,1697548488026,120,,,"[19, 1090, 282, 74, 71, 67, 68, 51, 62, 359, 73, 54, 72, 68, 66, 535, 65, 52, 65, 65, 63, 59, 568, 70, 52, 67, 52, 52, 64, 64, 845, 72, 72, 69, 67, 50, 65, 312, 74, 73, 72, 68, 63, 49, 304, 71, 68, 68, 52, 66, 49, 530, 67, 49, 51, 64, 59, 345, 68, 67, 51, 66, 65, 63, 593, 73, 70, 70, 69, 68, 63, 861, 512, 288, 75, 56, 63, 618, 251, 54, 254, 70, 68, 62, 613, 255, 72, 54, 71, 68, 63, 911, 76, 75, 72, 71, 68, 67, 587, 269, 63, 74, 264, 69, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 75, 75, 71, 57, 70, 837, 314, 308, 76, 76, 72, 70, 323, 59, 59, 79, 307, 71, 70]","[1697548465926, 1697548467016, 1697548467298, 1697548467372, 1697548467443, 1697548467510, 1697548467578, 1697548467629, 1697548467691, 1697548468050, 1697548468123, 1697548468177, 1697548468249, 1697548468317, 1697548468383, 1697548468918, 1697548468983, 1697548469035, 1697548469100, 1697548469165, 1697548469228, 1697548469287, 1697548469855, 1697548469925, 1697548469977, 1697548470044, 1697548470096, 1697548470148, 1697548470212, 1697548470276, 1697548471121, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473666, 1697548473725, 1697548474070, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475186, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478234, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479556, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484527, 1697548484584, 1697548484654, 1697548485491, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
1527,1527,99,26,[],200,llama-7b,64,1,436.0,1.0,1,A100,1697548498303,1697548498739,120,10.0,1.0,"[31, 404]","[1697548498334, 1697548498738]"
1528,1528,806,27,[],200,llama-7b,64,1,3668.0,1.0,1,A100,1697548498742,1697548502410,120,89.0,20.0,"[28, 649, 369, 61, 62, 59, 55, 51, 420, 65, 51, 49, 58, 55, 724, 63, 61, 57, 56, 55, 620]","[1697548498770, 1697548499419, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500496, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501498, 1697548501561, 1697548501622, 1697548501679, 1697548501735, 1697548501790, 1697548502410]"
1529,1529,487,15,[],200,llama-7b,64,1,3424.0,1.0,1,A100,1697548464019,1697548467443,120,123.0,17.0,"[13, 827, 459, 65, 53, 56, 45, 701, 72, 69, 54, 64, 64, 48, 691, 72, 71]","[1697548464032, 1697548464859, 1697548465318, 1697548465383, 1697548465436, 1697548465492, 1697548465537, 1697548466238, 1697548466310, 1697548466379, 1697548466433, 1697548466497, 1697548466561, 1697548466609, 1697548467300, 1697548467372, 1697548467443]"
1530,1530,343,9,[],200,llama-7b,64,1,3015.0,1.0,1,A100,1697548457696,1697548460711,120,84.0,20.0,"[16, 632, 272, 72, 62, 48, 49, 50, 204, 47, 58, 48, 57, 57, 619, 64, 49, 64, 59, 61, 427]","[1697548457712, 1697548458344, 1697548458616, 1697548458688, 1697548458750, 1697548458798, 1697548458847, 1697548458897, 1697548459101, 1697548459148, 1697548459206, 1697548459254, 1697548459311, 1697548459368, 1697548459987, 1697548460051, 1697548460100, 1697548460164, 1697548460223, 1697548460284, 1697548460711]"
1531,1531,732,22,[],200,llama-7b,64,1,1369.0,1.0,1,A100,1697548495952,1697548497321,120,345.0,12.0,"[11, 362, 56, 40, 41, 50, 50, 447, 67, 63, 61, 63, 57]","[1697548495963, 1697548496325, 1697548496381, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497009, 1697548497076, 1697548497139, 1697548497200, 1697548497263, 1697548497320]"
1532,1532,872,28,[],200,llama-7b,64,1,3064.0,1.0,1,A100,1697548522790,1697548525854,120,91.0,20.0,"[19, 856, 73, 52, 67, 51, 64, 64, 552, 66, 65, 65, 60, 57, 589, 67, 65, 51, 50, 65, 66]","[1697548522809, 1697548523665, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524036, 1697548524588, 1697548524654, 1697548524719, 1697548524784, 1697548524844, 1697548524901, 1697548525490, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525854]"
1533,1533,696,27,[],200,llama-7b,64,1,4129.0,1.0,1,A100,1697548508991,1697548513120,120,83.0,20.0,"[11, 1325, 66, 60, 59, 45, 59, 57, 231, 56, 42, 43, 52, 50, 656, 55, 55, 52, 52, 752, 351]","[1697548509002, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510557, 1697548510616, 1697548510673, 1697548510904, 1697548510960, 1697548511002, 1697548511045, 1697548511097, 1697548511147, 1697548511803, 1697548511858, 1697548511913, 1697548511965, 1697548512017, 1697548512769, 1697548513120]"
1534,1534,502,23,[],200,llama-7b,64,1,250.0,1.0,1,A100,1697548497323,1697548497573,120,19.0,1.0,"[10, 239]","[1697548497333, 1697548497572]"
1535,1535,157,24,[],200,llama-7b,64,1,8311.0,1.0,1,A100,1697548497576,1697548505887,120,563.0,55.0,"[17, 578, 66, 61, 52, 52, 50, 350, 60, 50, 50, 826, 61, 62, 59, 55, 51, 421, 65, 50, 49, 59, 54, 723, 64, 60, 58, 56, 54, 620, 66, 63, 62, 60, 55, 876, 62, 58, 58, 52, 41, 704, 63, 47, 60, 59, 58, 57, 352, 65, 51, 61, 60, 59, 284, 45]","[1697548497593, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498402, 1697548498452, 1697548498802, 1697548498862, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500497, 1697548500562, 1697548500612, 1697548500661, 1697548500720, 1697548500774, 1697548501497, 1697548501561, 1697548501621, 1697548501679, 1697548501735, 1697548501789, 1697548502409, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502715, 1697548503591, 1697548503653, 1697548503711, 1697548503769, 1697548503821, 1697548503862, 1697548504566, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504853, 1697548504910, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505558, 1697548505842, 1697548505887]"
1536,1536,102,15,[],200,llama-7b,64,1,2183.0,1.0,1,A100,1697548488030,1697548490213,120,84.0,20.0,"[21, 780, 57, 52, 35, 37, 285, 67, 50, 49, 40, 48, 40, 304, 52, 43, 42, 51, 49, 40, 41]","[1697548488051, 1697548488831, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489895, 1697548489947, 1697548489990, 1697548490032, 1697548490083, 1697548490132, 1697548490172, 1697548490213]"
1537,1537,279,50,[],200,llama-7b,64,1,3003.0,1.0,1,A100,1697548527276,1697548530279,120,67.0,18.0,"[14, 607, 401, 656, 52, 50, 48, 47, 39, 365, 51, 50, 41, 41, 41, 49, 347, 56, 48]","[1697548527290, 1697548527897, 1697548528298, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529190, 1697548529555, 1697548529606, 1697548529656, 1697548529697, 1697548529738, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279]"
1538,1538,886,20,[],200,llama-7b,64,1,504.0,1.0,1,A100,1697548491477,1697548491981,120,17.0,1.0,"[6, 497]","[1697548491483, 1697548491980]"
1539,1539,547,21,[],200,llama-7b,64,1,718.0,1.0,1,A100,1697548491984,1697548492702,120,12.0,1.0,"[13, 705]","[1697548491997, 1697548492702]"
1540,1540,316,22,[],200,llama-7b,64,1,2705.0,1.0,1,A100,1697548492704,1697548495409,120,86.0,20.0,"[19, 828, 61, 47, 38, 39, 310, 52, 42, 46, 432, 61, 47, 60, 55, 54, 51, 297, 49, 58, 59]","[1697548492723, 1697548493551, 1697548493612, 1697548493659, 1697548493697, 1697548493736, 1697548494046, 1697548494098, 1697548494140, 1697548494186, 1697548494618, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495409]"
1541,1541,693,16,[],200,llama-7b,64,1,563.0,1.0,1,A100,1697548490218,1697548490781,120,67.0,2.0,"[36, 472, 55]","[1697548490254, 1697548490726, 1697548490781]"
1542,1542,462,17,[],200,llama-7b,64,1,561.0,1.0,1,A100,1697548490784,1697548491345,120,52.0,1.0,"[6, 555]","[1697548490790, 1697548491345]"
1543,1543,705,35,[],200,llama-7b,64,1,4383.0,1.0,1,A100,1697548502413,1697548506796,120,79.0,27.0,"[22, 1156, 63, 58, 57, 53, 41, 704, 62, 47, 60, 60, 58, 56, 352, 66, 50, 61, 60, 60, 283, 45, 55, 51, 679, 65, 58]","[1697548502435, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504796, 1697548504854, 1697548504910, 1697548505262, 1697548505328, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505887, 1697548505942, 1697548505993, 1697548506672, 1697548506737, 1697548506795]"
1544,1544,868,51,[],200,llama-7b,64,1,3302.0,1.0,1,A100,1697548530282,1697548533584,120,85.0,20.0,"[6, 597, 51, 38, 473, 64, 65, 61, 59, 51, 737, 52, 59, 64, 51, 54, 576, 64, 63, 50, 66]","[1697548530288, 1697548530885, 1697548530936, 1697548530974, 1697548531447, 1697548531511, 1697548531576, 1697548531637, 1697548531696, 1697548531747, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533583]"
1545,1545,645,24,[],200,llama-7b,64,1,1860.0,1.0,1,A100,1697548494143,1697548496003,120,86.0,20.0,"[6, 403, 67, 60, 47, 60, 55, 54, 52, 296, 49, 58, 59, 54, 54, 41, 234, 55, 52, 52, 52]","[1697548494149, 1697548494552, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494947, 1697548495243, 1697548495292, 1697548495350, 1697548495409, 1697548495463, 1697548495517, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496003]"
1546,1546,474,36,[],200,llama-7b,64,1,4104.0,1.0,1,A100,1697548506798,1697548510902,120,109.0,33.0,"[7, 671, 64, 61, 59, 57, 57, 323, 65, 64, 61, 58, 56, 41, 305, 57, 55, 50, 40, 40, 580, 56, 54, 53, 53, 49, 559, 59, 59, 47, 58, 56, 230]","[1697548506805, 1697548507476, 1697548507540, 1697548507601, 1697548507660, 1697548507717, 1697548507774, 1697548508097, 1697548508162, 1697548508226, 1697548508287, 1697548508345, 1697548508401, 1697548508442, 1697548508747, 1697548508804, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509569, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509834, 1697548510393, 1697548510452, 1697548510511, 1697548510558, 1697548510616, 1697548510672, 1697548510902]"
1547,1547,118,18,[],200,llama-7b,64,1,2793.0,1.0,1,A100,1697548491348,1697548494141,120,85.0,20.0,"[15, 617, 58, 54, 53, 43, 51, 51, 49, 721, 59, 49, 48, 39, 357, 47, 38, 38, 312, 52, 42]","[1697548491363, 1697548491980, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492339, 1697548493060, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493612, 1697548493659, 1697548493697, 1697548493735, 1697548494047, 1697548494099, 1697548494141]"
1548,1548,461,15,[],200,llama-7b,64,1,4554.0,1.0,1,A100,1697548469585,1697548474139,120,216.0,30.0,"[14, 1522, 72, 72, 69, 67, 50, 65, 312, 74, 73, 72, 67, 64, 49, 305, 69, 70, 68, 51, 66, 48, 532, 66, 50, 50, 65, 57, 347, 68]","[1697548469599, 1697548471121, 1697548471193, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472227, 1697548472532, 1697548472601, 1697548472671, 1697548472739, 1697548472790, 1697548472856, 1697548472904, 1697548473436, 1697548473502, 1697548473552, 1697548473602, 1697548473667, 1697548473724, 1697548474071, 1697548474139]"
1549,1549,788,24,[],200,llama-7b,64,1,380.0,1.0,1,A100,1697548486268,1697548486648,120,31.0,1.0,"[11, 368]","[1697548486279, 1697548486647]"
1550,1550,309,25,[],200,llama-7b,64,1,2854.0,1.0,1,A100,1697548496007,1697548498861,120,52.0,20.0,"[33, 891, 76, 69, 64, 61, 62, 57, 56, 261, 61, 48, 46, 45, 400, 62, 51, 52, 49, 351, 59]","[1697548496040, 1697548496931, 1697548497007, 1697548497076, 1697548497140, 1697548497201, 1697548497263, 1697548497320, 1697548497376, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498861]"
1551,1551,823,19,[],200,llama-7b,64,1,1860.0,1.0,1,A100,1697548494143,1697548496003,120,90.0,20.0,"[11, 398, 67, 60, 47, 60, 55, 54, 52, 296, 49, 59, 58, 54, 54, 41, 234, 55, 52, 52, 52]","[1697548494154, 1697548494552, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494947, 1697548495243, 1697548495292, 1697548495351, 1697548495409, 1697548495463, 1697548495517, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496003]"
1552,1552,133,37,[],200,llama-7b,64,1,655.0,1.0,1,A100,1697548510905,1697548511560,120,15.0,1.0,"[6, 648]","[1697548510911, 1697548511559]"
1553,1553,831,38,[],200,llama-7b,64,1,896.0,1.0,1,A100,1697548511563,1697548512459,120,11.0,1.0,"[27, 869]","[1697548511590, 1697548512459]"
1554,1554,579,39,[],200,llama-7b,64,1,1319.0,1.0,1,A100,1697548512464,1697548513783,120,19.0,1.0,"[32, 1287]","[1697548512496, 1697548513783]"
1555,1555,477,20,[],200,llama-7b,64,1,7761.0,1.0,1,A100,1697548496008,1697548503769,120,244.0,50.0,"[47, 876, 77, 68, 64, 64, 59, 57, 55, 262, 61, 48, 46, 45, 400, 62, 51, 52, 49, 351, 59, 51, 49, 826, 62, 62, 59, 54, 52, 421, 64, 51, 49, 58, 55, 723, 64, 60, 58, 56, 54, 620, 66, 63, 62, 60, 55, 876, 62, 58, 58]","[1697548496055, 1697548496931, 1697548497008, 1697548497076, 1697548497140, 1697548497204, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499787, 1697548499849, 1697548499911, 1697548499970, 1697548500024, 1697548500076, 1697548500497, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501497, 1697548501561, 1697548501621, 1697548501679, 1697548501735, 1697548501789, 1697548502409, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502715, 1697548503591, 1697548503653, 1697548503711, 1697548503769]"
1556,1556,80,26,[],200,llama-7b,64,1,556.0,1.0,1,A100,1697548498864,1697548499420,120,13.0,1.0,"[17, 539]","[1697548498881, 1697548499420]"
1557,1557,667,27,[],200,llama-7b,64,1,3178.0,1.0,1,A100,1697548499423,1697548502601,120,364.0,17.0,"[27, 975, 72, 65, 50, 49, 59, 54, 723, 63, 63, 57, 54, 56, 621, 64, 64, 62]","[1697548499450, 1697548500425, 1697548500497, 1697548500562, 1697548500612, 1697548500661, 1697548500720, 1697548500774, 1697548501497, 1697548501560, 1697548501623, 1697548501680, 1697548501734, 1697548501790, 1697548502411, 1697548502475, 1697548502539, 1697548502601]"
1558,1558,638,52,[],200,llama-7b,64,1,2978.0,1.0,1,A100,1697548533591,1697548536569,120,88.0,20.0,"[21, 319, 65, 59, 46, 45, 59, 58, 368, 46, 59, 59, 56, 707, 59, 59, 58, 55, 668, 63, 48]","[1697548533612, 1697548533931, 1697548533996, 1697548534055, 1697548534101, 1697548534146, 1697548534205, 1697548534263, 1697548534631, 1697548534677, 1697548534736, 1697548534795, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568]"
1559,1559,437,28,[],200,llama-7b,64,1,4290.0,1.0,1,A100,1697548502604,1697548506894,120,91.0,29.0,"[8, 518, 462, 62, 58, 57, 53, 41, 704, 62, 47, 60, 59, 59, 57, 352, 65, 50, 61, 60, 60, 282, 46, 55, 52, 678, 65, 59, 55, 43]","[1697548502612, 1697548503130, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504854, 1697548504911, 1697548505263, 1697548505328, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505841, 1697548505887, 1697548505942, 1697548505994, 1697548506672, 1697548506737, 1697548506796, 1697548506851, 1697548506894]"
1560,1560,593,40,[],200,llama-7b,64,1,1454.0,1.0,1,A100,1697548521649,1697548523103,120,335.0,9.0,"[24, 472, 266, 69, 68, 66, 57, 66, 50, 316]","[1697548521673, 1697548522145, 1697548522411, 1697548522480, 1697548522548, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103]"
1561,1561,362,41,[],200,llama-7b,64,1,559.0,1.0,1,A100,1697548523107,1697548523666,120,14.0,1.0,"[6, 553]","[1697548523113, 1697548523666]"
1562,1562,24,42,[],200,llama-7b,64,1,1888.0,1.0,1,A100,1697548523668,1697548525556,120,79.0,9.0,"[6, 915, 64, 66, 65, 59, 58, 588, 67]","[1697548523674, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556]"
1563,1563,385,19,[],200,llama-7b,64,1,5224.0,1.0,1,A100,1697548488031,1697548493255,120,52.0,43.0,"[50, 751, 57, 51, 35, 37, 285, 67, 50, 49, 40, 49, 40, 303, 52, 43, 43, 50, 49, 40, 42, 193, 374, 49, 49, 42, 553, 50, 49, 40, 48, 48, 329, 54, 53, 42, 52, 50, 49, 722, 59, 48, 48, 40]","[1697548488081, 1697548488832, 1697548488889, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490214, 1697548490407, 1697548490781, 1697548490830, 1697548490879, 1697548490921, 1697548491474, 1697548491524, 1697548491573, 1697548491613, 1697548491661, 1697548491709, 1697548492038, 1697548492092, 1697548492145, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493060, 1697548493119, 1697548493167, 1697548493215, 1697548493255]"
1564,1564,858,47,[],200,llama-7b,64,1,2357.0,1.0,1,A100,1697548547423,1697548549780,120,182.0,12.0,"[42, 680, 49, 49, 810, 61, 51, 59, 47, 58, 55, 396]","[1697548547465, 1697548548145, 1697548548194, 1697548548243, 1697548549053, 1697548549114, 1697548549165, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549780]"
1565,1565,720,11,[],200,llama-7b,64,1,1316.0,1.0,1,A100,1697548490923,1697548492239,120,286.0,6.0,"[8, 1049, 58, 54, 53, 43, 51]","[1697548490931, 1697548491980, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239]"
1566,1566,717,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548525559,1697548527818,120,,,"[10, 460, 66, 48, 48, 60, 59, 653, 66, 50, 64, 65, 63]","[1697548525569, 1697548526029, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526963, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527271]"
1567,1567,836,15,[],200,llama-7b,64,1,799.0,1.0,1,A100,1697548488033,1697548488832,120,11.0,1.0,"[133, 666]","[1697548488166, 1697548488832]"
1568,1568,584,16,[],200,llama-7b,64,1,406.0,1.0,1,A100,1697548488836,1697548489242,120,10.0,1.0,"[36, 369]","[1697548488872, 1697548489241]"
1569,1569,241,17,[],200,llama-7b,64,1,586.0,1.0,1,A100,1697548489245,1697548489831,120,19.0,1.0,"[35, 551]","[1697548489280, 1697548489831]"
1570,1570,12,18,[],200,llama-7b,64,1,533.0,1.0,1,A100,1697548489834,1697548490367,120,11.0,1.0,"[19, 513]","[1697548489853, 1697548490366]"
1571,1571,602,19,[],200,llama-7b,64,1,356.0,1.0,1,A100,1697548490370,1697548490726,120,15.0,1.0,"[10, 346]","[1697548490380, 1697548490726]"
1572,1572,371,20,[],200,llama-7b,64,1,614.0,1.0,1,A100,1697548490731,1697548491345,120,13.0,1.0,"[23, 591]","[1697548490754, 1697548491345]"
1573,1573,33,21,[],200,llama-7b,64,1,943.0,1.0,1,A100,1697548491347,1697548492290,120,140.0,7.0,"[13, 620, 58, 54, 53, 43, 51, 51]","[1697548491360, 1697548491980, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492290]"
1574,1574,496,12,[],200,llama-7b,64,1,1806.0,1.0,1,A100,1697548492242,1697548494048,120,335.0,11.0,"[18, 441, 360, 59, 48, 48, 39, 357, 48, 38, 37, 313]","[1697548492260, 1697548492701, 1697548493061, 1697548493120, 1697548493168, 1697548493216, 1697548493255, 1697548493612, 1697548493660, 1697548493698, 1697548493735, 1697548494048]"
1575,1575,149,13,[],200,llama-7b,64,1,1241.0,1.0,1,A100,1697548494051,1697548495292,120,563.0,10.0,"[19, 481, 68, 60, 47, 60, 55, 54, 52, 296, 49]","[1697548494070, 1697548494551, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494947, 1697548495243, 1697548495292]"
1576,1576,851,14,[],200,llama-7b,64,1,432.0,1.0,1,A100,1697548495296,1697548495728,120,23.0,1.0,"[13, 418]","[1697548495309, 1697548495727]"
1577,1577,514,15,[],200,llama-7b,64,1,2567.0,1.0,1,A100,1697548495731,1697548498298,120,85.0,20.0,"[22, 572, 55, 41, 41, 50, 49, 447, 69, 64, 63, 60, 56, 55, 262, 60, 48, 46, 46, 400, 61]","[1697548495753, 1697548496325, 1697548496380, 1697548496421, 1697548496462, 1697548496512, 1697548496561, 1697548497008, 1697548497077, 1697548497141, 1697548497204, 1697548497264, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497791, 1697548497837, 1697548498237, 1697548498298]"
1578,1578,255,12,[],200,llama-7b,64,1,18215.0,1.0,1,A100,1697548466439,1697548484654,120,216.0,119.0,"[27, 550, 283, 74, 70, 68, 67, 51, 63, 359, 72, 54, 71, 70, 64, 536, 64, 51, 67, 64, 63, 59, 570, 69, 52, 68, 51, 52, 64, 64, 846, 72, 71, 69, 68, 49, 65, 312, 74, 73, 72, 68, 63, 49, 304, 71, 68, 68, 52, 66, 49, 530, 67, 49, 51, 64, 59, 345, 68, 67, 51, 66, 65, 62, 594, 72, 71, 70, 69, 68, 63, 861, 512, 288, 75, 56, 63, 618, 251, 55, 253, 70, 68, 62, 613, 255, 72, 54, 71, 68, 63, 911, 76, 75, 72, 71, 68, 67, 587, 269, 63, 74, 264, 69, 51, 68, 839, 75, 56, 72, 66, 65, 50, 338, 76, 75, 75, 71, 57, 70]","[1697548466466, 1697548467016, 1697548467299, 1697548467373, 1697548467443, 1697548467511, 1697548467578, 1697548467629, 1697548467692, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468318, 1697548468382, 1697548468918, 1697548468982, 1697548469033, 1697548469100, 1697548469164, 1697548469227, 1697548469286, 1697548469856, 1697548469925, 1697548469977, 1697548470045, 1697548470096, 1697548470148, 1697548470212, 1697548470276, 1697548471122, 1697548471194, 1697548471265, 1697548471334, 1697548471402, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472790, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473666, 1697548473725, 1697548474070, 1697548474138, 1697548474205, 1697548474256, 1697548474322, 1697548474387, 1697548474449, 1697548475043, 1697548475115, 1697548475186, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478626, 1697548478688, 1697548479301, 1697548479556, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483842, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484527, 1697548484584, 1697548484654]"
1579,1579,729,22,[],200,llama-7b,64,1,770.0,1.0,1,A100,1697548492292,1697548493062,120,874.0,2.0,"[15, 754]","[1697548492307, 1697548493061]"
1580,1580,776,16,[],200,llama-7b,64,1,973.0,1.0,1,A100,1697548479823,1697548480796,120,67.0,2.0,"[9, 470, 494]","[1697548479832, 1697548480302, 1697548480796]"
1581,1581,546,17,[],200,llama-7b,64,1,3657.0,1.0,1,A100,1697548480799,1697548484456,120,93.0,20.0,"[18, 705, 290, 268, 63, 74, 265, 68, 51, 68, 839, 75, 56, 72, 66, 66, 49, 338, 76, 76, 74]","[1697548480817, 1697548481522, 1697548481812, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484382, 1697548484456]"
1582,1582,378,44,[],200,llama-7b,64,1,6378.0,1.0,1,A100,1697548527827,1697548534205,120,93.0,47.0,"[56, 416, 656, 51, 50, 48, 47, 40, 365, 52, 48, 41, 42, 40, 49, 347, 56, 48, 59, 60, 50, 487, 39, 472, 65, 63, 62, 59, 51, 737, 52, 59, 64, 50, 56, 575, 65, 63, 50, 63, 58, 46, 311, 60, 45, 46, 59]","[1697548527883, 1697548528299, 1697548528955, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529556, 1697548529608, 1697548529656, 1697548529697, 1697548529739, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530398, 1697548530448, 1697548530935, 1697548530974, 1697548531446, 1697548531511, 1697548531574, 1697548531636, 1697548531695, 1697548531746, 1697548532483, 1697548532535, 1697548532594, 1697548532658, 1697548532708, 1697548532764, 1697548533339, 1697548533404, 1697548533467, 1697548533517, 1697548533580, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205]"
1583,1583,390,23,[],200,llama-7b,64,1,2345.0,1.0,1,A100,1697548493064,1697548495409,120,84.0,20.0,"[10, 476, 62, 47, 38, 39, 310, 52, 42, 46, 433, 60, 47, 60, 55, 54, 51, 297, 49, 58, 59]","[1697548493074, 1697548493550, 1697548493612, 1697548493659, 1697548493697, 1697548493736, 1697548494046, 1697548494098, 1697548494140, 1697548494186, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495409]"
1584,1584,202,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484457,1697548488027,120,,,"[6, 477, 553, 312, 308, 77, 76, 71, 70, 323, 59, 59, 79, 307, 71, 70]","[1697548484463, 1697548484940, 1697548485493, 1697548485805, 1697548486113, 1697548486190, 1697548486266, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
1585,1585,449,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548486650,1697548488025,120,,,"[13, 1145]","[1697548486663, 1697548487808]"
1586,1586,288,11,[],200,llama-7b,64,1,2180.0,1.0,1,A100,1697548488033,1697548490213,120,93.0,20.0,"[202, 598, 56, 51, 35, 38, 284, 68, 49, 49, 40, 48, 40, 305, 51, 43, 42, 52, 48, 40, 41]","[1697548488235, 1697548488833, 1697548488889, 1697548488940, 1697548488975, 1697548489013, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489896, 1697548489947, 1697548489990, 1697548490032, 1697548490084, 1697548490132, 1697548490172, 1697548490213]"
1587,1587,215,26,[],200,llama-7b,64,1,799.0,1.0,1,A100,1697548488033,1697548488832,120,12.0,1.0,"[123, 676]","[1697548488156, 1697548488832]"
1588,1588,800,27,[],200,llama-7b,64,1,2045.0,1.0,1,A100,1697548488835,1697548490880,120,140.0,20.0,"[22, 440, 67, 50, 49, 40, 49, 40, 303, 52, 43, 43, 50, 49, 40, 41, 194, 374, 50, 49]","[1697548488857, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490407, 1697548490781, 1697548490831, 1697548490880]"
1589,1589,503,16,[],200,llama-7b,64,1,3578.0,1.0,1,A100,1697548473670,1697548477248,120,109.0,20.0,"[15, 312, 74, 68, 67, 51, 66, 64, 63, 593, 73, 71, 69, 70, 67, 63, 862, 511, 288, 75, 56]","[1697548473685, 1697548473997, 1697548474071, 1697548474139, 1697548474206, 1697548474257, 1697548474323, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475187, 1697548475256, 1697548475326, 1697548475393, 1697548475456, 1697548476318, 1697548476829, 1697548477117, 1697548477192, 1697548477248]"
1590,1590,900,19,[],200,llama-7b,64,1,1266.0,1.0,1,A100,1697548488032,1697548489298,120,67.0,6.0,"[110, 689, 58, 51, 35, 37, 286]","[1697548488142, 1697548488831, 1697548488889, 1697548488940, 1697548488975, 1697548489012, 1697548489298]"
1591,1591,705,22,[],200,llama-7b,64,1,7022.0,1.0,1,A100,1697548475460,1697548482482,120,79.0,27.0,"[6, 2463, 251, 55, 253, 70, 69, 61, 614, 254, 71, 55, 72, 68, 61, 913, 76, 74, 72, 71, 68, 67, 588, 268, 64, 74, 264]","[1697548475466, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478627, 1697548478688, 1697548479302, 1697548479556, 1697548479627, 1697548479682, 1697548479754, 1697548479822, 1697548479883, 1697548480796, 1697548480872, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481812, 1697548482080, 1697548482144, 1697548482218, 1697548482482]"
1592,1592,561,20,[],200,llama-7b,64,1,2408.0,1.0,1,A100,1697548489302,1697548491710,120,87.0,20.0,"[13, 515, 66, 51, 44, 42, 51, 48, 41, 41, 193, 374, 50, 49, 41, 553, 51, 48, 40, 48, 49]","[1697548489315, 1697548489830, 1697548489896, 1697548489947, 1697548489991, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490214, 1697548490407, 1697548490781, 1697548490831, 1697548490880, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710]"
1593,1593,569,46,[],200,llama-7b,64,1,877.0,1.0,1,A100,1697548533688,1697548534565,120,16.0,1.0,"[24, 852]","[1697548533712, 1697548534564]"
1594,1594,681,26,[],200,llama-7b,64,1,315.0,1.0,1,A100,1697548495413,1697548495728,120,23.0,1.0,"[22, 292]","[1697548495435, 1697548495727]"
1595,1595,339,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548534566,1697548537856,120,,,"[15, 595, 382, 60, 59, 58, 55, 667, 63, 49, 54, 315, 52, 60, 66, 63, 60, 47]","[1697548534581, 1697548535176, 1697548535558, 1697548535618, 1697548535677, 1697548535735, 1697548535790, 1697548536457, 1697548536520, 1697548536569, 1697548536623, 1697548536938, 1697548536990, 1697548537050, 1697548537116, 1697548537179, 1697548537239, 1697548537286]"
1596,1596,457,28,[],200,llama-7b,64,1,1179.0,1.0,1,A100,1697548502413,1697548503592,120,874.0,2.0,"[12, 1166]","[1697548502425, 1697548503591]"
1597,1597,763,21,[],200,llama-7b,64,1,384.0,1.0,1,A100,1697548510455,1697548510839,120,20.0,1.0,"[15, 369]","[1697548510470, 1697548510839]"
1598,1598,165,17,[],200,llama-7b,64,1,3839.0,1.0,1,A100,1697548477251,1697548481090,120,83.0,20.0,"[22, 406, 251, 251, 54, 253, 71, 68, 61, 613, 255, 72, 54, 72, 67, 62, 913, 76, 75, 71, 72]","[1697548477273, 1697548477679, 1697548477930, 1697548478181, 1697548478235, 1697548478488, 1697548478559, 1697548478627, 1697548478688, 1697548479301, 1697548479556, 1697548479628, 1697548479682, 1697548479754, 1697548479821, 1697548479883, 1697548480796, 1697548480872, 1697548480947, 1697548481018, 1697548481090]"
1599,1599,540,22,[],200,llama-7b,64,1,1124.0,1.0,1,A100,1697548510841,1697548511965,120,140.0,5.0,"[6, 712, 242, 58, 53, 53]","[1697548510847, 1697548511559, 1697548511801, 1697548511859, 1697548511912, 1697548511965]"
1600,1600,233,29,[],200,llama-7b,64,1,718.0,1.0,1,A100,1697548503595,1697548504313,120,6.0,1.0,"[26, 692]","[1697548503621, 1697548504313]"
1601,1601,816,30,[],200,llama-7b,64,1,1064.0,1.0,1,A100,1697548504315,1697548505379,120,182.0,4.0,"[14, 864, 69, 65, 52]","[1697548504329, 1697548505193, 1697548505262, 1697548505327, 1697548505379]"
1602,1602,314,11,[],200,llama-7b,64,1,1213.0,1.0,1,A100,1697548494789,1697548496002,120,335.0,13.0,"[19, 371, 65, 49, 58, 59, 55, 52, 42, 232, 55, 53, 51, 52]","[1697548494808, 1697548495179, 1697548495244, 1697548495293, 1697548495351, 1697548495410, 1697548495465, 1697548495517, 1697548495559, 1697548495791, 1697548495846, 1697548495899, 1697548495950, 1697548496002]"
1603,1603,197,23,[],200,llama-7b,64,1,2196.0,1.0,1,A100,1697548511968,1697548514164,120,6.0,8.0,"[6, 485, 311, 350, 58, 55, 44, 54, 833]","[1697548511974, 1697548512459, 1697548512770, 1697548513120, 1697548513178, 1697548513233, 1697548513277, 1697548513331, 1697548514164]"
1604,1604,85,12,[],200,llama-7b,64,1,2854.0,1.0,1,A100,1697548496007,1697548498861,120,88.0,20.0,"[37, 887, 76, 69, 64, 61, 62, 57, 56, 261, 61, 48, 46, 45, 400, 62, 51, 52, 49, 351, 59]","[1697548496044, 1697548496931, 1697548497007, 1697548497076, 1697548497140, 1697548497201, 1697548497263, 1697548497320, 1697548497376, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498861]"
1605,1605,587,31,[],200,llama-7b,64,1,395.0,1.0,1,A100,1697548505385,1697548505780,120,13.0,1.0,"[15, 379]","[1697548505400, 1697548505779]"
1606,1606,898,24,[],200,llama-7b,64,1,1158.0,1.0,1,A100,1697548514167,1697548515325,120,79.0,2.0,"[6, 1152]","[1697548514173, 1697548515325]"
1607,1607,336,32,[],200,llama-7b,64,1,1694.0,1.0,1,A100,1697548505782,1697548507476,120,58.0,7.0,"[25, 593, 272, 66, 58, 55, 44, 581]","[1697548505807, 1697548506400, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507476]"
1608,1608,558,25,[],200,llama-7b,64,1,3700.0,1.0,1,A100,1697548515328,1697548519028,120,58.0,20.0,"[7, 734, 498, 270, 68, 66, 64, 64, 293, 66, 64, 59, 54, 672, 66, 65, 58, 57, 362, 63, 50]","[1697548515335, 1697548516069, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517635, 1697548518307, 1697548518373, 1697548518438, 1697548518496, 1697548518553, 1697548518915, 1697548518978, 1697548519028]"
1609,1609,327,26,[],200,llama-7b,64,1,1717.0,1.0,1,A100,1697548519030,1697548520747,120,563.0,10.0,"[19, 561, 185, 242, 67, 63, 62, 61, 61, 59, 337]","[1697548519049, 1697548519610, 1697548519795, 1697548520037, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520351, 1697548520410, 1697548520747]"
1610,1610,921,33,[],200,llama-7b,64,1,544.0,1.0,1,A100,1697548507483,1697548508027,120,31.0,1.0,"[18, 525]","[1697548507501, 1697548508026]"
1611,1611,675,13,[],200,llama-7b,64,1,1107.0,1.0,1,A100,1697548498863,1697548499970,120,563.0,5.0,"[8, 549, 368, 61, 62, 59]","[1697548498871, 1697548499420, 1697548499788, 1697548499849, 1697548499911, 1697548499970]"
1612,1612,170,30,[],200,llama-7b,64,1,2989.0,1.0,1,A100,1697548500665,1697548503654,120,335.0,15.0,"[19, 520, 294, 63, 61, 57, 56, 55, 621, 65, 62, 63, 59, 55, 877, 62]","[1697548500684, 1697548501204, 1697548501498, 1697548501561, 1697548501622, 1697548501679, 1697548501735, 1697548501790, 1697548502411, 1697548502476, 1697548502538, 1697548502601, 1697548502660, 1697548502715, 1697548503592, 1697548503654]"
1613,1613,697,34,[],200,llama-7b,64,1,1652.0,1.0,1,A100,1697548508028,1697548509680,120,123.0,10.0,"[17, 634, 69, 57, 54, 50, 40, 40, 579, 58, 54]","[1697548508045, 1697548508679, 1697548508748, 1697548508805, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509568, 1697548509626, 1697548509680]"
1614,1614,510,33,[],200,llama-7b,64,1,578.0,1.0,1,A100,1697548520169,1697548520747,120,79.0,2.0,"[7, 571]","[1697548520176, 1697548520747]"
1615,1615,196,14,[],200,llama-7b,64,1,802.0,1.0,1,A100,1697548488031,1697548488833,120,13.0,1.0,"[58, 743]","[1697548488089, 1697548488832]"
1616,1616,288,34,[],200,llama-7b,64,1,2673.0,1.0,1,A100,1697548520749,1697548523422,120,93.0,20.0,"[7, 607, 68, 62, 48, 60, 46, 54, 710, 70, 66, 68, 56, 66, 50, 317, 66, 64, 64, 62, 62]","[1697548520756, 1697548521363, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522481, 1697548522547, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422]"
1617,1617,900,15,[],200,llama-7b,64,1,669.0,1.0,1,A100,1697548488835,1697548489504,120,67.0,6.0,"[17, 389, 56, 68, 49, 49, 41]","[1697548488852, 1697548489241, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489504]"
1618,1618,889,27,[],200,llama-7b,64,1,2672.0,1.0,1,A100,1697548520750,1697548523422,120,86.0,20.0,"[6, 607, 68, 62, 48, 60, 46, 54, 710, 70, 66, 67, 57, 66, 50, 317, 66, 64, 64, 62, 62]","[1697548520756, 1697548521363, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522481, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422]"
1619,1619,861,18,[],200,llama-7b,64,1,432.0,1.0,1,A100,1697548481091,1697548481523,120,10.0,1.0,"[15, 417]","[1697548481106, 1697548481523]"
1620,1620,553,16,[],200,llama-7b,64,1,2205.0,1.0,1,A100,1697548489506,1697548491711,120,88.0,20.0,"[16, 308, 66, 51, 44, 42, 51, 48, 41, 41, 193, 374, 50, 49, 41, 553, 51, 48, 40, 49, 48]","[1697548489522, 1697548489830, 1697548489896, 1697548489947, 1697548489991, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490214, 1697548490407, 1697548490781, 1697548490831, 1697548490880, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491710]"
1621,1621,350,35,[],200,llama-7b,64,1,645.0,1.0,1,A100,1697548509682,1697548510327,120,216.0,1.0,"[18, 627]","[1697548509700, 1697548510327]"
1622,1622,445,14,[],200,llama-7b,64,1,526.0,1.0,1,A100,1697548499972,1697548500498,120,457.0,2.0,"[7, 447, 72]","[1697548499979, 1697548500426, 1697548500498]"
1623,1623,522,19,[],200,llama-7b,64,1,1433.0,1.0,1,A100,1697548481526,1697548482959,120,20.0,1.0,"[20, 1412]","[1697548481546, 1697548482958]"
1624,1624,126,36,[],200,llama-7b,64,1,508.0,1.0,1,A100,1697548510331,1697548510839,120,19.0,1.0,"[11, 497]","[1697548510342, 1697548510839]"
1625,1625,288,20,[],200,llama-7b,64,1,4274.0,1.0,1,A100,1697548482961,1697548487235,120,93.0,20.0,"[16, 1171, 82, 77, 78, 71, 71, 57, 70, 837, 314, 308, 76, 76, 72, 69, 325, 59, 59, 79, 307]","[1697548482977, 1697548484148, 1697548484230, 1697548484307, 1697548484385, 1697548484456, 1697548484527, 1697548484584, 1697548484654, 1697548485491, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486406, 1697548486731, 1697548486790, 1697548486849, 1697548486928, 1697548487235]"
1626,1626,711,37,[],200,llama-7b,64,1,1072.0,1.0,1,A100,1697548510841,1697548511913,120,457.0,4.0,"[19, 699, 243, 57, 54]","[1697548510860, 1697548511559, 1697548511802, 1697548511859, 1697548511913]"
1627,1627,659,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548469984,1697548488024,120,,,"[6, 715, 416, 73, 71, 69, 68, 49, 66, 311, 74, 73, 72, 67, 64, 49, 304, 70, 69, 69, 51, 66, 48, 532, 66, 50, 50, 65, 58, 346, 68, 66, 51, 67, 64, 63, 593, 73, 71, 69, 69, 68, 63, 861, 512, 288, 75, 56, 63, 618, 251, 55, 253, 70, 69, 61, 614, 255, 71, 54, 71, 68, 63, 911, 77, 74, 72, 71, 68, 67, 587, 269, 63, 74, 264, 69, 51, 68, 839, 75, 56, 72, 66, 65, 50, 338, 76, 75, 75, 71, 57, 70, 838, 313, 308, 76, 76, 72, 70, 323, 59, 60, 78, 307, 71, 70]","[1697548469990, 1697548470705, 1697548471121, 1697548471194, 1697548471265, 1697548471334, 1697548471402, 1697548471451, 1697548471517, 1697548471828, 1697548471902, 1697548471975, 1697548472047, 1697548472114, 1697548472178, 1697548472227, 1697548472531, 1697548472601, 1697548472670, 1697548472739, 1697548472790, 1697548472856, 1697548472904, 1697548473436, 1697548473502, 1697548473552, 1697548473602, 1697548473667, 1697548473725, 1697548474071, 1697548474139, 1697548474205, 1697548474256, 1697548474323, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475187, 1697548475256, 1697548475325, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248, 1697548477311, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478558, 1697548478627, 1697548478688, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480872, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482080, 1697548482143, 1697548482217, 1697548482481, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483842, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486849, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
1628,1628,327,44,[],200,llama-7b,64,1,1146.0,1.0,1,A100,1697548533590,1697548534736,120,563.0,10.0,"[9, 332, 64, 60, 46, 45, 59, 58, 368, 46, 59]","[1697548533599, 1697548533931, 1697548533995, 1697548534055, 1697548534101, 1697548534146, 1697548534205, 1697548534263, 1697548534631, 1697548534677, 1697548534736]"
1629,1629,351,28,[],200,llama-7b,64,1,1283.0,1.0,1,A100,1697548513123,1697548514406,120,216.0,6.0,"[16, 644, 381, 66, 60, 58, 58]","[1697548513139, 1697548513783, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406]"
1630,1630,910,45,[],200,llama-7b,64,1,437.0,1.0,1,A100,1697548534739,1697548535176,120,8.0,1.0,"[14, 423]","[1697548534753, 1697548535176]"
1631,1631,125,29,[],200,llama-7b,64,1,1658.0,1.0,1,A100,1697548514411,1697548516069,120,13.0,1.0,"[17, 1641]","[1697548514428, 1697548516069]"
1632,1632,685,46,[],200,llama-7b,64,1,1279.0,1.0,1,A100,1697548535178,1697548536457,120,364.0,2.0,"[14, 1081, 184]","[1697548535192, 1697548536273, 1697548536457]"
1633,1633,715,30,[],200,llama-7b,64,1,1249.0,1.0,1,A100,1697548516073,1697548517322,120,20.0,1.0,"[15, 1233]","[1697548516088, 1697548517321]"
1634,1634,122,16,[],200,llama-7b,64,1,4486.0,1.0,1,A100,1697548474142,1697548478628,120,88.0,20.0,"[8, 611, 283, 72, 71, 70, 69, 67, 63, 861, 512, 288, 75, 56, 63, 619, 251, 54, 253, 71, 68]","[1697548474150, 1697548474761, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248, 1697548477311, 1697548477930, 1697548478181, 1697548478235, 1697548478488, 1697548478559, 1697548478627]"
1635,1635,339,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536460,1697548537862,120,,,"[11, 395, 72, 51, 61, 66, 63, 60, 47]","[1697548536471, 1697548536866, 1697548536938, 1697548536989, 1697548537050, 1697548537116, 1697548537179, 1697548537239, 1697548537286]"
1636,1636,293,53,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536572,1697548537861,120,,,"[10, 284, 72, 51, 61, 66, 63, 60, 47]","[1697548536582, 1697548536866, 1697548536938, 1697548536989, 1697548537050, 1697548537116, 1697548537179, 1697548537239, 1697548537286]"
1637,1637,482,31,[],200,llama-7b,64,1,3086.0,1.0,1,A100,1697548517325,1697548520411,120,91.0,20.0,"[22, 717, 243, 65, 65, 59, 57, 362, 63, 49, 59, 57, 44, 607, 242, 68, 62, 62, 62, 60, 60]","[1697548517347, 1697548518064, 1697548518307, 1697548518372, 1697548518437, 1697548518496, 1697548518553, 1697548518915, 1697548518978, 1697548519027, 1697548519086, 1697548519143, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410]"
1638,1638,396,18,[],200,llama-7b,64,1,2313.0,1.0,1,A100,1697548506676,1697548508989,120,89.0,20.0,"[6, 600, 194, 63, 62, 59, 56, 57, 324, 66, 63, 62, 57, 55, 42, 306, 56, 55, 49, 41, 40]","[1697548506682, 1697548507282, 1697548507476, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508163, 1697548508226, 1697548508288, 1697548508345, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989]"
1639,1639,70,54,[],200,llama-7b,64,1,954.0,1.0,1,A100,1697548537877,1697548538831,120,39.0,1.0,"[266, 688]","[1697548538143, 1697548538831]"
1640,1640,653,55,[],200,llama-7b,64,1,1131.0,1.0,1,A100,1697548538834,1697548539965,120,96.0,6.0,"[44, 632, 241, 61, 55, 54, 44]","[1697548538878, 1697548539510, 1697548539751, 1697548539812, 1697548539867, 1697548539921, 1697548539965]"
1641,1641,143,32,[],200,llama-7b,64,1,2258.0,1.0,1,A100,1697548520413,1697548522671,120,6.0,12.0,"[11, 939, 68, 62, 48, 60, 46, 54, 710, 70, 66, 67, 57]","[1697548520424, 1697548521363, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522481, 1697548522547, 1697548522614, 1697548522671]"
1642,1642,425,56,[],200,llama-7b,64,1,3059.0,1.0,1,A100,1697548539968,1697548543027,120,88.0,20.0,"[6, 411, 74, 50, 59, 46, 58, 56, 52, 348, 49, 750, 60, 60, 50, 650, 63, 61, 56, 45, 55]","[1697548539974, 1697548540385, 1697548540459, 1697548540509, 1697548540568, 1697548540614, 1697548540672, 1697548540728, 1697548540780, 1697548541128, 1697548541177, 1697548541927, 1697548541987, 1697548542047, 1697548542097, 1697548542747, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027]"
1643,1643,97,29,[],200,llama-7b,64,1,2939.0,1.0,1,A100,1697548506896,1697548509835,120,6.0,20.0,"[14, 1116, 71, 65, 64, 60, 59, 56, 41, 305, 58, 54, 50, 40, 39, 581, 56, 54, 53, 53, 49]","[1697548506910, 1697548508026, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508401, 1697548508442, 1697548508747, 1697548508805, 1697548508859, 1697548508909, 1697548508949, 1697548508988, 1697548509569, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509834]"
1644,1644,820,17,[],200,llama-7b,64,1,2165.0,1.0,1,A100,1697548478630,1697548480795,120,161.0,9.0,"[19, 442, 211, 255, 71, 54, 71, 68, 63, 911]","[1697548478649, 1697548479091, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795]"
1645,1645,357,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548482485,1697548488026,120,,,"[20, 453, 550, 76, 56, 71, 67, 65, 49, 338, 77, 77, 73, 70, 57, 70, 837, 314, 307, 77, 76, 71, 70, 325, 59, 59, 79, 307, 70, 70]","[1697548482505, 1697548482958, 1697548483508, 1697548483584, 1697548483640, 1697548483711, 1697548483778, 1697548483843, 1697548483892, 1697548484230, 1697548484307, 1697548484384, 1697548484457, 1697548484527, 1697548484584, 1697548484654, 1697548485491, 1697548485805, 1697548486112, 1697548486189, 1697548486265, 1697548486336, 1697548486406, 1697548486731, 1697548486790, 1697548486849, 1697548486928, 1697548487235, 1697548487305, 1697548487375]"
1646,1646,475,18,[],200,llama-7b,64,1,3658.0,1.0,1,A100,1697548480798,1697548484456,120,89.0,20.0,"[8, 716, 290, 268, 63, 74, 265, 68, 51, 68, 839, 75, 55, 73, 66, 66, 49, 338, 76, 79, 71]","[1697548480806, 1697548481522, 1697548481812, 1697548482080, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484385, 1697548484456]"
1647,1647,901,35,[],200,llama-7b,64,1,1071.0,1.0,1,A100,1697548527825,1697548528896,120,17.0,1.0,"[143, 928]","[1697548527968, 1697548528896]"
1648,1648,562,36,[],200,llama-7b,64,1,5247.0,1.0,1,A100,1697548528900,1697548534147,120,67.0,39.0,"[33, 569, 54, 52, 49, 41, 41, 40, 50, 347, 55, 48, 60, 59, 51, 486, 39, 472, 65, 64, 61, 59, 52, 736, 53, 58, 64, 51, 55, 576, 64, 63, 50, 63, 58, 46, 311, 59, 46, 46]","[1697548528933, 1697548529502, 1697548529556, 1697548529608, 1697548529657, 1697548529698, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530279, 1697548530339, 1697548530398, 1697548530449, 1697548530935, 1697548530974, 1697548531446, 1697548531511, 1697548531575, 1697548531636, 1697548531695, 1697548531747, 1697548532483, 1697548532536, 1697548532594, 1697548532658, 1697548532709, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533580, 1697548533638, 1697548533684, 1697548533995, 1697548534054, 1697548534100, 1697548534146]"
1649,1649,650,38,[],200,llama-7b,64,1,866.0,1.0,1,A100,1697548504913,1697548505779,120,13.0,1.0,"[7, 859]","[1697548504920, 1697548505779]"
1650,1650,305,39,[],200,llama-7b,64,1,2967.0,1.0,1,A100,1697548505781,1697548508748,120,86.0,20.0,"[10, 609, 272, 66, 58, 55, 44, 580, 65, 61, 60, 55, 57, 325, 64, 65, 60, 59, 54, 42, 306]","[1697548505791, 1697548506400, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507475, 1697548507540, 1697548507601, 1697548507661, 1697548507716, 1697548507773, 1697548508098, 1697548508162, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748]"
1651,1651,224,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484458,1697548488027,120,,,"[10, 472, 553, 312, 309, 76, 76, 72, 69, 323, 59, 60, 78, 307, 71, 70]","[1697548484468, 1697548484940, 1697548485493, 1697548485805, 1697548486114, 1697548486190, 1697548486266, 1697548486338, 1697548486407, 1697548486730, 1697548486789, 1697548486849, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
1652,1652,806,20,[],200,llama-7b,64,1,2185.0,1.0,1,A100,1697548488029,1697548490214,120,89.0,20.0,"[50, 753, 57, 51, 35, 38, 284, 67, 50, 49, 40, 49, 40, 303, 52, 43, 43, 50, 49, 40, 42]","[1697548488079, 1697548488832, 1697548488889, 1697548488940, 1697548488975, 1697548489013, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490214]"
1653,1653,582,21,[],200,llama-7b,64,1,506.0,1.0,1,A100,1697548490221,1697548490727,120,19.0,1.0,"[48, 458]","[1697548490269, 1697548490727]"
1654,1654,231,22,[],200,llama-7b,64,1,614.0,1.0,1,A100,1697548490731,1697548491345,120,13.0,1.0,"[33, 581]","[1697548490764, 1697548491345]"
1655,1655,75,40,[],200,llama-7b,64,1,2346.0,1.0,1,A100,1697548508751,1697548511097,120,345.0,18.0,"[27, 661, 129, 57, 54, 53, 53, 50, 558, 59, 60, 45, 59, 56, 231, 57, 42, 43, 52]","[1697548508778, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835, 1697548510393, 1697548510452, 1697548510512, 1697548510557, 1697548510616, 1697548510672, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511097]"
1656,1656,8,23,[],200,llama-7b,64,1,744.0,1.0,1,A100,1697548491348,1697548492092,120,39.0,3.0,"[8, 624, 58, 54]","[1697548491356, 1697548491980, 1697548492038, 1697548492092]"
1657,1657,143,16,[],200,llama-7b,64,1,1719.0,1.0,1,A100,1697548467446,1697548469165,120,6.0,12.0,"[18, 507, 81, 72, 53, 72, 70, 64, 534, 65, 52, 66, 65]","[1697548467464, 1697548467971, 1697548468052, 1697548468124, 1697548468177, 1697548468249, 1697548468319, 1697548468383, 1697548468917, 1697548468982, 1697548469034, 1697548469100, 1697548469165]"
1658,1658,632,41,[],200,llama-7b,64,1,4368.0,1.0,1,A100,1697548511099,1697548515467,120,91.0,20.0,"[14, 446, 243, 57, 54, 52, 56, 748, 350, 59, 55, 43, 54, 833, 68, 59, 58, 57, 920, 71, 71]","[1697548511113, 1697548511559, 1697548511802, 1697548511859, 1697548511913, 1697548511965, 1697548512021, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514163, 1697548514231, 1697548514290, 1697548514348, 1697548514405, 1697548515325, 1697548515396, 1697548515467]"
1659,1659,798,30,[],200,llama-7b,64,1,1260.0,1.0,1,A100,1697548509837,1697548511097,120,79.0,6.0,"[18, 1048, 57, 42, 43, 52]","[1697548509855, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511097]"
1660,1660,451,31,[],200,llama-7b,64,1,461.0,1.0,1,A100,1697548511099,1697548511560,120,286.0,1.0,"[14, 446]","[1697548511113, 1697548511559]"
1661,1661,841,17,[],200,llama-7b,64,1,2283.0,1.0,1,A100,1697548469168,1697548471451,120,123.0,15.0,"[7, 681, 68, 53, 67, 51, 52, 65, 64, 844, 74, 70, 70, 67, 50]","[1697548469175, 1697548469856, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471194, 1697548471264, 1697548471334, 1697548471401, 1697548471451]"
1662,1662,199,32,[],200,llama-7b,64,1,895.0,1.0,1,A100,1697548511564,1697548512459,120,13.0,1.0,"[23, 872]","[1697548511587, 1697548512459]"
1663,1663,310,12,[],200,llama-7b,64,1,615.0,1.0,1,A100,1697548468056,1697548468671,120,26.0,1.0,"[7, 608]","[1697548468063, 1697548468671]"
1664,1664,893,33,[],200,llama-7b,64,1,3067.0,1.0,1,A100,1697548512462,1697548515529,120,335.0,10.0,"[29, 1292, 381, 66, 60, 58, 58, 918, 71, 72, 62]","[1697548512491, 1697548513783, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406, 1697548515324, 1697548515395, 1697548515467, 1697548515529]"
1665,1665,401,42,[],200,llama-7b,64,1,3558.0,1.0,1,A100,1697548515470,1697548519028,120,84.0,20.0,"[29, 570, 498, 270, 68, 66, 65, 63, 293, 66, 65, 58, 54, 672, 65, 66, 58, 57, 362, 63, 50]","[1697548515499, 1697548516069, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517036, 1697548517099, 1697548517392, 1697548517458, 1697548517523, 1697548517581, 1697548517635, 1697548518307, 1697548518372, 1697548518438, 1697548518496, 1697548518553, 1697548518915, 1697548518978, 1697548519028]"
1666,1666,156,24,[],200,llama-7b,64,1,2285.0,1.0,1,A100,1697548495412,1697548497697,120,86.0,20.0,"[11, 304, 65, 55, 52, 52, 52, 378, 40, 41, 50, 49, 447, 69, 63, 64, 59, 57, 55, 262, 60]","[1697548495423, 1697548495727, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496003, 1697548496381, 1697548496421, 1697548496462, 1697548496512, 1697548496561, 1697548497008, 1697548497077, 1697548497140, 1697548497204, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697]"
1667,1667,147,45,[],200,llama-7b,64,1,358.0,1.0,1,A100,1697548534207,1697548534565,120,182.0,1.0,"[12, 346]","[1697548534219, 1697548534565]"
1668,1668,731,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548534568,1697548537856,120,,,"[23, 585, 382, 60, 59, 57, 56, 667, 63, 48, 55, 315, 52, 60, 65, 64, 60, 47]","[1697548534591, 1697548535176, 1697548535558, 1697548535618, 1697548535677, 1697548535734, 1697548535790, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536938, 1697548536990, 1697548537050, 1697548537115, 1697548537179, 1697548537239, 1697548537286]"
1669,1669,740,25,[],200,llama-7b,64,1,2272.0,1.0,1,A100,1697548497699,1697548499971,120,563.0,14.0,"[9, 463, 66, 61, 52, 52, 49, 351, 59, 51, 50, 826, 61, 62, 60]","[1697548497708, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499911, 1697548499971]"
1670,1670,924,24,[],200,llama-7b,64,1,397.0,1.0,1,A100,1697548500029,1697548500426,120,9.0,1.0,"[19, 377]","[1697548500048, 1697548500425]"
1671,1671,577,25,[],200,llama-7b,64,1,2048.0,1.0,1,A100,1697548500428,1697548502476,120,93.0,9.0,"[20, 755, 295, 63, 60, 58, 56, 54, 622, 65]","[1697548500448, 1697548501203, 1697548501498, 1697548501561, 1697548501621, 1697548501679, 1697548501735, 1697548501789, 1697548502411, 1697548502476]"
1672,1672,323,26,[],200,llama-7b,64,1,3079.0,1.0,1,A100,1697548502480,1697548505559,120,84.0,20.0,"[5, 645, 461, 63, 58, 57, 53, 41, 704, 62, 47, 60, 59, 58, 57, 352, 66, 50, 61, 60, 60]","[1697548502485, 1697548503130, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504853, 1697548504910, 1697548505262, 1697548505328, 1697548505378, 1697548505439, 1697548505499, 1697548505559]"
1673,1673,63,23,[],200,llama-7b,64,1,315.0,1.0,1,A100,1697548495413,1697548495728,120,39.0,1.0,"[15, 299]","[1697548495428, 1697548495727]"
1674,1674,645,24,[],200,llama-7b,64,1,2567.0,1.0,1,A100,1697548495731,1697548498298,120,86.0,20.0,"[10, 583, 56, 41, 41, 50, 49, 447, 69, 63, 65, 59, 56, 55, 262, 60, 48, 46, 46, 400, 61]","[1697548495741, 1697548496324, 1697548496380, 1697548496421, 1697548496462, 1697548496512, 1697548496561, 1697548497008, 1697548497077, 1697548497140, 1697548497205, 1697548497264, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497791, 1697548497837, 1697548498237, 1697548498298]"
1675,1675,654,26,[],200,llama-7b,64,1,668.0,1.0,1,A100,1697548495794,1697548496462,120,47.0,4.0,"[6, 525, 55, 41, 41]","[1697548495800, 1697548496325, 1697548496380, 1697548496421, 1697548496462]"
1676,1676,401,38,[],200,llama-7b,64,1,2625.0,1.0,1,A100,1697548527826,1697548530451,120,84.0,20.0,"[155, 915, 59, 51, 50, 48, 47, 40, 364, 52, 50, 40, 41, 42, 49, 346, 56, 49, 59, 58, 54]","[1697548527981, 1697548528896, 1697548528955, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529657, 1697548529697, 1697548529738, 1697548529780, 1697548529829, 1697548530175, 1697548530231, 1697548530280, 1697548530339, 1697548530397, 1697548530451]"
1677,1677,892,31,[],200,llama-7b,64,1,2534.0,1.0,1,A100,1697548518440,1697548520974,120,87.0,20.0,"[16, 390, 69, 63, 50, 58, 56, 45, 607, 243, 68, 62, 62, 61, 61, 60, 336, 65, 63, 49, 50]","[1697548518456, 1697548518846, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520037, 1697548520105, 1697548520167, 1697548520229, 1697548520290, 1697548520351, 1697548520411, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520974]"
1678,1678,60,43,[],200,llama-7b,64,1,4759.0,1.0,1,A100,1697548519031,1697548523790,120,93.0,36.0,"[28, 736, 242, 67, 63, 62, 61, 61, 59, 337, 65, 63, 48, 49, 54, 404, 64, 47, 61, 45, 55, 709, 69, 67, 67, 57, 66, 50, 316, 67, 64, 64, 62, 61, 317, 52]","[1697548519059, 1697548519795, 1697548520037, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520351, 1697548520410, 1697548520747, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521026, 1697548521430, 1697548521494, 1697548521541, 1697548521602, 1697548521647, 1697548521702, 1697548522411, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523790]"
1679,1679,403,42,[],200,llama-7b,64,1,779.0,1.0,1,A100,1697548535678,1697548536457,120,874.0,2.0,"[5, 590, 184]","[1697548535683, 1697548536273, 1697548536457]"
1680,1680,488,26,[],200,llama-7b,64,1,451.0,1.0,1,A100,1697548499974,1697548500425,120,6.0,1.0,"[10, 441]","[1697548499984, 1697548500425]"
1681,1681,479,47,[],200,llama-7b,64,1,5962.0,1.0,1,A100,1697548537863,1697548543825,120,140.0,36.0,"[34, 277, 27, 683, 55, 41, 47, 45, 678, 61, 56, 53, 44, 52, 42, 400, 50, 59, 47, 57, 57, 50, 350, 48, 750, 60, 59, 52, 650, 62, 61, 57, 44, 55, 693, 59, 47]","[1697548537897, 1697548538174, 1697548538201, 1697548538884, 1697548538939, 1697548538980, 1697548539027, 1697548539072, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540778, 1697548541128, 1697548541176, 1697548541926, 1697548541986, 1697548542045, 1697548542097, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542971, 1697548543026, 1697548543719, 1697548543778, 1697548543825]"
1682,1682,761,42,[],200,llama-7b,64,1,3902.0,1.0,1,A100,1697548500666,1697548504568,120,85.0,20.0,"[27, 510, 295, 63, 61, 58, 55, 55, 621, 64, 63, 62, 60, 55, 877, 62, 58, 57, 53, 41, 705]","[1697548500693, 1697548501203, 1697548501498, 1697548501561, 1697548501622, 1697548501680, 1697548501735, 1697548501790, 1697548502411, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502715, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504568]"
1683,1683,257,27,[],200,llama-7b,64,1,776.0,1.0,1,A100,1697548500427,1697548501203,120,14.0,1.0,"[6, 770]","[1697548500433, 1697548501203]"
1684,1684,234,40,[],200,llama-7b,64,1,5241.0,1.0,1,A100,1697548513786,1697548519027,120,457.0,25.0,"[9, 1062, 468, 71, 69, 64, 61, 977, 271, 68, 65, 64, 64, 293, 67, 64, 58, 53, 672, 66, 65, 59, 57, 361, 63, 50]","[1697548513795, 1697548514857, 1697548515325, 1697548515396, 1697548515465, 1697548515529, 1697548515590, 1697548516567, 1697548516838, 1697548516906, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517459, 1697548517523, 1697548517581, 1697548517634, 1697548518306, 1697548518372, 1697548518437, 1697548518496, 1697548518553, 1697548518914, 1697548518977, 1697548519027]"
1685,1685,758,44,[],200,llama-7b,64,1,3171.0,1.0,1,A100,1697548523793,1697548526964,120,84.0,20.0,"[6, 553, 238, 64, 65, 65, 59, 58, 588, 67, 66, 50, 50, 66, 65, 242, 48, 48, 60, 59, 654]","[1697548523799, 1697548524352, 1697548524590, 1697548524654, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525622, 1697548525672, 1697548525722, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526964]"
1686,1686,331,17,[],200,llama-7b,64,1,988.0,1.0,1,A100,1697548491714,1697548492702,120,26.0,1.0,"[33, 955]","[1697548491747, 1697548492702]"
1687,1687,86,48,[],200,llama-7b,64,1,2739.0,1.0,1,A100,1697548537876,1697548540615,120,335.0,17.0,"[193, 761, 54, 55, 42, 46, 45, 679, 61, 56, 53, 44, 52, 41, 401, 50, 59, 47]","[1697548538069, 1697548538830, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539072, 1697548539751, 1697548539812, 1697548539868, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540459, 1697548540509, 1697548540568, 1697548540615]"
1688,1688,914,18,[],200,llama-7b,64,1,2705.0,1.0,1,A100,1697548492704,1697548495409,120,84.0,20.0,"[11, 835, 62, 47, 38, 39, 310, 52, 42, 46, 432, 61, 47, 60, 55, 55, 50, 297, 49, 58, 59]","[1697548492715, 1697548493550, 1697548493612, 1697548493659, 1697548493697, 1697548493736, 1697548494046, 1697548494098, 1697548494140, 1697548494186, 1697548494618, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494896, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495409]"
1689,1689,841,33,[],200,llama-7b,64,1,1980.0,1.0,1,A100,1697548522674,1697548524654,120,123.0,15.0,"[22, 408, 66, 64, 64, 62, 62, 316, 52, 67, 51, 63, 64, 555, 64]","[1697548522696, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523971, 1697548524035, 1697548524590, 1697548524654]"
1690,1690,684,19,[],200,llama-7b,64,1,2286.0,1.0,1,A100,1697548495412,1697548497698,120,100.0,20.0,"[6, 309, 65, 55, 52, 52, 52, 378, 40, 41, 50, 50, 445, 70, 63, 62, 61, 57, 55, 262, 60]","[1697548495418, 1697548495727, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496003, 1697548496381, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497007, 1697548497077, 1697548497140, 1697548497202, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697]"
1691,1691,45,20,[],200,llama-7b,64,1,729.0,1.0,1,A100,1697548493258,1697548493987,120,19.0,1.0,"[11, 718]","[1697548493269, 1697548493987]"
1692,1692,743,21,[],200,llama-7b,64,1,852.0,1.0,1,A100,1697548493989,1697548494841,120,123.0,6.0,"[10, 552, 67, 60, 47, 60, 56]","[1697548493999, 1697548494551, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494841]"
1693,1693,312,20,[],200,llama-7b,64,1,468.0,1.0,1,A100,1697548497704,1697548498172,120,23.0,1.0,"[29, 438]","[1697548497733, 1697548498171]"
1694,1694,404,22,[],200,llama-7b,64,1,2234.0,1.0,1,A100,1697548494843,1697548497077,120,87.0,20.0,"[12, 325, 64, 49, 59, 58, 55, 51, 43, 232, 55, 53, 51, 52, 379, 40, 40, 51, 50, 445, 69]","[1697548494855, 1697548495180, 1697548495244, 1697548495293, 1697548495352, 1697548495410, 1697548495465, 1697548495516, 1697548495559, 1697548495791, 1697548495846, 1697548495899, 1697548495950, 1697548496002, 1697548496381, 1697548496421, 1697548496461, 1697548496512, 1697548496562, 1697548497007, 1697548497076]"
1695,1695,80,21,[],200,llama-7b,64,1,563.0,1.0,1,A100,1697548498176,1697548498739,120,13.0,1.0,"[25, 538]","[1697548498201, 1697548498739]"
1696,1696,670,22,[],200,llama-7b,64,1,2994.0,1.0,1,A100,1697548498741,1697548501735,120,67.0,18.0,"[19, 660, 368, 61, 62, 59, 55, 51, 420, 65, 51, 51, 57, 54, 724, 63, 61, 57, 56]","[1697548498760, 1697548499420, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500496, 1697548500561, 1697548500612, 1697548500663, 1697548500720, 1697548500774, 1697548501498, 1697548501561, 1697548501622, 1697548501679, 1697548501735]"
1697,1697,589,18,[],200,llama-7b,64,1,2215.0,1.0,1,A100,1697548471452,1697548473667,120,92.0,20.0,"[8, 291, 78, 74, 73, 72, 67, 64, 48, 304, 71, 68, 68, 53, 65, 49, 530, 68, 48, 51, 65]","[1697548471460, 1697548471751, 1697548471829, 1697548471903, 1697548471976, 1697548472048, 1697548472115, 1697548472179, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472791, 1697548472856, 1697548472905, 1697548473435, 1697548473503, 1697548473551, 1697548473602, 1697548473667]"
1698,1698,866,35,[],200,llama-7b,64,1,3539.0,1.0,1,A100,1697548523425,1697548526964,120,93.0,20.0,"[6, 921, 237, 64, 66, 65, 59, 58, 587, 68, 65, 51, 51, 65, 65, 242, 48, 48, 60, 59, 654]","[1697548523431, 1697548524352, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525488, 1697548525556, 1697548525621, 1697548525672, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526964]"
1699,1699,172,23,[],200,llama-7b,64,1,491.0,1.0,1,A100,1697548497081,1697548497572,120,19.0,1.0,"[24, 467]","[1697548497105, 1697548497572]"
1700,1700,642,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526967,1697548527822,120,,,"[18, 592]","[1697548526985, 1697548527577]"
1701,1701,57,39,[],200,llama-7b,64,1,912.0,1.0,1,A100,1697548530456,1697548531368,120,13.0,1.0,"[94, 818]","[1697548530550, 1697548531368]"
1702,1702,754,24,[],200,llama-7b,64,1,1228.0,1.0,1,A100,1697548497575,1697548498803,120,88.0,7.0,"[19, 643, 61, 52, 52, 50, 350]","[1697548497594, 1697548498237, 1697548498298, 1697548498350, 1697548498402, 1697548498452, 1697548498802]"
1703,1703,643,29,[],200,llama-7b,64,1,853.0,1.0,1,A100,1697548525857,1697548526710,120,18.0,1.0,"[19, 834]","[1697548525876, 1697548526710]"
1704,1704,531,25,[],200,llama-7b,64,1,3605.0,1.0,1,A100,1697548498805,1697548502410,120,52.0,20.0,"[7, 607, 369, 61, 62, 59, 55, 51, 420, 65, 51, 49, 58, 55, 724, 63, 61, 57, 56, 55, 620]","[1697548498812, 1697548499419, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500496, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501498, 1697548501561, 1697548501622, 1697548501679, 1697548501735, 1697548501790, 1697548502410]"
1705,1705,304,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526713,1697548527820,120,,,"[10, 854]","[1697548526723, 1697548527577]"
1706,1706,69,31,[],200,llama-7b,64,1,2572.0,1.0,1,A100,1697548527826,1697548530398,120,85.0,20.0,"[42, 397, 34, 655, 52, 50, 48, 47, 40, 365, 51, 49, 41, 42, 40, 49, 347, 56, 48, 59, 60]","[1697548527868, 1697548528265, 1697548528299, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529556, 1697548529607, 1697548529656, 1697548529697, 1697548529739, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530398]"
1707,1707,755,40,[],200,llama-7b,64,1,3481.0,1.0,1,A100,1697548531370,1697548534851,120,286.0,25.0,"[7, 741, 365, 53, 59, 64, 50, 55, 575, 66, 63, 49, 66, 55, 46, 311, 60, 45, 46, 59, 58, 367, 46, 59, 59, 57]","[1697548531377, 1697548532118, 1697548532483, 1697548532536, 1697548532595, 1697548532659, 1697548532709, 1697548532764, 1697548533339, 1697548533405, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534676, 1697548534735, 1697548534794, 1697548534851]"
1708,1708,659,32,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548530406,1697548537858,120,,,"[33, 446, 51, 39, 471, 66, 62, 62, 59, 52, 738, 52, 58, 64, 51, 54, 576, 64, 63, 50, 67, 54, 46, 311, 60, 45, 46, 58, 58, 368, 46, 59, 59, 56, 708, 59, 60, 57, 55, 668, 63, 48, 55, 314, 52, 60, 66, 63, 60, 47]","[1697548530439, 1697548530885, 1697548530936, 1697548530975, 1697548531446, 1697548531512, 1697548531574, 1697548531636, 1697548531695, 1697548531747, 1697548532485, 1697548532537, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533584, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534204, 1697548534262, 1697548534630, 1697548534676, 1697548534735, 1697548534794, 1697548534850, 1697548535558, 1697548535617, 1697548535677, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
1709,1709,113,10,[],200,llama-7b,64,1,528.0,1.0,1,A100,1697548460715,1697548461243,120,13.0,1.0,"[23, 505]","[1697548460738, 1697548461243]"
1710,1710,96,27,[],200,llama-7b,64,1,413.0,1.0,1,A100,1697548516909,1697548517322,120,31.0,1.0,"[11, 402]","[1697548516920, 1697548517322]"
1711,1711,553,34,[],200,llama-7b,64,1,3496.0,1.0,1,A100,1697548515532,1697548519028,120,88.0,20.0,"[6, 531, 498, 271, 68, 65, 65, 63, 293, 66, 65, 58, 54, 672, 65, 66, 58, 57, 362, 63, 50]","[1697548515538, 1697548516069, 1697548516567, 1697548516838, 1697548516906, 1697548516971, 1697548517036, 1697548517099, 1697548517392, 1697548517458, 1697548517523, 1697548517581, 1697548517635, 1697548518307, 1697548518372, 1697548518438, 1697548518496, 1697548518553, 1697548518915, 1697548518978, 1697548519028]"
1712,1712,846,28,[],200,llama-7b,64,1,1454.0,1.0,1,A100,1697548501206,1697548502660,120,140.0,6.0,"[6, 944, 254, 65, 64, 62, 59]","[1697548501212, 1697548502156, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502660]"
1713,1713,703,11,[],200,llama-7b,64,1,688.0,1.0,1,A100,1697548461247,1697548461935,120,12.0,1.0,"[15, 673]","[1697548461262, 1697548461935]"
1714,1714,132,48,[],200,llama-7b,64,1,3592.0,1.0,1,A100,1697548543826,1697548547418,120,100.0,20.0,"[10, 686, 254, 56, 54, 46, 951, 72, 64, 65, 62, 62, 47, 63, 760, 65, 64, 59, 48, 58, 46]","[1697548543836, 1697548544522, 1697548544776, 1697548544832, 1697548544886, 1697548544932, 1697548545883, 1697548545955, 1697548546019, 1697548546084, 1697548546146, 1697548546208, 1697548546255, 1697548546318, 1697548547078, 1697548547143, 1697548547207, 1697548547266, 1697548547314, 1697548547372, 1697548547418]"
1715,1715,474,12,[],200,llama-7b,64,1,6240.0,1.0,1,A100,1697548461938,1697548468178,120,109.0,33.0,"[21, 947, 56, 57, 54, 820, 63, 60, 57, 56, 730, 460, 64, 48, 60, 45, 703, 71, 70, 54, 64, 63, 48, 690, 73, 70, 68, 67, 51, 63, 360, 72, 55]","[1697548461959, 1697548462906, 1697548462962, 1697548463019, 1697548463073, 1697548463893, 1697548463956, 1697548464016, 1697548464073, 1697548464129, 1697548464859, 1697548465319, 1697548465383, 1697548465431, 1697548465491, 1697548465536, 1697548466239, 1697548466310, 1697548466380, 1697548466434, 1697548466498, 1697548466561, 1697548466609, 1697548467299, 1697548467372, 1697548467442, 1697548467510, 1697548467577, 1697548467628, 1697548467691, 1697548468051, 1697548468123, 1697548468178]"
1716,1716,837,13,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484657,1697548488028,120,,,"[6, 1984, 83, 59, 60, 78, 307, 72, 70]","[1697548484663, 1697548486647, 1697548486730, 1697548486789, 1697548486849, 1697548486927, 1697548487234, 1697548487306, 1697548487376]"
1717,1717,836,49,[],200,llama-7b,64,1,660.0,1.0,1,A100,1697548547421,1697548548081,120,11.0,1.0,"[25, 635]","[1697548547446, 1697548548081]"
1718,1718,493,50,[],200,llama-7b,64,1,2546.0,1.0,1,A100,1697548548084,1697548550630,120,83.0,20.0,"[21, 655, 292, 61, 52, 58, 48, 57, 56, 395, 60, 47, 48, 47, 60, 55, 313, 64, 55, 48, 54]","[1697548548105, 1697548548760, 1697548549052, 1697548549113, 1697548549165, 1697548549223, 1697548549271, 1697548549328, 1697548549384, 1697548549779, 1697548549839, 1697548549886, 1697548549934, 1697548549981, 1697548550041, 1697548550096, 1697548550409, 1697548550473, 1697548550528, 1697548550576, 1697548550630]"
1719,1719,271,51,[],200,llama-7b,64,1,2864.0,1.0,1,A100,1697548550635,1697548553499,120,87.0,20.0,"[15, 910, 69, 63, 57, 44, 44, 55, 56, 712, 53, 67, 49, 50, 66, 64, 274, 49, 47, 60, 60]","[1697548550650, 1697548551560, 1697548551629, 1697548551692, 1697548551749, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552660, 1697548552713, 1697548552780, 1697548552829, 1697548552879, 1697548552945, 1697548553009, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553499]"
1720,1720,615,29,[],200,llama-7b,64,1,2896.0,1.0,1,A100,1697548502663,1697548505559,120,93.0,20.0,"[10, 457, 462, 62, 58, 57, 53, 41, 704, 62, 47, 60, 59, 59, 57, 352, 65, 50, 61, 60, 60]","[1697548502673, 1697548503130, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504854, 1697548504911, 1697548505263, 1697548505328, 1697548505378, 1697548505439, 1697548505499, 1697548505559]"
1721,1721,854,52,[],200,llama-7b,64,1,2888.0,1.0,1,A100,1697548553503,1697548556391,120,67.0,29.0,"[29, 902, 437, 251, 253, 58, 45, 44, 46, 58, 57, 43, 50, 69, 46, 37, 45, 42, 41, 39, 39, 35, 27, 33, 26, 26, 26, 27, 31, 26]","[1697548553532, 1697548554434, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555522, 1697548555568, 1697548555626, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556169, 1697548556196, 1697548556229, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556391]"
1722,1722,127,13,[],200,llama-7b,64,1,920.0,1.0,1,A100,1697548468181,1697548469101,120,100.0,5.0,"[15, 475, 246, 66, 51, 67]","[1697548468196, 1697548468671, 1697548468917, 1697548468983, 1697548469034, 1697548469101]"
1723,1723,227,21,[],200,llama-7b,64,1,3767.0,1.0,1,A100,1697548503772,1697548507539,120,364.0,25.0,"[15, 526, 255, 62, 47, 60, 59, 58, 57, 351, 65, 51, 61, 60, 61, 282, 46, 54, 52, 679, 65, 58, 55, 43, 581, 64]","[1697548503787, 1697548504313, 1697548504568, 1697548504630, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505560, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506673, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539]"
1724,1724,671,49,[],200,llama-7b,64,1,458.0,1.0,1,A100,1697548540618,1697548541076,120,12.0,1.0,"[10, 447]","[1697548540628, 1697548541075]"
1725,1725,88,23,[],200,llama-7b,64,1,3321.0,1.0,1,A100,1697548498301,1697548501622,120,58.0,20.0,"[18, 419, 65, 58, 51, 50, 826, 62, 60, 60, 54, 52, 421, 65, 51, 50, 57, 54, 724, 63, 61]","[1697548498319, 1697548498738, 1697548498803, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499850, 1697548499910, 1697548499970, 1697548500024, 1697548500076, 1697548500497, 1697548500562, 1697548500613, 1697548500663, 1697548500720, 1697548500774, 1697548501498, 1697548501561, 1697548501622]"
1726,1726,444,50,[],200,llama-7b,64,1,1671.0,1.0,1,A100,1697548541078,1697548542749,120,457.0,6.0,"[6, 506, 337, 60, 59, 51, 651]","[1697548541084, 1697548541590, 1697548541927, 1697548541987, 1697548542046, 1697548542097, 1697548542748]"
1727,1727,451,27,[],200,llama-7b,64,1,595.0,1.0,1,A100,1697548495730,1697548496325,120,286.0,1.0,"[18, 577]","[1697548495748, 1697548496325]"
1728,1728,419,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526966,1697548527821,120,,,"[11, 600]","[1697548526977, 1697548527577]"
1729,1729,111,28,[],200,llama-7b,64,1,879.0,1.0,1,A100,1697548496327,1697548497206,120,79.0,5.0,"[7, 674, 69, 63, 65]","[1697548496334, 1697548497008, 1697548497077, 1697548497140, 1697548497205]"
1730,1730,191,46,[],200,llama-7b,64,1,2625.0,1.0,1,A100,1697548527826,1697548530451,120,85.0,20.0,"[145, 925, 58, 52, 50, 48, 47, 40, 364, 52, 50, 40, 41, 42, 49, 347, 55, 49, 59, 59, 52]","[1697548527971, 1697548528896, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529657, 1697548529697, 1697548529738, 1697548529780, 1697548529829, 1697548530176, 1697548530231, 1697548530280, 1697548530339, 1697548530398, 1697548530450]"
1731,1731,592,24,[],200,llama-7b,64,1,5741.0,1.0,1,A100,1697548492096,1697548497837,120,15.0,50.0,"[14, 591, 360, 58, 49, 48, 39, 356, 49, 38, 37, 312, 52, 42, 46, 431, 61, 47, 59, 55, 55, 51, 297, 49, 58, 59, 54, 53, 42, 232, 56, 53, 51, 51, 379, 41, 40, 51, 49, 446, 68, 64, 61, 63, 57, 55, 262, 61, 47, 46, 46]","[1697548492110, 1697548492701, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493660, 1697548493698, 1697548493735, 1697548494047, 1697548494099, 1697548494141, 1697548494187, 1697548494618, 1697548494679, 1697548494726, 1697548494785, 1697548494840, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495409, 1697548495463, 1697548495516, 1697548495558, 1697548495790, 1697548495846, 1697548495899, 1697548495950, 1697548496001, 1697548496380, 1697548496421, 1697548496461, 1697548496512, 1697548496561, 1697548497007, 1697548497075, 1697548497139, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497745, 1697548497791, 1697548497837]"
1732,1732,295,37,[],200,llama-7b,64,1,1127.0,1.0,1,A100,1697548527828,1697548528955,120,52.0,2.0,"[236, 832, 59]","[1697548528064, 1697548528896, 1697548528955]"
1733,1733,894,13,[],200,llama-7b,64,1,908.0,1.0,1,A100,1697548468674,1697548469582,120,14.0,1.0,"[20, 888]","[1697548468694, 1697548469582]"
1734,1734,664,14,[],200,llama-7b,64,1,2242.0,1.0,1,A100,1697548469586,1697548471828,120,364.0,9.0,"[25, 1092, 418, 73, 71, 69, 67, 50, 66, 311]","[1697548469611, 1697548470703, 1697548471121, 1697548471194, 1697548471265, 1697548471334, 1697548471401, 1697548471451, 1697548471517, 1697548471828]"
1735,1735,648,24,[],200,llama-7b,64,1,3285.0,1.0,1,A100,1697548501626,1697548504911,120,84.0,20.0,"[12, 519, 253, 66, 63, 62, 60, 54, 875, 65, 56, 58, 52, 42, 705, 61, 48, 60, 59, 58, 57]","[1697548501638, 1697548502157, 1697548502410, 1697548502476, 1697548502539, 1697548502601, 1697548502661, 1697548502715, 1697548503590, 1697548503655, 1697548503711, 1697548503769, 1697548503821, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911]"
1736,1736,78,57,[],200,llama-7b,64,1,4387.0,1.0,1,A100,1697548543030,1697548547417,120,84.0,20.0,"[11, 1481, 254, 55, 54, 47, 951, 72, 65, 64, 62, 61, 48, 62, 760, 65, 64, 59, 49, 57, 46]","[1697548543041, 1697548544522, 1697548544776, 1697548544831, 1697548544885, 1697548544932, 1697548545883, 1697548545955, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547371, 1697548547417]"
1737,1737,321,15,[],200,llama-7b,64,1,840.0,1.0,1,A100,1697548471830,1697548472670,120,182.0,4.0,"[6, 620, 76, 70, 68]","[1697548471836, 1697548472456, 1697548472532, 1697548472602, 1697548472670]"
1738,1738,89,16,[],200,llama-7b,64,1,2722.0,1.0,1,A100,1697548472672,1697548475394,120,52.0,20.0,"[14, 499, 250, 67, 49, 51, 65, 58, 345, 68, 68, 50, 67, 64, 63, 594, 72, 71, 70, 69, 68]","[1697548472686, 1697548473185, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473667, 1697548473725, 1697548474070, 1697548474138, 1697548474206, 1697548474256, 1697548474323, 1697548474387, 1697548474450, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475394]"
1739,1739,416,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548534855,1697548537857,120,,,"[6, 1412, 184, 63, 48, 55, 315, 52, 59, 66, 64, 59, 48]","[1697548534861, 1697548536273, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536938, 1697548536990, 1697548537049, 1697548537115, 1697548537179, 1697548537238, 1697548537286]"
1740,1740,418,25,[],200,llama-7b,64,1,975.0,1.0,1,A100,1697548504913,1697548505888,120,286.0,3.0,"[7, 859, 63, 46]","[1697548504920, 1697548505779, 1697548505842, 1697548505888]"
1741,1741,80,26,[],200,llama-7b,64,1,508.0,1.0,1,A100,1697548505892,1697548506400,120,13.0,1.0,"[9, 499]","[1697548505901, 1697548506400]"
1742,1742,779,27,[],200,llama-7b,64,1,1821.0,1.0,1,A100,1697548506405,1697548508226,120,563.0,10.0,"[23, 854, 194, 63, 62, 59, 56, 57, 324, 66, 63]","[1697548506428, 1697548507282, 1697548507476, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508163, 1697548508226]"
1743,1743,440,28,[],200,llama-7b,64,1,2675.0,1.0,1,A100,1697548508228,1697548510903,120,84.0,20.0,"[15, 436, 69, 57, 54, 50, 40, 40, 579, 58, 52, 53, 53, 50, 559, 60, 59, 46, 58, 56, 231]","[1697548508243, 1697548508679, 1697548508748, 1697548508805, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509568, 1697548509626, 1697548509678, 1697548509731, 1697548509784, 1697548509834, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510672, 1697548510903]"
1744,1744,211,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548510911,1697548527819,120,,,"[16, 632, 243, 56, 55, 52, 55, 749, 350, 59, 55, 43, 54, 833, 68, 59, 58, 58, 919, 71, 70, 64, 60, 976, 271, 68, 66, 64, 64, 293, 66, 64, 59, 53, 672, 66, 65, 58, 58, 362, 62, 50, 58, 57, 45, 607, 242, 67, 63, 62, 62, 60, 60, 336, 66, 63, 48, 49, 53, 406, 62, 48, 60, 46, 54, 709, 70, 67, 67, 57, 66, 49, 317, 67, 64, 63, 63, 61, 317, 52, 67, 51, 63, 64, 554, 64, 66, 65, 59, 57, 589, 67, 66, 50, 51, 65, 65, 242, 48, 47, 60, 59, 654, 66, 49, 65, 65, 63]","[1697548510927, 1697548511559, 1697548511802, 1697548511858, 1697548511913, 1697548511965, 1697548512020, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514163, 1697548514231, 1697548514290, 1697548514348, 1697548514406, 1697548515325, 1697548515396, 1697548515466, 1697548515530, 1697548515590, 1697548516566, 1697548516837, 1697548516905, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517634, 1697548518306, 1697548518372, 1697548518437, 1697548518495, 1697548518553, 1697548518915, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520103, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521025, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522786, 1697548523103, 1697548523170, 1697548523234, 1697548523297, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523971, 1697548524035, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524900, 1697548525489, 1697548525556, 1697548525622, 1697548525672, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526963, 1697548527029, 1697548527078, 1697548527143, 1697548527208, 1697548527271]"
1745,1745,184,42,[],200,llama-7b,64,1,2905.0,1.0,1,A100,1697548537875,1697548540780,120,87.0,20.0,"[109, 847, 54, 55, 42, 45, 46, 677, 62, 55, 53, 45, 52, 41, 400, 50, 60, 46, 57, 57, 52]","[1697548537984, 1697548538831, 1697548538885, 1697548538940, 1697548538982, 1697548539027, 1697548539073, 1697548539750, 1697548539812, 1697548539867, 1697548539920, 1697548539965, 1697548540017, 1697548540058, 1697548540458, 1697548540508, 1697548540568, 1697548540614, 1697548540671, 1697548540728, 1697548540780]"
1746,1746,754,58,[],200,llama-7b,64,1,1744.0,1.0,1,A100,1697548547421,1697548549165,120,88.0,7.0,"[30, 694, 50, 48, 810, 61, 51]","[1697548547451, 1697548548145, 1697548548195, 1697548548243, 1697548549053, 1697548549114, 1697548549165]"
1747,1747,678,28,[],200,llama-7b,64,1,2965.0,1.0,1,A100,1697548517325,1697548520290,120,244.0,18.0,"[16, 723, 243, 65, 65, 58, 58, 362, 63, 50, 58, 56, 45, 607, 242, 68, 62, 62, 62]","[1697548517341, 1697548518064, 1697548518307, 1697548518372, 1697548518437, 1697548518495, 1697548518553, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290]"
1748,1748,480,38,[],200,llama-7b,64,1,542.0,1.0,1,A100,1697548511917,1697548512459,120,26.0,1.0,"[15, 527]","[1697548511932, 1697548512459]"
1749,1749,142,39,[],200,llama-7b,64,1,5062.0,1.0,1,A100,1697548512461,1697548517523,120,52.0,20.0,"[5, 1316, 382, 66, 60, 58, 57, 919, 73, 70, 63, 61, 976, 270, 68, 66, 64, 64, 293, 67, 64]","[1697548512466, 1697548513782, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514405, 1697548515324, 1697548515397, 1697548515467, 1697548515530, 1697548515591, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517459, 1697548517523]"
1750,1750,427,33,[],200,llama-7b,64,1,1118.0,1.0,1,A100,1697548537863,1697548538981,120,58.0,5.0,"[36, 275, 27, 683, 55, 42]","[1697548537899, 1697548538174, 1697548538201, 1697548538884, 1697548538939, 1697548538981]"
1751,1751,835,7,[],200,llama-7b,64,1,2185.0,1.0,1,A100,1697548488029,1697548490214,120,87.0,20.0,"[35, 768, 56, 52, 35, 37, 285, 67, 50, 49, 40, 48, 40, 304, 52, 43, 43, 50, 49, 40, 42]","[1697548488064, 1697548488832, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490214]"
1752,1752,795,23,[],200,llama-7b,64,1,801.0,1.0,1,A100,1697548488032,1697548488833,120,12.0,1.0,"[105, 696]","[1697548488137, 1697548488833]"
1753,1753,323,35,[],200,llama-7b,64,1,2616.0,1.0,1,A100,1697548519031,1697548521647,120,84.0,20.0,"[24, 555, 185, 242, 67, 63, 62, 61, 61, 59, 337, 65, 63, 49, 48, 54, 405, 63, 47, 61, 45]","[1697548519055, 1697548519610, 1697548519795, 1697548520037, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520351, 1697548520410, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520972, 1697548521026, 1697548521431, 1697548521494, 1697548521541, 1697548521602, 1697548521647]"
1754,1754,572,24,[],200,llama-7b,64,1,405.0,1.0,1,A100,1697548488837,1697548489242,120,16.0,1.0,"[51, 353]","[1697548488888, 1697548489241]"
1755,1755,225,25,[],200,llama-7b,64,1,585.0,1.0,1,A100,1697548489246,1697548489831,120,23.0,1.0,"[38, 547]","[1697548489284, 1697548489831]"
1756,1756,1,26,[],200,llama-7b,64,1,5575.0,1.0,1,A100,1697548489834,1697548495409,120,47.0,43.0,"[24, 508, 42, 373, 49, 49, 42, 553, 51, 48, 40, 49, 49, 327, 54, 53, 43, 51, 50, 49, 723, 58, 48, 49, 39, 356, 48, 38, 38, 311, 52, 42, 46, 432, 60, 47, 60, 55, 55, 51, 297, 49, 58, 59]","[1697548489858, 1697548490366, 1697548490408, 1697548490781, 1697548490830, 1697548490879, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491711, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492289, 1697548492338, 1697548493061, 1697548493119, 1697548493167, 1697548493216, 1697548493255, 1697548493611, 1697548493659, 1697548493697, 1697548493735, 1697548494046, 1697548494098, 1697548494140, 1697548494186, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494840, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495409]"
1757,1757,853,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548487237,1697548488026,120,,,"[10, 561]","[1697548487247, 1697548487808]"
1758,1758,907,36,[],200,llama-7b,64,1,496.0,1.0,1,A100,1697548521649,1697548522145,120,10.0,1.0,"[12, 484]","[1697548521661, 1697548522145]"
1759,1759,488,8,[],200,llama-7b,64,1,508.0,1.0,1,A100,1697548490219,1697548490727,120,6.0,1.0,"[41, 466]","[1697548490260, 1697548490726]"
1760,1760,259,9,[],200,llama-7b,64,1,2883.0,1.0,1,A100,1697548490729,1697548493612,120,87.0,20.0,"[15, 600, 131, 50, 48, 40, 48, 49, 329, 54, 52, 42, 52, 50, 49, 724, 59, 47, 48, 39, 356]","[1697548490744, 1697548491344, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492039, 1697548492093, 1697548492145, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493062, 1697548493121, 1697548493168, 1697548493216, 1697548493255, 1697548493611]"
1761,1761,684,37,[],200,llama-7b,64,1,3341.0,1.0,1,A100,1697548522148,1697548525489,120,100.0,20.0,"[17, 865, 73, 67, 64, 64, 62, 62, 317, 51, 68, 50, 63, 64, 554, 65, 65, 65, 59, 58, 588]","[1697548522165, 1697548523030, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422, 1697548523739, 1697548523790, 1697548523858, 1697548523908, 1697548523971, 1697548524035, 1697548524589, 1697548524654, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489]"
1762,1762,715,23,[],200,llama-7b,64,1,660.0,1.0,1,A100,1697548492042,1697548492702,120,20.0,1.0,"[6, 654]","[1697548492048, 1697548492702]"
1763,1763,463,24,[],200,llama-7b,64,1,845.0,1.0,1,A100,1697548492706,1697548493551,120,39.0,1.0,"[14, 831]","[1697548492720, 1697548493551]"
1764,1764,113,25,[],200,llama-7b,64,1,425.0,1.0,1,A100,1697548493562,1697548493987,120,13.0,1.0,"[28, 397]","[1697548493590, 1697548493987]"
1765,1765,171,14,[],200,llama-7b,64,1,515.0,1.0,1,A100,1697548504679,1697548505194,120,6.0,1.0,"[17, 497]","[1697548504696, 1697548505193]"
1766,1766,422,25,[],200,llama-7b,64,1,436.0,1.0,1,A100,1697548498302,1697548498738,120,26.0,1.0,"[27, 409]","[1697548498329, 1697548498738]"
1767,1767,583,27,[],200,llama-7b,64,1,2286.0,1.0,1,A100,1697548495412,1697548497698,120,96.0,20.0,"[6, 309, 65, 55, 52, 52, 51, 379, 40, 41, 50, 50, 445, 70, 63, 64, 59, 57, 55, 262, 60]","[1697548495418, 1697548495727, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496002, 1697548496381, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497007, 1697548497077, 1697548497140, 1697548497204, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697]"
1768,1768,308,27,[],200,llama-7b,64,1,2397.0,1.0,1,A100,1697548496465,1697548498862,120,87.0,20.0,"[7, 460, 76, 69, 63, 64, 59, 58, 54, 262, 61, 48, 46, 45, 400, 62, 51, 52, 49, 351, 60]","[1697548496472, 1697548496932, 1697548497008, 1697548497077, 1697548497140, 1697548497204, 1697548497263, 1697548497321, 1697548497375, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498862]"
1769,1769,849,10,[],200,llama-7b,64,1,372.0,1.0,1,A100,1697548493616,1697548493988,120,10.0,1.0,"[50, 322]","[1697548493666, 1697548493988]"
1770,1770,81,26,[],200,llama-7b,64,1,2035.0,1.0,1,A100,1697548498739,1697548500774,120,732.0,13.0,"[14, 667, 368, 62, 61, 59, 55, 51, 420, 65, 51, 51, 57, 54]","[1697548498753, 1697548499420, 1697548499788, 1697548499850, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500496, 1697548500561, 1697548500612, 1697548500663, 1697548500720, 1697548500774]"
1771,1771,615,11,[],200,llama-7b,64,1,2009.0,1.0,1,A100,1697548493993,1697548496002,120,93.0,20.0,"[30, 529, 67, 59, 48, 60, 55, 54, 51, 298, 48, 59, 59, 54, 52, 42, 234, 55, 52, 52, 51]","[1697548494023, 1697548494552, 1697548494619, 1697548494678, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494946, 1697548495244, 1697548495292, 1697548495351, 1697548495410, 1697548495464, 1697548495516, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496002]"
1772,1772,814,26,[],200,llama-7b,64,1,2012.0,1.0,1,A100,1697548493990,1697548496002,120,89.0,20.0,"[18, 543, 67, 60, 48, 60, 55, 54, 52, 297, 49, 58, 59, 54, 52, 42, 234, 55, 52, 52, 51]","[1697548494008, 1697548494551, 1697548494618, 1697548494678, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494947, 1697548495244, 1697548495293, 1697548495351, 1697548495410, 1697548495464, 1697548495516, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496002]"
1773,1773,331,28,[],200,llama-7b,64,1,471.0,1.0,1,A100,1697548497701,1697548498172,120,26.0,1.0,"[22, 448]","[1697548497723, 1697548498171]"
1774,1774,834,14,[],200,llama-7b,64,1,2944.0,1.0,1,A100,1697548469104,1697548472048,120,85.0,20.0,"[12, 466, 274, 68, 53, 67, 51, 52, 65, 64, 844, 74, 70, 70, 67, 50, 65, 312, 74, 73, 73]","[1697548469116, 1697548469582, 1697548469856, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471194, 1697548471264, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471828, 1697548471902, 1697548471975, 1697548472048]"
1775,1775,276,30,[],200,llama-7b,64,1,2536.0,1.0,1,A100,1697548505562,1697548508098,120,732.0,13.0,"[30, 1080, 66, 58, 55, 44, 580, 65, 61, 60, 55, 57, 325]","[1697548505592, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507475, 1697548507540, 1697548507601, 1697548507661, 1697548507716, 1697548507773, 1697548508098]"
1776,1776,274,12,[],200,llama-7b,64,1,1739.0,1.0,1,A100,1697548496007,1697548497746,120,364.0,11.0,"[27, 897, 76, 69, 64, 60, 63, 57, 55, 262, 61, 48]","[1697548496034, 1697548496931, 1697548497007, 1697548497076, 1697548497140, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497746]"
1777,1777,44,13,[],200,llama-7b,64,1,423.0,1.0,1,A100,1697548497748,1697548498171,120,12.0,1.0,"[14, 409]","[1697548497762, 1697548498171]"
1778,1778,721,14,[],200,llama-7b,64,1,788.0,1.0,1,A100,1697548498174,1697548498962,120,286.0,5.0,"[22, 543, 63, 59, 51, 50]","[1697548498196, 1697548498739, 1697548498802, 1697548498861, 1697548498912, 1697548498962]"
1779,1779,496,34,[],200,llama-7b,64,1,1534.0,1.0,1,A100,1697548524657,1697548526191,120,335.0,11.0,"[16, 576, 240, 68, 65, 51, 50, 65, 66, 242, 47, 48]","[1697548524673, 1697548525249, 1697548525489, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525854, 1697548526096, 1697548526143, 1697548526191]"
1780,1780,104,51,[],200,llama-7b,64,1,4326.0,1.0,1,A100,1697548542751,1697548547077,120,93.0,20.0,"[11, 587, 370, 59, 47, 58, 54, 52, 787, 55, 54, 47, 951, 71, 66, 64, 62, 61, 48, 62, 760]","[1697548542762, 1697548543349, 1697548543719, 1697548543778, 1697548543825, 1697548543883, 1697548543937, 1697548543989, 1697548544776, 1697548544831, 1697548544885, 1697548544932, 1697548545883, 1697548545954, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077]"
1781,1781,376,15,[],200,llama-7b,64,1,4626.0,1.0,1,A100,1697548498965,1697548503591,120,87.0,20.0,"[7, 1453, 72, 65, 50, 49, 58, 55, 723, 63, 63, 57, 54, 56, 620, 65, 64, 62, 59, 55, 876]","[1697548498972, 1697548500425, 1697548500497, 1697548500562, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501497, 1697548501560, 1697548501623, 1697548501680, 1697548501734, 1697548501790, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503591]"
1782,1782,314,14,[],200,llama-7b,64,1,1881.0,1.0,1,A100,1697548466437,1697548468318,120,335.0,13.0,"[24, 555, 283, 74, 70, 68, 67, 51, 63, 359, 72, 54, 71, 70]","[1697548466461, 1697548467016, 1697548467299, 1697548467373, 1697548467443, 1697548467511, 1697548467578, 1697548467629, 1697548467692, 1697548468051, 1697548468123, 1697548468177, 1697548468248, 1697548468318]"
1783,1783,895,15,[],200,llama-7b,64,1,348.0,1.0,1,A100,1697548468323,1697548468671,120,15.0,1.0,"[24, 324]","[1697548468347, 1697548468671]"
1784,1784,673,16,[],200,llama-7b,64,1,3374.0,1.0,1,A100,1697548468674,1697548472048,120,93.0,20.0,"[21, 887, 274, 68, 53, 67, 51, 52, 65, 64, 844, 74, 72, 68, 67, 50, 65, 313, 74, 73, 72]","[1697548468695, 1697548469582, 1697548469856, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471120, 1697548471194, 1697548471266, 1697548471334, 1697548471401, 1697548471451, 1697548471516, 1697548471829, 1697548471903, 1697548471976, 1697548472048]"
1785,1785,274,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526193,1697548527820,120,,,"[6, 511, 254, 66, 49, 64, 66, 63]","[1697548526199, 1697548526710, 1697548526964, 1697548527030, 1697548527079, 1697548527143, 1697548527209, 1697548527272]"
1786,1786,328,17,[],200,llama-7b,64,1,741.0,1.0,1,A100,1697548472050,1697548472791,120,109.0,6.0,"[19, 387, 76, 70, 69, 68, 52]","[1697548472069, 1697548472456, 1697548472532, 1697548472602, 1697548472671, 1697548472739, 1697548472791]"
1787,1787,852,36,[],200,llama-7b,64,1,2572.0,1.0,1,A100,1697548527825,1697548530397,120,100.0,20.0,"[32, 408, 34, 655, 52, 50, 48, 47, 40, 364, 52, 49, 41, 41, 41, 49, 347, 56, 48, 59, 59]","[1697548527857, 1697548528265, 1697548528299, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529656, 1697548529697, 1697548529738, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530397]"
1788,1788,801,52,[],200,llama-7b,64,1,2962.0,1.0,1,A100,1697548547080,1697548550042,120,47.0,20.0,"[18, 517, 59, 53, 48, 369, 50, 48, 811, 61, 50, 60, 47, 58, 54, 396, 60, 48, 47, 48, 59]","[1697548547098, 1697548547615, 1697548547674, 1697548547727, 1697548547775, 1697548548144, 1697548548194, 1697548548242, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549383, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041]"
1789,1789,98,18,[],200,llama-7b,64,1,394.0,1.0,1,A100,1697548472792,1697548473186,120,14.0,1.0,"[14, 380]","[1697548472806, 1697548473186]"
1790,1790,687,19,[],200,llama-7b,64,1,4060.0,1.0,1,A100,1697548473189,1697548477249,120,96.0,20.0,"[17, 791, 73, 69, 67, 50, 66, 65, 63, 594, 71, 72, 70, 68, 67, 65, 861, 512, 288, 74, 57]","[1697548473206, 1697548473997, 1697548474070, 1697548474139, 1697548474206, 1697548474256, 1697548474322, 1697548474387, 1697548474450, 1697548475044, 1697548475115, 1697548475187, 1697548475257, 1697548475325, 1697548475392, 1697548475457, 1697548476318, 1697548476830, 1697548477118, 1697548477192, 1697548477249]"
1791,1791,146,16,[],200,llama-7b,64,1,3144.0,1.0,1,A100,1697548503594,1697548506738,120,96.0,20.0,"[17, 701, 256, 61, 48, 60, 58, 59, 57, 351, 65, 51, 61, 60, 60, 282, 46, 55, 52, 678, 65]","[1697548503611, 1697548504312, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504795, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505841, 1697548505887, 1697548505942, 1697548505994, 1697548506672, 1697548506737]"
1792,1792,332,21,[],200,llama-7b,64,1,988.0,1.0,1,A100,1697548491714,1697548492702,120,39.0,1.0,"[20, 967]","[1697548491734, 1697548492701]"
1793,1793,599,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548530401,1697548537858,120,,,"[28, 456, 51, 39, 471, 66, 62, 62, 61, 51, 736, 52, 59, 64, 51, 54, 576, 64, 63, 50, 67, 54, 45, 312, 60, 45, 46, 59, 57, 368, 46, 59, 58, 57, 708, 59, 59, 58, 55, 668, 63, 48, 55, 314, 52, 60, 66, 63, 60, 47]","[1697548530429, 1697548530885, 1697548530936, 1697548530975, 1697548531446, 1697548531512, 1697548531574, 1697548531636, 1697548531697, 1697548531748, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533584, 1697548533638, 1697548533683, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534262, 1697548534630, 1697548534676, 1697548534735, 1697548534793, 1697548534850, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
1794,1794,915,22,[],200,llama-7b,64,1,846.0,1.0,1,A100,1697548492704,1697548493550,120,182.0,1.0,"[6, 840]","[1697548492710, 1697548493550]"
1795,1795,735,17,[],200,llama-7b,64,1,2250.0,1.0,1,A100,1697548506740,1697548508990,120,85.0,20.0,"[25, 517, 194, 63, 62, 59, 57, 56, 324, 65, 64, 60, 59, 55, 42, 305, 57, 55, 50, 40, 40]","[1697548506765, 1697548507282, 1697548507476, 1697548507539, 1697548507601, 1697548507660, 1697548507717, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508747, 1697548508804, 1697548508859, 1697548508909, 1697548508949, 1697548508989]"
1796,1796,691,23,[],200,llama-7b,64,1,430.0,1.0,1,A100,1697548493557,1697548493987,120,47.0,1.0,"[18, 412]","[1697548493575, 1697548493987]"
1797,1797,462,53,[],200,llama-7b,64,1,296.0,1.0,1,A100,1697548550045,1697548550341,120,52.0,1.0,"[19, 277]","[1697548550064, 1697548550341]"
1798,1798,910,14,[],200,llama-7b,64,1,1069.0,1.0,1,A100,1697548527827,1697548528896,120,8.0,1.0,"[219, 850]","[1697548528046, 1697548528896]"
1799,1799,571,15,[],200,llama-7b,64,1,657.0,1.0,1,A100,1697548528899,1697548529556,120,67.0,2.0,"[27, 576, 54]","[1697548528926, 1697548529502, 1697548529556]"
1800,1800,342,16,[],200,llama-7b,64,1,2137.0,1.0,1,A100,1697548529559,1697548531696,120,364.0,14.0,"[15, 534, 69, 55, 48, 59, 60, 51, 486, 39, 472, 64, 64, 61, 60]","[1697548529574, 1697548530108, 1697548530177, 1697548530232, 1697548530280, 1697548530339, 1697548530399, 1697548530450, 1697548530936, 1697548530975, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531696]"
1801,1801,345,24,[],200,llama-7b,64,1,2012.0,1.0,1,A100,1697548493990,1697548496002,120,39.0,20.0,"[14, 547, 67, 60, 47, 60, 56, 54, 51, 298, 49, 58, 59, 54, 52, 42, 234, 55, 52, 52, 51]","[1697548494004, 1697548494551, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494841, 1697548494895, 1697548494946, 1697548495244, 1697548495293, 1697548495351, 1697548495410, 1697548495464, 1697548495516, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496002]"
1802,1802,232,54,[],200,llama-7b,64,1,2666.0,1.0,1,A100,1697548550343,1697548553009,120,93.0,20.0,"[19, 578, 61, 53, 52, 50, 48, 426, 63, 56, 45, 44, 55, 55, 712, 54, 65, 51, 50, 64, 64]","[1697548550362, 1697548550940, 1697548551001, 1697548551054, 1697548551106, 1697548551156, 1697548551204, 1697548551630, 1697548551693, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552714, 1697548552779, 1697548552830, 1697548552880, 1697548552944, 1697548553008]"
1803,1803,508,18,[],200,llama-7b,64,1,4129.0,1.0,1,A100,1697548508991,1697548513120,120,86.0,20.0,"[18, 1318, 66, 60, 59, 46, 58, 57, 231, 55, 43, 42, 53, 50, 656, 55, 55, 53, 54, 749, 351]","[1697548509009, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510673, 1697548510904, 1697548510959, 1697548511002, 1697548511044, 1697548511097, 1697548511147, 1697548511803, 1697548511858, 1697548511913, 1697548511966, 1697548512020, 1697548512769, 1697548513120]"
1804,1804,924,17,[],200,llama-7b,64,1,420.0,1.0,1,A100,1697548531699,1697548532119,120,9.0,1.0,"[23, 397]","[1697548531722, 1697548532119]"
1805,1805,701,18,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548532122,1697548537867,120,,,"[15, 1012, 191, 64, 63, 50, 68, 53, 46, 311, 60, 45, 46, 59, 58, 367, 46, 59, 59, 57, 707, 59, 59, 58, 55, 668, 63, 48, 55, 314, 52, 60, 66, 63, 60, 47]","[1697548532137, 1697548533149, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533585, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534676, 1697548534735, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
1806,1806,285,16,[],200,llama-7b,64,1,4300.0,1.0,1,A100,1697548498301,1697548502601,120,100.0,27.0,"[23, 414, 65, 58, 51, 49, 827, 61, 62, 59, 55, 50, 422, 65, 51, 50, 57, 54, 724, 63, 61, 57, 56, 54, 620, 66, 63, 63]","[1697548498324, 1697548498738, 1697548498803, 1697548498861, 1697548498912, 1697548498961, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500075, 1697548500497, 1697548500562, 1697548500613, 1697548500663, 1697548500720, 1697548500774, 1697548501498, 1697548501561, 1697548501622, 1697548501679, 1697548501735, 1697548501789, 1697548502409, 1697548502475, 1697548502538, 1697548502601]"
1807,1807,349,19,[],200,llama-7b,64,1,2903.0,1.0,1,A100,1697548537878,1697548540781,120,88.0,20.0,"[270, 683, 53, 55, 42, 46, 46, 677, 61, 56, 54, 44, 52, 41, 401, 49, 60, 46, 58, 55, 54]","[1697548538148, 1697548538831, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539073, 1697548539750, 1697548539811, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540459, 1697548540508, 1697548540568, 1697548540614, 1697548540672, 1697548540727, 1697548540781]"
1808,1808,184,26,[],200,llama-7b,64,1,3146.0,1.0,1,A100,1697548502413,1697548505559,120,87.0,20.0,"[24, 693, 461, 63, 58, 57, 53, 41, 704, 62, 47, 60, 59, 59, 56, 352, 66, 50, 61, 60, 60]","[1697548502437, 1697548503130, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504854, 1697548504910, 1697548505262, 1697548505328, 1697548505378, 1697548505439, 1697548505499, 1697548505559]"
1809,1809,168,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548513131,1697548527820,120,,,"[23, 628, 382, 66, 60, 58, 58, 918, 71, 70, 64, 61, 977, 271, 68, 65, 64, 64, 293, 67, 64, 58, 53, 672, 66, 65, 59, 57, 361, 63, 50, 58, 57, 45, 607, 242, 68, 62, 62, 62, 60, 60, 336, 66, 63, 48, 49, 53, 406, 62, 48, 60, 46, 54, 709, 70, 67, 67, 57, 66, 49, 317, 67, 64, 63, 63, 61, 317, 52, 67, 51, 64, 63, 554, 64, 66, 65, 59, 57, 589, 67, 66, 50, 51, 65, 65, 241, 49, 47, 60, 59, 654, 66, 49, 65, 65, 63]","[1697548513154, 1697548513782, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406, 1697548515324, 1697548515395, 1697548515465, 1697548515529, 1697548515590, 1697548516567, 1697548516838, 1697548516906, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517459, 1697548517523, 1697548517581, 1697548517634, 1697548518306, 1697548518372, 1697548518437, 1697548518496, 1697548518553, 1697548518914, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521025, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522786, 1697548523103, 1697548523170, 1697548523234, 1697548523297, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524035, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524900, 1697548525489, 1697548525556, 1697548525622, 1697548525672, 1697548525723, 1697548525788, 1697548525853, 1697548526094, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526963, 1697548527029, 1697548527078, 1697548527143, 1697548527208, 1697548527271]"
1810,1810,126,20,[],200,llama-7b,64,1,804.0,1.0,1,A100,1697548540786,1697548541590,120,19.0,1.0,"[78, 726]","[1697548540864, 1697548541590]"
1811,1811,314,18,[],200,llama-7b,64,1,1866.0,1.0,1,A100,1697548488029,1697548489895,120,335.0,13.0,"[37, 766, 56, 52, 35, 37, 285, 67, 50, 49, 40, 48, 40, 304]","[1697548488066, 1697548488832, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489551, 1697548489591, 1697548489895]"
1812,1812,362,25,[],200,llama-7b,64,1,898.0,1.0,1,A100,1697548497841,1697548498739,120,14.0,1.0,"[14, 883]","[1697548497855, 1697548498738]"
1813,1813,708,21,[],200,llama-7b,64,1,906.0,1.0,1,A100,1697548541596,1697548542502,120,140.0,1.0,"[15, 891]","[1697548541611, 1697548542502]"
1814,1814,480,22,[],200,llama-7b,64,1,843.0,1.0,1,A100,1697548542506,1697548543349,120,26.0,1.0,"[20, 823]","[1697548542526, 1697548543349]"
1815,1815,836,40,[],200,llama-7b,64,1,539.0,1.0,1,A100,1697548517526,1697548518065,120,11.0,1.0,"[29, 510]","[1697548517555, 1697548518065]"
1816,1816,584,14,[],200,llama-7b,64,1,801.0,1.0,1,A100,1697548488033,1697548488834,120,10.0,1.0,"[163, 637]","[1697548488196, 1697548488833]"
1817,1817,467,41,[],200,llama-7b,64,1,2905.0,1.0,1,A100,1697548518068,1697548520973,120,93.0,20.0,"[21, 757, 69, 63, 49, 59, 56, 45, 607, 242, 68, 63, 62, 62, 59, 61, 335, 66, 63, 49, 49]","[1697548518089, 1697548518846, 1697548518915, 1697548518978, 1697548519027, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520167, 1697548520229, 1697548520291, 1697548520350, 1697548520411, 1697548520746, 1697548520812, 1697548520875, 1697548520924, 1697548520973]"
1818,1818,228,23,[],200,llama-7b,64,1,4066.0,1.0,1,A100,1697548543352,1697548547418,120,100.0,20.0,"[11, 1159, 254, 56, 53, 47, 952, 71, 66, 63, 62, 61, 48, 62, 760, 65, 64, 59, 49, 58, 45]","[1697548543363, 1697548544522, 1697548544776, 1697548544832, 1697548544885, 1697548544932, 1697548545884, 1697548545955, 1697548546021, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547372, 1697548547417]"
1819,1819,237,15,[],200,llama-7b,64,1,2043.0,1.0,1,A100,1697548488838,1697548490881,120,87.0,20.0,"[54, 349, 57, 67, 50, 48, 41, 48, 40, 303, 52, 43, 43, 50, 49, 40, 41, 194, 374, 50, 50]","[1697548488892, 1697548489241, 1697548489298, 1697548489365, 1697548489415, 1697548489463, 1697548489504, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490407, 1697548490781, 1697548490831, 1697548490881]"
1820,1820,5,41,[],200,llama-7b,64,1,2619.0,1.0,1,A100,1697548519029,1697548521648,120,84.0,20.0,"[15, 566, 185, 242, 67, 63, 62, 61, 61, 59, 337, 65, 63, 48, 49, 54, 405, 63, 47, 61, 46]","[1697548519044, 1697548519610, 1697548519795, 1697548520037, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520351, 1697548520410, 1697548520747, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521026, 1697548521431, 1697548521494, 1697548521541, 1697548521602, 1697548521648]"
1821,1821,753,15,[],200,llama-7b,64,1,3091.0,1.0,1,A100,1697548505196,1697548508287,120,83.0,20.0,"[17, 566, 63, 45, 55, 52, 678, 66, 58, 55, 43, 581, 65, 61, 60, 55, 57, 324, 65, 64, 61]","[1697548505213, 1697548505779, 1697548505842, 1697548505887, 1697548505942, 1697548505994, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507540, 1697548507601, 1697548507661, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508287]"
1822,1822,79,28,[],200,llama-7b,64,1,554.0,1.0,1,A100,1697548498866,1697548499420,120,12.0,1.0,"[22, 532]","[1697548498888, 1697548499420]"
1823,1823,13,16,[],200,llama-7b,64,1,2730.0,1.0,1,A100,1697548490883,1697548493613,120,90.0,20.0,"[15, 447, 130, 50, 48, 40, 49, 48, 328, 54, 53, 43, 51, 50, 49, 722, 59, 49, 48, 39, 357]","[1697548490898, 1697548491345, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491710, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492289, 1697548492338, 1697548493060, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493612]"
1824,1824,668,29,[],200,llama-7b,64,1,1297.0,1.0,1,A100,1697548499423,1697548500720,120,109.0,6.0,"[24, 978, 72, 65, 50, 49, 59]","[1697548499447, 1697548500425, 1697548500497, 1697548500562, 1697548500612, 1697548500661, 1697548500720]"
1825,1825,810,24,[],200,llama-7b,64,1,3050.0,1.0,1,A100,1697548547422,1697548550472,120,91.0,20.0,"[33, 626, 64, 50, 48, 810, 61, 51, 59, 47, 58, 55, 396, 59, 48, 47, 48, 59, 56, 312, 63]","[1697548547455, 1697548548081, 1697548548145, 1697548548195, 1697548548243, 1697548549053, 1697548549114, 1697548549165, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472]"
1826,1826,138,24,[],200,llama-7b,64,1,2182.0,1.0,1,A100,1697548488033,1697548490215,120,91.0,20.0,"[138, 661, 57, 51, 35, 37, 285, 68, 49, 49, 41, 47, 40, 305, 51, 43, 43, 51, 48, 41, 42]","[1697548488171, 1697548488832, 1697548488889, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489504, 1697548489551, 1697548489591, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490215]"
1827,1827,439,30,[],200,llama-7b,64,1,900.0,1.0,1,A100,1697548500722,1697548501622,120,13.0,4.0,"[10, 471, 295, 63, 61]","[1697548500732, 1697548501203, 1697548501498, 1697548501561, 1697548501622]"
1828,1828,483,15,[],200,llama-7b,64,1,2337.0,1.0,1,A100,1697548472051,1697548474388,120,84.0,20.0,"[21, 384, 76, 70, 69, 68, 52, 65, 49, 530, 67, 49, 51, 65, 58, 346, 68, 67, 51, 66, 64]","[1697548472072, 1697548472456, 1697548472532, 1697548472602, 1697548472671, 1697548472739, 1697548472791, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473667, 1697548473725, 1697548474071, 1697548474139, 1697548474206, 1697548474257, 1697548474323, 1697548474387]"
1829,1829,378,15,[],200,llama-7b,64,1,8858.0,1.0,1,A100,1697548477256,1697548486114,120,93.0,47.0,"[33, 641, 251, 54, 254, 70, 68, 62, 612, 256, 71, 54, 71, 68, 63, 912, 76, 75, 71, 72, 67, 66, 589, 269, 63, 74, 264, 68, 51, 69, 838, 76, 55, 72, 66, 66, 49, 338, 76, 75, 75, 71, 57, 71, 837, 314, 308]","[1697548477289, 1697548477930, 1697548478181, 1697548478235, 1697548478489, 1697548478559, 1697548478627, 1697548478689, 1697548479301, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480796, 1697548480872, 1697548480947, 1697548481018, 1697548481090, 1697548481157, 1697548481223, 1697548481812, 1697548482081, 1697548482144, 1697548482218, 1697548482482, 1697548482550, 1697548482601, 1697548482670, 1697548483508, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484381, 1697548484456, 1697548484527, 1697548484584, 1697548484655, 1697548485492, 1697548485806, 1697548486114]"
1830,1830,312,18,[],200,llama-7b,64,1,801.0,1.0,1,A100,1697548488032,1697548488833,120,23.0,1.0,"[67, 734]","[1697548488099, 1697548488833]"
1831,1831,59,19,[],200,llama-7b,64,1,2043.0,1.0,1,A100,1697548488838,1697548490881,120,91.0,20.0,"[49, 354, 57, 67, 50, 48, 41, 48, 40, 303, 52, 43, 43, 50, 49, 40, 41, 194, 374, 50, 50]","[1697548488887, 1697548489241, 1697548489298, 1697548489365, 1697548489415, 1697548489463, 1697548489504, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490407, 1697548490781, 1697548490831, 1697548490881]"
1832,1832,529,16,[],200,llama-7b,64,1,390.0,1.0,1,A100,1697548508290,1697548508680,120,10.0,1.0,"[19, 370]","[1697548508309, 1697548508679]"
1833,1833,719,25,[],200,llama-7b,64,1,1257.0,1.0,1,A100,1697548490218,1697548491475,120,182.0,6.0,"[47, 516, 50, 48, 42, 554]","[1697548490265, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491475]"
1834,1834,641,20,[],200,llama-7b,64,1,5628.0,1.0,1,A100,1697548490884,1697548496512,120,16.0,50.0,"[24, 437, 130, 50, 48, 40, 49, 49, 327, 54, 53, 43, 51, 51, 48, 722, 59, 49, 48, 39, 357, 47, 38, 38, 312, 52, 41, 46, 432, 60, 47, 60, 55, 55, 51, 297, 49, 58, 58, 55, 53, 42, 233, 55, 53, 51, 52, 378, 41, 40, 51]","[1697548490908, 1697548491345, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491711, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492338, 1697548493060, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493612, 1697548493659, 1697548493697, 1697548493735, 1697548494047, 1697548494099, 1697548494140, 1697548494186, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494840, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495408, 1697548495463, 1697548495516, 1697548495558, 1697548495791, 1697548495846, 1697548495899, 1697548495950, 1697548496002, 1697548496380, 1697548496421, 1697548496461, 1697548496512]"
1835,1835,182,17,[],200,llama-7b,64,1,5482.0,1.0,1,A100,1697548508682,1697548514164,120,47.0,31.0,"[6, 751, 129, 57, 54, 53, 53, 49, 560, 59, 59, 46, 59, 56, 230, 57, 42, 43, 51, 51, 655, 56, 54, 53, 51, 753, 350, 59, 55, 43, 54, 834]","[1697548508688, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509834, 1697548510394, 1697548510453, 1697548510512, 1697548510558, 1697548510617, 1697548510673, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511802, 1697548511858, 1697548511912, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514164]"
1836,1836,51,31,[],200,llama-7b,64,1,5229.0,1.0,1,A100,1697548508101,1697548513330,120,364.0,36.0,"[9, 569, 69, 57, 54, 50, 40, 40, 579, 58, 53, 52, 54, 49, 559, 60, 59, 46, 58, 56, 230, 57, 42, 44, 51, 51, 654, 57, 54, 53, 51, 753, 350, 59, 55, 43, 54]","[1697548508110, 1697548508679, 1697548508748, 1697548508805, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509568, 1697548509626, 1697548509679, 1697548509731, 1697548509785, 1697548509834, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510672, 1697548510902, 1697548510959, 1697548511001, 1697548511045, 1697548511096, 1697548511147, 1697548511801, 1697548511858, 1697548511912, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330]"
1837,1837,492,26,[],200,llama-7b,64,1,2663.0,1.0,1,A100,1697548491478,1697548494141,120,47.0,20.0,"[10, 493, 58, 54, 52, 44, 51, 50, 49, 722, 58, 49, 48, 39, 358, 46, 39, 37, 312, 52, 42]","[1697548491488, 1697548491981, 1697548492039, 1697548492093, 1697548492145, 1697548492189, 1697548492240, 1697548492290, 1697548492339, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493613, 1697548493659, 1697548493698, 1697548493735, 1697548494047, 1697548494099, 1697548494141]"
1838,1838,812,29,[],200,llama-7b,64,1,360.0,1.0,1,A100,1697548497212,1697548497572,120,16.0,1.0,"[12, 348]","[1697548497224, 1697548497572]"
1839,1839,465,30,[],200,llama-7b,64,1,724.0,1.0,1,A100,1697548497574,1697548498298,120,364.0,3.0,"[14, 583, 66, 61]","[1697548497588, 1697548498171, 1697548498237, 1697548498298]"
1840,1840,238,31,[],200,llama-7b,64,1,1488.0,1.0,1,A100,1697548498300,1697548499788,120,563.0,6.0,"[19, 419, 65, 58, 51, 50, 826]","[1697548498319, 1697548498738, 1697548498803, 1697548498861, 1697548498912, 1697548498962, 1697548499788]"
1841,1841,333,37,[],200,llama-7b,64,1,1641.0,1.0,1,A100,1697548534149,1697548535790,120,563.0,11.0,"[8, 408, 65, 47, 59, 58, 57, 707, 59, 59, 58, 56]","[1697548534157, 1697548534565, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535790]"
1842,1842,259,16,[],200,llama-7b,64,1,4238.0,1.0,1,A100,1697548474390,1697548478628,120,87.0,20.0,"[9, 361, 284, 72, 71, 70, 69, 67, 64, 860, 512, 288, 75, 56, 63, 619, 251, 53, 254, 70, 70]","[1697548474399, 1697548474760, 1697548475044, 1697548475116, 1697548475187, 1697548475257, 1697548475326, 1697548475393, 1697548475457, 1697548476317, 1697548476829, 1697548477117, 1697548477192, 1697548477248, 1697548477311, 1697548477930, 1697548478181, 1697548478234, 1697548478488, 1697548478558, 1697548478628]"
1843,1843,626,48,[],200,llama-7b,64,1,558.0,1.0,1,A100,1697548549783,1697548550341,120,10.0,1.0,"[19, 539]","[1697548549802, 1697548550341]"
1844,1844,774,47,[],200,llama-7b,64,1,915.0,1.0,1,A100,1697548530453,1697548531368,120,8.0,1.0,"[83, 832]","[1697548530536, 1697548531368]"
1845,1845,288,49,[],200,llama-7b,64,1,2665.0,1.0,1,A100,1697548550344,1697548553009,120,93.0,20.0,"[19, 577, 61, 53, 51, 51, 48, 425, 64, 56, 44, 45, 55, 55, 712, 54, 65, 51, 50, 64, 64]","[1697548550363, 1697548550940, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551204, 1697548551629, 1697548551693, 1697548551749, 1697548551793, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552714, 1697548552779, 1697548552830, 1697548552880, 1697548552944, 1697548553008]"
1846,1846,554,48,[],200,llama-7b,64,1,748.0,1.0,1,A100,1697548531371,1697548532119,120,26.0,1.0,"[37, 711]","[1697548531408, 1697548532119]"
1847,1847,206,49,[],200,llama-7b,64,1,1026.0,1.0,1,A100,1697548532123,1697548533149,120,16.0,1.0,"[19, 1007]","[1697548532142, 1697548533149]"
1848,1848,882,50,[],200,llama-7b,64,1,1643.0,1.0,1,A100,1697548533152,1697548534795,120,345.0,11.0,"[7, 772, 64, 60, 45, 46, 59, 58, 367, 46, 59, 59]","[1697548533159, 1697548533931, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534676, 1697548534735, 1697548534794]"
1849,1849,656,28,[],200,llama-7b,64,1,926.0,1.0,1,A100,1697548523427,1697548524353,120,26.0,1.0,"[14, 912]","[1697548523441, 1697548524353]"
1850,1850,123,27,[],200,llama-7b,64,1,407.0,1.0,1,A100,1697548494145,1697548494552,120,14.0,1.0,"[19, 388]","[1697548494164, 1697548494552]"
1851,1851,317,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548524356,1697548527820,120,,,"[17, 876, 240, 68, 65, 51, 50, 65, 66, 241, 48, 48, 59, 59, 656, 65, 49, 65, 64, 64]","[1697548524373, 1697548525249, 1697548525489, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525854, 1697548526095, 1697548526143, 1697548526191, 1697548526250, 1697548526309, 1697548526965, 1697548527030, 1697548527079, 1697548527144, 1697548527208, 1697548527272]"
1852,1852,822,28,[],200,llama-7b,64,1,2521.0,1.0,1,A100,1697548494555,1697548497076,120,88.0,20.0,"[22, 602, 64, 50, 58, 58, 55, 53, 41, 233, 55, 54, 51, 52, 378, 40, 40, 51, 50, 445, 69]","[1697548494577, 1697548495179, 1697548495243, 1697548495293, 1697548495351, 1697548495409, 1697548495464, 1697548495517, 1697548495558, 1697548495791, 1697548495846, 1697548495900, 1697548495951, 1697548496003, 1697548496381, 1697548496421, 1697548496461, 1697548496512, 1697548496562, 1697548497007, 1697548497076]"
1853,1853,240,19,[],200,llama-7b,64,1,3578.0,1.0,1,A100,1697548473670,1697548477248,120,83.0,20.0,"[11, 316, 74, 68, 67, 51, 66, 64, 63, 593, 73, 71, 69, 70, 67, 63, 862, 511, 288, 75, 56]","[1697548473681, 1697548473997, 1697548474071, 1697548474139, 1697548474206, 1697548474257, 1697548474323, 1697548474387, 1697548474450, 1697548475043, 1697548475116, 1697548475187, 1697548475256, 1697548475326, 1697548475393, 1697548475456, 1697548476318, 1697548476829, 1697548477117, 1697548477192, 1697548477248]"
1854,1854,440,23,[],200,llama-7b,64,1,3174.0,1.0,1,A100,1697548501737,1697548504911,120,84.0,20.0,"[8, 412, 254, 65, 63, 62, 60, 54, 876, 63, 58, 57, 52, 42, 705, 61, 48, 60, 59, 58, 57]","[1697548501745, 1697548502157, 1697548502411, 1697548502476, 1697548502539, 1697548502601, 1697548502661, 1697548502715, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911]"
1855,1855,17,20,[],200,llama-7b,64,1,427.0,1.0,1,A100,1697548477252,1697548477679,120,23.0,1.0,"[40, 387]","[1697548477292, 1697548477679]"
1856,1856,55,50,[],200,llama-7b,64,1,768.0,1.0,1,A100,1697548553012,1697548553780,120,12.0,1.0,"[25, 743]","[1697548553037, 1697548553780]"
1857,1857,600,21,[],200,llama-7b,64,1,1408.0,1.0,1,A100,1697548477683,1697548479091,120,23.0,1.0,"[25, 1383]","[1697548477708, 1697548479091]"
1858,1858,83,30,[],200,llama-7b,64,1,2004.0,1.0,1,A100,1697548527825,1697548529829,120,123.0,15.0,"[27, 413, 34, 655, 52, 50, 48, 47, 40, 364, 52, 49, 41, 41, 41, 49]","[1697548527852, 1697548528265, 1697548528299, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529656, 1697548529697, 1697548529738, 1697548529779, 1697548529828]"
1859,1859,645,51,[],200,llama-7b,64,1,2351.0,1.0,1,A100,1697548553783,1697548556134,120,86.0,20.0,"[6, 645, 437, 251, 253, 58, 45, 45, 45, 58, 57, 43, 50, 69, 46, 37, 45, 42, 41, 39, 39]","[1697548553789, 1697548554434, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555523, 1697548555568, 1697548555626, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134]"
1860,1860,95,24,[],200,llama-7b,64,1,865.0,1.0,1,A100,1697548504915,1697548505780,120,12.0,1.0,"[17, 848]","[1697548504932, 1697548505780]"
1861,1861,860,27,[],200,llama-7b,64,1,3186.0,1.0,1,A100,1697548505562,1697548508748,120,85.0,20.0,"[25, 813, 272, 66, 58, 55, 44, 580, 65, 62, 59, 55, 57, 324, 65, 65, 60, 59, 54, 42, 306]","[1697548505587, 1697548506400, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507475, 1697548507540, 1697548507602, 1697548507661, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748]"
1862,1862,679,17,[],200,llama-7b,64,1,439.0,1.0,1,A100,1697548475398,1697548475837,120,15.0,1.0,"[18, 421]","[1697548475416, 1697548475837]"
1863,1863,660,32,[],200,llama-7b,64,1,2995.0,1.0,1,A100,1697548520977,1697548523972,120,732.0,25.0,"[24, 362, 68, 63, 47, 60, 46, 54, 710, 70, 66, 68, 56, 66, 50, 317, 66, 64, 63, 63, 62, 316, 52, 67, 51, 64]","[1697548521001, 1697548521363, 1697548521431, 1697548521494, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522481, 1697548522547, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523297, 1697548523360, 1697548523422, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972]"
1864,1864,156,24,[],200,llama-7b,64,1,2503.0,1.0,1,A100,1697548495795,1697548498298,120,86.0,20.0,"[25, 505, 55, 41, 41, 50, 50, 447, 68, 64, 59, 64, 56, 55, 262, 61, 47, 47, 45, 400, 61]","[1697548495820, 1697548496325, 1697548496380, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497009, 1697548497077, 1697548497141, 1697548497200, 1697548497264, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497745, 1697548497792, 1697548497837, 1697548498237, 1697548498298]"
1865,1865,375,22,[],200,llama-7b,64,1,4415.0,1.0,1,A100,1697548479093,1697548483508,120,874.0,17.0,"[16, 1687, 75, 75, 72, 71, 68, 67, 587, 270, 63, 74, 265, 67, 52, 68, 838]","[1697548479109, 1697548480796, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482081, 1697548482144, 1697548482218, 1697548482483, 1697548482550, 1697548482602, 1697548482670, 1697548483508]"
1866,1866,448,18,[],200,llama-7b,64,1,3843.0,1.0,1,A100,1697548475840,1697548479683,120,335.0,12.0,"[17, 1822, 250, 251, 55, 253, 71, 68, 61, 614, 254, 71, 55]","[1697548475857, 1697548477679, 1697548477929, 1697548478180, 1697548478235, 1697548478488, 1697548478559, 1697548478627, 1697548478688, 1697548479302, 1697548479556, 1697548479627, 1697548479682]"
1867,1867,800,25,[],200,llama-7b,64,1,2964.0,1.0,1,A100,1697548505784,1697548508748,120,140.0,20.0,"[28, 860, 66, 58, 55, 44, 581, 63, 62, 59, 56, 57, 325, 64, 65, 60, 59, 54, 42, 306]","[1697548505812, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507476, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508098, 1697548508162, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748]"
1868,1868,36,23,[],200,llama-7b,64,1,3722.0,1.0,1,A100,1697548483511,1697548487233,120,457.0,20.0,"[13, 624, 82, 77, 78, 72, 70, 57, 70, 837, 314, 308, 76, 76, 72, 70, 323, 59, 59, 79, 306]","[1697548483524, 1697548484148, 1697548484230, 1697548484307, 1697548484385, 1697548484457, 1697548484527, 1697548484584, 1697548484654, 1697548485491, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487233]"
1869,1869,104,19,[],200,llama-7b,64,1,4026.0,1.0,1,A100,1697548479685,1697548483711,120,93.0,20.0,"[16, 601, 494, 75, 75, 72, 72, 67, 67, 587, 270, 62, 74, 265, 68, 51, 68, 839, 75, 55, 73]","[1697548479701, 1697548480302, 1697548480796, 1697548480871, 1697548480946, 1697548481018, 1697548481090, 1697548481157, 1697548481224, 1697548481811, 1697548482081, 1697548482143, 1697548482217, 1697548482482, 1697548482550, 1697548482601, 1697548482669, 1697548483508, 1697548483583, 1697548483638, 1697548483711]"
1870,1870,734,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548487236,1697548488026,120,,,"[7, 565]","[1697548487243, 1697548487808]"
1871,1871,92,27,[],200,llama-7b,64,1,3186.0,1.0,1,A100,1697548505562,1697548508748,120,85.0,20.0,"[33, 805, 273, 65, 58, 55, 44, 580, 65, 61, 60, 55, 57, 325, 64, 65, 60, 59, 54, 42, 306]","[1697548505595, 1697548506400, 1697548506673, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507475, 1697548507540, 1697548507601, 1697548507661, 1697548507716, 1697548507773, 1697548508098, 1697548508162, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748]"
1872,1872,395,25,[],200,llama-7b,64,1,2184.0,1.0,1,A100,1697548488029,1697548490213,120,88.0,20.0,"[13, 789, 57, 52, 35, 37, 285, 67, 50, 48, 41, 48, 40, 304, 52, 43, 42, 51, 49, 40, 41]","[1697548488042, 1697548488831, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414, 1697548489462, 1697548489503, 1697548489551, 1697548489591, 1697548489895, 1697548489947, 1697548489990, 1697548490032, 1697548490083, 1697548490132, 1697548490172, 1697548490213]"
1873,1873,172,43,[],200,llama-7b,64,1,404.0,1.0,1,A100,1697548536462,1697548536866,120,19.0,1.0,"[17, 387]","[1697548536479, 1697548536866]"
1874,1874,576,28,[],200,llama-7b,64,1,461.0,1.0,1,A100,1697548490884,1697548491345,120,14.0,1.0,"[32, 429]","[1697548490916, 1697548491345]"
1875,1875,453,29,[],200,llama-7b,64,1,384.0,1.0,1,A100,1697548520292,1697548520676,120,26.0,1.0,"[16, 367]","[1697548520308, 1697548520675]"
1876,1876,110,30,[],200,llama-7b,64,1,862.0,1.0,1,A100,1697548520679,1697548521541,120,96.0,4.0,"[22, 662, 68, 62, 48]","[1697548520701, 1697548521363, 1697548521431, 1697548521493, 1697548521541]"
1877,1877,816,31,[],200,llama-7b,64,1,1005.0,1.0,1,A100,1697548521543,1697548522548,120,182.0,4.0,"[12, 590, 265, 70, 68]","[1697548521555, 1697548522145, 1697548522410, 1697548522480, 1697548522548]"
1878,1878,230,29,[],200,llama-7b,64,1,840.0,1.0,1,A100,1697548491348,1697548492188,120,86.0,5.0,"[29, 603, 58, 54, 53, 43]","[1697548491377, 1697548491980, 1697548492038, 1697548492092, 1697548492145, 1697548492188]"
1879,1879,57,12,[],200,llama-7b,64,1,509.0,1.0,1,A100,1697548490218,1697548490727,120,13.0,1.0,"[41, 467]","[1697548490259, 1697548490726]"
1880,1880,780,20,[],200,llama-7b,64,1,3520.0,1.0,1,A100,1697548483714,1697548487234,120,85.0,20.0,"[13, 421, 83, 76, 77, 73, 70, 57, 70, 838, 313, 308, 76, 76, 72, 70, 323, 59, 59, 79, 307]","[1697548483727, 1697548484148, 1697548484231, 1697548484307, 1697548484384, 1697548484457, 1697548484527, 1697548484584, 1697548484654, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486265, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234]"
1881,1881,648,13,[],200,llama-7b,64,1,2883.0,1.0,1,A100,1697548490729,1697548493612,120,84.0,20.0,"[20, 595, 131, 50, 48, 40, 48, 49, 329, 54, 52, 42, 52, 50, 49, 724, 59, 47, 48, 39, 357]","[1697548490749, 1697548491344, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492039, 1697548492093, 1697548492145, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493062, 1697548493121, 1697548493168, 1697548493216, 1697548493255, 1697548493612]"
1882,1882,7,30,[],200,llama-7b,64,1,1856.0,1.0,1,A100,1697548492191,1697548494047,120,345.0,11.0,"[7, 503, 360, 58, 49, 48, 39, 356, 49, 38, 37, 312]","[1697548492198, 1697548492701, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493660, 1697548493698, 1697548493735, 1697548494047]"
1883,1883,419,14,[],200,llama-7b,64,1,2179.0,1.0,1,A100,1697548493613,1697548495792,120,88.0,20.0,"[8, 367, 59, 52, 42, 46, 432, 59, 47, 61, 54, 56, 50, 298, 49, 58, 59, 54, 52, 42, 233]","[1697548493621, 1697548493988, 1697548494047, 1697548494099, 1697548494141, 1697548494187, 1697548494619, 1697548494678, 1697548494725, 1697548494786, 1697548494840, 1697548494896, 1697548494946, 1697548495244, 1697548495293, 1697548495351, 1697548495410, 1697548495464, 1697548495516, 1697548495558, 1697548495791]"
1884,1884,587,31,[],200,llama-7b,64,1,503.0,1.0,1,A100,1697548494049,1697548494552,120,13.0,1.0,"[6, 497]","[1697548494055, 1697548494552]"
1885,1885,335,32,[],200,llama-7b,64,1,8104.0,1.0,1,A100,1697548494556,1697548502660,120,58.0,62.0,"[26, 597, 65, 49, 58, 58, 55, 53, 41, 233, 55, 53, 51, 54, 377, 40, 40, 51, 50, 445, 69, 64, 61, 62, 57, 55, 262, 60, 48, 46, 46, 400, 61, 52, 51, 50, 351, 59, 51, 50, 826, 61, 61, 60, 54, 51, 421, 65, 51, 49, 58, 55, 723, 63, 61, 58, 55, 55, 620, 66, 63, 62, 60]","[1697548494582, 1697548495179, 1697548495244, 1697548495293, 1697548495351, 1697548495409, 1697548495464, 1697548495517, 1697548495558, 1697548495791, 1697548495846, 1697548495899, 1697548495950, 1697548496004, 1697548496381, 1697548496421, 1697548496461, 1697548496512, 1697548496562, 1697548497007, 1697548497076, 1697548497140, 1697548497201, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497791, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499910, 1697548499970, 1697548500024, 1697548500075, 1697548500496, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501497, 1697548501560, 1697548501621, 1697548501679, 1697548501734, 1697548501789, 1697548502409, 1697548502475, 1697548502538, 1697548502600, 1697548502660]"
1886,1886,337,38,[],200,llama-7b,64,1,536.0,1.0,1,A100,1697548525493,1697548526029,120,12.0,1.0,"[6, 530]","[1697548525499, 1697548526029]"
1887,1887,598,42,[],200,llama-7b,64,1,1649.0,1.0,1,A100,1697548521649,1697548523298,120,345.0,12.0,"[20, 476, 266, 69, 68, 66, 57, 66, 50, 316, 67, 64, 64]","[1697548521669, 1697548522145, 1697548522411, 1697548522480, 1697548522548, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103, 1697548523170, 1697548523234, 1697548523298]"
1888,1888,365,43,[],200,llama-7b,64,1,365.0,1.0,1,A100,1697548523301,1697548523666,120,23.0,1.0,"[11, 354]","[1697548523312, 1697548523666]"
1889,1889,779,27,[],200,llama-7b,64,1,2936.0,1.0,1,A100,1697548500777,1697548503713,120,563.0,10.0,"[14, 1365, 254, 65, 64, 61, 60, 55, 875, 64, 58]","[1697548500791, 1697548502156, 1697548502410, 1697548502475, 1697548502539, 1697548502600, 1697548502660, 1697548502715, 1697548503590, 1697548503654, 1697548503712]"
1890,1890,26,44,[],200,llama-7b,64,1,683.0,1.0,1,A100,1697548523670,1697548524353,120,18.0,1.0,"[15, 668]","[1697548523685, 1697548524353]"
1891,1891,725,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548524356,1697548527820,120,,,"[7, 886, 240, 68, 65, 51, 50, 65, 65, 242, 48, 47, 60, 59, 656, 64, 50, 64, 65, 64]","[1697548524363, 1697548525249, 1697548525489, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526965, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
1892,1892,440,28,[],200,llama-7b,64,1,3022.0,1.0,1,A100,1697548503716,1697548506738,120,84.0,20.0,"[11, 586, 255, 61, 48, 60, 59, 58, 57, 351, 65, 51, 61, 60, 60, 283, 46, 54, 52, 679, 64]","[1697548503727, 1697548504313, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506673, 1697548506737]"
1893,1893,209,29,[],200,llama-7b,64,1,542.0,1.0,1,A100,1697548506741,1697548507283,120,20.0,1.0,"[27, 514]","[1697548506768, 1697548507282]"
1894,1894,380,46,[],200,llama-7b,64,1,6851.0,1.0,1,A100,1697548527826,1697548534677,120,216.0,50.0,"[61, 412, 656, 52, 50, 47, 47, 40, 365, 52, 48, 41, 42, 40, 50, 347, 55, 48, 59, 60, 50, 487, 39, 472, 65, 63, 61, 60, 52, 736, 52, 59, 64, 51, 55, 575, 65, 63, 50, 66, 55, 46, 311, 60, 45, 46, 59, 57, 368, 47]","[1697548527887, 1697548528299, 1697548528955, 1697548529007, 1697548529057, 1697548529104, 1697548529151, 1697548529191, 1697548529556, 1697548529608, 1697548529656, 1697548529697, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530279, 1697548530338, 1697548530398, 1697548530448, 1697548530935, 1697548530974, 1697548531446, 1697548531511, 1697548531574, 1697548531635, 1697548531695, 1697548531747, 1697548532483, 1697548532535, 1697548532594, 1697548532658, 1697548532709, 1697548532764, 1697548533339, 1697548533404, 1697548533467, 1697548533517, 1697548533583, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534262, 1697548534630, 1697548534677]"
1895,1895,798,30,[],200,llama-7b,64,1,1061.0,1.0,1,A100,1697548507285,1697548508346,120,79.0,6.0,"[14, 799, 64, 64, 61, 58]","[1697548507299, 1697548508098, 1697548508162, 1697548508226, 1697548508287, 1697548508345]"
1896,1896,921,33,[],200,llama-7b,64,1,467.0,1.0,1,A100,1697548502663,1697548503130,120,31.0,1.0,"[7, 460]","[1697548502670, 1697548503130]"
1897,1897,72,15,[],200,llama-7b,64,1,2504.0,1.0,1,A100,1697548495794,1697548498298,120,84.0,20.0,"[16, 514, 56, 41, 41, 50, 50, 446, 69, 64, 64, 59, 56, 55, 262, 60, 48, 46, 46, 400, 61]","[1697548495810, 1697548496324, 1697548496380, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497008, 1697548497077, 1697548497141, 1697548497205, 1697548497264, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497791, 1697548497837, 1697548498237, 1697548498298]"
1898,1898,550,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548487237,1697548488027,120,,,"[20, 551]","[1697548487257, 1697548487808]"
1899,1899,567,31,[],200,llama-7b,64,1,2554.0,1.0,1,A100,1697548508349,1697548510903,120,90.0,20.0,"[8, 323, 68, 57, 54, 50, 40, 41, 578, 57, 54, 53, 52, 50, 560, 59, 59, 46, 58, 56, 231]","[1697548508357, 1697548508680, 1697548508748, 1697548508805, 1697548508859, 1697548508909, 1697548508949, 1697548508990, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509784, 1697548509834, 1697548510394, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510672, 1697548510903]"
1900,1900,693,34,[],200,llama-7b,64,1,1434.0,1.0,1,A100,1697548503134,1697548504568,120,67.0,2.0,"[20, 1159, 254]","[1697548503154, 1697548504313, 1697548504567]"
1901,1901,353,35,[],200,llama-7b,64,1,809.0,1.0,1,A100,1697548504570,1697548505379,120,52.0,4.0,"[12, 611, 69, 66, 51]","[1697548504582, 1697548505193, 1697548505262, 1697548505328, 1697548505379]"
1902,1902,749,16,[],200,llama-7b,64,1,3322.0,1.0,1,A100,1697548498300,1697548501622,120,47.0,20.0,"[11, 427, 64, 59, 51, 50, 826, 61, 61, 60, 54, 52, 421, 65, 51, 50, 57, 54, 724, 63, 61]","[1697548498311, 1697548498738, 1697548498802, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499910, 1697548499970, 1697548500024, 1697548500076, 1697548500497, 1697548500562, 1697548500613, 1697548500663, 1697548500720, 1697548500774, 1697548501498, 1697548501561, 1697548501622]"
1903,1903,123,36,[],200,llama-7b,64,1,400.0,1.0,1,A100,1697548505380,1697548505780,120,14.0,1.0,"[16, 383]","[1697548505396, 1697548505779]"
1904,1904,214,22,[],200,llama-7b,64,1,2182.0,1.0,1,A100,1697548488033,1697548490215,120,52.0,20.0,"[118, 681, 57, 51, 35, 37, 286, 67, 50, 48, 41, 48, 40, 304, 51, 43, 43, 50, 49, 40, 43]","[1697548488151, 1697548488832, 1697548488889, 1697548488940, 1697548488975, 1697548489012, 1697548489298, 1697548489365, 1697548489415, 1697548489463, 1697548489504, 1697548489552, 1697548489592, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490215]"
1905,1905,101,31,[],200,llama-7b,64,1,525.0,1.0,1,A100,1697548501632,1697548502157,120,13.0,1.0,"[20, 504]","[1697548501652, 1697548502156]"
1906,1906,799,32,[],200,llama-7b,64,1,3401.0,1.0,1,A100,1697548502157,1697548505558,120,84.0,20.0,"[8, 964, 462, 63, 58, 57, 52, 42, 705, 61, 48, 60, 59, 58, 57, 351, 65, 50, 62, 60, 59]","[1697548502165, 1697548503129, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505377, 1697548505439, 1697548505499, 1697548505558]"
1907,1907,711,37,[],200,llama-7b,64,1,1014.0,1.0,1,A100,1697548505782,1697548506796,120,457.0,4.0,"[15, 603, 272, 66, 58]","[1697548505797, 1697548506400, 1697548506672, 1697548506738, 1697548506796]"
1908,1908,912,23,[],200,llama-7b,64,1,2901.0,1.0,1,A100,1697548490218,1697548493119,120,92.0,20.0,"[7, 501, 55, 50, 48, 42, 554, 50, 48, 40, 49, 49, 327, 54, 53, 43, 51, 50, 49, 723, 58]","[1697548490225, 1697548490726, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491711, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492289, 1697548492338, 1697548493061, 1697548493119]"
1909,1909,474,27,[],200,llama-7b,64,1,4713.0,1.0,1,A100,1697548496007,1697548500720,120,109.0,33.0,"[32, 968, 69, 64, 61, 62, 57, 56, 261, 61, 48, 46, 45, 400, 62, 51, 51, 50, 351, 59, 51, 49, 827, 61, 61, 60, 54, 51, 422, 64, 51, 49, 58]","[1697548496039, 1697548497007, 1697548497076, 1697548497140, 1697548497201, 1697548497263, 1697548497320, 1697548497376, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499788, 1697548499849, 1697548499910, 1697548499970, 1697548500024, 1697548500075, 1697548500497, 1697548500561, 1697548500612, 1697548500661, 1697548500719]"
1910,1910,404,17,[],200,llama-7b,64,1,3284.0,1.0,1,A100,1697548501627,1697548504911,120,87.0,20.0,"[20, 509, 254, 66, 63, 62, 60, 54, 876, 64, 57, 57, 52, 42, 705, 61, 48, 60, 59, 58, 57]","[1697548501647, 1697548502156, 1697548502410, 1697548502476, 1697548502539, 1697548502601, 1697548502661, 1697548502715, 1697548503591, 1697548503655, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911]"
1911,1911,431,33,[],200,llama-7b,64,1,3297.0,1.0,1,A100,1697548505562,1697548508859,120,732.0,22.0,"[24, 814, 272, 66, 58, 55, 44, 580, 65, 62, 58, 57, 55, 325, 65, 65, 60, 59, 54, 42, 306, 56, 55]","[1697548505586, 1697548506400, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507475, 1697548507540, 1697548507602, 1697548507660, 1697548507717, 1697548507772, 1697548508097, 1697548508162, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859]"
1912,1912,585,25,[],200,llama-7b,64,1,5722.0,1.0,1,A100,1697548550474,1697548556196,120,244.0,50.0,"[14, 452, 61, 53, 51, 51, 48, 425, 64, 56, 45, 44, 55, 55, 712, 55, 65, 50, 50, 64, 65, 274, 48, 48, 60, 60, 348, 59, 46, 59, 860, 250, 254, 58, 44, 45, 45, 58, 58, 43, 50, 69, 46, 37, 45, 42, 41, 39, 39, 35, 27]","[1697548550488, 1697548550940, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551204, 1697548551629, 1697548551693, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552715, 1697548552780, 1697548552830, 1697548552880, 1697548552944, 1697548553009, 1697548553283, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554871, 1697548555121, 1697548555375, 1697548555433, 1697548555477, 1697548555522, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556169, 1697548556196]"
1913,1913,480,38,[],200,llama-7b,64,1,482.0,1.0,1,A100,1697548506801,1697548507283,120,26.0,1.0,"[6, 475]","[1697548506807, 1697548507282]"
1914,1914,141,39,[],200,llama-7b,64,1,2551.0,1.0,1,A100,1697548507284,1697548509835,120,89.0,20.0,"[5, 737, 71, 65, 64, 61, 58, 56, 41, 306, 56, 55, 50, 40, 40, 580, 56, 54, 53, 53, 50]","[1697548507289, 1697548508026, 1697548508097, 1697548508162, 1697548508226, 1697548508287, 1697548508345, 1697548508401, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509569, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835]"
1915,1915,887,18,[],200,llama-7b,64,1,8313.0,1.0,1,A100,1697548514167,1697548522480,120,244.0,50.0,"[6, 685, 467, 71, 70, 64, 60, 977, 269, 70, 64, 65, 63, 295, 66, 64, 58, 53, 672, 67, 64, 59, 57, 362, 62, 50, 58, 57, 45, 607, 243, 67, 62, 62, 62, 60, 60, 336, 66, 63, 48, 49, 54, 405, 62, 48, 60, 46, 54, 709, 70]","[1697548514173, 1697548514858, 1697548515325, 1697548515396, 1697548515466, 1697548515530, 1697548515590, 1697548516567, 1697548516836, 1697548516906, 1697548516970, 1697548517035, 1697548517098, 1697548517393, 1697548517459, 1697548517523, 1697548517581, 1697548517634, 1697548518306, 1697548518373, 1697548518437, 1697548518496, 1697548518553, 1697548518915, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520037, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480]"
1916,1916,642,20,[],200,llama-7b,64,1,3014.0,1.0,1,A100,1697548468321,1697548471335,120,89.0,20.0,"[15, 335, 246, 66, 51, 67, 65, 61, 60, 568, 69, 53, 67, 51, 52, 65, 64, 846, 72, 71, 70]","[1697548468336, 1697548468671, 1697548468917, 1697548468983, 1697548469034, 1697548469101, 1697548469166, 1697548469227, 1697548469287, 1697548469855, 1697548469924, 1697548469977, 1697548470044, 1697548470095, 1697548470147, 1697548470212, 1697548470276, 1697548471122, 1697548471194, 1697548471265, 1697548471335]"
1917,1917,838,40,[],200,llama-7b,64,1,4393.0,1.0,1,A100,1697548509837,1697548514230,120,90.0,20.0,"[10, 992, 64, 57, 42, 43, 51, 51, 654, 58, 54, 52, 51, 754, 351, 57, 55, 44, 53, 834, 66]","[1697548509847, 1697548510839, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511801, 1697548511859, 1697548511913, 1697548511965, 1697548512016, 1697548512770, 1697548513121, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514164, 1697548514230]"
1918,1918,414,21,[],200,llama-7b,64,1,2330.0,1.0,1,A100,1697548471337,1697548473667,120,87.0,20.0,"[13, 401, 78, 74, 73, 72, 67, 63, 49, 304, 71, 68, 68, 53, 65, 49, 530, 67, 49, 51, 64]","[1697548471350, 1697548471751, 1697548471829, 1697548471903, 1697548471976, 1697548472048, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472791, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473602, 1697548473666]"
1919,1919,68,22,[],200,llama-7b,64,1,328.0,1.0,1,A100,1697548473670,1697548473998,120,12.0,1.0,"[17, 311]","[1697548473687, 1697548473998]"
1920,1920,772,23,[],200,llama-7b,64,1,4627.0,1.0,1,A100,1697548474000,1697548478627,120,83.0,20.0,"[6, 754, 283, 73, 71, 69, 70, 67, 63, 861, 512, 288, 74, 57, 64, 617, 252, 54, 253, 71, 68]","[1697548474006, 1697548474760, 1697548475043, 1697548475116, 1697548475187, 1697548475256, 1697548475326, 1697548475393, 1697548475456, 1697548476317, 1697548476829, 1697548477117, 1697548477191, 1697548477248, 1697548477312, 1697548477929, 1697548478181, 1697548478235, 1697548478488, 1697548478559, 1697548478627]"
1921,1921,524,59,[],200,llama-7b,64,1,3714.0,1.0,1,A100,1697548549166,1697548552880,120,100.0,30.0,"[13, 520, 81, 59, 48, 47, 48, 59, 56, 312, 63, 56, 49, 53, 371, 53, 51, 51, 48, 426, 62, 57, 44, 45, 55, 55, 712, 54, 65, 51, 50]","[1697548549179, 1697548549699, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472, 1697548550528, 1697548550577, 1697548550630, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551204, 1697548551630, 1697548551692, 1697548551749, 1697548551793, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552714, 1697548552779, 1697548552830, 1697548552880]"
1922,1922,492,41,[],200,llama-7b,64,1,4204.0,1.0,1,A100,1697548514233,1697548518437,120,47.0,20.0,"[10, 615, 467, 71, 70, 64, 60, 977, 270, 69, 64, 66, 62, 295, 66, 64, 58, 53, 673, 66, 64]","[1697548514243, 1697548514858, 1697548515325, 1697548515396, 1697548515466, 1697548515530, 1697548515590, 1697548516567, 1697548516837, 1697548516906, 1697548516970, 1697548517036, 1697548517098, 1697548517393, 1697548517459, 1697548517523, 1697548517581, 1697548517634, 1697548518307, 1697548518373, 1697548518437]"
1923,1923,239,42,[],200,llama-7b,64,1,3262.0,1.0,1,A100,1697548518440,1697548521702,120,39.0,27.0,"[26, 380, 69, 63, 50, 58, 56, 45, 607, 242, 69, 62, 62, 61, 60, 61, 336, 65, 63, 49, 50, 52, 405, 62, 48, 60, 46, 54]","[1697548518466, 1697548518846, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520105, 1697548520167, 1697548520229, 1697548520290, 1697548520350, 1697548520411, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520974, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701]"
1924,1924,769,43,[],200,llama-7b,64,1,4103.0,1.0,1,A100,1697548540783,1697548544886,120,47.0,20.0,"[13, 794, 337, 60, 59, 51, 651, 61, 61, 57, 44, 55, 694, 58, 47, 58, 54, 52, 788, 55, 54]","[1697548540796, 1697548541590, 1697548541927, 1697548541987, 1697548542046, 1697548542097, 1697548542748, 1697548542809, 1697548542870, 1697548542927, 1697548542971, 1697548543026, 1697548543720, 1697548543778, 1697548543825, 1697548543883, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544886]"
1925,1925,513,28,[],200,llama-7b,64,1,3052.0,1.0,1,A100,1697548508750,1697548511802,120,83.0,20.0,"[7, 682, 129, 57, 54, 53, 53, 50, 559, 59, 59, 46, 59, 55, 231, 57, 42, 43, 51, 51, 655]","[1697548508757, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835, 1697548510394, 1697548510453, 1697548510512, 1697548510558, 1697548510617, 1697548510672, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511802]"
1926,1926,539,44,[],200,llama-7b,64,1,3256.0,1.0,1,A100,1697548544889,1697548548145,120,83.0,20.0,"[23, 603, 369, 71, 65, 64, 63, 61, 48, 62, 759, 67, 62, 60, 48, 58, 45, 257, 53, 47, 371]","[1697548544912, 1697548545515, 1697548545884, 1697548545955, 1697548546020, 1697548546084, 1697548546147, 1697548546208, 1697548546256, 1697548546318, 1697548547077, 1697548547144, 1697548547206, 1697548547266, 1697548547314, 1697548547372, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145]"
1927,1927,84,19,[],200,llama-7b,64,1,469.0,1.0,1,A100,1697548489898,1697548490367,120,26.0,1.0,"[6, 463]","[1697548489904, 1697548490367]"
1928,1928,760,20,[],200,llama-7b,64,1,7327.0,1.0,1,A100,1697548490371,1697548497698,120,335.0,64.0,"[19, 337, 55, 49, 49, 41, 553, 51, 48, 40, 48, 49, 329, 54, 52, 43, 50, 51, 49, 724, 59, 47, 48, 39, 356, 48, 38, 38, 312, 52, 41, 46, 432, 60, 47, 60, 55, 54, 52, 297, 49, 58, 58, 55, 53, 42, 233, 55, 53, 51, 52, 378, 41, 40, 50, 50, 446, 69, 63, 61, 63, 57, 55, 262, 60]","[1697548490390, 1697548490727, 1697548490782, 1697548490831, 1697548490880, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492039, 1697548492093, 1697548492145, 1697548492188, 1697548492238, 1697548492289, 1697548492338, 1697548493062, 1697548493121, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493659, 1697548493697, 1697548493735, 1697548494047, 1697548494099, 1697548494140, 1697548494186, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494840, 1697548494894, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495408, 1697548495463, 1697548495516, 1697548495558, 1697548495791, 1697548495846, 1697548495899, 1697548495950, 1697548496002, 1697548496380, 1697548496421, 1697548496461, 1697548496511, 1697548496561, 1697548497007, 1697548497076, 1697548497139, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697]"
1929,1929,635,18,[],200,llama-7b,64,1,659.0,1.0,1,A100,1697548463897,1697548464556,120,23.0,1.0,"[18, 640]","[1697548463915, 1697548464555]"
1930,1930,471,32,[],200,llama-7b,64,1,2939.0,1.0,1,A100,1697548522551,1697548525490,120,86.0,20.0,"[16, 463, 73, 67, 64, 64, 62, 62, 316, 52, 67, 51, 64, 63, 555, 64, 65, 65, 60, 57, 589]","[1697548522567, 1697548523030, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524035, 1697548524590, 1697548524654, 1697548524719, 1697548524784, 1697548524844, 1697548524901, 1697548525490]"
1931,1931,410,19,[],200,llama-7b,64,1,2952.0,1.0,1,A100,1697548464558,1697548467510,120,364.0,12.0,"[23, 1321, 336, 73, 69, 53, 65, 63, 48, 689, 73, 71, 68]","[1697548464581, 1697548465902, 1697548466238, 1697548466311, 1697548466380, 1697548466433, 1697548466498, 1697548466561, 1697548466609, 1697548467298, 1697548467371, 1697548467442, 1697548467510]"
1932,1932,290,29,[],200,llama-7b,64,1,653.0,1.0,1,A100,1697548511806,1697548512459,120,14.0,1.0,"[14, 639]","[1697548511820, 1697548512459]"
1933,1933,63,20,[],200,llama-7b,64,1,457.0,1.0,1,A100,1697548467515,1697548467972,120,39.0,1.0,"[11, 446]","[1697548467526, 1697548467972]"
1934,1934,7,43,[],200,llama-7b,64,1,2205.0,1.0,1,A100,1697548521704,1697548523909,120,345.0,11.0,"[6, 1320, 73, 68, 63, 64, 62, 62, 317, 51, 68, 51]","[1697548521710, 1697548523030, 1697548523103, 1697548523171, 1697548523234, 1697548523298, 1697548523360, 1697548523422, 1697548523739, 1697548523790, 1697548523858, 1697548523909]"
1935,1935,878,30,[],200,llama-7b,64,1,5063.0,1.0,1,A100,1697548512460,1697548517523,120,83.0,20.0,"[21, 1301, 382, 66, 60, 58, 58, 918, 71, 72, 63, 60, 977, 270, 69, 65, 64, 64, 293, 67, 64]","[1697548512481, 1697548513782, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406, 1697548515324, 1697548515395, 1697548515467, 1697548515530, 1697548515590, 1697548516567, 1697548516837, 1697548516906, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517459, 1697548517523]"
1936,1936,114,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526032,1697548527819,120,,,"[9, 669, 254, 65, 50, 64, 66, 63]","[1697548526041, 1697548526710, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527209, 1697548527272]"
1937,1937,769,21,[],200,llama-7b,64,1,3360.0,1.0,1,A100,1697548467975,1697548471335,120,47.0,20.0,"[6, 690, 246, 66, 51, 67, 64, 62, 60, 568, 69, 52, 68, 51, 51, 66, 64, 846, 72, 71, 70]","[1697548467981, 1697548468671, 1697548468917, 1697548468983, 1697548469034, 1697548469101, 1697548469165, 1697548469227, 1697548469287, 1697548469855, 1697548469924, 1697548469976, 1697548470044, 1697548470095, 1697548470146, 1697548470212, 1697548470276, 1697548471122, 1697548471194, 1697548471265, 1697548471335]"
1938,1938,247,26,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548504570,1697548527817,120,,,"[23, 600, 69, 66, 51, 60, 60, 60, 283, 46, 54, 52, 679, 65, 58, 55, 43, 581, 64, 62, 59, 56, 57, 324, 65, 64, 60, 59, 55, 42, 305, 57, 54, 50, 41, 40, 579, 57, 54, 52, 53, 50, 559, 59, 59, 46, 59, 56, 230, 57, 42, 43, 52, 51, 654, 57, 54, 53, 51, 753, 350, 58, 56, 43, 53, 834, 66, 60, 58, 58, 919, 71, 70, 64, 61, 976, 271, 68, 65, 65, 63, 294, 66, 64, 58, 54, 672, 66, 64, 59, 58, 361, 63, 50, 58, 57, 45, 607, 242, 68, 62, 62, 62, 60, 60, 336, 66, 63, 48, 49, 53, 405, 63, 47, 61, 46, 54, 709, 70, 67, 67, 57, 66, 49, 317, 67, 64, 64, 62, 61, 317, 52, 67, 51, 63, 64, 554, 64, 66, 65, 59, 57, 588, 68, 65, 51, 50, 65, 66, 242, 48, 47, 60, 59, 654, 66, 49, 65, 64, 64]","[1697548504593, 1697548505193, 1697548505262, 1697548505328, 1697548505379, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506673, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508747, 1697548508804, 1697548508858, 1697548508908, 1697548508949, 1697548508989, 1697548509568, 1697548509625, 1697548509679, 1697548509731, 1697548509784, 1697548509834, 1697548510393, 1697548510452, 1697548510511, 1697548510557, 1697548510616, 1697548510672, 1697548510902, 1697548510959, 1697548511001, 1697548511044, 1697548511096, 1697548511147, 1697548511801, 1697548511858, 1697548511912, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513177, 1697548513233, 1697548513276, 1697548513329, 1697548514163, 1697548514229, 1697548514289, 1697548514347, 1697548514405, 1697548515324, 1697548515395, 1697548515465, 1697548515529, 1697548515590, 1697548516566, 1697548516837, 1697548516905, 1697548516970, 1697548517035, 1697548517098, 1697548517392, 1697548517458, 1697548517522, 1697548517580, 1697548517634, 1697548518306, 1697548518372, 1697548518436, 1697548518495, 1697548518553, 1697548518914, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521025, 1697548521430, 1697548521493, 1697548521540, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522786, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523971, 1697548524035, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524900, 1697548525488, 1697548525556, 1697548525621, 1697548525672, 1697548525722, 1697548525787, 1697548525853, 1697548526095, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526963, 1697548527029, 1697548527078, 1697548527143, 1697548527207, 1697548527271]"
1939,1939,99,29,[],200,llama-7b,64,1,563.0,1.0,1,A100,1697548498176,1697548498739,120,10.0,1.0,"[23, 540]","[1697548498199, 1697548498739]"
1940,1940,241,33,[],200,llama-7b,64,1,536.0,1.0,1,A100,1697548525493,1697548526029,120,19.0,1.0,"[26, 510]","[1697548525519, 1697548526029]"
1941,1941,831,34,[],200,llama-7b,64,1,678.0,1.0,1,A100,1697548526032,1697548526710,120,11.0,1.0,"[14, 664]","[1697548526046, 1697548526710]"
1942,1942,647,31,[],200,llama-7b,64,1,2885.0,1.0,1,A100,1697548517526,1697548520411,120,83.0,20.0,"[25, 514, 242, 66, 64, 59, 58, 361, 63, 49, 58, 57, 45, 607, 242, 68, 62, 62, 62, 60, 61]","[1697548517551, 1697548518065, 1697548518307, 1697548518373, 1697548518437, 1697548518496, 1697548518554, 1697548518915, 1697548518978, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520411]"
1943,1943,682,30,[],200,llama-7b,64,1,7995.0,1.0,1,A100,1697548498742,1697548506737,120,244.0,50.0,"[24, 653, 369, 61, 62, 59, 55, 51, 420, 65, 51, 51, 56, 55, 724, 63, 61, 58, 55, 55, 620, 65, 64, 62, 59, 55, 876, 63, 58, 57, 52, 42, 704, 62, 47, 60, 59, 58, 57, 352, 65, 51, 61, 60, 59, 284, 45, 55, 51, 679, 65]","[1697548498766, 1697548499419, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500496, 1697548500561, 1697548500612, 1697548500663, 1697548500719, 1697548500774, 1697548501498, 1697548501561, 1697548501622, 1697548501680, 1697548501735, 1697548501790, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504853, 1697548504910, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505558, 1697548505842, 1697548505887, 1697548505942, 1697548505993, 1697548506672, 1697548506737]"
1944,1944,599,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526713,1697548527821,120,,,"[25, 839]","[1697548526738, 1697548527577]"
1945,1945,236,42,[],200,llama-7b,64,1,387.0,1.0,1,A100,1697548520977,1697548521364,120,8.0,1.0,"[39, 348]","[1697548521016, 1697548521364]"
1946,1946,346,36,[],200,llama-7b,64,1,2623.0,1.0,1,A100,1697548527826,1697548530449,120,85.0,20.0,"[211, 859, 59, 51, 51, 47, 48, 39, 364, 52, 49, 41, 41, 42, 49, 346, 56, 48, 60, 58, 51]","[1697548528037, 1697548528896, 1697548528955, 1697548529006, 1697548529057, 1697548529104, 1697548529152, 1697548529191, 1697548529555, 1697548529607, 1697548529656, 1697548529697, 1697548529738, 1697548529780, 1697548529829, 1697548530175, 1697548530231, 1697548530279, 1697548530339, 1697548530397, 1697548530448]"
1947,1947,425,22,[],200,llama-7b,64,1,2331.0,1.0,1,A100,1697548471336,1697548473667,120,88.0,20.0,"[12, 403, 78, 73, 74, 72, 67, 63, 49, 304, 71, 68, 68, 53, 65, 49, 530, 67, 49, 50, 65]","[1697548471348, 1697548471751, 1697548471829, 1697548471902, 1697548471976, 1697548472048, 1697548472115, 1697548472178, 1697548472227, 1697548472531, 1697548472602, 1697548472670, 1697548472738, 1697548472791, 1697548472856, 1697548472905, 1697548473435, 1697548473502, 1697548473551, 1697548473601, 1697548473666]"
1948,1948,197,23,[],200,llama-7b,64,1,781.0,1.0,1,A100,1697548473669,1697548474450,120,6.0,8.0,"[6, 322, 74, 68, 67, 51, 66, 64, 63]","[1697548473675, 1697548473997, 1697548474071, 1697548474139, 1697548474206, 1697548474257, 1697548474323, 1697548474387, 1697548474450]"
1949,1949,787,24,[],200,llama-7b,64,1,2796.0,1.0,1,A100,1697548474453,1697548477249,120,123.0,6.0,"[15, 1368, 481, 512, 289, 74, 57]","[1697548474468, 1697548475836, 1697548476317, 1697548476829, 1697548477118, 1697548477192, 1697548477249]"
1950,1950,555,25,[],200,llama-7b,64,1,424.0,1.0,1,A100,1697548477255,1697548477679,120,11.0,1.0,"[29, 395]","[1697548477284, 1697548477679]"
1951,1951,302,26,[],200,llama-7b,64,1,4800.0,1.0,1,A100,1697548477683,1697548482483,120,85.0,20.0,"[13, 1395, 211, 255, 71, 55, 70, 68, 63, 911, 77, 75, 72, 70, 67, 68, 588, 269, 63, 74, 264]","[1697548477696, 1697548479091, 1697548479302, 1697548479557, 1697548479628, 1697548479683, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480872, 1697548480947, 1697548481019, 1697548481089, 1697548481156, 1697548481224, 1697548481812, 1697548482081, 1697548482144, 1697548482218, 1697548482482]"
1952,1952,308,32,[],200,llama-7b,64,1,3009.0,1.0,1,A100,1697548520413,1697548523422,120,87.0,20.0,"[6, 944, 68, 62, 48, 60, 46, 54, 710, 70, 66, 67, 57, 66, 50, 317, 66, 64, 64, 62, 62]","[1697548520419, 1697548521363, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522481, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422]"
1953,1953,826,43,[],200,llama-7b,64,1,2669.0,1.0,1,A100,1697548521367,1697548524036,120,87.0,20.0,"[25, 753, 266, 70, 67, 67, 56, 66, 50, 317, 66, 64, 64, 62, 61, 317, 52, 67, 51, 64, 64]","[1697548521392, 1697548522145, 1697548522411, 1697548522481, 1697548522548, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524036]"
1954,1954,75,33,[],200,llama-7b,64,1,2827.0,1.0,1,A100,1697548523424,1697548526251,120,345.0,18.0,"[15, 913, 237, 64, 66, 65, 59, 58, 587, 68, 65, 51, 51, 66, 64, 242, 48, 48, 60]","[1697548523439, 1697548524352, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525488, 1697548525556, 1697548525621, 1697548525672, 1697548525723, 1697548525789, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251]"
1955,1955,572,24,[],200,llama-7b,64,1,429.0,1.0,1,A100,1697548493122,1697548493551,120,16.0,1.0,"[7, 421]","[1697548493129, 1697548493550]"
1956,1956,341,25,[],200,llama-7b,64,1,2235.0,1.0,1,A100,1697548493556,1697548495791,120,87.0,20.0,"[16, 415, 60, 52, 41, 47, 432, 60, 46, 61, 55, 55, 51, 297, 48, 59, 59, 53, 53, 42, 233]","[1697548493572, 1697548493987, 1697548494047, 1697548494099, 1697548494140, 1697548494187, 1697548494619, 1697548494679, 1697548494725, 1697548494786, 1697548494841, 1697548494896, 1697548494947, 1697548495244, 1697548495292, 1697548495351, 1697548495410, 1697548495463, 1697548495516, 1697548495558, 1697548495791]"
1957,1957,86,48,[],200,llama-7b,64,1,2751.0,1.0,1,A100,1697548537864,1697548540615,120,335.0,17.0,"[94, 872, 54, 55, 42, 46, 46, 677, 61, 56, 53, 44, 52, 42, 400, 50, 59, 47]","[1697548537958, 1697548538830, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539073, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614]"
1958,1958,33,16,[],200,llama-7b,64,1,1189.0,1.0,1,A100,1697548486117,1697548487306,120,140.0,7.0,"[16, 515, 82, 59, 60, 78, 307, 72]","[1697548486133, 1697548486648, 1697548486730, 1697548486789, 1697548486849, 1697548486927, 1697548487234, 1697548487306]"
1959,1959,672,49,[],200,llama-7b,64,1,4159.0,1.0,1,A100,1697548540617,1697548544776,120,93.0,20.0,"[15, 443, 54, 48, 750, 59, 59, 52, 650, 63, 61, 56, 45, 55, 692, 59, 47, 57, 55, 52, 787]","[1697548540632, 1697548541075, 1697548541129, 1697548541177, 1697548541927, 1697548541986, 1697548542045, 1697548542097, 1697548542747, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776]"
1960,1960,732,17,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548487309,1697548488027,120,,,"[6, 493]","[1697548487315, 1697548487808]"
1961,1961,820,32,[],200,llama-7b,64,1,1770.0,1.0,1,A100,1697548499791,1697548501561,120,161.0,9.0,"[7, 627, 72, 65, 51, 48, 59, 54, 723, 64]","[1697548499798, 1697548500425, 1697548500497, 1697548500562, 1697548500613, 1697548500661, 1697548500720, 1697548500774, 1697548501497, 1697548501561]"
1962,1962,193,15,[],200,llama-7b,64,1,4067.0,1.0,1,A100,1697548500500,1697548504567,120,79.0,20.0,"[6, 992, 63, 60, 58, 56, 55, 621, 65, 63, 62, 59, 54, 878, 62, 58, 57, 53, 41, 704]","[1697548500506, 1697548501498, 1697548501561, 1697548501621, 1697548501679, 1697548501735, 1697548501790, 1697548502411, 1697548502476, 1697548502539, 1697548502601, 1697548502660, 1697548502714, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567]"
1963,1963,567,33,[],200,llama-7b,64,1,3348.0,1.0,1,A100,1697548501563,1697548504911,120,90.0,20.0,"[6, 588, 253, 66, 63, 62, 60, 54, 875, 64, 57, 58, 52, 42, 705, 61, 48, 60, 59, 58, 57]","[1697548501569, 1697548502157, 1697548502410, 1697548502476, 1697548502539, 1697548502601, 1697548502661, 1697548502715, 1697548503590, 1697548503654, 1697548503711, 1697548503769, 1697548503821, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911]"
1964,1964,841,17,[],200,llama-7b,64,1,2593.0,1.0,1,A100,1697548478631,1697548481224,120,123.0,15.0,"[23, 648, 255, 71, 55, 71, 67, 63, 911, 76, 75, 72, 71, 68, 67]","[1697548478654, 1697548479302, 1697548479557, 1697548479628, 1697548479683, 1697548479754, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224]"
1965,1965,221,34,[],200,llama-7b,64,1,5479.0,1.0,1,A100,1697548504914,1697548510393,120,364.0,36.0,"[12, 853, 63, 45, 55, 52, 678, 66, 58, 55, 43, 581, 64, 62, 60, 55, 57, 324, 65, 64, 61, 58, 55, 42, 305, 57, 55, 49, 41, 40, 580, 56, 54, 52, 53, 50, 559]","[1697548504926, 1697548505779, 1697548505842, 1697548505887, 1697548505942, 1697548505994, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507661, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508287, 1697548508345, 1697548508400, 1697548508442, 1697548508747, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989, 1697548509569, 1697548509625, 1697548509679, 1697548509731, 1697548509784, 1697548509834, 1697548510393]"
1966,1966,634,32,[],200,llama-7b,64,1,1524.0,1.0,1,A100,1697548513334,1697548514858,120,13.0,1.0,"[18, 1505]","[1697548513352, 1697548514857]"
1967,1967,410,33,[],200,llama-7b,64,1,2776.0,1.0,1,A100,1697548514859,1697548517635,120,364.0,12.0,"[13, 1197, 498, 270, 68, 66, 64, 64, 293, 66, 64, 59, 54]","[1697548514872, 1697548516069, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517635]"
1968,1968,62,34,[],200,llama-7b,64,1,3336.0,1.0,1,A100,1697548517637,1697548520973,120,91.0,20.0,"[15, 1194, 69, 63, 49, 58, 57, 45, 607, 242, 68, 62, 63, 62, 59, 61, 335, 66, 63, 49, 49]","[1697548517652, 1697548518846, 1697548518915, 1697548518978, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520229, 1697548520291, 1697548520350, 1697548520411, 1697548520746, 1697548520812, 1697548520875, 1697548520924, 1697548520973]"
1969,1969,589,18,[],200,llama-7b,64,1,5039.0,1.0,1,A100,1697548481226,1697548486265,120,92.0,20.0,"[10, 1722, 551, 75, 55, 72, 66, 66, 49, 338, 77, 76, 74, 69, 58, 70, 838, 314, 308, 76, 75]","[1697548481236, 1697548482958, 1697548483509, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484307, 1697548484383, 1697548484457, 1697548484526, 1697548484584, 1697548484654, 1697548485492, 1697548485806, 1697548486114, 1697548486190, 1697548486265]"
1970,1970,414,21,[],200,llama-7b,64,1,2347.0,1.0,1,A100,1697548496515,1697548498862,120,87.0,20.0,"[10, 406, 77, 69, 63, 64, 59, 58, 54, 262, 61, 48, 46, 45, 400, 62, 51, 52, 49, 351, 60]","[1697548496525, 1697548496931, 1697548497008, 1697548497077, 1697548497140, 1697548497204, 1697548497263, 1697548497321, 1697548497375, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498862]"
1971,1971,368,38,[],200,llama-7b,64,1,2913.0,1.0,1,A100,1697548537866,1697548540779,120,88.0,20.0,"[87, 877, 54, 55, 42, 46, 45, 678, 61, 56, 53, 44, 52, 42, 400, 50, 59, 47, 57, 57, 51]","[1697548537953, 1697548538830, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539072, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779]"
1972,1972,385,19,[],200,llama-7b,64,1,5222.0,1.0,1,A100,1697548488033,1697548493255,120,52.0,43.0,"[124, 675, 57, 51, 35, 37, 286, 67, 50, 48, 41, 48, 40, 304, 51, 43, 43, 51, 48, 41, 41, 192, 375, 49, 49, 42, 553, 50, 49, 40, 48, 48, 329, 54, 52, 43, 52, 50, 49, 722, 59, 48, 49, 39]","[1697548488157, 1697548488832, 1697548488889, 1697548488940, 1697548488975, 1697548489012, 1697548489298, 1697548489365, 1697548489415, 1697548489463, 1697548489504, 1697548489552, 1697548489592, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490214, 1697548490406, 1697548490781, 1697548490830, 1697548490879, 1697548490921, 1697548491474, 1697548491524, 1697548491573, 1697548491613, 1697548491661, 1697548491709, 1697548492038, 1697548492092, 1697548492144, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493060, 1697548493119, 1697548493167, 1697548493216, 1697548493255]"
1973,1973,244,19,[],200,llama-7b,64,1,380.0,1.0,1,A100,1697548486268,1697548486648,120,9.0,1.0,"[12, 367]","[1697548486280, 1697548486647]"
1974,1974,14,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548486651,1697548488025,120,,,"[17, 1140]","[1697548486668, 1697548487808]"
1975,1975,921,35,[],200,llama-7b,64,1,442.0,1.0,1,A100,1697548510397,1697548510839,120,31.0,1.0,"[6, 436]","[1697548510403, 1697548510839]"
1976,1976,603,21,[],200,llama-7b,64,1,800.0,1.0,1,A100,1697548488033,1697548488833,120,9.0,1.0,"[159, 641]","[1697548488192, 1697548488833]"
1977,1977,374,22,[],200,llama-7b,64,1,2045.0,1.0,1,A100,1697548488835,1697548490880,120,85.0,20.0,"[28, 378, 56, 68, 49, 49, 40, 49, 40, 303, 52, 43, 43, 50, 49, 40, 41, 194, 374, 50, 49]","[1697548488863, 1697548489241, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489503, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490213, 1697548490407, 1697548490781, 1697548490831, 1697548490880]"
1978,1978,582,36,[],200,llama-7b,64,1,715.0,1.0,1,A100,1697548510844,1697548511559,120,19.0,1.0,"[31, 684]","[1697548510875, 1697548511559]"
1979,1979,351,37,[],200,llama-7b,64,1,1716.0,1.0,1,A100,1697548511561,1697548513277,120,216.0,6.0,"[6, 892, 310, 350, 59, 55, 44]","[1697548511567, 1697548512459, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513277]"
1980,1980,531,43,[],200,llama-7b,64,1,3090.0,1.0,1,A100,1697548504571,1697548507661,120,52.0,20.0,"[32, 590, 69, 66, 51, 60, 60, 60, 283, 46, 54, 52, 679, 65, 58, 55, 43, 581, 65, 61, 60]","[1697548504603, 1697548505193, 1697548505262, 1697548505328, 1697548505379, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506673, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507540, 1697548507601, 1697548507661]"
1981,1981,483,24,[],200,llama-7b,64,1,2184.0,1.0,1,A100,1697548488031,1697548490215,120,84.0,20.0,"[60, 741, 57, 51, 36, 36, 286, 67, 50, 48, 41, 48, 40, 303, 52, 43, 43, 50, 49, 40, 43]","[1697548488091, 1697548488832, 1697548488889, 1697548488940, 1697548488976, 1697548489012, 1697548489298, 1697548489365, 1697548489415, 1697548489463, 1697548489504, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490215]"
1982,1982,886,27,[],200,llama-7b,64,1,472.0,1.0,1,A100,1697548482487,1697548482959,120,17.0,1.0,"[19, 452]","[1697548482506, 1697548482958]"
1983,1983,656,28,[],200,llama-7b,64,1,1186.0,1.0,1,A100,1697548482963,1697548484149,120,26.0,1.0,"[24, 1161]","[1697548482987, 1697548484148]"
1984,1984,318,29,[],200,llama-7b,64,1,2115.0,1.0,1,A100,1697548484151,1697548486266,120,6.0,6.0,"[7, 782, 552, 313, 308, 76, 77]","[1697548484158, 1697548484940, 1697548485492, 1697548485805, 1697548486113, 1697548486189, 1697548486266]"
1985,1985,156,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548534680,1697548537857,120,,,"[6, 490, 382, 60, 59, 57, 56, 667, 63, 48, 55, 315, 52, 60, 65, 64, 59, 48]","[1697548534686, 1697548535176, 1697548535558, 1697548535618, 1697548535677, 1697548535734, 1697548535790, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536938, 1697548536990, 1697548537050, 1697548537115, 1697548537179, 1697548537238, 1697548537286]"
1986,1986,83,30,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548486269,1697548488024,120,,,"[18, 361, 82, 59, 60, 79, 307, 71, 70]","[1697548486287, 1697548486648, 1697548486730, 1697548486789, 1697548486849, 1697548486928, 1697548487235, 1697548487306, 1697548487376]"
1987,1987,926,26,[],200,llama-7b,64,1,4117.0,1.0,1,A100,1697548495794,1697548499911,120,563.0,30.0,"[11, 520, 55, 41, 41, 50, 50, 446, 69, 64, 63, 60, 56, 55, 262, 60, 48, 46, 46, 400, 61, 52, 51, 50, 351, 59, 51, 49, 826, 62, 61]","[1697548495805, 1697548496325, 1697548496380, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497008, 1697548497077, 1697548497141, 1697548497204, 1697548497264, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497791, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499787, 1697548499849, 1697548499910]"
1988,1988,396,18,[],200,llama-7b,64,1,2847.0,1.0,1,A100,1697548488033,1697548490880,120,89.0,20.0,"[210, 997, 57, 68, 49, 49, 40, 49, 40, 304, 51, 43, 43, 51, 48, 40, 41, 194, 374, 49, 50]","[1697548488243, 1697548489240, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489503, 1697548489552, 1697548489592, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490132, 1697548490172, 1697548490213, 1697548490407, 1697548490781, 1697548490830, 1697548490880]"
1989,1989,672,31,[],200,llama-7b,64,1,2183.0,1.0,1,A100,1697548488031,1697548490214,120,93.0,20.0,"[45, 756, 57, 51, 35, 38, 284, 67, 50, 49, 40, 49, 40, 303, 52, 43, 43, 50, 49, 40, 42]","[1697548488076, 1697548488832, 1697548488889, 1697548488940, 1697548488975, 1697548489013, 1697548489297, 1697548489364, 1697548489414, 1697548489463, 1697548489503, 1697548489552, 1697548489592, 1697548489895, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490172, 1697548490214]"
1990,1990,739,48,[],200,llama-7b,64,1,310.0,1.0,1,A100,1697548537864,1697548538174,120,216.0,1.0,"[23, 287]","[1697548537887, 1697548538174]"
1991,1991,457,22,[],200,llama-7b,64,1,1128.0,1.0,1,A100,1697548527827,1697548528955,120,874.0,2.0,"[224, 904]","[1697548528051, 1697548528955]"
1992,1992,666,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526253,1697548527820,120,,,"[14, 443, 254, 66, 49, 65, 65, 63]","[1697548526267, 1697548526710, 1697548526964, 1697548527030, 1697548527079, 1697548527144, 1697548527209, 1697548527272]"
1993,1993,108,23,[],200,llama-7b,64,1,598.0,1.0,1,A100,1697548528958,1697548529556,120,182.0,2.0,"[13, 531, 54]","[1697548528971, 1697548529502, 1697548529556]"
1994,1994,814,24,[],200,llama-7b,64,1,3151.0,1.0,1,A100,1697548529559,1697548532710,120,89.0,20.0,"[10, 538, 69, 56, 48, 59, 60, 51, 486, 39, 472, 64, 64, 61, 60, 51, 737, 52, 59, 63, 51]","[1697548529569, 1697548530107, 1697548530176, 1697548530232, 1697548530280, 1697548530339, 1697548530399, 1697548530450, 1697548530936, 1697548530975, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531696, 1697548531747, 1697548532484, 1697548532536, 1697548532595, 1697548532658, 1697548532709]"
1995,1995,451,20,[],200,llama-7b,64,1,429.0,1.0,1,A100,1697548477251,1697548477680,120,286.0,1.0,"[26, 403]","[1697548477277, 1697548477680]"
1996,1996,482,49,[],200,llama-7b,64,1,2602.0,1.0,1,A100,1697548538177,1697548540779,120,91.0,20.0,"[16, 639, 52, 56, 41, 46, 46, 677, 61, 56, 53, 45, 51, 42, 401, 49, 59, 47, 58, 55, 52]","[1697548538193, 1697548538832, 1697548538884, 1697548538940, 1697548538981, 1697548539027, 1697548539073, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539965, 1697548540016, 1697548540058, 1697548540459, 1697548540508, 1697548540567, 1697548540614, 1697548540672, 1697548540727, 1697548540779]"
1997,1997,83,21,[],200,llama-7b,64,1,3541.0,1.0,1,A100,1697548477683,1697548481224,120,123.0,15.0,"[10, 1397, 212, 255, 71, 54, 71, 68, 63, 911, 77, 75, 71, 71, 67, 68]","[1697548477693, 1697548479090, 1697548479302, 1697548479557, 1697548479628, 1697548479682, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480872, 1697548480947, 1697548481018, 1697548481089, 1697548481156, 1697548481224]"
1998,1998,42,38,[],200,llama-7b,64,1,544.0,1.0,1,A100,1697548528959,1697548529503,120,10.0,1.0,"[9, 534]","[1697548528968, 1697548529502]"
1999,1999,435,35,[],200,llama-7b,64,1,3868.0,1.0,1,A100,1697548527827,1697548531695,120,563.0,27.0,"[75, 994, 59, 52, 50, 48, 47, 39, 365, 52, 49, 40, 42, 40, 50, 347, 55, 48, 60, 59, 51, 486, 39, 472, 64, 64, 62, 59]","[1697548527902, 1697548528896, 1697548528955, 1697548529007, 1697548529057, 1697548529105, 1697548529152, 1697548529191, 1697548529556, 1697548529608, 1697548529657, 1697548529697, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530279, 1697548530339, 1697548530398, 1697548530449, 1697548530935, 1697548530974, 1697548531446, 1697548531510, 1697548531574, 1697548531636, 1697548531695]"
2000,2000,482,29,[],200,llama-7b,64,1,2945.0,1.0,1,A100,1697548497080,1697548500025,120,91.0,20.0,"[13, 480, 64, 61, 48, 46, 45, 400, 62, 51, 52, 49, 351, 60, 50, 49, 827, 61, 62, 59, 55]","[1697548497093, 1697548497573, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498862, 1697548498912, 1697548498961, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025]"
2001,2001,627,39,[],200,llama-7b,64,1,3203.0,1.0,1,A100,1697548529506,1697548532709,120,93.0,20.0,"[14, 587, 69, 56, 47, 60, 59, 52, 485, 39, 473, 64, 64, 61, 59, 52, 737, 52, 58, 64, 51]","[1697548529520, 1697548530107, 1697548530176, 1697548530232, 1697548530279, 1697548530339, 1697548530398, 1697548530450, 1697548530935, 1697548530974, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531695, 1697548531747, 1697548532484, 1697548532536, 1697548532594, 1697548532658, 1697548532709]"
2002,2002,468,25,[],200,llama-7b,64,1,2846.0,1.0,1,A100,1697548532712,1697548535558,120,31.0,20.0,"[11, 427, 190, 64, 63, 50, 68, 53, 46, 311, 60, 45, 46, 59, 58, 367, 46, 60, 58, 57, 707]","[1697548532723, 1697548533150, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533585, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534676, 1697548534736, 1697548534794, 1697548534851, 1697548535558]"
2003,2003,203,34,[],200,llama-7b,64,1,1812.0,1.0,1,A100,1697548508861,1697548510673,120,364.0,13.0,"[15, 564, 128, 57, 54, 53, 54, 49, 558, 60, 58, 46, 59, 56]","[1697548508876, 1697548509440, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509786, 1697548509835, 1697548510393, 1697548510453, 1697548510511, 1697548510557, 1697548510616, 1697548510672]"
2004,2004,739,35,[],200,llama-7b,64,1,389.0,1.0,1,A100,1697548520975,1697548521364,120,216.0,1.0,"[16, 372]","[1697548520991, 1697548521363]"
2005,2005,392,36,[],200,llama-7b,64,1,777.0,1.0,1,A100,1697548521368,1697548522145,120,20.0,1.0,"[34, 743]","[1697548521402, 1697548522145]"
2006,2006,164,37,[],200,llama-7b,64,1,880.0,1.0,1,A100,1697548522150,1697548523030,120,15.0,1.0,"[18, 862]","[1697548522168, 1697548523030]"
2007,2007,91,36,[],200,llama-7b,64,1,421.0,1.0,1,A100,1697548531698,1697548532119,120,23.0,1.0,"[12, 408]","[1697548531710, 1697548532118]"
2008,2008,750,38,[],200,llama-7b,64,1,2821.0,1.0,1,A100,1697548523033,1697548525854,120,88.0,20.0,"[8, 624, 73, 52, 67, 52, 63, 64, 553, 64, 66, 65, 59, 58, 589, 67, 65, 51, 50, 65, 66]","[1697548523041, 1697548523665, 1697548523738, 1697548523790, 1697548523857, 1697548523909, 1697548523972, 1697548524036, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525490, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525854]"
2009,2009,764,37,[],200,llama-7b,64,1,1027.0,1.0,1,A100,1697548532122,1697548533149,120,39.0,1.0,"[20, 1007]","[1697548532142, 1697548533149]"
2010,2010,251,30,[],200,llama-7b,64,1,397.0,1.0,1,A100,1697548500028,1697548500425,120,31.0,1.0,"[15, 382]","[1697548500043, 1697548500425]"
2011,2011,29,39,[],200,llama-7b,64,1,1965.0,1.0,1,A100,1697548540783,1697548542748,120,161.0,6.0,"[25, 782, 336, 61, 59, 51, 651]","[1697548540808, 1697548541590, 1697548541926, 1697548541987, 1697548542046, 1697548542097, 1697548542748]"
2012,2012,534,38,[],200,llama-7b,64,1,3416.0,1.0,1,A100,1697548533152,1697548536568,120,96.0,20.0,"[12, 767, 64, 60, 46, 45, 59, 58, 367, 47, 59, 58, 57, 707, 59, 59, 58, 55, 668, 63, 48]","[1697548533164, 1697548533931, 1697548533995, 1697548534055, 1697548534101, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568]"
2013,2013,396,40,[],200,llama-7b,64,1,2846.0,1.0,1,A100,1697548532712,1697548535558,120,89.0,20.0,"[16, 421, 191, 64, 63, 50, 64, 58, 46, 310, 60, 46, 45, 59, 58, 367, 46, 60, 58, 57, 707]","[1697548532728, 1697548533149, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533581, 1697548533639, 1697548533685, 1697548533995, 1697548534055, 1697548534101, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534676, 1697548534736, 1697548534794, 1697548534851, 1697548535558]"
2014,2014,192,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536569,1697548537856,120,,,"[13, 284, 72, 52, 61, 65, 63, 60, 47]","[1697548536582, 1697548536866, 1697548536938, 1697548536990, 1697548537051, 1697548537116, 1697548537179, 1697548537239, 1697548537286]"
2015,2015,836,31,[],200,llama-7b,64,1,774.0,1.0,1,A100,1697548500429,1697548501203,120,11.0,1.0,"[9, 765]","[1697548500438, 1697548501203]"
2016,2016,57,41,[],200,llama-7b,64,1,710.0,1.0,1,A100,1697548535563,1697548536273,120,13.0,1.0,"[24, 686]","[1697548535587, 1697548536273]"
2017,2017,758,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536276,1697548537862,120,,,"[16, 574, 72, 51, 61, 66, 62, 61, 46]","[1697548536292, 1697548536866, 1697548536938, 1697548536989, 1697548537050, 1697548537116, 1697548537178, 1697548537239, 1697548537285]"
2018,2018,454,26,[],200,llama-7b,64,1,1036.0,1.0,1,A100,1697548508750,1697548509786,120,182.0,6.0,"[18, 671, 129, 57, 54, 53, 53]","[1697548508768, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785]"
2019,2019,605,32,[],200,llama-7b,64,1,952.0,1.0,1,A100,1697548501205,1697548502157,120,8.0,1.0,"[6, 945]","[1697548501211, 1697548502156]"
2020,2020,419,43,[],200,llama-7b,64,1,2910.0,1.0,1,A100,1697548537870,1697548540780,120,88.0,20.0,"[172, 789, 54, 55, 42, 44, 47, 677, 62, 55, 53, 45, 52, 41, 400, 50, 60, 46, 57, 57, 52]","[1697548538042, 1697548538831, 1697548538885, 1697548538940, 1697548538982, 1697548539026, 1697548539073, 1697548539750, 1697548539812, 1697548539867, 1697548539920, 1697548539965, 1697548540017, 1697548540058, 1697548540458, 1697548540508, 1697548540568, 1697548540614, 1697548540671, 1697548540728, 1697548540780]"
2021,2021,261,33,[],200,llama-7b,64,1,1432.0,1.0,1,A100,1697548502159,1697548503591,120,874.0,2.0,"[7, 1425]","[1697548502166, 1697548503591]"
2022,2022,234,27,[],200,llama-7b,64,1,4376.0,1.0,1,A100,1697548509788,1697548514164,120,457.0,25.0,"[15, 524, 66, 60, 59, 46, 59, 56, 230, 57, 43, 42, 51, 51, 654, 58, 54, 52, 51, 754, 350, 58, 55, 44, 53, 834]","[1697548509803, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510617, 1697548510673, 1697548510903, 1697548510960, 1697548511003, 1697548511045, 1697548511096, 1697548511147, 1697548511801, 1697548511859, 1697548511913, 1697548511965, 1697548512016, 1697548512770, 1697548513120, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514164]"
2023,2023,38,34,[],200,llama-7b,64,1,3144.0,1.0,1,A100,1697548503594,1697548506738,120,88.0,20.0,"[22, 696, 256, 61, 48, 60, 58, 59, 57, 351, 65, 51, 61, 60, 60, 283, 45, 55, 52, 678, 65]","[1697548503616, 1697548504312, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504795, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505887, 1697548505942, 1697548505994, 1697548506672, 1697548506737]"
2024,2024,762,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536868,1697548537856,120,,,"[6, 893]","[1697548536874, 1697548537767]"
2025,2025,815,28,[],200,llama-7b,64,1,1299.0,1.0,1,A100,1697548514167,1697548515466,120,52.0,4.0,"[11, 680, 467, 71, 70]","[1697548514178, 1697548514858, 1697548515325, 1697548515396, 1697548515466]"
2026,2026,794,30,[],200,llama-7b,64,1,1069.0,1.0,1,A100,1697548527827,1697548528896,120,11.0,1.0,"[70, 998]","[1697548527897, 1697548528895]"
2027,2027,570,31,[],200,llama-7b,64,1,603.0,1.0,1,A100,1697548528899,1697548529502,120,18.0,1.0,"[32, 571]","[1697548528931, 1697548529502]"
2028,2028,530,45,[],200,llama-7b,64,1,963.0,1.0,1,A100,1697548537867,1697548538830,120,26.0,1.0,"[96, 867]","[1697548537963, 1697548538830]"
2029,2029,223,32,[],200,llama-7b,64,1,602.0,1.0,1,A100,1697548529506,1697548530108,120,16.0,1.0,"[24, 578]","[1697548529530, 1697548530108]"
2030,2030,895,33,[],200,llama-7b,64,1,773.0,1.0,1,A100,1697548530112,1697548530885,120,15.0,1.0,"[26, 747]","[1697548530138, 1697548530885]"
2031,2031,190,46,[],200,llama-7b,64,1,1675.0,1.0,1,A100,1697548538833,1697548540508,120,335.0,10.0,"[13, 663, 241, 61, 56, 53, 45, 51, 42, 401, 49]","[1697548538846, 1697548539509, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539965, 1697548540016, 1697548540058, 1697548540459, 1697548540508]"
2032,2032,665,34,[],200,llama-7b,64,1,2796.0,1.0,1,A100,1697548530888,1697548533684,120,90.0,20.0,"[15, 465, 79, 65, 63, 61, 60, 51, 736, 53, 59, 64, 50, 55, 575, 65, 64, 49, 67, 54, 46]","[1697548530903, 1697548531368, 1697548531447, 1697548531512, 1697548531575, 1697548531636, 1697548531696, 1697548531747, 1697548532483, 1697548532536, 1697548532595, 1697548532659, 1697548532709, 1697548532764, 1697548533339, 1697548533404, 1697548533468, 1697548533517, 1697548533584, 1697548533638, 1697548533684]"
2033,2033,597,44,[],200,llama-7b,64,1,441.0,1.0,1,A100,1697548523912,1697548524353,120,39.0,1.0,"[25, 416]","[1697548523937, 1697548524353]"
2034,2034,925,37,[],200,llama-7b,64,1,3230.0,1.0,1,A100,1697548530454,1697548533684,120,87.0,20.0,"[34, 879, 80, 64, 64, 61, 59, 52, 736, 53, 60, 62, 51, 56, 575, 65, 63, 49, 66, 55, 46]","[1697548530488, 1697548531367, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531695, 1697548531747, 1697548532483, 1697548532536, 1697548532596, 1697548532658, 1697548532709, 1697548532765, 1697548533340, 1697548533405, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533684]"
2035,2035,889,47,[],200,llama-7b,64,1,4265.0,1.0,1,A100,1697548540511,1697548544776,120,86.0,20.0,"[6, 558, 54, 48, 750, 59, 61, 50, 650, 63, 61, 56, 45, 55, 692, 59, 47, 57, 55, 52, 787]","[1697548540517, 1697548541075, 1697548541129, 1697548541177, 1697548541927, 1697548541986, 1697548542047, 1697548542097, 1697548542747, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776]"
2036,2036,830,29,[],200,llama-7b,64,1,1729.0,1.0,1,A100,1697548527826,1697548529555,120,140.0,9.0,"[16, 423, 33, 656, 52, 50, 48, 47, 40, 364]","[1697548527842, 1697548528265, 1697548528298, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555]"
2037,2037,601,30,[],200,llama-7b,64,1,3151.0,1.0,1,A100,1697548529558,1697548532709,120,83.0,20.0,"[7, 542, 69, 56, 48, 59, 59, 52, 486, 38, 473, 64, 64, 61, 60, 51, 737, 52, 58, 64, 51]","[1697548529565, 1697548530107, 1697548530176, 1697548530232, 1697548530280, 1697548530339, 1697548530398, 1697548530450, 1697548530936, 1697548530974, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531696, 1697548531747, 1697548532484, 1697548532536, 1697548532594, 1697548532658, 1697548532709]"
2038,2038,697,38,[],200,llama-7b,64,1,2049.0,1.0,1,A100,1697548533685,1697548535734,120,123.0,10.0,"[12, 867, 67, 46, 59, 58, 57, 707, 59, 59, 58]","[1697548533697, 1697548534564, 1697548534631, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734]"
2039,2039,328,10,[],200,llama-7b,64,1,990.0,1.0,1,A100,1697548509627,1697548510617,120,109.0,6.0,"[13, 687, 66, 60, 59, 46, 59]","[1697548509640, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510617]"
2040,2040,99,11,[],200,llama-7b,64,1,218.0,1.0,1,A100,1697548510621,1697548510839,120,10.0,1.0,"[10, 208]","[1697548510631, 1697548510839]"
2041,2041,357,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548535735,1697548537858,120,,,"[7, 531, 184, 64, 47, 55, 315, 51, 61, 66, 62, 60, 47]","[1697548535742, 1697548536273, 1697548536457, 1697548536521, 1697548536568, 1697548536623, 1697548536938, 1697548536989, 1697548537050, 1697548537116, 1697548537178, 1697548537238, 1697548537285]"
2042,2042,448,50,[],200,llama-7b,64,1,2427.0,1.0,1,A100,1697548544779,1697548547206,120,335.0,12.0,"[12, 724, 368, 71, 65, 65, 62, 61, 48, 62, 759, 67, 63]","[1697548544791, 1697548545515, 1697548545883, 1697548545954, 1697548546019, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547076, 1697548547143, 1697548547206]"
2043,2043,689,12,[],200,llama-7b,64,1,715.0,1.0,1,A100,1697548510844,1697548511559,120,15.0,1.0,"[18, 697]","[1697548510862, 1697548511559]"
2044,2044,127,40,[],200,llama-7b,64,1,1112.0,1.0,1,A100,1697548537869,1697548538981,120,100.0,5.0,"[45, 260, 27, 683, 55, 42]","[1697548537914, 1697548538174, 1697548538201, 1697548538884, 1697548538939, 1697548538981]"
2045,2045,715,41,[],200,llama-7b,64,1,525.0,1.0,1,A100,1697548538985,1697548539510,120,20.0,1.0,"[10, 515]","[1697548538995, 1697548539510]"
2046,2046,458,13,[],200,llama-7b,64,1,896.0,1.0,1,A100,1697548511563,1697548512459,120,11.0,1.0,"[14, 882]","[1697548511577, 1697548512459]"
2047,2047,485,42,[],200,llama-7b,64,1,997.0,1.0,1,A100,1697548539512,1697548540509,120,67.0,3.0,"[21, 852, 74, 50]","[1697548539533, 1697548540385, 1697548540459, 1697548540509]"
2048,2048,117,14,[],200,llama-7b,64,1,1702.0,1.0,1,A100,1697548512462,1697548514164,120,364.0,2.0,"[14, 1306, 382]","[1697548512476, 1697548513782, 1697548514164]"
2049,2049,101,51,[],200,llama-7b,64,1,401.0,1.0,1,A100,1697548547214,1697548547615,120,13.0,1.0,"[14, 387]","[1697548547228, 1697548547615]"
2050,2050,807,52,[],200,llama-7b,64,1,2855.0,1.0,1,A100,1697548547618,1697548550473,120,90.0,20.0,"[29, 434, 64, 50, 47, 811, 61, 51, 58, 47, 58, 55, 397, 60, 47, 47, 48, 60, 55, 312, 64]","[1697548547647, 1697548548081, 1697548548145, 1697548548195, 1697548548242, 1697548549053, 1697548549114, 1697548549165, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549780, 1697548549840, 1697548549887, 1697548549934, 1697548549982, 1697548550042, 1697548550097, 1697548550409, 1697548550473]"
2051,2051,145,43,[],200,llama-7b,64,1,2299.0,1.0,1,A100,1697548540511,1697548542810,120,161.0,9.0,"[6, 558, 54, 48, 750, 59, 61, 50, 650, 63]","[1697548540517, 1697548541075, 1697548541129, 1697548541177, 1697548541927, 1697548541986, 1697548542047, 1697548542097, 1697548542747, 1697548542810]"
2052,2052,462,53,[],200,llama-7b,64,1,463.0,1.0,1,A100,1697548550477,1697548550940,120,52.0,1.0,"[29, 434]","[1697548550506, 1697548550940]"
2053,2053,232,54,[],200,llama-7b,64,1,2557.0,1.0,1,A100,1697548550943,1697548553500,120,93.0,20.0,"[9, 608, 70, 62, 57, 45, 44, 55, 55, 712, 53, 66, 51, 49, 65, 64, 275, 49, 46, 61, 61]","[1697548550952, 1697548551560, 1697548551630, 1697548551692, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552713, 1697548552779, 1697548552830, 1697548552879, 1697548552944, 1697548553008, 1697548553283, 1697548553332, 1697548553378, 1697548553439, 1697548553500]"
2054,2054,846,44,[],200,llama-7b,64,1,1125.0,1.0,1,A100,1697548542812,1697548543937,120,140.0,6.0,"[6, 531, 371, 58, 47, 58, 54]","[1697548542818, 1697548543349, 1697548543720, 1697548543778, 1697548543825, 1697548543883, 1697548543937]"
2055,2055,441,32,[],200,llama-7b,64,1,507.0,1.0,1,A100,1697548490219,1697548490726,120,6.0,1.0,"[30, 477]","[1697548490249, 1697548490726]"
2056,2056,181,18,[],200,llama-7b,64,1,5644.0,1.0,1,A100,1697548504914,1697548510558,120,91.0,39.0,"[10, 856, 62, 45, 55, 52, 678, 66, 58, 55, 43, 581, 65, 61, 60, 55, 57, 324, 65, 64, 61, 58, 54, 43, 305, 57, 54, 50, 41, 40, 580, 56, 54, 52, 53, 50, 559, 59, 59, 47]","[1697548504924, 1697548505780, 1697548505842, 1697548505887, 1697548505942, 1697548505994, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507540, 1697548507601, 1697548507661, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508287, 1697548508345, 1697548508399, 1697548508442, 1697548508747, 1697548508804, 1697548508858, 1697548508908, 1697548508949, 1697548508989, 1697548509569, 1697548509625, 1697548509679, 1697548509731, 1697548509784, 1697548509834, 1697548510393, 1697548510452, 1697548510511, 1697548510558]"
2057,2057,652,51,[],200,llama-7b,64,1,378.0,1.0,1,A100,1697548534798,1697548535176,120,14.0,1.0,"[8, 370]","[1697548534806, 1697548535176]"
2058,2058,101,33,[],200,llama-7b,64,1,615.0,1.0,1,A100,1697548490730,1697548491345,120,13.0,1.0,"[29, 586]","[1697548490759, 1697548491345]"
2059,2059,802,34,[],200,llama-7b,64,1,631.0,1.0,1,A100,1697548491349,1697548491980,120,9.0,1.0,"[24, 607]","[1697548491373, 1697548491980]"
2060,2060,455,35,[],200,llama-7b,64,1,2912.0,1.0,1,A100,1697548491983,1697548494895,120,91.0,20.0,"[14, 705, 359, 58, 49, 48, 39, 356, 49, 38, 37, 312, 52, 42, 46, 431, 60, 48, 59, 55, 55]","[1697548491997, 1697548492702, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493660, 1697548493698, 1697548493735, 1697548494047, 1697548494099, 1697548494141, 1697548494187, 1697548494618, 1697548494678, 1697548494726, 1697548494785, 1697548494840, 1697548494895]"
2061,2061,503,45,[],200,llama-7b,64,1,3480.0,1.0,1,A100,1697548543938,1697548547418,120,109.0,20.0,"[7, 577, 255, 55, 54, 46, 951, 71, 65, 66, 61, 62, 47, 63, 760, 65, 64, 59, 48, 58, 46]","[1697548543945, 1697548544522, 1697548544777, 1697548544832, 1697548544886, 1697548544932, 1697548545883, 1697548545954, 1697548546019, 1697548546085, 1697548546146, 1697548546208, 1697548546255, 1697548546318, 1697548547078, 1697548547143, 1697548547207, 1697548547266, 1697548547314, 1697548547372, 1697548547418]"
2062,2062,668,31,[],200,llama-7b,64,1,1745.0,1.0,1,A100,1697548529831,1697548531576,120,109.0,6.0,"[15, 1039, 51, 38, 473, 64, 65]","[1697548529846, 1697548530885, 1697548530936, 1697548530974, 1697548531447, 1697548531511, 1697548531576]"
2063,2063,70,22,[],200,llama-7b,64,1,555.0,1.0,1,A100,1697548498865,1697548499420,120,39.0,1.0,"[26, 529]","[1697548498891, 1697548499420]"
2064,2064,768,23,[],200,llama-7b,64,1,1297.0,1.0,1,A100,1697548499423,1697548500720,120,47.0,6.0,"[22, 980, 72, 65, 50, 49, 59]","[1697548499445, 1697548500425, 1697548500497, 1697548500562, 1697548500612, 1697548500661, 1697548500720]"
2065,2065,204,36,[],200,llama-7b,64,1,568.0,1.0,1,A100,1697548494897,1697548495465,120,67.0,6.0,"[15, 267, 65, 49, 59, 58, 55]","[1697548494912, 1697548495179, 1697548495244, 1697548495293, 1697548495352, 1697548495410, 1697548495465]"
2066,2066,444,32,[],200,llama-7b,64,1,1131.0,1.0,1,A100,1697548531579,1697548532710,120,457.0,6.0,"[16, 524, 365, 52, 59, 64, 51]","[1697548531595, 1697548532119, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710]"
2067,2067,321,33,[],200,llama-7b,64,1,746.0,1.0,1,A100,1697548523974,1697548524720,120,182.0,4.0,"[6, 373, 237, 64, 66]","[1697548523980, 1697548524353, 1697548524590, 1697548524654, 1697548524720]"
2068,2068,98,33,[],200,llama-7b,64,1,437.0,1.0,1,A100,1697548532713,1697548533150,120,14.0,1.0,"[25, 412]","[1697548532738, 1697548533150]"
2069,2069,802,34,[],200,llama-7b,64,1,777.0,1.0,1,A100,1697548533154,1697548533931,120,9.0,1.0,"[14, 763]","[1697548533168, 1697548533931]"
2070,2070,455,35,[],200,llama-7b,64,1,3247.0,1.0,1,A100,1697548533932,1697548537179,120,91.0,20.0,"[6, 626, 66, 47, 59, 58, 57, 707, 59, 59, 58, 55, 669, 62, 49, 54, 315, 51, 61, 65, 64]","[1697548533938, 1697548534564, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536458, 1697548536520, 1697548536569, 1697548536623, 1697548536938, 1697548536989, 1697548537050, 1697548537115, 1697548537179]"
2071,2071,789,37,[],200,llama-7b,64,1,6321.0,1.0,1,A100,1697548495468,1697548501789,120,6.0,50.0,"[7, 252, 65, 55, 53, 51, 52, 377, 41, 41, 50, 49, 447, 69, 63, 64, 59, 57, 55, 262, 60, 48, 46, 46, 400, 61, 52, 51, 50, 351, 59, 51, 49, 826, 62, 61, 60, 54, 51, 421, 65, 51, 49, 58, 55, 723, 63, 61, 58, 55, 55]","[1697548495475, 1697548495727, 1697548495792, 1697548495847, 1697548495900, 1697548495951, 1697548496003, 1697548496380, 1697548496421, 1697548496462, 1697548496512, 1697548496561, 1697548497008, 1697548497077, 1697548497140, 1697548497204, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497791, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499787, 1697548499849, 1697548499910, 1697548499970, 1697548500024, 1697548500075, 1697548500496, 1697548500561, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501497, 1697548501560, 1697548501621, 1697548501679, 1697548501734, 1697548501789]"
2072,2072,250,46,[],200,llama-7b,64,1,660.0,1.0,1,A100,1697548547422,1697548548082,120,31.0,1.0,"[38, 621]","[1697548547460, 1697548548081]"
2073,2073,203,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548537181,1697548537857,120,,,"[6, 581]","[1697548537187, 1697548537768]"
2074,2074,833,47,[],200,llama-7b,64,1,1300.0,1.0,1,A100,1697548548084,1697548549384,120,563.0,8.0,"[16, 660, 292, 61, 52, 58, 47, 58, 56]","[1697548548100, 1697548548760, 1697548549052, 1697548549113, 1697548549165, 1697548549223, 1697548549270, 1697548549328, 1697548549384]"
2075,2075,608,48,[],200,llama-7b,64,1,3327.0,1.0,1,A100,1697548549387,1697548552714,120,96.0,20.0,"[15, 938, 69, 63, 56, 48, 54, 371, 53, 51, 51, 48, 426, 62, 57, 44, 45, 55, 55, 712, 54]","[1697548549402, 1697548550340, 1697548550409, 1697548550472, 1697548550528, 1697548550576, 1697548550630, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551204, 1697548551630, 1697548551692, 1697548551749, 1697548551793, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552714]"
2076,2076,35,23,[],200,llama-7b,64,1,2730.0,1.0,1,A100,1697548490882,1697548493612,120,87.0,20.0,"[6, 457, 130, 50, 48, 40, 49, 48, 329, 53, 53, 42, 52, 50, 49, 724, 57, 49, 48, 39, 357]","[1697548490888, 1697548491345, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491710, 1697548492039, 1697548492092, 1697548492145, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493062, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493612]"
2077,2077,405,42,[],200,llama-7b,64,1,2744.0,1.0,1,A100,1697548547729,1697548550473,120,87.0,20.0,"[16, 336, 64, 50, 47, 811, 60, 52, 58, 48, 57, 55, 397, 59, 47, 47, 48, 60, 55, 313, 64]","[1697548547745, 1697548548081, 1697548548145, 1697548548195, 1697548548242, 1697548549053, 1697548549113, 1697548549165, 1697548549223, 1697548549271, 1697548549328, 1697548549383, 1697548549780, 1697548549839, 1697548549886, 1697548549933, 1697548549981, 1697548550041, 1697548550096, 1697548550409, 1697548550473]"
2078,2078,367,45,[],200,llama-7b,64,1,1367.0,1.0,1,A100,1697548524356,1697548525723,120,92.0,6.0,"[17, 876, 240, 68, 65, 51, 50]","[1697548524373, 1697548525249, 1697548525489, 1697548525557, 1697548525622, 1697548525673, 1697548525723]"
2079,2079,22,46,[],200,llama-7b,64,1,302.0,1.0,1,A100,1697548525727,1697548526029,120,16.0,1.0,"[6, 296]","[1697548525733, 1697548526029]"
2080,2080,458,31,[],200,llama-7b,64,1,541.0,1.0,1,A100,1697548506742,1697548507283,120,11.0,1.0,"[28, 513]","[1697548506770, 1697548507283]"
2081,2081,111,32,[],200,llama-7b,64,1,1002.0,1.0,1,A100,1697548507285,1697548508287,120,79.0,5.0,"[7, 805, 65, 64, 61]","[1697548507292, 1697548508097, 1697548508162, 1697548508226, 1697548508287]"
2082,2082,731,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526030,1697548527820,120,,,"[6, 674, 254, 65, 50, 64, 65, 64]","[1697548526036, 1697548526710, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2083,2083,816,33,[],200,llama-7b,64,1,570.0,1.0,1,A100,1697548508289,1697548508859,120,182.0,4.0,"[7, 383, 70, 56, 54]","[1697548508296, 1697548508679, 1697548508749, 1697548508805, 1697548508859]"
2084,2084,469,34,[],200,llama-7b,64,1,577.0,1.0,1,A100,1697548508863,1697548509440,120,17.0,1.0,"[18, 559]","[1697548508881, 1697548509440]"
2085,2085,246,35,[],200,llama-7b,64,1,8929.0,1.0,1,A100,1697548509443,1697548518372,120,58.0,47.0,"[20, 864, 66, 60, 59, 46, 58, 57, 230, 56, 43, 43, 51, 51, 656, 55, 55, 53, 54, 750, 350, 58, 55, 44, 53, 834, 65, 61, 58, 58, 919, 71, 69, 65, 60, 976, 271, 68, 66, 64, 64, 293, 66, 64, 59, 53, 672, 66]","[1697548509463, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510673, 1697548510903, 1697548510959, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511803, 1697548511858, 1697548511913, 1697548511966, 1697548512020, 1697548512770, 1697548513120, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514164, 1697548514229, 1697548514290, 1697548514348, 1697548514406, 1697548515325, 1697548515396, 1697548515465, 1697548515530, 1697548515590, 1697548516566, 1697548516837, 1697548516905, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517634, 1697548518306, 1697548518372]"
2086,2086,809,22,[],200,llama-7b,64,1,487.0,1.0,1,A100,1697548507540,1697548508027,120,16.0,1.0,"[14, 472]","[1697548507554, 1697548508026]"
2087,2087,586,23,[],200,llama-7b,64,1,2874.0,1.0,1,A100,1697548508029,1697548510903,120,85.0,20.0,"[21, 629, 69, 57, 54, 50, 40, 40, 579, 58, 54, 52, 53, 49, 559, 60, 59, 46, 58, 56, 231]","[1697548508050, 1697548508679, 1697548508748, 1697548508805, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509568, 1697548509626, 1697548509680, 1697548509732, 1697548509785, 1697548509834, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510672, 1697548510903]"
2088,2088,834,36,[],200,llama-7b,64,1,2598.0,1.0,1,A100,1697548518375,1697548520973,120,85.0,20.0,"[12, 459, 69, 63, 50, 58, 56, 45, 607, 243, 68, 61, 63, 61, 60, 61, 336, 65, 63, 49, 49]","[1697548518387, 1697548518846, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520037, 1697548520105, 1697548520166, 1697548520229, 1697548520290, 1697548520350, 1697548520411, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520973]"
2089,2089,773,16,[],200,llama-7b,64,1,3091.0,1.0,1,A100,1697548504570,1697548507661,120,90.0,20.0,"[6, 617, 69, 65, 52, 60, 60, 60, 283, 46, 54, 52, 679, 65, 58, 55, 43, 581, 65, 61, 59]","[1697548504576, 1697548505193, 1697548505262, 1697548505327, 1697548505379, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506673, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507540, 1697548507601, 1697548507660]"
2090,2090,729,40,[],200,llama-7b,64,1,969.0,1.0,1,A100,1697548542751,1697548543720,120,874.0,2.0,"[20, 949]","[1697548542771, 1697548543720]"
2091,2091,188,44,[],200,llama-7b,64,1,4103.0,1.0,1,A100,1697548540783,1697548544886,120,85.0,20.0,"[30, 777, 336, 61, 59, 51, 651, 61, 61, 58, 43, 55, 694, 58, 47, 58, 54, 52, 788, 55, 54]","[1697548540813, 1697548541590, 1697548541926, 1697548541987, 1697548542046, 1697548542097, 1697548542748, 1697548542809, 1697548542870, 1697548542928, 1697548542971, 1697548543026, 1697548543720, 1697548543778, 1697548543825, 1697548543883, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544886]"
2092,2092,61,43,[],200,llama-7b,64,1,463.0,1.0,1,A100,1697548550477,1697548550940,120,9.0,1.0,"[28, 435]","[1697548550505, 1697548550940]"
2093,2093,759,44,[],200,llama-7b,64,1,2557.0,1.0,1,A100,1697548550943,1697548553500,120,92.0,20.0,"[6, 611, 70, 62, 57, 45, 44, 55, 55, 712, 53, 67, 50, 49, 65, 64, 275, 49, 47, 60, 61]","[1697548550949, 1697548551560, 1697548551630, 1697548551692, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552713, 1697548552780, 1697548552830, 1697548552879, 1697548552944, 1697548553008, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553500]"
2094,2094,596,17,[],200,llama-7b,64,1,2177.0,1.0,1,A100,1697548493615,1697548495792,120,87.0,20.0,"[25, 347, 60, 52, 42, 46, 431, 60, 47, 60, 55, 55, 51, 298, 49, 58, 59, 54, 52, 42, 234]","[1697548493640, 1697548493987, 1697548494047, 1697548494099, 1697548494141, 1697548494187, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494840, 1697548494895, 1697548494946, 1697548495244, 1697548495293, 1697548495351, 1697548495410, 1697548495464, 1697548495516, 1697548495558, 1697548495792]"
2095,2095,162,26,[],200,llama-7b,64,1,2901.0,1.0,1,A100,1697548490219,1697548493120,120,90.0,20.0,"[15, 492, 55, 50, 48, 42, 554, 50, 48, 40, 49, 47, 329, 54, 53, 43, 51, 51, 48, 723, 59]","[1697548490234, 1697548490726, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491475, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491709, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492338, 1697548493061, 1697548493120]"
2096,2096,724,27,[],200,llama-7b,64,1,427.0,1.0,1,A100,1697548493124,1697548493551,120,11.0,1.0,"[31, 396]","[1697548493155, 1697548493551]"
2097,2097,493,28,[],200,llama-7b,64,1,2226.0,1.0,1,A100,1697548493566,1697548495792,120,83.0,20.0,"[29, 392, 60, 52, 42, 46, 432, 59, 47, 61, 54, 56, 50, 298, 49, 58, 59, 54, 52, 42, 233]","[1697548493595, 1697548493987, 1697548494047, 1697548494099, 1697548494141, 1697548494187, 1697548494619, 1697548494678, 1697548494725, 1697548494786, 1697548494840, 1697548494896, 1697548494946, 1697548495244, 1697548495293, 1697548495351, 1697548495410, 1697548495464, 1697548495516, 1697548495558, 1697548495791]"
2098,2098,548,48,[],200,llama-7b,64,1,3366.0,1.0,1,A100,1697548544779,1697548548145,120,86.0,20.0,"[7, 728, 369, 71, 65, 65, 62, 61, 48, 62, 759, 67, 63, 60, 48, 57, 46, 257, 53, 47, 371]","[1697548544786, 1697548545514, 1697548545883, 1697548545954, 1697548546019, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547076, 1697548547143, 1697548547206, 1697548547266, 1697548547314, 1697548547371, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145]"
2099,2099,325,49,[],200,llama-7b,64,1,2483.0,1.0,1,A100,1697548548147,1697548550630,120,85.0,20.0,"[14, 600, 292, 61, 50, 60, 47, 58, 55, 395, 60, 48, 47, 48, 59, 56, 313, 63, 55, 49, 53]","[1697548548161, 1697548548761, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550410, 1697548550473, 1697548550528, 1697548550577, 1697548550630]"
2100,2100,420,45,[],200,llama-7b,64,1,2632.0,1.0,1,A100,1697548553502,1697548556134,120,52.0,20.0,"[25, 907, 437, 251, 253, 58, 45, 44, 46, 58, 57, 43, 50, 69, 46, 37, 45, 42, 41, 40, 38]","[1697548553527, 1697548554434, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555522, 1697548555568, 1697548555626, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556096, 1697548556134]"
2101,2101,773,45,[],200,llama-7b,64,1,3258.0,1.0,1,A100,1697548544887,1697548548145,120,90.0,20.0,"[12, 616, 368, 72, 65, 64, 63, 60, 49, 62, 759, 66, 63, 60, 48, 58, 45, 257, 53, 47, 371]","[1697548544899, 1697548545515, 1697548545883, 1697548545955, 1697548546020, 1697548546084, 1697548546147, 1697548546207, 1697548546256, 1697548546318, 1697548547077, 1697548547143, 1697548547206, 1697548547266, 1697548547314, 1697548547372, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145]"
2102,2102,911,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548535792,1697548537862,120,,,"[8, 1066, 72, 51, 61, 65, 63, 61, 46]","[1697548535800, 1697548536866, 1697548536938, 1697548536989, 1697548537050, 1697548537115, 1697548537178, 1697548537239, 1697548537285]"
2103,2103,194,45,[],200,llama-7b,64,1,2262.0,1.0,1,A100,1697548548148,1697548550410,120,335.0,16.0,"[9, 604, 292, 61, 50, 60, 47, 57, 56, 395, 60, 48, 47, 48, 59, 55, 314]","[1697548548157, 1697548548761, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549328, 1697548549384, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550096, 1697548550410]"
2104,2104,816,15,[],200,llama-7b,64,1,1299.0,1.0,1,A100,1697548514167,1697548515466,120,182.0,4.0,"[11, 680, 467, 71, 70]","[1697548514178, 1697548514858, 1697548515325, 1697548515396, 1697548515466]"
2105,2105,444,16,[],200,llama-7b,64,1,1568.0,1.0,1,A100,1697548515468,1697548517036,120,457.0,6.0,"[7, 594, 498, 270, 68, 66, 65]","[1697548515475, 1697548516069, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517036]"
2106,2106,816,55,[],200,llama-7b,64,1,940.0,1.0,1,A100,1697548553012,1697548553952,120,182.0,4.0,"[13, 755, 67, 59, 46]","[1697548553025, 1697548553780, 1697548553847, 1697548553906, 1697548553952]"
2107,2107,689,39,[],200,llama-7b,64,1,953.0,1.0,1,A100,1697548537878,1697548538831,120,15.0,1.0,"[216, 737]","[1697548538094, 1697548538831]"
2108,2108,144,29,[],200,llama-7b,64,1,2504.0,1.0,1,A100,1697548495794,1697548498298,120,96.0,20.0,"[11, 520, 55, 41, 41, 50, 50, 446, 69, 64, 62, 61, 56, 55, 262, 60, 48, 46, 46, 400, 61]","[1697548495805, 1697548496325, 1697548496380, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497008, 1697548497077, 1697548497141, 1697548497203, 1697548497264, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497791, 1697548497837, 1697548498237, 1697548498298]"
2109,2109,549,46,[],200,llama-7b,64,1,2481.0,1.0,1,A100,1697548548148,1697548550629,120,93.0,20.0,"[23, 590, 292, 61, 50, 60, 47, 58, 55, 395, 60, 48, 47, 48, 59, 56, 313, 64, 55, 48, 52]","[1697548548171, 1697548548761, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550410, 1697548550474, 1697548550529, 1697548550577, 1697548550629]"
2110,2110,340,40,[],200,llama-7b,64,1,3212.0,1.0,1,A100,1697548538834,1697548542046,120,85.0,20.0,"[37, 639, 241, 60, 56, 54, 44, 52, 41, 400, 50, 59, 47, 57, 56, 52, 350, 48, 749, 60, 60]","[1697548538871, 1697548539510, 1697548539751, 1697548539811, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540727, 1697548540779, 1697548541129, 1697548541177, 1697548541926, 1697548541986, 1697548542046]"
2111,2111,216,17,[],200,llama-7b,64,1,3065.0,1.0,1,A100,1697548517039,1697548520104,120,91.0,20.0,"[10, 273, 71, 66, 64, 58, 54, 671, 66, 65, 58, 58, 362, 63, 50, 58, 56, 45, 607, 242, 68]","[1697548517049, 1697548517322, 1697548517393, 1697548517459, 1697548517523, 1697548517581, 1697548517635, 1697548518306, 1697548518372, 1697548518437, 1697548518495, 1697548518553, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104]"
2112,2112,202,47,[],200,llama-7b,64,1,6267.0,1.0,1,A100,1697548550633,1697548556900,120,874.0,72.0,"[12, 914, 70, 63, 57, 44, 44, 55, 56, 712, 53, 67, 50, 50, 65, 64, 274, 48, 48, 60, 60, 348, 59, 46, 59, 860, 251, 253, 58, 44, 45, 46, 57, 58, 43, 50, 69, 46, 37, 45, 42, 41, 39, 39, 34, 28, 32, 27, 26, 26, 27, 31, 25, 31, 25, 31, 24, 25, 25, 24, 30, 24, 24, 29, 23, 24, 23, 24, 24, 23, 29, 24, 23]","[1697548550645, 1697548551559, 1697548551629, 1697548551692, 1697548551749, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552660, 1697548552713, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009, 1697548553283, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555477, 1697548555522, 1697548555568, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556168, 1697548556196, 1697548556228, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556390, 1697548556421, 1697548556446, 1697548556477, 1697548556501, 1697548556526, 1697548556551, 1697548556575, 1697548556605, 1697548556629, 1697548556653, 1697548556682, 1697548556705, 1697548556729, 1697548556752, 1697548556776, 1697548556800, 1697548556823, 1697548556852, 1697548556876, 1697548556899]"
2113,2113,907,37,[],200,llama-7b,64,1,311.0,1.0,1,A100,1697548537863,1697548538174,120,10.0,1.0,"[12, 299]","[1697548537875, 1697548538174]"
2114,2114,561,38,[],200,llama-7b,64,1,2603.0,1.0,1,A100,1697548538176,1697548540779,120,87.0,20.0,"[9, 646, 53, 55, 42, 46, 46, 677, 61, 56, 54, 44, 52, 41, 401, 49, 60, 46, 58, 55, 52]","[1697548538185, 1697548538831, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539073, 1697548539750, 1697548539811, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540459, 1697548540508, 1697548540568, 1697548540614, 1697548540672, 1697548540727, 1697548540779]"
2115,2115,117,41,[],200,llama-7b,64,1,700.0,1.0,1,A100,1697548542048,1697548542748,120,364.0,2.0,"[14, 440, 246]","[1697548542062, 1697548542502, 1697548542748]"
2116,2116,851,30,[],200,llama-7b,64,1,435.0,1.0,1,A100,1697548498303,1697548498738,120,23.0,1.0,"[31, 404]","[1697548498334, 1697548498738]"
2117,2117,504,31,[],200,llama-7b,64,1,3669.0,1.0,1,A100,1697548498741,1697548502410,120,58.0,20.0,"[19, 660, 368, 62, 61, 59, 55, 51, 420, 65, 51, 51, 57, 54, 724, 63, 61, 57, 56, 55, 620]","[1697548498760, 1697548499420, 1697548499788, 1697548499850, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500496, 1697548500561, 1697548500612, 1697548500663, 1697548500720, 1697548500774, 1697548501498, 1697548501561, 1697548501622, 1697548501679, 1697548501735, 1697548501790, 1697548502410]"
2118,2118,429,24,[],200,llama-7b,64,1,7504.0,1.0,1,A100,1697548500722,1697548508226,120,244.0,50.0,"[11, 471, 294, 63, 61, 58, 55, 55, 620, 65, 63, 62, 60, 55, 877, 62, 58, 57, 53, 41, 705, 61, 48, 59, 59, 59, 57, 351, 65, 51, 61, 60, 59, 283, 46, 55, 51, 679, 65, 58, 56, 43, 581, 64, 62, 59, 56, 57, 324, 65, 64]","[1697548500733, 1697548501204, 1697548501498, 1697548501561, 1697548501622, 1697548501680, 1697548501735, 1697548501790, 1697548502410, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502715, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504736, 1697548504795, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505558, 1697548505841, 1697548505887, 1697548505942, 1697548505993, 1697548506672, 1697548506737, 1697548506795, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226]"
2119,2119,94,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548524723,1697548527820,120,,,"[12, 514, 240, 68, 65, 51, 50, 65, 66, 241, 48, 48, 59, 59, 656, 65, 49, 65, 63, 65]","[1697548524735, 1697548525249, 1697548525489, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525854, 1697548526095, 1697548526143, 1697548526191, 1697548526250, 1697548526309, 1697548526965, 1697548527030, 1697548527079, 1697548527144, 1697548527207, 1697548527272]"
2120,2120,699,42,[],200,llama-7b,64,1,598.0,1.0,1,A100,1697548542751,1697548543349,120,39.0,1.0,"[12, 586]","[1697548542763, 1697548543349]"
2121,2121,471,43,[],200,llama-7b,64,1,4066.0,1.0,1,A100,1697548543352,1697548547418,120,86.0,20.0,"[7, 1163, 254, 56, 53, 47, 952, 71, 65, 64, 62, 61, 48, 62, 760, 65, 64, 59, 49, 57, 46]","[1697548543359, 1697548544522, 1697548544776, 1697548544832, 1697548544885, 1697548544932, 1697548545884, 1697548545955, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547371, 1697548547417]"
2122,2122,679,35,[],200,llama-7b,64,1,1069.0,1.0,1,A100,1697548527827,1697548528896,120,15.0,1.0,"[227, 842]","[1697548528054, 1697548528896]"
2123,2123,520,39,[],200,llama-7b,64,1,851.0,1.0,1,A100,1697548525859,1697548526710,120,11.0,1.0,"[32, 819]","[1697548525891, 1697548526710]"
2124,2124,699,40,[],200,llama-7b,64,1,439.0,1.0,1,A100,1697548527827,1697548528266,120,39.0,1.0,"[55, 384]","[1697548527882, 1697548528266]"
2125,2125,621,35,[],200,llama-7b,64,1,2250.0,1.0,1,A100,1697548506740,1697548508990,120,88.0,20.0,"[23, 520, 193, 64, 61, 59, 57, 56, 324, 65, 64, 60, 59, 55, 42, 305, 57, 55, 50, 40, 40]","[1697548506763, 1697548507283, 1697548507476, 1697548507540, 1697548507601, 1697548507660, 1697548507717, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508747, 1697548508804, 1697548508859, 1697548508909, 1697548508949, 1697548508989]"
2126,2126,11,38,[],200,llama-7b,64,1,3820.0,1.0,1,A100,1697548513279,1697548517099,120,732.0,17.0,"[13, 491, 381, 66, 60, 58, 58, 918, 72, 69, 64, 61, 977, 271, 68, 65, 64, 64]","[1697548513292, 1697548513783, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406, 1697548515324, 1697548515396, 1697548515465, 1697548515529, 1697548515590, 1697548516567, 1697548516838, 1697548516906, 1697548516971, 1697548517035, 1697548517099]"
2127,2127,563,29,[],200,llama-7b,64,1,3447.0,1.0,1,A100,1697548515468,1697548518915,120,874.0,18.0,"[14, 587, 498, 270, 68, 66, 65, 63, 293, 66, 65, 58, 54, 672, 65, 66, 58, 57, 362]","[1697548515482, 1697548516069, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517036, 1697548517099, 1697548517392, 1697548517458, 1697548517523, 1697548517581, 1697548517635, 1697548518307, 1697548518372, 1697548518438, 1697548518496, 1697548518553, 1697548518915]"
2128,2128,682,28,[],200,llama-7b,64,1,8830.0,1.0,1,A100,1697548508751,1697548517581,120,244.0,50.0,"[32, 656, 129, 57, 54, 53, 53, 50, 558, 59, 59, 46, 59, 56, 231, 57, 42, 43, 52, 50, 655, 56, 54, 53, 54, 750, 350, 59, 55, 43, 54, 834, 65, 61, 58, 57, 919, 72, 69, 65, 60, 976, 271, 68, 65, 65, 63, 294, 66, 64, 59]","[1697548508783, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835, 1697548510393, 1697548510452, 1697548510511, 1697548510557, 1697548510616, 1697548510672, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511097, 1697548511147, 1697548511802, 1697548511858, 1697548511912, 1697548511965, 1697548512019, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514164, 1697548514229, 1697548514290, 1697548514348, 1697548514405, 1697548515324, 1697548515396, 1697548515465, 1697548515530, 1697548515590, 1697548516566, 1697548516837, 1697548516905, 1697548516970, 1697548517035, 1697548517098, 1697548517392, 1697548517458, 1697548517522, 1697548517581]"
2129,2129,333,30,[],200,llama-7b,64,1,1892.0,1.0,1,A100,1697548518920,1697548520812,120,563.0,11.0,"[16, 674, 185, 242, 67, 63, 62, 61, 61, 59, 337, 65]","[1697548518936, 1697548519610, 1697548519795, 1697548520037, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520351, 1697548520410, 1697548520747, 1697548520812]"
2130,2130,702,27,[],200,llama-7b,64,1,3679.0,1.0,1,A100,1697548499913,1697548503592,120,89.0,20.0,"[6, 507, 71, 65, 51, 48, 59, 54, 723, 64, 60, 58, 56, 54, 622, 64, 64, 62, 59, 55, 877]","[1697548499919, 1697548500426, 1697548500497, 1697548500562, 1697548500613, 1697548500661, 1697548500720, 1697548500774, 1697548501497, 1697548501561, 1697548501621, 1697548501679, 1697548501735, 1697548501789, 1697548502411, 1697548502475, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503592]"
2131,2131,916,31,[],200,llama-7b,64,1,548.0,1.0,1,A100,1697548520816,1697548521364,120,8.0,1.0,"[16, 531]","[1697548520832, 1697548521363]"
2132,2132,694,32,[],200,llama-7b,64,1,1992.0,1.0,1,A100,1697548521368,1697548523360,120,161.0,13.0,"[28, 1015, 70, 67, 67, 56, 66, 50, 317, 67, 63, 64, 62]","[1697548521396, 1697548522411, 1697548522481, 1697548522548, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523171, 1697548523234, 1697548523298, 1697548523360]"
2133,2133,872,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548487238,1697548488027,120,,,"[24, 546]","[1697548487262, 1697548487808]"
2134,2134,620,22,[],200,llama-7b,64,1,1385.0,1.0,1,A100,1697548488029,1697548489414,120,100.0,8.0,"[12, 790, 57, 52, 35, 37, 285, 67, 50]","[1697548488041, 1697548488831, 1697548488888, 1697548488940, 1697548488975, 1697548489012, 1697548489297, 1697548489364, 1697548489414]"
2135,2135,347,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548523364,1697548527820,120,,,"[15, 360, 51, 68, 51, 63, 64, 553, 64, 66, 65, 59, 58, 587, 69, 66, 50, 50, 65, 65, 242, 48, 48, 60, 59, 654, 65, 50, 64, 65, 64]","[1697548523379, 1697548523739, 1697548523790, 1697548523858, 1697548523909, 1697548523972, 1697548524036, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525488, 1697548525557, 1697548525623, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2136,2136,307,52,[],200,llama-7b,64,1,1094.0,1.0,1,A100,1697548535179,1697548536273,120,26.0,1.0,"[22, 1072]","[1697548535201, 1697548536273]"
2137,2137,84,53,[],200,llama-7b,64,1,588.0,1.0,1,A100,1697548536278,1697548536866,120,26.0,1.0,"[29, 559]","[1697548536307, 1697548536866]"
2138,2138,667,54,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536869,1697548537856,120,,,"[20, 878]","[1697548536889, 1697548537767]"
2139,2139,438,55,[],200,llama-7b,64,1,956.0,1.0,1,A100,1697548537875,1697548538831,120,9.0,1.0,"[160, 796]","[1697548538035, 1697548538831]"
2140,2140,781,22,[],200,llama-7b,64,1,3080.0,1.0,1,A100,1697548481227,1697548484307,120,335.0,10.0,"[6, 1725, 551, 74, 56, 72, 66, 66, 49, 338, 77]","[1697548481233, 1697548482958, 1697548483509, 1697548483583, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484307]"
2141,2141,92,56,[],200,llama-7b,64,1,3212.0,1.0,1,A100,1697548538834,1697548542046,120,85.0,20.0,"[39, 637, 241, 61, 55, 54, 44, 52, 41, 400, 50, 59, 47, 57, 56, 52, 350, 48, 749, 61, 59]","[1697548538873, 1697548539510, 1697548539751, 1697548539812, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540727, 1697548540779, 1697548541129, 1697548541177, 1697548541926, 1697548541987, 1697548542046]"
2142,2142,798,18,[],200,llama-7b,64,1,865.0,1.0,1,A100,1697548520108,1697548520973,120,79.0,6.0,"[15, 624, 65, 63, 49, 49]","[1697548520123, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520973]"
2143,2143,575,19,[],200,llama-7b,64,1,2446.0,1.0,1,A100,1697548520976,1697548523422,120,86.0,20.0,"[30, 358, 67, 63, 47, 60, 46, 54, 710, 70, 66, 68, 56, 66, 50, 317, 66, 64, 64, 62, 62]","[1697548521006, 1697548521364, 1697548521431, 1697548521494, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522481, 1697548522547, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422]"
2144,2144,764,19,[],200,llama-7b,64,1,279.0,1.0,1,A100,1697548510560,1697548510839,120,39.0,1.0,"[15, 264]","[1697548510575, 1697548510839]"
2145,2145,797,57,[],200,llama-7b,64,1,452.0,1.0,1,A100,1697548542050,1697548542502,120,26.0,1.0,"[28, 424]","[1697548542078, 1697548542502]"
2146,2146,453,58,[],200,llama-7b,64,1,843.0,1.0,1,A100,1697548542506,1697548543349,120,26.0,1.0,"[25, 818]","[1697548542531, 1697548543349]"
2147,2147,535,20,[],200,llama-7b,64,1,4624.0,1.0,1,A100,1697548510842,1697548515466,120,84.0,20.0,"[15, 702, 243, 57, 53, 53, 53, 751, 350, 58, 57, 42, 54, 834, 67, 59, 58, 58, 919, 71, 70]","[1697548510857, 1697548511559, 1697548511802, 1697548511859, 1697548511912, 1697548511965, 1697548512018, 1697548512769, 1697548513119, 1697548513177, 1697548513234, 1697548513276, 1697548513330, 1697548514164, 1697548514231, 1697548514290, 1697548514348, 1697548514406, 1697548515325, 1697548515396, 1697548515466]"
2148,2148,222,59,[],200,llama-7b,64,1,4064.0,1.0,1,A100,1697548543354,1697548547418,120,96.0,20.0,"[28, 1140, 254, 56, 53, 47, 952, 71, 64, 65, 62, 62, 48, 62, 759, 65, 64, 59, 49, 58, 46]","[1697548543382, 1697548544522, 1697548544776, 1697548544832, 1697548544885, 1697548544932, 1697548545884, 1697548545955, 1697548546019, 1697548546084, 1697548546146, 1697548546208, 1697548546256, 1697548546318, 1697548547077, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547372, 1697548547418]"
2149,2149,175,34,[],200,llama-7b,64,1,1077.0,1.0,1,A100,1697548538982,1697548540059,120,140.0,8.0,"[7, 762, 61, 56, 53, 44, 52, 42]","[1697548538989, 1697548539751, 1697548539812, 1697548539868, 1697548539921, 1697548539965, 1697548540017, 1697548540059]"
2150,2150,179,60,[],200,llama-7b,64,1,498.0,1.0,1,A100,1697548552882,1697548553380,120,161.0,4.0,"[7, 328, 66, 49, 47]","[1697548552889, 1697548553217, 1697548553283, 1697548553332, 1697548553379]"
2151,2151,757,35,[],200,llama-7b,64,1,1013.0,1.0,1,A100,1697548540062,1697548541075,120,20.0,1.0,"[18, 995]","[1697548540080, 1697548541075]"
2152,2152,528,36,[],200,llama-7b,64,1,3809.0,1.0,1,A100,1697548541077,1697548544886,120,52.0,20.0,"[6, 507, 337, 60, 59, 51, 651, 61, 61, 58, 43, 56, 693, 58, 46, 58, 54, 53, 788, 55, 54]","[1697548541083, 1697548541590, 1697548541927, 1697548541987, 1697548542046, 1697548542097, 1697548542748, 1697548542809, 1697548542870, 1697548542928, 1697548542971, 1697548543027, 1697548543720, 1697548543778, 1697548543824, 1697548543882, 1697548543936, 1697548543989, 1697548544777, 1697548544832, 1697548544886]"
2153,2153,885,61,[],200,llama-7b,64,1,3270.0,1.0,1,A100,1697548553383,1697548556653,120,84.0,43.0,"[6, 391, 68, 59, 46, 58, 860, 251, 252, 60, 43, 45, 45, 58, 58, 43, 50, 69, 46, 36, 46, 42, 41, 39, 39, 35, 27, 32, 27, 26, 26, 27, 31, 26, 30, 25, 31, 24, 25, 25, 24, 30, 24, 24]","[1697548553389, 1697548553780, 1697548553848, 1697548553907, 1697548553953, 1697548554011, 1697548554871, 1697548555122, 1697548555374, 1697548555434, 1697548555477, 1697548555522, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555927, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556169, 1697548556196, 1697548556228, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556391, 1697548556421, 1697548556446, 1697548556477, 1697548556501, 1697548556526, 1697548556551, 1697548556575, 1697548556605, 1697548556629, 1697548556653]"
2154,2154,382,41,[],200,llama-7b,64,1,3695.0,1.0,1,A100,1697548543723,1697548547418,120,47.0,20.0,"[6, 793, 254, 56, 53, 47, 951, 72, 64, 65, 62, 62, 48, 62, 759, 66, 63, 59, 49, 58, 46]","[1697548543729, 1697548544522, 1697548544776, 1697548544832, 1697548544885, 1697548544932, 1697548545883, 1697548545955, 1697548546019, 1697548546084, 1697548546146, 1697548546208, 1697548546256, 1697548546318, 1697548547077, 1697548547143, 1697548547206, 1697548547265, 1697548547314, 1697548547372, 1697548547418]"
2155,2155,715,25,[],200,llama-7b,64,1,438.0,1.0,1,A100,1697548498300,1697548498738,120,20.0,1.0,"[14, 424]","[1697548498314, 1697548498738]"
2156,2156,41,20,[],200,llama-7b,64,1,5042.0,1.0,1,A100,1697548493256,1697548498298,120,39.0,43.0,"[11, 720, 60, 52, 41, 46, 433, 60, 47, 60, 55, 55, 51, 296, 49, 59, 58, 54, 53, 42, 233, 55, 53, 51, 52, 378, 41, 40, 51, 49, 446, 69, 64, 60, 63, 57, 55, 262, 61, 47, 47, 45, 400, 61]","[1697548493267, 1697548493987, 1697548494047, 1697548494099, 1697548494140, 1697548494186, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494896, 1697548494947, 1697548495243, 1697548495292, 1697548495351, 1697548495409, 1697548495463, 1697548495516, 1697548495558, 1697548495791, 1697548495846, 1697548495899, 1697548495950, 1697548496002, 1697548496380, 1697548496421, 1697548496461, 1697548496512, 1697548496561, 1697548497007, 1697548497076, 1697548497140, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497745, 1697548497792, 1697548497837, 1697548498237, 1697548498298]"
2157,2157,485,26,[],200,llama-7b,64,1,1109.0,1.0,1,A100,1697548498741,1697548499850,120,67.0,3.0,"[12, 667, 368, 62]","[1697548498753, 1697548499420, 1697548499788, 1697548499850]"
2158,2158,145,27,[],200,llama-7b,64,1,1710.0,1.0,1,A100,1697548499851,1697548501561,120,161.0,9.0,"[10, 564, 72, 65, 51, 48, 59, 54, 723, 64]","[1697548499861, 1697548500425, 1697548500497, 1697548500562, 1697548500613, 1697548500661, 1697548500720, 1697548500774, 1697548501497, 1697548501561]"
2159,2159,242,26,[],200,llama-7b,64,1,1555.0,1.0,1,A100,1697548535560,1697548537115,120,345.0,9.0,"[7, 706, 184, 64, 47, 55, 315, 51, 60, 66]","[1697548535567, 1697548536273, 1697548536457, 1697548536521, 1697548536568, 1697548536623, 1697548536938, 1697548536989, 1697548537049, 1697548537115]"
2160,2160,180,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526713,1697548527821,120,,,"[20, 844]","[1697548526733, 1697548527577]"
2161,2161,168,19,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548509000,1697548527818,120,,,"[19, 1308, 66, 60, 59, 46, 58, 57, 231, 55, 43, 42, 52, 51, 656, 55, 55, 53, 54, 749, 351, 58, 55, 44, 53, 834, 66, 60, 58, 57, 920, 71, 69, 65, 60, 976, 271, 68, 66, 64, 64, 293, 66, 64, 59, 53, 672, 66, 64, 59, 58, 361, 63, 50, 58, 57, 45, 607, 242, 68, 62, 62, 62, 60, 60, 336, 66, 63, 48, 49, 53, 406, 62, 48, 60, 46, 54, 709, 70, 67, 67, 57, 66, 49, 317, 67, 64, 63, 63, 61, 317, 52, 67, 51, 63, 64, 554, 64, 65, 66, 59, 57, 588, 68, 66, 50, 50, 66, 65, 242, 48, 47, 60, 59, 654, 66, 49, 65, 65, 63]","[1697548509019, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510673, 1697548510904, 1697548510959, 1697548511002, 1697548511044, 1697548511096, 1697548511147, 1697548511803, 1697548511858, 1697548511913, 1697548511966, 1697548512020, 1697548512769, 1697548513120, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514405, 1697548515325, 1697548515396, 1697548515465, 1697548515530, 1697548515590, 1697548516566, 1697548516837, 1697548516905, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517634, 1697548518306, 1697548518372, 1697548518436, 1697548518495, 1697548518553, 1697548518914, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521025, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522786, 1697548523103, 1697548523170, 1697548523234, 1697548523297, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523971, 1697548524035, 1697548524589, 1697548524653, 1697548524718, 1697548524784, 1697548524843, 1697548524900, 1697548525488, 1697548525556, 1697548525622, 1697548525672, 1697548525722, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526963, 1697548527029, 1697548527078, 1697548527143, 1697548527208, 1697548527271]"
2162,2162,877,41,[],200,llama-7b,64,1,2624.0,1.0,1,A100,1697548527826,1697548530450,120,85.0,20.0,"[128, 942, 59, 51, 51, 48, 47, 39, 365, 52, 49, 41, 41, 41, 49, 347, 55, 49, 59, 59, 52]","[1697548527954, 1697548528896, 1697548528955, 1697548529006, 1697548529057, 1697548529105, 1697548529152, 1697548529191, 1697548529556, 1697548529608, 1697548529657, 1697548529698, 1697548529739, 1697548529780, 1697548529829, 1697548530176, 1697548530231, 1697548530280, 1697548530339, 1697548530398, 1697548530450]"
2163,2163,538,42,[],200,llama-7b,64,1,3231.0,1.0,1,A100,1697548530453,1697548533684,120,89.0,20.0,"[15, 899, 80, 65, 62, 62, 59, 52, 738, 52, 58, 64, 51, 54, 576, 64, 63, 50, 67, 54, 46]","[1697548530468, 1697548531367, 1697548531447, 1697548531512, 1697548531574, 1697548531636, 1697548531695, 1697548531747, 1697548532485, 1697548532537, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533584, 1697548533638, 1697548533684]"
2164,2164,307,43,[],200,llama-7b,64,1,880.0,1.0,1,A100,1697548533685,1697548534565,120,26.0,1.0,"[12, 867]","[1697548533697, 1697548534564]"
2165,2165,891,44,[],200,llama-7b,64,1,992.0,1.0,1,A100,1697548534567,1697548535559,120,52.0,2.0,"[19, 590, 382]","[1697548534586, 1697548535176, 1697548535558]"
2166,2166,643,45,[],200,llama-7b,64,1,711.0,1.0,1,A100,1697548535562,1697548536273,120,18.0,1.0,"[15, 696]","[1697548535577, 1697548536273]"
2167,2167,413,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536276,1697548537862,120,,,"[21, 569, 72, 51, 61, 66, 62, 61, 47]","[1697548536297, 1697548536866, 1697548536938, 1697548536989, 1697548537050, 1697548537116, 1697548537178, 1697548537239, 1697548537286]"
2168,2168,135,50,[],200,llama-7b,64,1,1143.0,1.0,1,A100,1697548540783,1697548541926,120,52.0,2.0,"[20, 787, 336]","[1697548540803, 1697548541590, 1697548541926]"
2169,2169,437,23,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484310,1697548488027,120,,,"[6, 624, 552, 313, 308, 77, 76, 71, 70, 323, 59, 59, 79, 307, 71, 70]","[1697548484316, 1697548484940, 1697548485492, 1697548485805, 1697548486113, 1697548486190, 1697548486266, 1697548486337, 1697548486407, 1697548486730, 1697548486789, 1697548486848, 1697548486927, 1697548487234, 1697548487305, 1697548487375]"
2170,2170,846,28,[],200,llama-7b,64,1,1097.0,1.0,1,A100,1697548501564,1697548502661,120,140.0,6.0,"[10, 582, 254, 66, 63, 62, 60]","[1697548501574, 1697548502156, 1697548502410, 1697548502476, 1697548502539, 1697548502601, 1697548502661]"
2171,2171,262,49,[],200,llama-7b,64,1,499.0,1.0,1,A100,1697548552718,1697548553217,120,39.0,1.0,"[15, 483]","[1697548552733, 1697548553216]"
2172,2172,841,51,[],200,llama-7b,64,1,2904.0,1.0,1,A100,1697548541928,1697548544832,120,123.0,15.0,"[7, 812, 62, 62, 56, 45, 55, 692, 59, 47, 57, 55, 52, 788, 55]","[1697548541935, 1697548542747, 1697548542809, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544777, 1697548544832]"
2173,2173,102,44,[],200,llama-7b,64,1,3053.0,1.0,1,A100,1697548547420,1697548550473,120,84.0,20.0,"[16, 645, 64, 49, 49, 810, 61, 50, 60, 47, 58, 55, 396, 59, 48, 47, 48, 59, 56, 312, 63]","[1697548547436, 1697548548081, 1697548548145, 1697548548194, 1697548548243, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472]"
2174,2174,546,17,[],200,llama-7b,64,1,2172.0,1.0,1,A100,1697548507663,1697548509835,120,93.0,20.0,"[13, 350, 72, 65, 64, 60, 59, 54, 42, 306, 56, 55, 49, 41, 40, 579, 57, 54, 53, 53, 50]","[1697548507676, 1697548508026, 1697548508098, 1697548508163, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835]"
2175,2175,455,36,[],200,llama-7b,64,1,2737.0,1.0,1,A100,1697548528899,1697548531636,120,91.0,20.0,"[19, 584, 54, 51, 50, 41, 41, 40, 50, 347, 55, 48, 59, 60, 51, 486, 39, 472, 65, 64, 61]","[1697548528918, 1697548529502, 1697548529556, 1697548529607, 1697548529657, 1697548529698, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530279, 1697548530338, 1697548530398, 1697548530449, 1697548530935, 1697548530974, 1697548531446, 1697548531511, 1697548531575, 1697548531636]"
2176,2176,447,41,[],200,llama-7b,64,1,1501.0,1.0,1,A100,1697548528279,1697548529780,120,161.0,13.0,"[9, 667, 51, 51, 48, 47, 39, 365, 51, 50, 41, 41, 40]","[1697548528288, 1697548528955, 1697548529006, 1697548529057, 1697548529105, 1697548529152, 1697548529191, 1697548529556, 1697548529607, 1697548529657, 1697548529698, 1697548529739, 1697548529779]"
2177,2177,256,25,[],200,llama-7b,64,1,5781.0,1.0,1,A100,1697548490221,1697548496002,120,47.0,50.0,"[59, 447, 54, 50, 48, 42, 553, 52, 47, 40, 48, 49, 329, 54, 52, 43, 51, 51, 49, 723, 58, 48, 48, 39, 356, 48, 38, 38, 312, 52, 41, 46, 432, 60, 47, 60, 55, 55, 51, 297, 49, 58, 58, 55, 53, 42, 232, 56, 53, 51, 51]","[1697548490280, 1697548490727, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491474, 1697548491526, 1697548491573, 1697548491613, 1697548491661, 1697548491710, 1697548492039, 1697548492093, 1697548492145, 1697548492188, 1697548492239, 1697548492290, 1697548492339, 1697548493062, 1697548493120, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493659, 1697548493697, 1697548493735, 1697548494047, 1697548494099, 1697548494140, 1697548494186, 1697548494618, 1697548494678, 1697548494725, 1697548494785, 1697548494840, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495408, 1697548495463, 1697548495516, 1697548495558, 1697548495790, 1697548495846, 1697548495899, 1697548495950, 1697548496001]"
2178,2178,71,47,[],200,llama-7b,64,1,2089.0,1.0,1,A100,1697548537876,1697548539965,120,364.0,11.0,"[203, 752, 53, 55, 42, 46, 45, 679, 60, 57, 53, 44]","[1697548538079, 1697548538831, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539072, 1697548539751, 1697548539811, 1697548539868, 1697548539921, 1697548539965]"
2179,2179,273,23,[],200,llama-7b,64,1,413.0,1.0,1,A100,1697548489418,1697548489831,120,19.0,1.0,"[15, 398]","[1697548489433, 1697548489831]"
2180,2180,51,24,[],200,llama-7b,64,1,4951.0,1.0,1,A100,1697548489834,1697548494785,120,364.0,36.0,"[19, 513, 42, 373, 50, 48, 42, 553, 51, 48, 40, 49, 48, 328, 54, 53, 42, 52, 50, 49, 723, 58, 48, 49, 39, 356, 48, 38, 38, 311, 52, 42, 46, 432, 60, 47, 60]","[1697548489853, 1697548490366, 1697548490408, 1697548490781, 1697548490831, 1697548490879, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491710, 1697548492038, 1697548492092, 1697548492145, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493061, 1697548493119, 1697548493167, 1697548493216, 1697548493255, 1697548493611, 1697548493659, 1697548493697, 1697548493735, 1697548494046, 1697548494098, 1697548494140, 1697548494186, 1697548494618, 1697548494678, 1697548494725, 1697548494785]"
2181,2181,770,48,[],200,llama-7b,64,1,417.0,1.0,1,A100,1697548539969,1697548540386,120,13.0,1.0,"[10, 407]","[1697548539979, 1697548540386]"
2182,2182,856,25,[],200,llama-7b,64,1,11567.0,1.0,1,A100,1697548505891,1697548517458,120,286.0,72.0,"[6, 503, 273, 65, 58, 55, 44, 581, 63, 62, 59, 56, 57, 325, 65, 64, 60, 59, 54, 42, 306, 56, 55, 49, 41, 40, 580, 56, 54, 52, 54, 49, 559, 59, 59, 47, 58, 56, 230, 57, 43, 42, 52, 51, 654, 57, 54, 53, 51, 753, 350, 59, 55, 43, 54, 833, 66, 60, 58, 58, 919, 71, 70, 64, 61, 976, 271, 68, 65, 65, 63, 294, 66]","[1697548505897, 1697548506400, 1697548506673, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507476, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508098, 1697548508163, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989, 1697548509569, 1697548509625, 1697548509679, 1697548509731, 1697548509785, 1697548509834, 1697548510393, 1697548510452, 1697548510511, 1697548510558, 1697548510616, 1697548510672, 1697548510902, 1697548510959, 1697548511002, 1697548511044, 1697548511096, 1697548511147, 1697548511801, 1697548511858, 1697548511912, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514163, 1697548514229, 1697548514289, 1697548514347, 1697548514405, 1697548515324, 1697548515395, 1697548515465, 1697548515529, 1697548515590, 1697548516566, 1697548516837, 1697548516905, 1697548516970, 1697548517035, 1697548517098, 1697548517392, 1697548517458]"
2183,2183,243,28,[],200,llama-7b,64,1,900.0,1.0,1,A100,1697548500722,1697548501622,120,67.0,4.0,"[6, 475, 295, 63, 61]","[1697548500728, 1697548501203, 1697548501498, 1697548501561, 1697548501622]"
2184,2184,832,29,[],200,llama-7b,64,1,530.0,1.0,1,A100,1697548501627,1697548502157,120,15.0,1.0,"[16, 514]","[1697548501643, 1697548502157]"
2185,2185,601,30,[],200,llama-7b,64,1,3398.0,1.0,1,A100,1697548502160,1697548505558,120,83.0,20.0,"[11, 958, 462, 63, 58, 57, 52, 42, 705, 61, 48, 60, 59, 58, 57, 351, 66, 50, 61, 60, 59]","[1697548502171, 1697548503129, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505328, 1697548505378, 1697548505439, 1697548505499, 1697548505558]"
2186,2186,712,39,[],200,llama-7b,64,1,3308.0,1.0,1,A100,1697548517102,1697548520410,120,88.0,20.0,"[16, 946, 242, 66, 65, 58, 58, 362, 63, 50, 58, 56, 45, 607, 242, 68, 62, 62, 62, 60, 60]","[1697548517118, 1697548518064, 1697548518306, 1697548518372, 1697548518437, 1697548518495, 1697548518553, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410]"
2187,2187,631,25,[],200,llama-7b,64,1,5931.0,1.0,1,A100,1697548494788,1697548500719,120,216.0,50.0,"[15, 441, 49, 58, 59, 55, 52, 42, 232, 55, 53, 51, 52, 379, 40, 40, 51, 50, 445, 69, 64, 61, 62, 57, 55, 262, 60, 48, 46, 46, 400, 61, 52, 51, 50, 351, 59, 51, 49, 827, 61, 61, 60, 54, 51, 421, 65, 51, 49, 58]","[1697548494803, 1697548495244, 1697548495293, 1697548495351, 1697548495410, 1697548495465, 1697548495517, 1697548495559, 1697548495791, 1697548495846, 1697548495899, 1697548495950, 1697548496002, 1697548496381, 1697548496421, 1697548496461, 1697548496512, 1697548496562, 1697548497007, 1697548497076, 1697548497140, 1697548497201, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497791, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498961, 1697548499788, 1697548499849, 1697548499910, 1697548499970, 1697548500024, 1697548500075, 1697548500496, 1697548500561, 1697548500612, 1697548500661, 1697548500719]"
2188,2188,95,25,[],200,llama-7b,64,1,924.0,1.0,1,A100,1697548496008,1697548496932,120,12.0,1.0,"[56, 868]","[1697548496064, 1697548496932]"
2189,2189,385,48,[],200,llama-7b,64,1,6169.0,1.0,1,A100,1697548527826,1697548533995,120,52.0,43.0,"[51, 388, 34, 656, 51, 50, 48, 47, 40, 365, 52, 48, 41, 42, 40, 49, 347, 56, 48, 59, 60, 50, 487, 39, 472, 65, 63, 62, 59, 52, 736, 52, 59, 64, 50, 55, 576, 65, 63, 50, 63, 58, 46, 311]","[1697548527877, 1697548528265, 1697548528299, 1697548528955, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529556, 1697548529608, 1697548529656, 1697548529697, 1697548529739, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530398, 1697548530448, 1697548530935, 1697548530974, 1697548531446, 1697548531511, 1697548531574, 1697548531636, 1697548531695, 1697548531747, 1697548532483, 1697548532535, 1697548532594, 1697548532658, 1697548532708, 1697548532763, 1697548533339, 1697548533404, 1697548533467, 1697548533517, 1697548533580, 1697548533638, 1697548533684, 1697548533995]"
2190,2190,793,26,[],200,llama-7b,64,1,4744.0,1.0,1,A100,1697548496935,1697548501679,120,92.0,31.0,"[16, 686, 61, 48, 46, 45, 400, 62, 51, 52, 49, 351, 60, 50, 49, 827, 61, 62, 59, 55, 51, 421, 64, 51, 49, 59, 54, 723, 64, 60, 58]","[1697548496951, 1697548497637, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498402, 1697548498451, 1697548498802, 1697548498862, 1697548498912, 1697548498961, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500497, 1697548500561, 1697548500612, 1697548500661, 1697548500720, 1697548500774, 1697548501497, 1697548501561, 1697548501621, 1697548501679]"
2191,2191,822,55,[],200,llama-7b,64,1,2631.0,1.0,1,A100,1697548553503,1697548556134,120,88.0,20.0,"[34, 897, 437, 251, 253, 58, 45, 45, 45, 58, 57, 43, 50, 69, 46, 37, 45, 42, 41, 39, 39]","[1697548553537, 1697548554434, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555523, 1697548555568, 1697548555626, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134]"
2192,2192,191,44,[],200,llama-7b,64,1,2172.0,1.0,1,A100,1697548507663,1697548509835,120,85.0,20.0,"[10, 353, 72, 65, 64, 60, 59, 54, 42, 306, 56, 55, 49, 41, 40, 579, 57, 54, 53, 53, 50]","[1697548507673, 1697548508026, 1697548508098, 1697548508163, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835]"
2193,2193,363,18,[],200,llama-7b,64,1,2607.0,1.0,1,A100,1697548495795,1697548498402,120,286.0,22.0,"[15, 515, 55, 41, 41, 50, 50, 446, 69, 64, 64, 59, 56, 55, 262, 60, 48, 46, 46, 400, 61, 52, 51]","[1697548495810, 1697548496325, 1697548496380, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497008, 1697548497077, 1697548497141, 1697548497205, 1697548497264, 1697548497320, 1697548497375, 1697548497637, 1697548497697, 1697548497745, 1697548497791, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498401]"
2194,2194,604,37,[],200,llama-7b,64,1,566.0,1.0,1,A100,1697548520975,1697548521541,120,161.0,4.0,"[15, 373, 68, 62, 48]","[1697548520990, 1697548521363, 1697548521431, 1697548521493, 1697548521541]"
2195,2195,228,20,[],200,llama-7b,64,1,3539.0,1.0,1,A100,1697548523425,1697548526964,120,100.0,20.0,"[19, 909, 236, 64, 66, 65, 59, 58, 588, 67, 65, 51, 51, 64, 66, 242, 48, 48, 60, 59, 654]","[1697548523444, 1697548524353, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525621, 1697548525672, 1697548525723, 1697548525787, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526964]"
2196,2196,25,19,[],200,llama-7b,64,1,334.0,1.0,1,A100,1697548498405,1697548498739,120,12.0,1.0,"[6, 328]","[1697548498411, 1697548498739]"
2197,2197,118,34,[],200,llama-7b,64,1,2623.0,1.0,1,A100,1697548527826,1697548530449,120,85.0,20.0,"[120, 950, 59, 51, 51, 48, 47, 39, 365, 52, 49, 41, 41, 41, 49, 347, 55, 49, 59, 59, 51]","[1697548527946, 1697548528896, 1697548528955, 1697548529006, 1697548529057, 1697548529105, 1697548529152, 1697548529191, 1697548529556, 1697548529608, 1697548529657, 1697548529698, 1697548529739, 1697548529780, 1697548529829, 1697548530176, 1697548530231, 1697548530280, 1697548530339, 1697548530398, 1697548530449]"
2198,2198,196,21,[],200,llama-7b,64,1,600.0,1.0,1,A100,1697548515469,1697548516069,120,13.0,1.0,"[19, 581]","[1697548515488, 1697548516069]"
2199,2199,895,22,[],200,llama-7b,64,1,1249.0,1.0,1,A100,1697548516073,1697548517322,120,15.0,1.0,"[25, 1224]","[1697548516098, 1697548517322]"
2200,2200,555,23,[],200,llama-7b,64,1,739.0,1.0,1,A100,1697548517326,1697548518065,120,11.0,1.0,"[26, 712]","[1697548517352, 1697548518064]"
2201,2201,319,24,[],200,llama-7b,64,1,778.0,1.0,1,A100,1697548518068,1697548518846,120,31.0,1.0,"[15, 763]","[1697548518083, 1697548518846]"
2202,2202,279,32,[],200,llama-7b,64,1,3027.0,1.0,1,A100,1697548502412,1697548505439,120,67.0,18.0,"[14, 703, 462, 63, 58, 57, 52, 42, 703, 64, 47, 60, 59, 58, 57, 351, 66, 50, 61]","[1697548502426, 1697548503129, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504566, 1697548504630, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505328, 1697548505378, 1697548505439]"
2203,2203,67,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548518850,1697548527818,120,,,"[30, 730, 184, 243, 67, 63, 62, 61, 61, 59, 337, 65, 63, 48, 49, 54, 405, 62, 48, 61, 45, 55, 708, 70, 67, 67, 57, 66, 50, 316, 67, 64, 63, 63, 61, 317, 52, 67, 51, 64, 63, 554, 64, 66, 65, 59, 58, 588, 67, 66, 51, 50, 65, 65, 242, 48, 47, 60, 59, 654, 66, 50, 64, 65, 63]","[1697548518880, 1697548519610, 1697548519794, 1697548520037, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520351, 1697548520410, 1697548520747, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521602, 1697548521647, 1697548521702, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103, 1697548523170, 1697548523234, 1697548523297, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524035, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526963, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527271]"
2204,2204,845,17,[],200,llama-7b,64,1,7022.0,1.0,1,A100,1697548502603,1697548509625,120,244.0,50.0,"[14, 513, 462, 62, 58, 57, 53, 41, 704, 62, 47, 60, 59, 59, 57, 352, 65, 50, 61, 60, 60, 283, 45, 55, 51, 679, 65, 59, 55, 43, 581, 64, 62, 59, 56, 57, 324, 65, 64, 60, 59, 55, 42, 305, 57, 54, 50, 40, 41, 579, 57]","[1697548502617, 1697548503130, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504854, 1697548504911, 1697548505263, 1697548505328, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505887, 1697548505942, 1697548505993, 1697548506672, 1697548506737, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508747, 1697548508804, 1697548508858, 1697548508908, 1697548508948, 1697548508989, 1697548509568, 1697548509625]"
2205,2205,833,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548537117,1697548537857,120,,,"[14, 637]","[1697548537131, 1697548537768]"
2206,2206,701,35,[],200,llama-7b,64,1,6596.0,1.0,1,A100,1697548530453,1697548537049,120,58.0,43.0,"[15, 899, 80, 65, 62, 62, 59, 52, 738, 52, 58, 64, 51, 55, 575, 64, 63, 50, 66, 55, 46, 311, 60, 45, 46, 58, 59, 367, 46, 59, 59, 56, 708, 59, 60, 57, 55, 668, 63, 48, 55, 314, 52, 60]","[1697548530468, 1697548531367, 1697548531447, 1697548531512, 1697548531574, 1697548531636, 1697548531695, 1697548531747, 1697548532485, 1697548532537, 1697548532595, 1697548532659, 1697548532710, 1697548532765, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533583, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534204, 1697548534263, 1697548534630, 1697548534676, 1697548534735, 1697548534794, 1697548534850, 1697548535558, 1697548535617, 1697548535677, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049]"
2207,2207,602,28,[],200,llama-7b,64,1,961.0,1.0,1,A100,1697548537869,1697548538830,120,15.0,1.0,"[90, 871]","[1697548537959, 1697548538830]"
2208,2208,261,29,[],200,llama-7b,64,1,918.0,1.0,1,A100,1697548538833,1697548539751,120,874.0,2.0,"[18, 900]","[1697548538851, 1697548539751]"
2209,2209,30,30,[],200,llama-7b,64,1,3273.0,1.0,1,A100,1697548539754,1697548543027,120,93.0,20.0,"[6, 625, 74, 50, 58, 47, 57, 57, 52, 348, 49, 749, 61, 60, 50, 650, 62, 61, 57, 45, 55]","[1697548539760, 1697548540385, 1697548540459, 1697548540509, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540780, 1697548541128, 1697548541177, 1697548541926, 1697548541987, 1697548542047, 1697548542097, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542972, 1697548543027]"
2210,2210,593,56,[],200,llama-7b,64,1,1671.0,1.0,1,A100,1697548553955,1697548555626,120,335.0,9.0,"[6, 473, 437, 251, 253, 59, 44, 45, 45, 58]","[1697548553961, 1697548554434, 1697548554871, 1697548555122, 1697548555375, 1697548555434, 1697548555478, 1697548555523, 1697548555568, 1697548555626]"
2211,2211,366,36,[],200,llama-7b,64,1,1624.0,1.0,1,A100,1697548508992,1697548510616,120,85.0,6.0,"[6, 1329, 66, 60, 59, 45, 59]","[1697548508998, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510557, 1697548510616]"
2212,2212,597,44,[],200,llama-7b,64,1,1207.0,1.0,1,A100,1697548524042,1697548525249,120,39.0,1.0,"[13, 1194]","[1697548524055, 1697548525249]"
2213,2213,166,19,[],200,llama-7b,64,1,461.0,1.0,1,A100,1697548490884,1697548491345,120,14.0,1.0,"[19, 442]","[1697548490903, 1697548491345]"
2214,2214,754,20,[],200,llama-7b,64,1,942.0,1.0,1,A100,1697548491348,1697548492290,120,88.0,7.0,"[19, 671, 54, 53, 43, 51, 51]","[1697548491367, 1697548492038, 1697548492092, 1697548492145, 1697548492188, 1697548492239, 1697548492290]"
2215,2215,431,49,[],200,llama-7b,64,1,4497.0,1.0,1,A100,1697548540388,1697548544885,120,732.0,22.0,"[17, 670, 54, 48, 750, 59, 61, 50, 650, 63, 61, 56, 45, 55, 692, 59, 47, 57, 55, 52, 787, 56, 53]","[1697548540405, 1697548541075, 1697548541129, 1697548541177, 1697548541927, 1697548541986, 1697548542047, 1697548542097, 1697548542747, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776, 1697548544832, 1697548544885]"
2216,2216,235,38,[],200,llama-7b,64,1,1753.0,1.0,1,A100,1697548521545,1697548523298,120,161.0,12.0,"[17, 848, 70, 68, 66, 57, 66, 50, 316, 67, 64, 64]","[1697548521562, 1697548522410, 1697548522480, 1697548522548, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103, 1697548523170, 1697548523234, 1697548523298]"
2217,2217,524,21,[],200,llama-7b,64,1,3554.0,1.0,1,A100,1697548492293,1697548495847,120,100.0,30.0,"[6, 402, 360, 59, 48, 48, 39, 356, 49, 37, 39, 312, 50, 42, 47, 431, 61, 47, 59, 56, 54, 51, 297, 49, 58, 59, 54, 53, 42, 232, 56]","[1697548492299, 1697548492701, 1697548493061, 1697548493120, 1697548493168, 1697548493216, 1697548493255, 1697548493611, 1697548493660, 1697548493697, 1697548493736, 1697548494048, 1697548494098, 1697548494140, 1697548494187, 1697548494618, 1697548494679, 1697548494726, 1697548494785, 1697548494841, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495350, 1697548495409, 1697548495463, 1697548495516, 1697548495558, 1697548495790, 1697548495846]"
2218,2218,3,39,[],200,llama-7b,64,1,2554.0,1.0,1,A100,1697548523301,1697548525855,120,89.0,20.0,"[15, 350, 73, 51, 67, 52, 63, 64, 553, 64, 66, 65, 59, 58, 589, 67, 65, 51, 50, 65, 67]","[1697548523316, 1697548523666, 1697548523739, 1697548523790, 1697548523857, 1697548523909, 1697548523972, 1697548524036, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525490, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525855]"
2219,2219,356,28,[],200,llama-7b,64,1,974.0,1.0,1,A100,1697548503594,1697548504568,120,874.0,2.0,"[11, 707, 256]","[1697548503605, 1697548504312, 1697548504568]"
2220,2220,262,31,[],200,llama-7b,64,1,838.0,1.0,1,A100,1697548505562,1697548506400,120,39.0,1.0,"[29, 809]","[1697548505591, 1697548506400]"
2221,2221,37,32,[],200,llama-7b,64,1,878.0,1.0,1,A100,1697548506404,1697548507282,120,20.0,1.0,"[19, 859]","[1697548506423, 1697548507282]"
2222,2222,620,33,[],200,llama-7b,64,1,1158.0,1.0,1,A100,1697548507284,1697548508442,120,100.0,8.0,"[10, 732, 71, 65, 64, 61, 58, 55, 42]","[1697548507294, 1697548508026, 1697548508097, 1697548508162, 1697548508226, 1697548508287, 1697548508345, 1697548508400, 1697548508442]"
2223,2223,426,24,[],200,llama-7b,64,1,5954.0,1.0,1,A100,1697548478630,1697548484584,120,79.0,36.0,"[24, 648, 255, 71, 55, 70, 68, 63, 911, 76, 75, 72, 71, 68, 67, 587, 270, 63, 74, 265, 67, 52, 68, 838, 76, 55, 72, 66, 66, 49, 338, 76, 79, 71, 72, 56]","[1697548478654, 1697548479302, 1697548479557, 1697548479628, 1697548479683, 1697548479753, 1697548479821, 1697548479884, 1697548480795, 1697548480871, 1697548480946, 1697548481018, 1697548481089, 1697548481157, 1697548481224, 1697548481811, 1697548482081, 1697548482144, 1697548482218, 1697548482483, 1697548482550, 1697548482602, 1697548482670, 1697548483508, 1697548483584, 1697548483639, 1697548483711, 1697548483777, 1697548483843, 1697548483892, 1697548484230, 1697548484306, 1697548484385, 1697548484456, 1697548484528, 1697548484584]"
2224,2224,367,34,[],200,llama-7b,64,1,1340.0,1.0,1,A100,1697548508445,1697548509785,120,92.0,6.0,"[15, 979, 129, 57, 54, 53, 53]","[1697548508460, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785]"
2225,2225,98,29,[],200,llama-7b,64,1,622.0,1.0,1,A100,1697548504571,1697548505193,120,14.0,1.0,"[23, 599]","[1697548504594, 1697548505193]"
2226,2226,681,30,[],200,llama-7b,64,1,581.0,1.0,1,A100,1697548505198,1697548505779,120,23.0,1.0,"[20, 561]","[1697548505218, 1697548505779]"
2227,2227,38,50,[],200,llama-7b,64,1,2755.0,1.0,1,A100,1697548553219,1697548555974,120,88.0,20.0,"[12, 550, 66, 60, 45, 59, 860, 250, 253, 59, 45, 45, 44, 58, 58, 43, 50, 69, 47, 35, 47]","[1697548553231, 1697548553781, 1697548553847, 1697548553907, 1697548553952, 1697548554011, 1697548554871, 1697548555121, 1697548555374, 1697548555433, 1697548555478, 1697548555523, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555892, 1697548555927, 1697548555974]"
2228,2228,458,31,[],200,llama-7b,64,1,617.0,1.0,1,A100,1697548505783,1697548506400,120,11.0,1.0,"[18, 599]","[1697548505801, 1697548506400]"
2229,2229,155,22,[],200,llama-7b,64,1,2450.0,1.0,1,A100,1697548495849,1697548498299,120,90.0,20.0,"[7, 469, 55, 41, 41, 50, 50, 447, 67, 65, 59, 64, 56, 55, 262, 61, 47, 47, 45, 400, 62]","[1697548495856, 1697548496325, 1697548496380, 1697548496421, 1697548496462, 1697548496512, 1697548496562, 1697548497009, 1697548497076, 1697548497141, 1697548497200, 1697548497264, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497745, 1697548497792, 1697548497837, 1697548498237, 1697548498299]"
2230,2230,783,60,[],200,llama-7b,64,1,660.0,1.0,1,A100,1697548547421,1697548548081,120,286.0,1.0,"[30, 630]","[1697548547451, 1697548548081]"
2231,2231,553,61,[],200,llama-7b,64,1,2546.0,1.0,1,A100,1697548548084,1697548550630,120,88.0,20.0,"[16, 660, 292, 61, 52, 58, 48, 57, 56, 395, 60, 47, 47, 48, 60, 55, 313, 64, 55, 48, 54]","[1697548548100, 1697548548760, 1697548549052, 1697548549113, 1697548549165, 1697548549223, 1697548549271, 1697548549328, 1697548549384, 1697548549779, 1697548549839, 1697548549886, 1697548549933, 1697548549981, 1697548550041, 1697548550096, 1697548550409, 1697548550473, 1697548550528, 1697548550576, 1697548550630]"
2232,2232,113,32,[],200,llama-7b,64,1,879.0,1.0,1,A100,1697548506404,1697548507283,120,13.0,1.0,"[17, 861]","[1697548506421, 1697548507282]"
2233,2233,202,25,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 7.92 GiB. GPU 0 has a total capacty of 39.39 GiB of which 7.74 GiB is free. Process 1412106 has 31.65 GiB memory in use. Of the allocated memory 23.36 GiB is allocated by PyTorch, and 6.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548484588,1697548488027,120,,,"[6, 2053, 83, 59, 60, 78, 307, 72, 70]","[1697548484594, 1697548486647, 1697548486730, 1697548486789, 1697548486849, 1697548486927, 1697548487234, 1697548487306, 1697548487376]"
2234,2234,801,45,[],200,llama-7b,64,1,2533.0,1.0,1,A100,1697548550476,1697548553009,120,47.0,20.0,"[35, 429, 62, 52, 52, 50, 49, 424, 63, 57, 44, 44, 56, 55, 712, 55, 65, 50, 50, 65, 64]","[1697548550511, 1697548550940, 1697548551002, 1697548551054, 1697548551106, 1697548551156, 1697548551205, 1697548551629, 1697548551692, 1697548551749, 1697548551793, 1697548551837, 1697548551893, 1697548551948, 1697548552660, 1697548552715, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009]"
2235,2235,812,33,[],200,llama-7b,64,1,737.0,1.0,1,A100,1697548507289,1697548508026,120,16.0,1.0,"[16, 721]","[1697548507305, 1697548508026]"
2236,2236,787,26,[],200,llama-7b,64,1,1265.0,1.0,1,A100,1697548488033,1697548489298,120,123.0,6.0,"[119, 680, 57, 51, 36, 36, 286]","[1697548488152, 1697548488832, 1697548488889, 1697548488940, 1697548488976, 1697548489012, 1697548489298]"
2237,2237,214,24,[],200,llama-7b,64,1,2182.0,1.0,1,A100,1697548488033,1697548490215,120,52.0,20.0,"[144, 655, 56, 52, 35, 38, 284, 68, 49, 49, 41, 47, 40, 305, 51, 43, 43, 51, 48, 41, 42]","[1697548488177, 1697548488832, 1697548488888, 1697548488940, 1697548488975, 1697548489013, 1697548489297, 1697548489365, 1697548489414, 1697548489463, 1697548489504, 1697548489551, 1697548489591, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490084, 1697548490132, 1697548490173, 1697548490215]"
2238,2238,472,34,[],200,llama-7b,64,1,2874.0,1.0,1,A100,1697548508029,1697548510903,120,85.0,20.0,"[16, 634, 69, 57, 54, 49, 41, 40, 579, 58, 53, 53, 53, 49, 559, 60, 59, 46, 58, 56, 231]","[1697548508045, 1697548508679, 1697548508748, 1697548508805, 1697548508859, 1697548508908, 1697548508949, 1697548508989, 1697548509568, 1697548509626, 1697548509679, 1697548509732, 1697548509785, 1697548509834, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510672, 1697548510903]"
2239,2239,797,25,[],200,llama-7b,64,1,506.0,1.0,1,A100,1697548490221,1697548490727,120,26.0,1.0,"[53, 453]","[1697548490274, 1697548490727]"
2240,2240,199,32,[],200,llama-7b,64,1,651.0,1.0,1,A100,1697548510908,1697548511559,120,13.0,1.0,"[18, 633]","[1697548510926, 1697548511559]"
2241,2241,573,26,[],200,llama-7b,64,1,745.0,1.0,1,A100,1697548490730,1697548491475,120,874.0,2.0,"[14, 731]","[1697548490744, 1697548491475]"
2242,2242,228,27,[],200,llama-7b,64,1,2664.0,1.0,1,A100,1697548491477,1697548494141,120,100.0,20.0,"[10, 493, 59, 54, 52, 43, 52, 50, 49, 722, 58, 49, 48, 39, 357, 48, 38, 37, 312, 52, 42]","[1697548491487, 1697548491980, 1697548492039, 1697548492093, 1697548492145, 1697548492188, 1697548492240, 1697548492290, 1697548492339, 1697548493061, 1697548493119, 1697548493168, 1697548493216, 1697548493255, 1697548493612, 1697548493660, 1697548493698, 1697548493735, 1697548494047, 1697548494099, 1697548494141]"
2243,2243,896,33,[],200,llama-7b,64,1,896.0,1.0,1,A100,1697548511563,1697548512459,120,15.0,1.0,"[19, 877]","[1697548511582, 1697548512459]"
2244,2244,550,34,[],200,llama-7b,64,1,5061.0,1.0,1,A100,1697548512462,1697548517523,120,91.0,20.0,"[20, 1300, 382, 66, 60, 58, 58, 918, 71, 72, 63, 60, 977, 270, 69, 65, 64, 64, 293, 67, 64]","[1697548512482, 1697548513782, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406, 1697548515324, 1697548515395, 1697548515467, 1697548515530, 1697548515590, 1697548516567, 1697548516837, 1697548516906, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517459, 1697548517523]"
2245,2245,312,4,[],200,llama-7b,64,1,1208.0,1.0,1,A100,1697548488033,1697548489241,120,23.0,1.0,"[216, 992]","[1697548488249, 1697548489241]"
2246,2246,907,50,[],200,llama-7b,64,1,924.0,1.0,1,A100,1697548550636,1697548551560,120,10.0,1.0,"[19, 904]","[1697548550655, 1697548551559]"
2247,2247,655,51,[],200,llama-7b,64,1,1817.0,1.0,1,A100,1697548551562,1697548553379,120,335.0,11.0,"[11, 713, 374, 54, 66, 50, 50, 64, 64, 275, 48, 48]","[1697548551573, 1697548552286, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552944, 1697548553008, 1697548553283, 1697548553331, 1697548553379]"
2248,2248,6,28,[],200,llama-7b,64,1,3057.0,1.0,1,A100,1697548494144,1697548497201,120,100.0,29.0,"[15, 460, 60, 47, 60, 55, 54, 52, 296, 49, 59, 58, 55, 53, 41, 234, 55, 52, 52, 52, 378, 40, 40, 51, 49, 446, 69, 64, 61]","[1697548494159, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494947, 1697548495243, 1697548495292, 1697548495351, 1697548495409, 1697548495464, 1697548495517, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496003, 1697548496381, 1697548496421, 1697548496461, 1697548496512, 1697548496561, 1697548497007, 1697548497076, 1697548497140, 1697548497201]"
2249,2249,89,5,[],200,llama-7b,64,1,2466.0,1.0,1,A100,1697548489244,1697548491710,120,52.0,20.0,"[21, 565, 66, 51, 43, 43, 50, 49, 41, 41, 193, 374, 50, 49, 41, 553, 51, 48, 40, 48, 49]","[1697548489265, 1697548489830, 1697548489896, 1697548489947, 1697548489990, 1697548490033, 1697548490083, 1697548490132, 1697548490173, 1697548490214, 1697548490407, 1697548490781, 1697548490831, 1697548490880, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491661, 1697548491710]"
2250,2250,250,45,[],200,llama-7b,64,1,775.0,1.0,1,A100,1697548525254,1697548526029,120,31.0,1.0,"[29, 746]","[1697548525283, 1697548526029]"
2251,2251,28,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526032,1697548527820,120,,,"[14, 664, 254, 65, 50, 64, 66, 63]","[1697548526046, 1697548526710, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527209, 1697548527272]"
2252,2252,307,52,[],200,llama-7b,64,1,398.0,1.0,1,A100,1697548553383,1697548553781,120,26.0,1.0,"[6, 392]","[1697548553389, 1697548553781]"
2253,2253,611,47,[],200,llama-7b,64,1,440.0,1.0,1,A100,1697548527826,1697548528266,120,14.0,1.0,"[66, 374]","[1697548527892, 1697548528266]"
2254,2254,83,53,[],200,llama-7b,64,1,2145.0,1.0,1,A100,1697548553783,1697548555928,120,123.0,15.0,"[10, 640, 438, 251, 253, 58, 45, 45, 45, 58, 57, 44, 49, 69, 46, 37]","[1697548553793, 1697548554433, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555523, 1697548555568, 1697548555626, 1697548555683, 1697548555727, 1697548555776, 1697548555845, 1697548555891, 1697548555928]"
2255,2255,389,48,[],200,llama-7b,64,1,625.0,1.0,1,A100,1697548528271,1697548528896,120,8.0,1.0,"[10, 615]","[1697548528281, 1697548528896]"
2256,2256,723,20,[],200,llama-7b,64,1,675.0,1.0,1,A100,1697548498745,1697548499420,120,14.0,1.0,"[31, 643]","[1697548498776, 1697548499419]"
2257,2257,381,21,[],200,llama-7b,64,1,1075.0,1.0,1,A100,1697548499422,1697548500497,120,140.0,2.0,"[18, 985, 72]","[1697548499440, 1697548500425, 1697548500497]"
2258,2258,4,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526966,1697548527821,120,,,"[6, 605]","[1697548526972, 1697548527577]"
2259,2259,586,22,[],200,llama-7b,64,1,2624.0,1.0,1,A100,1697548527826,1697548530450,120,85.0,20.0,"[133, 937, 58, 52, 50, 48, 48, 39, 364, 53, 49, 41, 41, 41, 49, 347, 55, 49, 59, 59, 52]","[1697548527959, 1697548528896, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529152, 1697548529191, 1697548529555, 1697548529608, 1697548529657, 1697548529698, 1697548529739, 1697548529780, 1697548529829, 1697548530176, 1697548530231, 1697548530280, 1697548530339, 1697548530398, 1697548530450]"
2260,2260,871,46,[],200,llama-7b,64,1,793.0,1.0,1,A100,1697548550412,1697548551205,120,123.0,6.0,"[6, 522, 61, 53, 51, 51, 48]","[1697548550418, 1697548550940, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551204]"
2261,2261,640,47,[],200,llama-7b,64,1,1079.0,1.0,1,A100,1697548551208,1697548552287,120,15.0,1.0,"[19, 1060]","[1697548551227, 1697548552287]"
2262,2262,366,23,[],200,llama-7b,64,1,1243.0,1.0,1,A100,1697548530453,1697548531696,120,85.0,6.0,"[20, 894, 80, 64, 64, 61, 59]","[1697548530473, 1697548531367, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531695]"
2263,2263,137,35,[],200,llama-7b,64,1,3334.0,1.0,1,A100,1697548509787,1697548513121,120,86.0,20.0,"[7, 533, 66, 60, 59, 46, 59, 56, 230, 56, 43, 43, 51, 51, 656, 56, 54, 52, 56, 749, 351]","[1697548509794, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510617, 1697548510673, 1697548510903, 1697548510959, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511803, 1697548511859, 1697548511913, 1697548511965, 1697548512021, 1697548512770, 1697548513121]"
2264,2264,587,29,[],200,llama-7b,64,1,365.0,1.0,1,A100,1697548497207,1697548497572,120,13.0,1.0,"[10, 355]","[1697548497217, 1697548497572]"
2265,2265,196,25,[],200,llama-7b,64,1,450.0,1.0,1,A100,1697548508229,1697548508679,120,13.0,1.0,"[15, 435]","[1697548508244, 1697548508679]"
2266,2266,359,30,[],200,llama-7b,64,1,594.0,1.0,1,A100,1697548497577,1697548498171,120,10.0,1.0,"[32, 562]","[1697548497609, 1697548498171]"
2267,2267,787,26,[],200,llama-7b,64,1,1105.0,1.0,1,A100,1697548508680,1697548509785,120,123.0,6.0,"[15, 744, 129, 57, 54, 53, 53]","[1697548508695, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785]"
2268,2268,556,27,[],200,llama-7b,64,1,538.0,1.0,1,A100,1697548509789,1697548510327,120,9.0,1.0,"[10, 528]","[1697548509799, 1697548510327]"
2269,2269,454,27,[],200,llama-7b,64,1,979.0,1.0,1,A100,1697548501682,1697548502661,120,182.0,6.0,"[6, 468, 255, 65, 63, 62, 60]","[1697548501688, 1697548502156, 1697548502411, 1697548502476, 1697548502539, 1697548502601, 1697548502661]"
2270,2270,215,28,[],200,llama-7b,64,1,508.0,1.0,1,A100,1697548510331,1697548510839,120,12.0,1.0,"[18, 490]","[1697548510349, 1697548510839]"
2271,2271,734,24,[],200,llama-7b,64,1,1004.0,1.0,1,A100,1697548493614,1697548494618,120,100.0,6.0,"[27, 346, 61, 51, 42, 46, 431]","[1697548493641, 1697548493987, 1697548494048, 1697548494099, 1697548494141, 1697548494187, 1697548494618]"
2272,2272,892,29,[],200,llama-7b,64,1,4624.0,1.0,1,A100,1697548510842,1697548515466,120,87.0,20.0,"[10, 707, 242, 58, 53, 53, 55, 749, 350, 59, 56, 42, 54, 834, 66, 60, 58, 58, 919, 71, 70]","[1697548510852, 1697548511559, 1697548511801, 1697548511859, 1697548511912, 1697548511965, 1697548512020, 1697548512769, 1697548513119, 1697548513178, 1697548513234, 1697548513276, 1697548513330, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406, 1697548515325, 1697548515396, 1697548515466]"
2273,2273,215,62,[],200,llama-7b,64,1,925.0,1.0,1,A100,1697548550635,1697548551560,120,12.0,1.0,"[38, 887]","[1697548550673, 1697548551560]"
2274,2274,320,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548533686,1697548537855,120,,,"[6, 938, 47, 59, 59, 56, 707, 59, 59, 58, 55, 668, 63, 49, 54, 314, 52, 60, 66, 63, 61, 46]","[1697548533692, 1697548534630, 1697548534677, 1697548534736, 1697548534795, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536569, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537239, 1697548537285]"
2275,2275,707,31,[],200,llama-7b,64,1,1491.0,1.0,1,A100,1697548543031,1697548544522,120,8.0,1.0,"[14, 1477]","[1697548543045, 1697548544522]"
2276,2276,565,38,[],200,llama-7b,64,1,3766.0,1.0,1,A100,1697548501792,1697548505558,120,91.0,20.0,"[15, 1322, 462, 63, 58, 57, 52, 42, 705, 61, 48, 60, 59, 58, 57, 351, 65, 51, 61, 60, 59]","[1697548501807, 1697548503129, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503821, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505558]"
2277,2277,151,22,[],200,llama-7b,64,1,705.0,1.0,1,A100,1697548500499,1697548501204,120,39.0,1.0,"[7, 698]","[1697548500506, 1697548501204]"
2278,2278,828,23,[],200,llama-7b,64,1,1455.0,1.0,1,A100,1697548501206,1697548502661,120,182.0,6.0,"[16, 1188, 65, 64, 62, 60]","[1697548501222, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502661]"
2279,2279,484,24,[],200,llama-7b,64,1,5434.0,1.0,1,A100,1697548502663,1697548508097,120,86.0,36.0,"[13, 916, 62, 58, 57, 53, 41, 704, 62, 47, 60, 59, 59, 57, 352, 65, 50, 61, 60, 60, 283, 45, 55, 52, 678, 65, 59, 55, 43, 581, 64, 62, 59, 56, 57, 324]","[1697548502676, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504854, 1697548504911, 1697548505263, 1697548505328, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505887, 1697548505942, 1697548505994, 1697548506672, 1697548506737, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097]"
2280,2280,890,40,[],200,llama-7b,64,1,2865.0,1.0,1,A100,1697548537863,1697548540728,120,93.0,20.0,"[31, 280, 27, 683, 55, 41, 47, 45, 678, 61, 55, 54, 44, 52, 42, 400, 50, 59, 47, 57, 57]","[1697548537894, 1697548538174, 1697548538201, 1697548538884, 1697548538939, 1697548538980, 1697548539027, 1697548539072, 1697548539750, 1697548539811, 1697548539866, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728]"
2281,2281,541,19,[],200,llama-7b,64,1,3008.0,1.0,1,A100,1697548522482,1697548525490,120,90.0,20.0,"[9, 539, 74, 66, 64, 64, 62, 62, 317, 51, 67, 51, 63, 64, 554, 65, 65, 65, 59, 58, 588]","[1697548522491, 1697548523030, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422, 1697548523739, 1697548523790, 1697548523857, 1697548523908, 1697548523971, 1697548524035, 1697548524589, 1697548524654, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489]"
2282,2282,160,42,[],200,llama-7b,64,1,661.0,1.0,1,A100,1697548547420,1697548548081,120,13.0,1.0,"[20, 641]","[1697548547440, 1697548548081]"
2283,2283,755,31,[],200,llama-7b,64,1,3882.0,1.0,1,A100,1697548503657,1697548507539,120,286.0,25.0,"[6, 650, 255, 61, 48, 60, 59, 58, 57, 351, 65, 51, 61, 60, 60, 283, 46, 54, 51, 679, 65, 59, 55, 43, 581, 64]","[1697548503663, 1697548504313, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505888, 1697548505942, 1697548505993, 1697548506672, 1697548506737, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539]"
2284,2284,225,28,[],200,llama-7b,64,1,465.0,1.0,1,A100,1697548502665,1697548503130,120,23.0,1.0,"[17, 448]","[1697548502682, 1697548503130]"
2285,2285,914,63,[],200,llama-7b,64,1,3812.0,1.0,1,A100,1697548551563,1697548555375,120,84.0,20.0,"[9, 715, 373, 54, 66, 50, 50, 64, 64, 274, 49, 48, 60, 60, 348, 60, 46, 58, 860, 251, 253]","[1697548551572, 1697548552287, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552944, 1697548553008, 1697548553282, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553907, 1697548553953, 1697548554011, 1697548554871, 1697548555122, 1697548555375]"
2286,2286,808,29,[],200,llama-7b,64,1,1433.0,1.0,1,A100,1697548503134,1697548504567,120,286.0,2.0,"[15, 1164, 254]","[1697548503149, 1697548504313, 1697548504567]"
2287,2287,584,30,[],200,llama-7b,64,1,622.0,1.0,1,A100,1697548504571,1697548505193,120,10.0,1.0,"[36, 586]","[1697548504607, 1697548505193]"
2288,2288,236,31,[],200,llama-7b,64,1,581.0,1.0,1,A100,1697548505198,1697548505779,120,8.0,1.0,"[16, 565]","[1697548505214, 1697548505779]"
2289,2289,363,32,[],200,llama-7b,64,1,3717.0,1.0,1,A100,1697548544525,1697548548242,120,286.0,22.0,"[16, 973, 369, 71, 65, 64, 63, 62, 47, 62, 761, 65, 65, 58, 48, 58, 46, 255, 54, 47, 370, 50, 48]","[1697548544541, 1697548545514, 1697548545883, 1697548545954, 1697548546019, 1697548546083, 1697548546146, 1697548546208, 1697548546255, 1697548546317, 1697548547078, 1697548547143, 1697548547208, 1697548547266, 1697548547314, 1697548547372, 1697548547418, 1697548547673, 1697548547727, 1697548547774, 1697548548144, 1697548548194, 1697548548242]"
2290,2290,9,32,[],200,llama-7b,64,1,2966.0,1.0,1,A100,1697548505782,1697548508748,120,85.0,20.0,"[20, 598, 272, 66, 58, 55, 44, 580, 64, 62, 59, 56, 57, 325, 64, 65, 60, 59, 54, 42, 306]","[1697548505802, 1697548506400, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508098, 1697548508162, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748]"
2291,2291,97,36,[],200,llama-7b,64,1,2904.0,1.0,1,A100,1697548537876,1697548540780,120,6.0,20.0,"[161, 794, 54, 55, 42, 44, 47, 677, 62, 55, 53, 45, 52, 41, 401, 49, 60, 47, 56, 57, 52]","[1697548538037, 1697548538831, 1697548538885, 1697548538940, 1697548538982, 1697548539026, 1697548539073, 1697548539750, 1697548539812, 1697548539867, 1697548539920, 1697548539965, 1697548540017, 1697548540058, 1697548540459, 1697548540508, 1697548540568, 1697548540615, 1697548540671, 1697548540728, 1697548540780]"
2292,2292,128,33,[],200,llama-7b,64,1,1453.0,1.0,1,A100,1697548548246,1697548549699,120,9.0,1.0,"[13, 1440]","[1697548548259, 1697548549699]"
2293,2293,592,33,[],200,llama-7b,64,1,8830.0,1.0,1,A100,1697548508751,1697548517581,120,15.0,50.0,"[22, 666, 129, 57, 54, 53, 53, 50, 559, 58, 60, 45, 59, 56, 231, 57, 42, 43, 52, 50, 655, 56, 54, 53, 52, 752, 350, 59, 55, 43, 54, 834, 65, 61, 57, 58, 919, 72, 69, 65, 60, 976, 271, 68, 65, 65, 63, 294, 66, 64, 59]","[1697548508773, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835, 1697548510394, 1697548510452, 1697548510512, 1697548510557, 1697548510616, 1697548510672, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511097, 1697548511147, 1697548511802, 1697548511858, 1697548511912, 1697548511965, 1697548512017, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514164, 1697548514229, 1697548514290, 1697548514347, 1697548514405, 1697548515324, 1697548515396, 1697548515465, 1697548515530, 1697548515590, 1697548516566, 1697548516837, 1697548516905, 1697548516970, 1697548517035, 1697548517098, 1697548517392, 1697548517458, 1697548517522, 1697548517581]"
2294,2294,218,39,[],200,llama-7b,64,1,1913.0,1.0,1,A100,1697548505562,1697548507475,120,109.0,7.0,"[15, 1095, 66, 58, 55, 44, 580]","[1697548505577, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507475]"
2295,2295,719,34,[],200,llama-7b,64,1,928.0,1.0,1,A100,1697548549702,1697548550630,120,182.0,6.0,"[7, 700, 63, 56, 48, 54]","[1697548549709, 1697548550409, 1697548550472, 1697548550528, 1697548550576, 1697548550630]"
2296,2296,923,40,[],200,llama-7b,64,1,865.0,1.0,1,A100,1697548507481,1697548508346,120,140.0,6.0,"[8, 537, 72, 64, 64, 61, 59]","[1697548507489, 1697548508026, 1697548508098, 1697548508162, 1697548508226, 1697548508287, 1697548508346]"
2297,2297,486,35,[],200,llama-7b,64,1,2865.0,1.0,1,A100,1697548550635,1697548553500,120,14.0,20.0,"[33, 892, 70, 62, 57, 45, 44, 55, 55, 712, 53, 67, 49, 50, 65, 64, 275, 49, 47, 60, 61]","[1697548550668, 1697548551560, 1697548551630, 1697548551692, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552713, 1697548552780, 1697548552829, 1697548552879, 1697548552944, 1697548553008, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553500]"
2298,2298,680,37,[],200,llama-7b,64,1,2244.0,1.0,1,A100,1697548540783,1697548543027,120,123.0,11.0,"[16, 791, 337, 60, 59, 51, 651, 61, 61, 57, 44, 55]","[1697548540799, 1697548541590, 1697548541927, 1697548541987, 1697548542046, 1697548542097, 1697548542748, 1697548542809, 1697548542870, 1697548542927, 1697548542971, 1697548543026]"
2299,2299,456,38,[],200,llama-7b,64,1,4388.0,1.0,1,A100,1697548543029,1697548547417,120,90.0,20.0,"[20, 1473, 254, 56, 53, 47, 952, 71, 65, 64, 62, 61, 48, 62, 760, 65, 64, 59, 49, 57, 46]","[1697548543049, 1697548544522, 1697548544776, 1697548544832, 1697548544885, 1697548544932, 1697548545884, 1697548545955, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547371, 1697548547417]"
2300,2300,147,36,[],200,llama-7b,64,1,932.0,1.0,1,A100,1697548553502,1697548554434,120,182.0,1.0,"[28, 904]","[1697548553530, 1697548554434]"
2301,2301,263,31,[],200,llama-7b,64,1,436.0,1.0,1,A100,1697548532713,1697548533149,120,15.0,1.0,"[20, 416]","[1697548532733, 1697548533149]"
2302,2302,593,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548525858,1697548527818,120,,,"[10, 842, 254, 65, 50, 64, 65, 64]","[1697548525868, 1697548526710, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2303,2303,363,34,[],200,llama-7b,64,1,3228.0,1.0,1,A100,1697548517584,1697548520812,120,286.0,22.0,"[6, 475, 242, 66, 64, 59, 58, 361, 62, 50, 58, 57, 45, 607, 242, 68, 62, 63, 62, 59, 61, 335, 66]","[1697548517590, 1697548518065, 1697548518307, 1697548518373, 1697548518437, 1697548518496, 1697548518554, 1697548518915, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520229, 1697548520291, 1697548520350, 1697548520411, 1697548520746, 1697548520812]"
2304,2304,488,26,[],200,llama-7b,64,1,603.0,1.0,1,A100,1697548517462,1697548518065,120,6.0,1.0,"[9, 594]","[1697548517471, 1697548518065]"
2305,2305,161,49,[],200,llama-7b,64,1,1562.0,1.0,1,A100,1697548533997,1697548535559,120,109.0,7.0,"[6, 627, 47, 59, 58, 57, 708]","[1697548534003, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535559]"
2306,2306,362,41,[],200,llama-7b,64,1,1070.0,1.0,1,A100,1697548527827,1697548528897,120,14.0,1.0,"[215, 855]","[1697548528042, 1697548528897]"
2307,2307,18,42,[],200,llama-7b,64,1,604.0,1.0,1,A100,1697548528898,1697548529502,120,15.0,1.0,"[23, 581]","[1697548528921, 1697548529502]"
2308,2308,742,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548535563,1697548537858,120,,,"[19, 691, 184, 64, 47, 55, 315, 51, 60, 66, 63, 60, 47]","[1697548535582, 1697548536273, 1697548536457, 1697548536521, 1697548536568, 1697548536623, 1697548536938, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
2309,2309,22,24,[],200,llama-7b,64,1,419.0,1.0,1,A100,1697548531700,1697548532119,120,16.0,1.0,"[15, 404]","[1697548531715, 1697548532119]"
2310,2310,499,29,[],200,llama-7b,64,1,2895.0,1.0,1,A100,1697548502664,1697548505559,120,88.0,20.0,"[16, 450, 462, 62, 58, 58, 52, 42, 703, 62, 47, 60, 59, 59, 57, 352, 64, 51, 61, 60, 60]","[1697548502680, 1697548503130, 1697548503592, 1697548503654, 1697548503712, 1697548503770, 1697548503822, 1697548503864, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504854, 1697548504911, 1697548505263, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559]"
2311,2311,720,43,[],200,llama-7b,64,1,893.0,1.0,1,A100,1697548529505,1697548530398,120,286.0,6.0,"[10, 592, 69, 56, 47, 60, 59]","[1697548529515, 1697548530107, 1697548530176, 1697548530232, 1697548530279, 1697548530339, 1697548530398]"
2312,2312,259,27,[],200,llama-7b,64,1,2905.0,1.0,1,A100,1697548518068,1697548520973,120,87.0,20.0,"[26, 752, 69, 63, 49, 59, 56, 45, 607, 242, 68, 63, 62, 61, 60, 61, 335, 66, 63, 49, 49]","[1697548518094, 1697548518846, 1697548518915, 1697548518978, 1697548519027, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520350, 1697548520411, 1697548520746, 1697548520812, 1697548520875, 1697548520924, 1697548520973]"
2313,2313,373,44,[],200,llama-7b,64,1,482.0,1.0,1,A100,1697548530403,1697548530885,120,15.0,1.0,"[36, 446]","[1697548530439, 1697548530885]"
2314,2314,924,35,[],200,llama-7b,64,1,548.0,1.0,1,A100,1697548520816,1697548521364,120,9.0,1.0,"[12, 535]","[1697548520828, 1697548521363]"
2315,2315,720,25,[],200,llama-7b,64,1,1464.0,1.0,1,A100,1697548532121,1697548533585,120,286.0,6.0,"[14, 1014, 191, 64, 63, 50, 67]","[1697548532135, 1697548533149, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533584]"
2316,2316,861,33,[],200,llama-7b,64,1,340.0,1.0,1,A100,1697548505440,1697548505780,120,10.0,1.0,"[6, 333]","[1697548505446, 1697548505779]"
2317,2317,277,30,[],200,llama-7b,64,1,838.0,1.0,1,A100,1697548505562,1697548506400,120,18.0,1.0,"[19, 819]","[1697548505581, 1697548506400]"
2318,2318,639,34,[],200,llama-7b,64,1,1115.0,1.0,1,A100,1697548505780,1697548506895,120,100.0,6.0,"[7, 613, 272, 66, 58, 55, 44]","[1697548505787, 1697548506400, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895]"
2319,2319,408,26,[],200,llama-7b,64,1,481.0,1.0,1,A100,1697548500723,1697548501204,120,16.0,1.0,"[14, 467]","[1697548500737, 1697548501204]"
2320,2320,862,31,[],200,llama-7b,64,1,1072.0,1.0,1,A100,1697548506404,1697548507476,120,216.0,2.0,"[22, 1050]","[1697548506426, 1697548507476]"
2321,2321,65,27,[],200,llama-7b,64,1,4787.0,1.0,1,A100,1697548501207,1697548505994,120,39.0,30.0,"[18, 931, 254, 65, 64, 62, 59, 55, 875, 64, 57, 59, 52, 41, 705, 61, 48, 60, 59, 58, 57, 351, 65, 51, 60, 61, 59, 283, 46, 55, 52]","[1697548501225, 1697548502156, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503590, 1697548503654, 1697548503711, 1697548503770, 1697548503822, 1697548503863, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505438, 1697548505499, 1697548505558, 1697548505841, 1697548505887, 1697548505942, 1697548505994]"
2322,2322,315,20,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548525492,1697548527817,120,,,"[17, 520, 66, 48, 48, 60, 59, 655, 64, 49, 65, 65, 63]","[1697548525509, 1697548526029, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526965, 1697548527029, 1697548527078, 1697548527143, 1697548527208, 1697548527271]"
2323,2323,637,32,[],200,llama-7b,64,1,2355.0,1.0,1,A100,1697548507480,1697548509835,120,96.0,20.0,"[16, 530, 72, 64, 64, 61, 59, 54, 42, 306, 56, 55, 50, 40, 40, 579, 57, 54, 53, 53, 50]","[1697548507496, 1697548508026, 1697548508098, 1697548508162, 1697548508226, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835]"
2324,2324,524,32,[],200,llama-7b,64,1,3504.0,1.0,1,A100,1697548507541,1697548511045,120,100.0,30.0,"[6, 479, 72, 65, 63, 61, 59, 54, 42, 306, 56, 55, 49, 41, 40, 579, 57, 54, 53, 53, 50, 558, 59, 59, 47, 58, 56, 230, 57, 43, 43]","[1697548507547, 1697548508026, 1697548508098, 1697548508163, 1697548508226, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835, 1697548510393, 1697548510452, 1697548510511, 1697548510558, 1697548510616, 1697548510672, 1697548510902, 1697548510959, 1697548511002, 1697548511045]"
2325,2325,293,35,[],200,llama-7b,64,1,2937.0,1.0,1,A100,1697548506898,1697548509835,120,91.0,20.0,"[13, 1115, 71, 65, 64, 61, 58, 56, 41, 305, 57, 55, 50, 40, 40, 580, 56, 54, 53, 53, 50]","[1697548506911, 1697548508026, 1697548508097, 1697548508162, 1697548508226, 1697548508287, 1697548508345, 1697548508401, 1697548508442, 1697548508747, 1697548508804, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509569, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835]"
2326,2326,579,41,[],200,llama-7b,64,1,328.0,1.0,1,A100,1697548508352,1697548508680,120,19.0,1.0,"[17, 311]","[1697548508369, 1697548508680]"
2327,2327,348,42,[],200,llama-7b,64,1,3119.0,1.0,1,A100,1697548508683,1697548511802,120,91.0,20.0,"[17, 739, 129, 57, 54, 53, 53, 50, 559, 59, 59, 46, 59, 55, 231, 57, 42, 43, 51, 51, 655]","[1697548508700, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835, 1697548510394, 1697548510453, 1697548510512, 1697548510558, 1697548510617, 1697548510672, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511802]"
2328,2328,414,21,[],200,llama-7b,64,1,2962.0,1.0,1,A100,1697548497700,1697548500662,120,87.0,20.0,"[18, 453, 66, 61, 52, 51, 50, 351, 59, 51, 50, 826, 61, 62, 58, 57, 50, 421, 65, 50, 50]","[1697548497718, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499911, 1697548499969, 1697548500026, 1697548500076, 1697548500497, 1697548500562, 1697548500612, 1697548500662]"
2329,2329,64,21,[],200,llama-7b,64,1,2623.0,1.0,1,A100,1697548527826,1697548530449,120,89.0,20.0,"[72, 997, 60, 52, 50, 48, 47, 39, 365, 52, 49, 41, 41, 40, 50, 347, 55, 48, 59, 60, 51]","[1697548527898, 1697548528895, 1697548528955, 1697548529007, 1697548529057, 1697548529105, 1697548529152, 1697548529191, 1697548529556, 1697548529608, 1697548529657, 1697548529698, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530279, 1697548530338, 1697548530398, 1697548530449]"
2330,2330,23,26,[],200,llama-7b,64,1,674.0,1.0,1,A100,1697548498745,1697548499419,120,26.0,1.0,"[30, 644]","[1697548498775, 1697548499419]"
2331,2331,719,27,[],200,llama-7b,64,1,1300.0,1.0,1,A100,1697548499420,1697548500720,120,182.0,6.0,"[13, 1064, 65, 50, 49, 59]","[1697548499433, 1697548500497, 1697548500562, 1697548500612, 1697548500661, 1697548500720]"
2332,2332,186,22,[],200,llama-7b,64,1,4012.0,1.0,1,A100,1697548500665,1697548504677,120,123.0,22.0,"[24, 514, 295, 63, 61, 58, 55, 55, 621, 64, 63, 62, 60, 55, 877, 62, 58, 57, 53, 41, 704, 62, 48]","[1697548500689, 1697548501203, 1697548501498, 1697548501561, 1697548501622, 1697548501680, 1697548501735, 1697548501790, 1697548502411, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502715, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504677]"
2333,2333,613,18,[],200,llama-7b,64,1,3492.0,1.0,1,A100,1697548509628,1697548513120,120,90.0,20.0,"[19, 680, 66, 60, 59, 46, 59, 56, 230, 56, 43, 43, 51, 51, 656, 56, 54, 53, 54, 750, 350]","[1697548509647, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510617, 1697548510673, 1697548510903, 1697548510959, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511803, 1697548511859, 1697548511913, 1697548511966, 1697548512020, 1697548512770, 1697548513120]"
2334,2334,107,31,[],200,llama-7b,64,1,629.0,1.0,1,A100,1697548498174,1697548498803,120,216.0,2.0,"[17, 612]","[1697548498191, 1697548498803]"
2335,2335,256,25,[],200,llama-7b,64,1,8870.0,1.0,1,A100,1697548508100,1697548516970,120,47.0,50.0,"[6, 573, 69, 57, 54, 50, 40, 40, 579, 58, 52, 53, 54, 49, 559, 60, 59, 46, 58, 56, 231, 56, 42, 44, 51, 51, 654, 57, 54, 53, 51, 753, 350, 58, 56, 43, 54, 833, 66, 60, 58, 58, 919, 72, 69, 64, 61, 976, 271, 68, 65]","[1697548508106, 1697548508679, 1697548508748, 1697548508805, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509568, 1697548509626, 1697548509678, 1697548509731, 1697548509785, 1697548509834, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510672, 1697548510903, 1697548510959, 1697548511001, 1697548511045, 1697548511096, 1697548511147, 1697548511801, 1697548511858, 1697548511912, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513177, 1697548513233, 1697548513276, 1697548513330, 1697548514163, 1697548514229, 1697548514289, 1697548514347, 1697548514405, 1697548515324, 1697548515396, 1697548515465, 1697548515529, 1697548515590, 1697548516566, 1697548516837, 1697548516905, 1697548516970]"
2336,2336,190,37,[],200,llama-7b,64,1,2189.0,1.0,1,A100,1697548544888,1697548547077,120,335.0,10.0,"[19, 608, 369, 71, 65, 64, 63, 61, 48, 62, 759]","[1697548544907, 1697548545515, 1697548545884, 1697548545955, 1697548546020, 1697548546084, 1697548546147, 1697548546208, 1697548546256, 1697548546318, 1697548547077]"
2337,2337,768,23,[],200,llama-7b,64,1,821.0,1.0,1,A100,1697548504679,1697548505500,120,47.0,6.0,"[7, 507, 69, 66, 51, 60, 61]","[1697548504686, 1697548505193, 1697548505262, 1697548505328, 1697548505379, 1697548505439, 1697548505500]"
2338,2338,842,28,[],200,llama-7b,64,1,2195.0,1.0,1,A100,1697548520976,1697548523171,120,161.0,16.0,"[20, 367, 68, 63, 47, 60, 46, 54, 710, 70, 66, 68, 56, 66, 50, 317, 66]","[1697548520996, 1697548521363, 1697548521431, 1697548521494, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522481, 1697548522547, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170]"
2339,2339,551,41,[],200,llama-7b,64,1,4046.0,1.0,1,A100,1697548540731,1697548544777,120,90.0,20.0,"[8, 336, 54, 48, 750, 59, 60, 51, 651, 62, 61, 56, 45, 55, 692, 59, 47, 57, 55, 52, 787]","[1697548540739, 1697548541075, 1697548541129, 1697548541177, 1697548541927, 1697548541986, 1697548542046, 1697548542097, 1697548542748, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776]"
2340,2340,891,38,[],200,llama-7b,64,1,595.0,1.0,1,A100,1697548547080,1697548547675,120,52.0,2.0,"[19, 516, 59]","[1697548547099, 1697548547615, 1697548547674]"
2341,2341,551,39,[],200,llama-7b,64,1,2796.0,1.0,1,A100,1697548547677,1697548550473,120,90.0,20.0,"[9, 395, 64, 50, 47, 811, 61, 51, 58, 47, 58, 55, 397, 60, 47, 47, 47, 61, 54, 313, 64]","[1697548547686, 1697548548081, 1697548548145, 1697548548195, 1697548548242, 1697548549053, 1697548549114, 1697548549165, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549780, 1697548549840, 1697548549887, 1697548549934, 1697548549981, 1697548550042, 1697548550096, 1697548550409, 1697548550473]"
2342,2342,468,26,[],200,llama-7b,64,1,2976.0,1.0,1,A100,1697548533593,1697548536569,120,31.0,20.0,"[20, 318, 65, 59, 46, 45, 59, 58, 367, 47, 59, 59, 56, 707, 59, 59, 58, 55, 668, 63, 49]","[1697548533613, 1697548533931, 1697548533996, 1697548534055, 1697548534101, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534677, 1697548534736, 1697548534795, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536569]"
2343,2343,618,29,[],200,llama-7b,64,1,492.0,1.0,1,A100,1697548523174,1697548523666,120,9.0,1.0,"[8, 483]","[1697548523182, 1697548523665]"
2344,2344,545,24,[],200,llama-7b,64,1,492.0,1.0,1,A100,1697548505502,1697548505994,120,216.0,5.0,"[5, 272, 63, 46, 54, 52]","[1697548505507, 1697548505779, 1697548505842, 1697548505888, 1697548505942, 1697548505994]"
2345,2345,272,30,[],200,llama-7b,64,1,3296.0,1.0,1,A100,1697548523668,1697548526964,120,86.0,20.0,"[13, 672, 237, 64, 65, 65, 59, 58, 588, 67, 66, 50, 51, 65, 65, 242, 48, 48, 60, 59, 654]","[1697548523681, 1697548524353, 1697548524590, 1697548524654, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525622, 1697548525672, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526964]"
2346,2346,646,22,[],200,llama-7b,64,1,914.0,1.0,1,A100,1697548530453,1697548531367,120,14.0,1.0,"[25, 889]","[1697548530478, 1697548531367]"
2347,2347,337,39,[],200,llama-7b,64,1,807.0,1.0,1,A100,1697548540784,1697548541591,120,12.0,1.0,"[25, 782]","[1697548540809, 1697548541591]"
2348,2348,421,23,[],200,llama-7b,64,1,2893.0,1.0,1,A100,1697548531370,1697548534263,120,85.0,20.0,"[23, 725, 366, 52, 59, 64, 50, 55, 576, 64, 64, 49, 63, 58, 46, 311, 60, 45, 46, 59, 58]","[1697548531393, 1697548532118, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532709, 1697548532764, 1697548533340, 1697548533404, 1697548533468, 1697548533517, 1697548533580, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263]"
2349,2349,918,40,[],200,llama-7b,64,1,905.0,1.0,1,A100,1697548541597,1697548542502,120,23.0,1.0,"[34, 871]","[1697548541631, 1697548542502]"
2350,2350,695,41,[],200,llama-7b,64,1,4572.0,1.0,1,A100,1697548542505,1697548547077,120,92.0,20.0,"[9, 835, 370, 59, 47, 57, 55, 52, 787, 55, 54, 47, 951, 71, 66, 64, 62, 61, 48, 62, 760]","[1697548542514, 1697548543349, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776, 1697548544831, 1697548544885, 1697548544932, 1697548545883, 1697548545954, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077]"
2351,2351,240,35,[],200,llama-7b,64,1,4560.0,1.0,1,A100,1697548510906,1697548515466,120,83.0,20.0,"[10, 643, 243, 56, 55, 52, 55, 749, 350, 58, 56, 43, 54, 835, 66, 59, 58, 58, 919, 71, 70]","[1697548510916, 1697548511559, 1697548511802, 1697548511858, 1697548511913, 1697548511965, 1697548512020, 1697548512769, 1697548513119, 1697548513177, 1697548513233, 1697548513276, 1697548513330, 1697548514165, 1697548514231, 1697548514290, 1697548514348, 1697548514406, 1697548515325, 1697548515396, 1697548515466]"
2352,2352,855,23,[],200,llama-7b,64,1,3320.0,1.0,1,A100,1697548498302,1697548501622,120,83.0,20.0,"[27, 409, 65, 58, 51, 50, 826, 61, 62, 59, 55, 51, 421, 64, 51, 51, 57, 54, 724, 63, 61]","[1697548498329, 1697548498738, 1697548498803, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500497, 1697548500561, 1697548500612, 1697548500663, 1697548500720, 1697548500774, 1697548501498, 1697548501561, 1697548501622]"
2353,2353,833,36,[],200,llama-7b,64,1,1923.0,1.0,1,A100,1697548515469,1697548517392,120,563.0,8.0,"[25, 575, 498, 270, 68, 66, 65, 63, 293]","[1697548515494, 1697548516069, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517036, 1697548517099, 1697548517392]"
2354,2354,509,24,[],200,llama-7b,64,1,852.0,1.0,1,A100,1697548501624,1697548502476,120,286.0,3.0,"[17, 516, 253, 66]","[1697548501641, 1697548502157, 1697548502410, 1697548502476]"
2355,2355,42,49,[],200,llama-7b,64,1,602.0,1.0,1,A100,1697548528900,1697548529502,120,10.0,1.0,"[23, 579]","[1697548528923, 1697548529502]"
2356,2356,603,37,[],200,llama-7b,64,1,669.0,1.0,1,A100,1697548517396,1697548518065,120,9.0,1.0,"[6, 663]","[1697548517402, 1697548518065]"
2357,2357,282,25,[],200,llama-7b,64,1,3080.0,1.0,1,A100,1697548502479,1697548505559,120,87.0,20.0,"[6, 645, 461, 63, 58, 57, 53, 41, 704, 62, 47, 60, 59, 58, 57, 352, 66, 50, 61, 60, 60]","[1697548502485, 1697548503130, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504853, 1697548504910, 1697548505262, 1697548505328, 1697548505378, 1697548505439, 1697548505499, 1697548505559]"
2358,2358,661,30,[],200,llama-7b,64,1,2053.0,1.0,1,A100,1697548515470,1697548517523,120,161.0,10.0,"[19, 580, 498, 270, 68, 66, 65, 63, 293, 66, 65]","[1697548515489, 1697548516069, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517036, 1697548517099, 1697548517392, 1697548517458, 1697548517523]"
2359,2359,258,38,[],200,llama-7b,64,1,6716.0,1.0,1,A100,1697548518068,1697548524784,120,244.0,50.0,"[20, 758, 69, 63, 49, 59, 56, 45, 607, 242, 68, 63, 62, 61, 60, 61, 336, 65, 63, 49, 49, 53, 405, 62, 48, 60, 46, 54, 709, 70, 67, 67, 57, 66, 50, 316, 67, 64, 63, 63, 61, 317, 52, 67, 51, 64, 63, 554, 64, 66, 65]","[1697548518088, 1697548518846, 1697548518915, 1697548518978, 1697548519027, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520350, 1697548520411, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520973, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103, 1697548523170, 1697548523234, 1697548523297, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524035, 1697548524589, 1697548524653, 1697548524719, 1697548524784]"
2360,2360,677,6,[],200,llama-7b,64,1,988.0,1.0,1,A100,1697548491713,1697548492701,120,9.0,1.0,"[11, 977]","[1697548491724, 1697548492701]"
2361,2361,863,26,[],200,llama-7b,64,1,837.0,1.0,1,A100,1697548505563,1697548506400,120,10.0,1.0,"[39, 798]","[1697548505602, 1697548506400]"
2362,2362,322,31,[],200,llama-7b,64,1,2886.0,1.0,1,A100,1697548517525,1697548520411,120,93.0,20.0,"[16, 524, 242, 66, 64, 59, 57, 362, 62, 50, 58, 58, 44, 607, 242, 68, 62, 63, 61, 60, 61]","[1697548517541, 1697548518065, 1697548518307, 1697548518373, 1697548518437, 1697548518496, 1697548518553, 1697548518915, 1697548518977, 1697548519027, 1697548519085, 1697548519143, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520229, 1697548520290, 1697548520350, 1697548520411]"
2363,2363,640,27,[],200,llama-7b,64,1,882.0,1.0,1,A100,1697548506401,1697548507283,120,15.0,1.0,"[15, 866]","[1697548506416, 1697548507282]"
2364,2364,108,39,[],200,llama-7b,64,1,725.0,1.0,1,A100,1697548547420,1697548548145,120,182.0,2.0,"[25, 636, 64]","[1697548547445, 1697548548081, 1697548548145]"
2365,2365,294,28,[],200,llama-7b,64,1,813.0,1.0,1,A100,1697548507285,1697548508098,120,9.0,2.0,"[14, 799]","[1697548507299, 1697548508098]"
2366,2366,817,40,[],200,llama-7b,64,1,2482.0,1.0,1,A100,1697548548148,1697548550630,120,86.0,20.0,"[14, 599, 292, 61, 50, 60, 47, 58, 55, 395, 60, 48, 47, 48, 59, 55, 314, 63, 55, 49, 53]","[1697548548162, 1697548548761, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550096, 1697548550410, 1697548550473, 1697548550528, 1697548550577, 1697548550630]"
2367,2367,323,42,[],200,llama-7b,64,1,3366.0,1.0,1,A100,1697548544779,1697548548145,120,84.0,20.0,"[22, 714, 368, 71, 65, 65, 62, 61, 48, 62, 759, 67, 63, 60, 48, 57, 46, 257, 53, 47, 371]","[1697548544801, 1697548545515, 1697548545883, 1697548545954, 1697548546019, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547076, 1697548547143, 1697548547206, 1697548547266, 1697548547314, 1697548547371, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145]"
2368,2368,65,29,[],200,llama-7b,64,1,3917.0,1.0,1,A100,1697548508099,1697548512016,120,39.0,30.0,"[6, 574, 69, 57, 54, 50, 40, 40, 579, 58, 54, 52, 53, 49, 559, 60, 59, 46, 58, 56, 231, 56, 42, 44, 51, 51, 654, 57, 54, 53, 51]","[1697548508105, 1697548508679, 1697548508748, 1697548508805, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509568, 1697548509626, 1697548509680, 1697548509732, 1697548509785, 1697548509834, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510672, 1697548510903, 1697548510959, 1697548511001, 1697548511045, 1697548511096, 1697548511147, 1697548511801, 1697548511858, 1697548511912, 1697548511965, 1697548512016]"
2369,2369,35,32,[],200,llama-7b,64,1,3416.0,1.0,1,A100,1697548533152,1697548536568,120,87.0,20.0,"[17, 762, 64, 60, 46, 45, 59, 58, 368, 46, 58, 59, 57, 707, 59, 59, 58, 55, 668, 63, 48]","[1697548533169, 1697548533931, 1697548533995, 1697548534055, 1697548534101, 1697548534146, 1697548534205, 1697548534263, 1697548534631, 1697548534677, 1697548534735, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568]"
2370,2370,595,33,[],200,llama-7b,64,1,295.0,1.0,1,A100,1697548536572,1697548536867,120,8.0,1.0,"[22, 273]","[1697548536594, 1697548536867]"
2371,2371,364,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536872,1697548537857,120,,,"[32, 863]","[1697548536904, 1697548537767]"
2372,2372,373,40,[],200,llama-7b,64,1,951.0,1.0,1,A100,1697548520413,1697548521364,120,15.0,1.0,"[6, 945]","[1697548520419, 1697548521364]"
2373,2373,24,35,[],200,llama-7b,64,1,1992.0,1.0,1,A100,1697548537876,1697548539868,120,79.0,9.0,"[178, 830, 55, 41, 47, 45, 678, 62, 55]","[1697548538054, 1697548538884, 1697548538939, 1697548538980, 1697548539027, 1697548539072, 1697548539750, 1697548539812, 1697548539867]"
2374,2374,90,32,[],200,llama-7b,64,1,949.0,1.0,1,A100,1697548520414,1697548521363,120,19.0,1.0,"[10, 939]","[1697548520424, 1697548521363]"
2375,2375,690,32,[],200,llama-7b,64,1,613.0,1.0,1,A100,1697548498807,1697548499420,120,39.0,1.0,"[7, 605]","[1697548498814, 1697548499419]"
2376,2376,100,42,[],200,llama-7b,64,1,1913.0,1.0,1,A100,1697548529783,1697548531696,120,732.0,14.0,"[6, 319, 69, 55, 48, 59, 60, 51, 486, 38, 473, 64, 65, 60, 60]","[1697548529789, 1697548530108, 1697548530177, 1697548530232, 1697548530280, 1697548530339, 1697548530399, 1697548530450, 1697548530936, 1697548530974, 1697548531447, 1697548531511, 1697548531576, 1697548531636, 1697548531696]"
2377,2377,464,33,[],200,llama-7b,64,1,1001.0,1.0,1,A100,1697548499424,1697548500425,120,12.0,1.0,"[28, 973]","[1697548499452, 1697548500425]"
2378,2378,680,33,[],200,llama-7b,64,1,1868.0,1.0,1,A100,1697548521366,1697548523234,120,123.0,11.0,"[15, 764, 266, 70, 67, 67, 56, 66, 50, 317, 66, 64]","[1697548521381, 1697548522145, 1697548522411, 1697548522481, 1697548522548, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234]"
2379,2379,490,51,[],200,llama-7b,64,1,1117.0,1.0,1,A100,1697548537864,1697548538981,120,11.0,5.0,"[43, 267, 27, 683, 55, 42]","[1697548537907, 1697548538174, 1697548538201, 1697548538884, 1697548538939, 1697548538981]"
2380,2380,449,34,[],200,llama-7b,64,1,2618.0,1.0,1,A100,1697548523237,1697548525855,120,86.0,20.0,"[6, 423, 73, 51, 67, 52, 63, 64, 553, 64, 66, 65, 59, 58, 589, 67, 65, 51, 50, 65, 67]","[1697548523243, 1697548523666, 1697548523739, 1697548523790, 1697548523857, 1697548523909, 1697548523972, 1697548524036, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525490, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525855]"
2381,2381,800,43,[],200,llama-7b,64,1,2565.0,1.0,1,A100,1697548531699,1697548534264,120,140.0,20.0,"[17, 768, 52, 59, 64, 51, 54, 576, 64, 63, 50, 64, 57, 46, 311, 60, 45, 46, 59, 59]","[1697548531716, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533581, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534264]"
2382,2382,693,36,[],200,llama-7b,64,1,1043.0,1.0,1,A100,1697548521368,1697548522411,120,67.0,2.0,"[29, 748, 266]","[1697548521397, 1697548522145, 1697548522411]"
2383,2383,352,37,[],200,llama-7b,64,1,755.0,1.0,1,A100,1697548522415,1697548523170,120,11.0,3.0,"[10, 605, 74, 66]","[1697548522425, 1697548523030, 1697548523104, 1697548523170]"
2384,2384,390,25,[],200,llama-7b,64,1,2456.0,1.0,1,A100,1697548494621,1697548497077,120,84.0,20.0,"[7, 551, 65, 49, 58, 58, 55, 53, 41, 233, 55, 53, 51, 54, 377, 40, 40, 51, 50, 445, 69]","[1697548494628, 1697548495179, 1697548495244, 1697548495293, 1697548495351, 1697548495409, 1697548495464, 1697548495517, 1697548495558, 1697548495791, 1697548495846, 1697548495899, 1697548495950, 1697548496004, 1697548496381, 1697548496421, 1697548496461, 1697548496512, 1697548496562, 1697548497007, 1697548497076]"
2385,2385,136,37,[],200,llama-7b,64,1,220.0,1.0,1,A100,1697548510619,1697548510839,120,31.0,1.0,"[7, 213]","[1697548510626, 1697548510839]"
2386,2386,729,38,[],200,llama-7b,64,1,961.0,1.0,1,A100,1697548510841,1697548511802,120,874.0,2.0,"[14, 947]","[1697548510855, 1697548511802]"
2387,2387,122,38,[],200,llama-7b,64,1,2683.0,1.0,1,A100,1697548523172,1697548525855,120,88.0,20.0,"[5, 488, 74, 51, 67, 52, 63, 64, 553, 64, 66, 65, 59, 58, 589, 67, 65, 51, 50, 65, 67]","[1697548523177, 1697548523665, 1697548523739, 1697548523790, 1697548523857, 1697548523909, 1697548523972, 1697548524036, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525490, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525855]"
2388,2388,501,39,[],200,llama-7b,64,1,655.0,1.0,1,A100,1697548511804,1697548512459,120,19.0,1.0,"[6, 649]","[1697548511810, 1697548512459]"
2389,2389,743,50,[],200,llama-7b,64,1,894.0,1.0,1,A100,1697548529505,1697548530399,120,123.0,6.0,"[15, 587, 69, 56, 47, 60, 59]","[1697548529520, 1697548530107, 1697548530176, 1697548530232, 1697548530279, 1697548530339, 1697548530398]"
2390,2390,490,51,[],200,llama-7b,64,1,1112.0,1.0,1,A100,1697548530400,1697548531512,120,11.0,5.0,"[19, 466, 50, 39, 473, 65]","[1697548530419, 1697548530885, 1697548530935, 1697548530974, 1697548531447, 1697548531512]"
2391,2391,144,52,[],200,llama-7b,64,1,2750.0,1.0,1,A100,1697548531514,1697548534264,120,96.0,20.0,"[10, 594, 366, 52, 59, 64, 50, 55, 576, 64, 63, 50, 66, 55, 46, 311, 60, 45, 46, 59, 59]","[1697548531524, 1697548532118, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532709, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533583, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534264]"
2392,2392,202,50,[],200,llama-7b,64,1,10488.0,1.0,1,A100,1697548544887,1697548555375,120,874.0,72.0,"[7, 621, 368, 72, 65, 64, 63, 60, 49, 62, 759, 66, 63, 60, 48, 57, 46, 257, 53, 47, 371, 49, 48, 810, 61, 51, 59, 47, 58, 55, 396, 60, 47, 48, 47, 60, 56, 311, 64, 55, 49, 53, 372, 52, 52, 50, 49, 425, 62, 57, 45, 44, 55, 56, 711, 54, 66, 50, 50, 65, 64, 274, 49, 47, 60, 60, 349, 59, 46, 59, 859, 251, 253]","[1697548544894, 1697548545515, 1697548545883, 1697548545955, 1697548546020, 1697548546084, 1697548546147, 1697548546207, 1697548546256, 1697548546318, 1697548547077, 1697548547143, 1697548547206, 1697548547266, 1697548547314, 1697548547371, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549779, 1697548549839, 1697548549886, 1697548549934, 1697548549981, 1697548550041, 1697548550097, 1697548550408, 1697548550472, 1697548550527, 1697548550576, 1697548550629, 1697548551001, 1697548551053, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551691, 1697548551748, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552659, 1697548552713, 1697548552779, 1697548552829, 1697548552879, 1697548552944, 1697548553008, 1697548553282, 1697548553331, 1697548553378, 1697548553438, 1697548553498, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554870, 1697548555121, 1697548555374]"
2393,2393,154,40,[],200,llama-7b,64,1,1320.0,1.0,1,A100,1697548512463,1697548513783,120,13.0,1.0,"[24, 1296]","[1697548512487, 1697548513783]"
2394,2394,860,41,[],200,llama-7b,64,1,4651.0,1.0,1,A100,1697548513786,1697548518437,120,85.0,20.0,"[11, 1060, 468, 71, 69, 65, 60, 977, 271, 68, 65, 64, 64, 293, 67, 64, 58, 53, 672, 66, 65]","[1697548513797, 1697548514857, 1697548515325, 1697548515396, 1697548515465, 1697548515530, 1697548515590, 1697548516567, 1697548516838, 1697548516906, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517459, 1697548517523, 1697548517581, 1697548517634, 1697548518306, 1697548518372, 1697548518437]"
2395,2395,715,39,[],200,llama-7b,64,1,852.0,1.0,1,A100,1697548525858,1697548526710,120,20.0,1.0,"[20, 832]","[1697548525878, 1697548526710]"
2396,2396,487,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526713,1697548527821,120,,,[15],[1697548526728]
2397,2397,166,26,[],200,llama-7b,64,1,492.0,1.0,1,A100,1697548497081,1697548497573,120,14.0,1.0,"[25, 466]","[1697548497106, 1697548497572]"
2398,2398,848,53,[],200,llama-7b,64,1,908.0,1.0,1,A100,1697548534268,1697548535176,120,47.0,1.0,"[35, 873]","[1697548534303, 1697548535176]"
2399,2399,497,54,[],200,llama-7b,64,1,1279.0,1.0,1,A100,1697548535178,1697548536457,120,67.0,2.0,"[18, 1077, 184]","[1697548535196, 1697548536273, 1697548536457]"
2400,2400,749,27,[],200,llama-7b,64,1,3087.0,1.0,1,A100,1697548497575,1697548500662,120,47.0,20.0,"[24, 572, 66, 61, 52, 51, 51, 350, 60, 50, 50, 826, 61, 62, 59, 55, 51, 421, 65, 50, 50]","[1697548497599, 1697548498171, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498452, 1697548498802, 1697548498862, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500497, 1697548500562, 1697548500612, 1697548500662]"
2401,2401,140,41,[],200,llama-7b,64,1,2622.0,1.0,1,A100,1697548527827,1697548530449,120,96.0,20.0,"[232, 837, 59, 51, 51, 47, 48, 39, 365, 51, 49, 41, 42, 40, 50, 346, 56, 48, 59, 59, 52]","[1697548528059, 1697548528896, 1697548528955, 1697548529006, 1697548529057, 1697548529104, 1697548529152, 1697548529191, 1697548529556, 1697548529607, 1697548529656, 1697548529697, 1697548529739, 1697548529779, 1697548529829, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530397, 1697548530449]"
2402,2402,512,42,[],200,llama-7b,64,1,406.0,1.0,1,A100,1697548518440,1697548518846,120,11.0,1.0,"[24, 382]","[1697548518464, 1697548518846]"
2403,2403,300,48,[],200,llama-7b,64,1,927.0,1.0,1,A100,1697548552290,1697548553217,120,9.0,1.0,"[12, 914]","[1697548552302, 1697548553216]"
2404,2404,69,49,[],200,llama-7b,64,1,2755.0,1.0,1,A100,1697548553219,1697548555974,120,85.0,20.0,"[11, 551, 66, 60, 45, 59, 860, 250, 253, 59, 45, 45, 44, 58, 58, 43, 50, 69, 47, 36, 46]","[1697548553230, 1697548553781, 1697548553847, 1697548553907, 1697548553952, 1697548554011, 1697548554871, 1697548555121, 1697548555374, 1697548555433, 1697548555478, 1697548555523, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555892, 1697548555928, 1697548555974]"
2405,2405,7,39,[],200,llama-7b,64,1,1405.0,1.0,1,A100,1697548524786,1697548526191,120,345.0,11.0,"[8, 455, 240, 68, 65, 51, 50, 65, 66, 241, 48, 48]","[1697548524794, 1697548525249, 1697548525489, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525854, 1697548526095, 1697548526143, 1697548526191]"
2406,2406,472,41,[],200,llama-7b,64,1,2865.0,1.0,1,A100,1697548550634,1697548553499,120,85.0,20.0,"[19, 906, 70, 63, 58, 43, 44, 56, 55, 713, 52, 67, 49, 50, 66, 64, 274, 49, 47, 60, 60]","[1697548550653, 1697548551559, 1697548551629, 1697548551692, 1697548551750, 1697548551793, 1697548551837, 1697548551893, 1697548551948, 1697548552661, 1697548552713, 1697548552780, 1697548552829, 1697548552879, 1697548552945, 1697548553009, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553499]"
2407,2407,241,24,[],200,llama-7b,64,1,654.0,1.0,1,A100,1697548510906,1697548511560,120,19.0,1.0,"[16, 638]","[1697548510922, 1697548511560]"
2408,2408,186,33,[],200,llama-7b,64,1,4543.0,1.0,1,A100,1697548511048,1697548515591,120,123.0,22.0,"[6, 505, 243, 56, 55, 52, 55, 749, 350, 59, 55, 43, 54, 833, 68, 59, 58, 58, 919, 71, 70, 64, 60]","[1697548511054, 1697548511559, 1697548511802, 1697548511858, 1697548511913, 1697548511965, 1697548512020, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514163, 1697548514231, 1697548514290, 1697548514348, 1697548514406, 1697548515325, 1697548515396, 1697548515466, 1697548515530, 1697548515590]"
2409,2409,11,25,[],200,llama-7b,64,1,4028.0,1.0,1,A100,1697548511563,1697548515591,120,732.0,17.0,"[22, 874, 310, 351, 58, 55, 44, 53, 833, 66, 61, 58, 57, 920, 71, 71, 63, 61]","[1697548511585, 1697548512459, 1697548512769, 1697548513120, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514163, 1697548514229, 1697548514290, 1697548514348, 1697548514405, 1697548515325, 1697548515396, 1697548515467, 1697548515530, 1697548515591]"
2410,2410,880,34,[],200,llama-7b,64,1,1800.0,1.0,1,A100,1697548515593,1697548517393,120,84.0,2.0,"[15, 1713, 71]","[1697548515608, 1697548517321, 1697548517392]"
2411,2411,454,29,[],200,llama-7b,64,1,970.0,1.0,1,A100,1697548517584,1697548518554,120,182.0,6.0,"[6, 475, 242, 66, 64, 59, 58]","[1697548517590, 1697548518065, 1697548518307, 1697548518373, 1697548518437, 1697548518496, 1697548518554]"
2412,2412,145,41,[],200,llama-7b,64,1,1736.0,1.0,1,A100,1697548521368,1697548523104,120,161.0,9.0,"[33, 744, 265, 71, 67, 67, 56, 66, 50, 317]","[1697548521401, 1697548522145, 1697548522410, 1697548522481, 1697548522548, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104]"
2413,2413,540,35,[],200,llama-7b,64,1,1102.0,1.0,1,A100,1697548517394,1697548518496,120,140.0,5.0,"[6, 665, 242, 66, 64, 59]","[1697548517400, 1697548518065, 1697548518307, 1697548518373, 1697548518437, 1697548518496]"
2414,2414,727,42,[],200,llama-7b,64,1,802.0,1.0,1,A100,1697548523107,1697548523909,120,58.0,5.0,"[17, 541, 74, 51, 67, 52]","[1697548523124, 1697548523665, 1697548523739, 1697548523790, 1697548523857, 1697548523909]"
2415,2415,309,36,[],200,llama-7b,64,1,2475.0,1.0,1,A100,1697548518499,1697548520974,120,52.0,20.0,"[6, 341, 69, 63, 50, 58, 56, 46, 606, 243, 68, 61, 63, 61, 60, 60, 336, 66, 63, 48, 51]","[1697548518505, 1697548518846, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519188, 1697548519794, 1697548520037, 1697548520105, 1697548520166, 1697548520229, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520974]"
2416,2416,110,30,[],200,llama-7b,64,1,1548.0,1.0,1,A100,1697548518556,1697548520104,120,96.0,4.0,"[14, 1040, 184, 242, 68]","[1697548518570, 1697548519610, 1697548519794, 1697548520036, 1697548520104]"
2417,2417,475,43,[],200,llama-7b,64,1,3053.0,1.0,1,A100,1697548523912,1697548526965,120,89.0,20.0,"[20, 421, 237, 64, 65, 65, 59, 58, 588, 67, 66, 51, 50, 65, 65, 242, 48, 48, 60, 58, 655]","[1697548523932, 1697548524353, 1697548524590, 1697548524654, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526309, 1697548526964]"
2418,2418,148,45,[],200,llama-7b,64,1,479.0,1.0,1,A100,1697548530889,1697548531368,120,16.0,1.0,"[19, 460]","[1697548530908, 1697548531368]"
2419,2419,731,46,[],200,llama-7b,64,1,2892.0,1.0,1,A100,1697548531371,1697548534263,120,89.0,20.0,"[27, 720, 366, 52, 59, 64, 50, 55, 575, 65, 64, 49, 68, 53, 46, 311, 60, 45, 46, 59, 58]","[1697548531398, 1697548532118, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532709, 1697548532764, 1697548533339, 1697548533404, 1697548533468, 1697548533517, 1697548533585, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263]"
2420,2420,600,26,[],200,llama-7b,64,1,1728.0,1.0,1,A100,1697548515594,1697548517322,120,23.0,1.0,"[19, 1708]","[1697548515613, 1697548517321]"
2421,2421,705,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526193,1697548527820,120,,,"[11, 760, 66, 49, 64, 66, 63]","[1697548526204, 1697548526964, 1697548527030, 1697548527079, 1697548527143, 1697548527209, 1697548527272]"
2422,2422,370,27,[],200,llama-7b,64,1,739.0,1.0,1,A100,1697548517325,1697548518064,120,31.0,1.0,"[11, 728]","[1697548517336, 1697548518064]"
2423,2423,25,28,[],200,llama-7b,64,1,778.0,1.0,1,A100,1697548518068,1697548518846,120,12.0,1.0,"[31, 747]","[1697548518099, 1697548518846]"
2424,2424,724,29,[],200,llama-7b,64,1,760.0,1.0,1,A100,1697548518850,1697548519610,120,11.0,1.0,"[25, 735]","[1697548518875, 1697548519610]"
2425,2425,472,30,[],200,llama-7b,64,1,3176.0,1.0,1,A100,1697548519611,1697548522787,120,85.0,20.0,"[16, 1048, 72, 65, 63, 49, 48, 54, 405, 63, 47, 60, 46, 54, 710, 69, 67, 67, 57, 66, 50]","[1697548519627, 1697548520675, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520972, 1697548521026, 1697548521431, 1697548521494, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787]"
2426,2426,817,31,[],200,llama-7b,64,1,2680.0,1.0,1,A100,1697548520107,1697548522787,120,86.0,20.0,"[22, 546, 72, 65, 63, 49, 49, 53, 405, 62, 48, 60, 46, 54, 710, 69, 67, 67, 57, 66, 50]","[1697548520129, 1697548520675, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520973, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787]"
2427,2427,365,41,[],200,llama-7b,64,1,440.0,1.0,1,A100,1697548527826,1697548528266,120,23.0,1.0,"[46, 393]","[1697548527872, 1697548528265]"
2428,2428,745,43,[],200,llama-7b,64,1,675.0,1.0,1,A100,1697548548086,1697548548761,120,17.0,1.0,"[34, 641]","[1697548548120, 1697548548761]"
2429,2429,134,42,[],200,llama-7b,64,1,2177.0,1.0,1,A100,1697548528273,1697548530450,120,86.0,20.0,"[17, 606, 59, 52, 50, 48, 47, 39, 365, 51, 50, 41, 41, 40, 50, 347, 55, 48, 59, 59, 52]","[1697548528290, 1697548528896, 1697548528955, 1697548529007, 1697548529057, 1697548529105, 1697548529152, 1697548529191, 1697548529556, 1697548529607, 1697548529657, 1697548529698, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530279, 1697548530338, 1697548530397, 1697548530449]"
2430,2430,898,37,[],200,llama-7b,64,1,455.0,1.0,1,A100,1697548520977,1697548521432,120,79.0,2.0,"[19, 435]","[1697548520996, 1697548521431]"
2431,2431,200,25,[],200,llama-7b,64,1,2166.0,1.0,1,A100,1697548505997,1697548508163,120,6.0,9.0,"[15, 1270, 194, 63, 62, 59, 56, 57, 325, 65]","[1697548506012, 1697548507282, 1697548507476, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508098, 1697548508163]"
2432,2432,200,18,[],200,llama-7b,64,1,2021.0,1.0,1,A100,1697548509838,1697548511859,120,6.0,9.0,"[10, 991, 64, 57, 42, 43, 51, 51, 654, 58]","[1697548509848, 1697548510839, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511801, 1697548511859]"
2433,2433,900,19,[],200,llama-7b,64,1,1415.0,1.0,1,A100,1697548511862,1697548513277,120,67.0,6.0,"[6, 591, 311, 350, 58, 55, 44]","[1697548511868, 1697548512459, 1697548512770, 1697548513120, 1697548513178, 1697548513233, 1697548513277]"
2434,2434,561,20,[],200,llama-7b,64,1,4243.0,1.0,1,A100,1697548513280,1697548517523,120,87.0,20.0,"[6, 496, 382, 66, 60, 58, 58, 918, 72, 69, 64, 61, 977, 271, 68, 65, 64, 64, 293, 67, 64]","[1697548513286, 1697548513782, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406, 1697548515324, 1697548515396, 1697548515465, 1697548515529, 1697548515590, 1697548516567, 1697548516838, 1697548516906, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517459, 1697548517523]"
2435,2435,109,37,[],200,llama-7b,64,1,2625.0,1.0,1,A100,1697548531639,1697548534264,120,90.0,20.0,"[16, 464, 365, 52, 59, 64, 51, 54, 576, 64, 63, 50, 67, 54, 46, 311, 60, 45, 46, 59, 59]","[1697548531655, 1697548532119, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533584, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534264]"
2436,2436,348,42,[],200,llama-7b,64,1,2961.0,1.0,1,A100,1697548547080,1697548550041,120,91.0,20.0,"[12, 523, 59, 53, 48, 369, 50, 48, 811, 61, 50, 60, 47, 58, 55, 395, 60, 48, 47, 48, 59]","[1697548547092, 1697548547615, 1697548547674, 1697548547727, 1697548547775, 1697548548144, 1697548548194, 1697548548242, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041]"
2437,2437,326,35,[],200,llama-7b,64,1,1662.0,1.0,1,A100,1697548517525,1697548519187,120,345.0,12.0,"[15, 525, 242, 66, 64, 59, 57, 362, 63, 49, 58, 58, 44]","[1697548517540, 1697548518065, 1697548518307, 1697548518373, 1697548518437, 1697548518496, 1697548518553, 1697548518915, 1697548518978, 1697548519027, 1697548519085, 1697548519143, 1697548519187]"
2438,2438,888,45,[],200,llama-7b,64,1,1001.0,1.0,1,A100,1697548509838,1697548510839,120,19.0,1.0,"[15, 986]","[1697548509853, 1697548510839]"
2439,2439,649,26,[],200,llama-7b,64,1,2625.0,1.0,1,A100,1697548527826,1697548530451,120,244.0,20.0,"[152, 918, 59, 51, 50, 48, 47, 40, 364, 52, 50, 40, 41, 42, 49, 346, 56, 49, 59, 58, 54]","[1697548527978, 1697548528896, 1697548528955, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529657, 1697548529697, 1697548529738, 1697548529780, 1697548529829, 1697548530175, 1697548530231, 1697548530280, 1697548530339, 1697548530397, 1697548530451]"
2440,2440,120,43,[],200,llama-7b,64,1,297.0,1.0,1,A100,1697548550044,1697548550341,120,17.0,1.0,"[13, 283]","[1697548550057, 1697548550340]"
2441,2441,549,46,[],200,llama-7b,64,1,4624.0,1.0,1,A100,1697548510842,1697548515466,120,93.0,20.0,"[23, 694, 243, 57, 54, 52, 55, 749, 350, 58, 56, 43, 54, 834, 67, 59, 58, 58, 919, 71, 70]","[1697548510865, 1697548511559, 1697548511802, 1697548511859, 1697548511913, 1697548511965, 1697548512020, 1697548512769, 1697548513119, 1697548513177, 1697548513233, 1697548513276, 1697548513330, 1697548514164, 1697548514231, 1697548514290, 1697548514348, 1697548514406, 1697548515325, 1697548515396, 1697548515466]"
2442,2442,330,21,[],200,llama-7b,64,1,2511.0,1.0,1,A100,1697548517526,1697548520037,120,345.0,14.0,"[24, 515, 242, 66, 64, 59, 58, 361, 62, 50, 58, 57, 45, 607, 242]","[1697548517550, 1697548518065, 1697548518307, 1697548518373, 1697548518437, 1697548518496, 1697548518554, 1697548518915, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036]"
2443,2443,275,55,[],200,llama-7b,64,1,591.0,1.0,1,A100,1697548536460,1697548537051,120,161.0,4.0,"[11, 395, 72, 51, 61]","[1697548536471, 1697548536866, 1697548536938, 1697548536989, 1697548537050]"
2444,2444,906,36,[],200,llama-7b,64,1,3598.0,1.0,1,A100,1697548519189,1697548522787,120,86.0,20.0,"[16, 1470, 72, 65, 63, 49, 48, 54, 405, 63, 47, 60, 46, 55, 709, 69, 68, 66, 57, 66, 50]","[1697548519205, 1697548520675, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520972, 1697548521026, 1697548521431, 1697548521494, 1697548521541, 1697548521601, 1697548521647, 1697548521702, 1697548522411, 1697548522480, 1697548522548, 1697548522614, 1697548522671, 1697548522737, 1697548522787]"
2445,2445,846,26,[],200,llama-7b,64,1,1256.0,1.0,1,A100,1697548496007,1697548497263,120,140.0,6.0,"[9, 915, 78, 67, 63, 61, 63]","[1697548496016, 1697548496931, 1697548497009, 1697548497076, 1697548497139, 1697548497200, 1697548497263]"
2446,2446,421,27,[],200,llama-7b,64,1,3229.0,1.0,1,A100,1697548530456,1697548533685,120,85.0,20.0,"[90, 821, 80, 64, 64, 61, 60, 51, 736, 53, 59, 63, 51, 55, 576, 64, 64, 49, 66, 55, 46]","[1697548530546, 1697548531367, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531696, 1697548531747, 1697548532483, 1697548532536, 1697548532595, 1697548532658, 1697548532709, 1697548532764, 1697548533340, 1697548533404, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533684]"
2447,2447,846,42,[],200,llama-7b,64,1,1242.0,1.0,1,A100,1697548530454,1697548531696,120,140.0,6.0,"[44, 869, 80, 64, 64, 61, 60]","[1697548530498, 1697548531367, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531696]"
2448,2448,82,28,[],200,llama-7b,64,1,3492.0,1.0,1,A100,1697548533687,1697548537179,120,67.0,20.0,"[20, 857, 66, 47, 59, 58, 57, 707, 59, 59, 58, 55, 668, 63, 49, 54, 314, 52, 60, 66, 64]","[1697548533707, 1697548534564, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536569, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537179]"
2449,2449,919,22,[],200,llama-7b,64,1,635.0,1.0,1,A100,1697548520040,1697548520675,120,14.0,1.0,"[16, 619]","[1697548520056, 1697548520675]"
2450,2450,688,23,[],200,llama-7b,64,1,863.0,1.0,1,A100,1697548520678,1697548521541,120,345.0,4.0,"[10, 675, 68, 62, 48]","[1697548520688, 1697548521363, 1697548521431, 1697548521493, 1697548521541]"
2451,2451,500,43,[],200,llama-7b,64,1,1819.0,1.0,1,A100,1697548531698,1697548533517,120,335.0,11.0,"[7, 413, 366, 52, 59, 64, 51, 54, 576, 64, 63, 50]","[1697548531705, 1697548532118, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517]"
2452,2452,617,27,[],200,llama-7b,64,1,2761.0,1.0,1,A100,1697548497264,1697548500025,120,87.0,20.0,"[8, 300, 66, 60, 48, 46, 45, 400, 61, 52, 51, 51, 350, 59, 51, 50, 826, 61, 62, 59, 55]","[1697548497272, 1697548497572, 1697548497638, 1697548497698, 1697548497746, 1697548497792, 1697548497837, 1697548498237, 1697548498298, 1697548498350, 1697548498401, 1697548498452, 1697548498802, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500025]"
2453,2453,271,28,[],200,llama-7b,64,1,3564.0,1.0,1,A100,1697548500028,1697548503592,120,87.0,20.0,"[16, 381, 73, 64, 51, 48, 59, 54, 724, 63, 60, 58, 56, 54, 622, 65, 63, 62, 59, 55, 877]","[1697548500044, 1697548500425, 1697548500498, 1697548500562, 1697548500613, 1697548500661, 1697548500720, 1697548500774, 1697548501498, 1697548501561, 1697548501621, 1697548501679, 1697548501735, 1697548501789, 1697548502411, 1697548502476, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503592]"
2454,2454,245,44,[],200,llama-7b,64,1,3048.0,1.0,1,A100,1697548533520,1697548536568,120,100.0,20.0,"[15, 396, 64, 60, 46, 45, 59, 58, 368, 46, 58, 60, 56, 707, 59, 59, 58, 55, 668, 63, 48]","[1697548533535, 1697548533931, 1697548533995, 1697548534055, 1697548534101, 1697548534146, 1697548534205, 1697548534263, 1697548534631, 1697548534677, 1697548534735, 1697548534795, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568]"
2455,2455,781,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548537181,1697548537857,120,,,"[14, 572]","[1697548537195, 1697548537767]"
2456,2456,439,30,[],200,llama-7b,64,1,1106.0,1.0,1,A100,1697548537876,1697548538982,120,13.0,4.0,"[93, 862, 53, 55, 43]","[1697548537969, 1697548538831, 1697548538884, 1697548538939, 1697548538982]"
2457,2457,212,31,[],200,llama-7b,64,1,526.0,1.0,1,A100,1697548538984,1697548539510,120,31.0,1.0,"[10, 516]","[1697548538994, 1697548539510]"
2458,2458,803,32,[],200,llama-7b,64,1,872.0,1.0,1,A100,1697548539514,1697548540386,120,20.0,1.0,"[29, 843]","[1697548539543, 1697548540386]"
2459,2459,574,33,[],200,llama-7b,64,1,741.0,1.0,1,A100,1697548540388,1697548541129,120,364.0,2.0,"[16, 670, 55]","[1697548540404, 1697548541074, 1697548541129]"
2460,2460,227,34,[],200,llama-7b,64,1,4952.0,1.0,1,A100,1697548541132,1697548546084,120,364.0,25.0,"[6, 453, 336, 60, 59, 51, 651, 61, 61, 58, 43, 56, 693, 59, 45, 58, 55, 52, 788, 55, 54, 46, 951, 71, 65, 65]","[1697548541138, 1697548541591, 1697548541927, 1697548541987, 1697548542046, 1697548542097, 1697548542748, 1697548542809, 1697548542870, 1697548542928, 1697548542971, 1697548543027, 1697548543720, 1697548543779, 1697548543824, 1697548543882, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544886, 1697548544932, 1697548545883, 1697548545954, 1697548546019, 1697548546084]"
2461,2461,684,37,[],200,llama-7b,64,1,3064.0,1.0,1,A100,1697548522790,1697548525854,120,100.0,20.0,"[12, 863, 73, 52, 67, 51, 64, 64, 554, 64, 65, 65, 60, 57, 589, 66, 66, 51, 50, 65, 66]","[1697548522802, 1697548523665, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524036, 1697548524590, 1697548524654, 1697548524719, 1697548524784, 1697548524844, 1697548524901, 1697548525490, 1697548525556, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525854]"
2462,2462,242,42,[],200,llama-7b,64,1,2124.0,1.0,1,A100,1697548553502,1697548555626,120,345.0,9.0,"[15, 916, 438, 251, 253, 58, 44, 45, 45, 59]","[1697548553517, 1697548554433, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555477, 1697548555522, 1697548555567, 1697548555626]"
2463,2463,904,35,[],200,llama-7b,64,1,993.0,1.0,1,A100,1697548546085,1697548547078,120,563.0,2.0,"[7, 769, 217]","[1697548546092, 1697548546861, 1697548547078]"
2464,2464,725,36,[],200,llama-7b,64,1,3156.0,1.0,1,A100,1697548539871,1697548543027,120,90.0,20.0,"[17, 497, 74, 50, 58, 47, 58, 56, 52, 348, 48, 751, 60, 60, 50, 650, 63, 61, 56, 45, 55]","[1697548539888, 1697548540385, 1697548540459, 1697548540509, 1697548540567, 1697548540614, 1697548540672, 1697548540728, 1697548540780, 1697548541128, 1697548541176, 1697548541927, 1697548541987, 1697548542047, 1697548542097, 1697548542747, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027]"
2465,2465,479,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548534266,1697548537856,120,,,"[6, 903, 384, 58, 59, 59, 55, 668, 62, 48, 55, 315, 52, 60, 65, 64, 60, 46]","[1697548534272, 1697548535175, 1697548535559, 1697548535617, 1697548535676, 1697548535735, 1697548535790, 1697548536458, 1697548536520, 1697548536568, 1697548536623, 1697548536938, 1697548536990, 1697548537050, 1697548537115, 1697548537179, 1697548537239, 1697548537285]"
2466,2466,250,48,[],200,llama-7b,64,1,954.0,1.0,1,A100,1697548537877,1697548538831,120,31.0,1.0,"[206, 748]","[1697548538083, 1697548538831]"
2467,2467,833,49,[],200,llama-7b,64,1,1224.0,1.0,1,A100,1697548538834,1697548540058,120,563.0,8.0,"[25, 650, 242, 60, 56, 53, 45, 52, 41]","[1697548538859, 1697548539509, 1697548539751, 1697548539811, 1697548539867, 1697548539920, 1697548539965, 1697548540017, 1697548540058]"
2468,2468,319,47,[],200,llama-7b,64,1,597.0,1.0,1,A100,1697548515472,1697548516069,120,31.0,1.0,"[31, 566]","[1697548515503, 1697548516069]"
2469,2469,378,37,[],200,llama-7b,64,1,8024.0,1.0,1,A100,1697548543030,1697548551054,120,93.0,47.0,"[24, 1722, 55, 54, 47, 952, 71, 65, 64, 62, 61, 48, 62, 760, 65, 64, 59, 49, 57, 46, 256, 54, 47, 370, 51, 47, 810, 61, 51, 59, 47, 58, 55, 396, 60, 47, 47, 48, 60, 55, 312, 64, 55, 49, 53, 372, 53]","[1697548543054, 1697548544776, 1697548544831, 1697548544885, 1697548544932, 1697548545884, 1697548545955, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547371, 1697548547417, 1697548547673, 1697548547727, 1697548547774, 1697548548144, 1697548548195, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549779, 1697548549839, 1697548549886, 1697548549933, 1697548549981, 1697548550041, 1697548550096, 1697548550408, 1697548550472, 1697548550527, 1697548550576, 1697548550629, 1697548551001, 1697548551054]"
2470,2470,521,44,[],200,llama-7b,64,1,934.0,1.0,1,A100,1697548548765,1697548549699,120,18.0,1.0,"[22, 912]","[1697548548787, 1697548549699]"
2471,2471,903,48,[],200,llama-7b,64,1,2236.0,1.0,1,A100,1697548516072,1697548518308,120,244.0,7.0,"[21, 1228, 72, 66, 64, 58, 54, 673]","[1697548516093, 1697548517321, 1697548517393, 1697548517459, 1697548517523, 1697548517581, 1697548517635, 1697548518308]"
2472,2472,174,45,[],200,llama-7b,64,1,3012.0,1.0,1,A100,1697548549702,1697548552714,120,87.0,20.0,"[21, 617, 69, 63, 56, 49, 53, 371, 53, 51, 50, 49, 426, 62, 57, 44, 45, 55, 55, 712, 54]","[1697548549723, 1697548550340, 1697548550409, 1697548550472, 1697548550528, 1697548550577, 1697548550630, 1697548551001, 1697548551054, 1697548551105, 1697548551155, 1697548551204, 1697548551630, 1697548551692, 1697548551749, 1697548551793, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552714]"
2473,2473,118,34,[],200,llama-7b,64,1,4139.0,1.0,1,A100,1697548500428,1697548504567,120,85.0,20.0,"[14, 761, 295, 63, 60, 58, 56, 54, 622, 65, 63, 62, 59, 55, 877, 62, 58, 57, 53, 41, 704]","[1697548500442, 1697548501203, 1697548501498, 1697548501561, 1697548501621, 1697548501679, 1697548501735, 1697548501789, 1697548502411, 1697548502476, 1697548502539, 1697548502601, 1697548502660, 1697548502715, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567]"
2474,2474,647,49,[],200,llama-7b,64,1,2662.0,1.0,1,A100,1697548518311,1697548520973,120,83.0,20.0,"[11, 524, 69, 63, 49, 59, 56, 45, 607, 243, 68, 62, 62, 62, 59, 61, 335, 66, 63, 49, 49]","[1697548518322, 1697548518846, 1697548518915, 1697548518978, 1697548519027, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520037, 1697548520105, 1697548520167, 1697548520229, 1697548520291, 1697548520350, 1697548520411, 1697548520746, 1697548520812, 1697548520875, 1697548520924, 1697548520973]"
2475,2475,461,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548534268,1697548537856,120,,,"[30, 1260, 59, 60, 57, 56, 668, 62, 49, 54, 315, 52, 60, 66, 63, 60, 47]","[1697548534298, 1697548535558, 1697548535617, 1697548535677, 1697548535734, 1697548535790, 1697548536458, 1697548536520, 1697548536569, 1697548536623, 1697548536938, 1697548536990, 1697548537050, 1697548537116, 1697548537179, 1697548537239, 1697548537286]"
2476,2476,230,45,[],200,llama-7b,64,1,1163.0,1.0,1,A100,1697548537864,1697548539027,120,86.0,5.0,"[85, 881, 54, 55, 42, 46]","[1697548537949, 1697548538830, 1697548538884, 1697548538939, 1697548538981, 1697548539027]"
2477,2477,819,46,[],200,llama-7b,64,1,480.0,1.0,1,A100,1697548539030,1697548539510,120,13.0,1.0,"[13, 467]","[1697548539043, 1697548539510]"
2478,2478,584,47,[],200,llama-7b,64,1,871.0,1.0,1,A100,1697548539514,1697548540385,120,10.0,1.0,"[24, 847]","[1697548539538, 1697548540385]"
2479,2479,246,48,[],200,llama-7b,64,1,8775.0,1.0,1,A100,1697548540389,1697548549164,120,58.0,47.0,"[25, 661, 54, 48, 750, 59, 61, 50, 650, 63, 61, 56, 45, 55, 692, 59, 47, 57, 55, 52, 787, 56, 53, 46, 952, 71, 65, 64, 63, 61, 48, 62, 759, 66, 64, 59, 49, 57, 46, 256, 54, 47, 370, 50, 48, 810, 61, 51]","[1697548540414, 1697548541075, 1697548541129, 1697548541177, 1697548541927, 1697548541986, 1697548542047, 1697548542097, 1697548542747, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776, 1697548544832, 1697548544885, 1697548544931, 1697548545883, 1697548545954, 1697548546019, 1697548546083, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547076, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547371, 1697548547417, 1697548547673, 1697548547727, 1697548547774, 1697548548144, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164]"
2480,2480,300,50,[],200,llama-7b,64,1,387.0,1.0,1,A100,1697548520977,1697548521364,120,9.0,1.0,"[24, 363]","[1697548521001, 1697548521364]"
2481,2481,75,51,[],200,llama-7b,64,1,2543.0,1.0,1,A100,1697548521366,1697548523909,120,345.0,18.0,"[21, 758, 266, 70, 67, 67, 56, 66, 50, 317, 66, 64, 64, 62, 61, 317, 52, 67, 51]","[1697548521387, 1697548522145, 1697548522411, 1697548522481, 1697548522548, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908]"
2482,2482,847,26,[],200,llama-7b,64,1,349.0,1.0,1,A100,1697548516973,1697548517322,120,10.0,1.0,"[7, 342]","[1697548516980, 1697548517322]"
2483,2483,616,27,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548517330,1697548527818,120,,,"[27, 707, 243, 65, 65, 59, 57, 362, 63, 49, 58, 57, 45, 607, 242, 68, 62, 62, 63, 59, 60, 336, 66, 63, 49, 49, 53, 405, 62, 48, 60, 46, 54, 709, 70, 67, 67, 57, 66, 50, 316, 67, 64, 64, 62, 61, 317, 52, 67, 51, 64, 63, 554, 64, 66, 65, 59, 57, 589, 67, 66, 50, 51, 65, 65, 241, 49, 47, 60, 59, 654, 66, 50, 64, 65, 63]","[1697548517357, 1697548518064, 1697548518307, 1697548518372, 1697548518437, 1697548518496, 1697548518553, 1697548518915, 1697548518978, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520291, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520924, 1697548520973, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524035, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524900, 1697548525489, 1697548525556, 1697548525622, 1697548525672, 1697548525723, 1697548525788, 1697548525853, 1697548526094, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526963, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527271]"
2484,2484,156,38,[],200,llama-7b,64,1,2444.0,1.0,1,A100,1697548551056,1697548553500,120,86.0,20.0,"[14, 490, 70, 62, 57, 45, 44, 55, 56, 711, 54, 65, 51, 49, 65, 64, 275, 48, 47, 61, 61]","[1697548551070, 1697548551560, 1697548551630, 1697548551692, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551949, 1697548552660, 1697548552714, 1697548552779, 1697548552830, 1697548552879, 1697548552944, 1697548553008, 1697548553283, 1697548553331, 1697548553378, 1697548553439, 1697548553500]"
2485,2485,17,49,[],200,llama-7b,64,1,533.0,1.0,1,A100,1697548549167,1697548549700,120,23.0,1.0,"[12, 521]","[1697548549179, 1697548549700]"
2486,2486,599,50,[],200,llama-7b,64,1,6492.0,1.0,1,A100,1697548549704,1697548556196,120,58.0,55.0,"[10, 626, 69, 63, 56, 48, 54, 372, 52, 51, 51, 48, 426, 62, 57, 44, 45, 55, 55, 712, 54, 65, 51, 50, 64, 64, 275, 48, 48, 60, 60, 348, 59, 46, 59, 860, 250, 254, 58, 44, 45, 45, 58, 58, 43, 50, 69, 46, 37, 45, 42, 41, 39, 39, 35, 27]","[1697548549714, 1697548550340, 1697548550409, 1697548550472, 1697548550528, 1697548550576, 1697548550630, 1697548551002, 1697548551054, 1697548551105, 1697548551156, 1697548551204, 1697548551630, 1697548551692, 1697548551749, 1697548551793, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552714, 1697548552779, 1697548552830, 1697548552880, 1697548552944, 1697548553008, 1697548553283, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554871, 1697548555121, 1697548555375, 1697548555433, 1697548555477, 1697548555522, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556169, 1697548556196]"
2487,2487,813,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548534268,1697548537856,120,,,"[25, 883, 382, 59, 60, 58, 55, 668, 62, 49, 54, 315, 52, 60, 66, 63, 60, 47]","[1697548534293, 1697548535176, 1697548535558, 1697548535617, 1697548535677, 1697548535735, 1697548535790, 1697548536458, 1697548536520, 1697548536569, 1697548536623, 1697548536938, 1697548536990, 1697548537050, 1697548537116, 1697548537179, 1697548537239, 1697548537286]"
2488,2488,785,35,[],200,llama-7b,64,1,882.0,1.0,1,A100,1697548510677,1697548511559,120,10.0,1.0,"[14, 868]","[1697548510691, 1697548511559]"
2489,2489,446,7,[],200,llama-7b,64,1,848.0,1.0,1,A100,1697548492703,1697548493551,120,26.0,1.0,"[11, 836]","[1697548492714, 1697548493550]"
2490,2490,107,8,[],200,llama-7b,64,1,490.0,1.0,1,A100,1697548493557,1697548494047,120,216.0,2.0,"[25, 465]","[1697548493582, 1697548494047]"
2491,2491,806,9,[],200,llama-7b,64,1,1954.0,1.0,1,A100,1697548494049,1697548496003,120,89.0,20.0,"[6, 497, 67, 59, 48, 60, 55, 54, 51, 297, 49, 59, 59, 54, 52, 42, 234, 55, 52, 52, 51]","[1697548494055, 1697548494552, 1697548494619, 1697548494678, 1697548494726, 1697548494786, 1697548494841, 1697548494895, 1697548494946, 1697548495243, 1697548495292, 1697548495351, 1697548495410, 1697548495464, 1697548495516, 1697548495558, 1697548495792, 1697548495847, 1697548495899, 1697548495951, 1697548496002]"
2492,2492,494,28,[],200,llama-7b,64,1,1876.0,1.0,1,A100,1697548500663,1697548502539,120,6.0,10.0,"[11, 530, 294, 63, 60, 59, 55, 54, 622, 65, 63]","[1697548500674, 1697548501204, 1697548501498, 1697548501561, 1697548501621, 1697548501680, 1697548501735, 1697548501789, 1697548502411, 1697548502476, 1697548502539]"
2493,2493,467,39,[],200,llama-7b,64,1,2863.0,1.0,1,A100,1697548537865,1697548540728,120,93.0,20.0,"[47, 262, 27, 683, 55, 42, 46, 45, 678, 61, 56, 53, 44, 52, 42, 400, 50, 59, 47, 57, 57]","[1697548537912, 1697548538174, 1697548538201, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539072, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728]"
2494,2494,561,36,[],200,llama-7b,64,1,5343.0,1.0,1,A100,1697548511562,1697548516905,120,87.0,20.0,"[18, 879, 310, 350, 59, 55, 44, 53, 833, 66, 61, 58, 57, 920, 71, 71, 63, 60, 977, 270, 68]","[1697548511580, 1697548512459, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514163, 1697548514229, 1697548514290, 1697548514348, 1697548514405, 1697548515325, 1697548515396, 1697548515467, 1697548515530, 1697548515590, 1697548516567, 1697548516837, 1697548516905]"
2495,2495,260,29,[],200,llama-7b,64,1,3017.0,1.0,1,A100,1697548502542,1697548505559,120,86.0,20.0,"[8, 579, 462, 63, 58, 57, 53, 41, 704, 62, 47, 60, 59, 58, 58, 352, 65, 50, 61, 60, 60]","[1697548502550, 1697548503129, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504567, 1697548504629, 1697548504676, 1697548504736, 1697548504795, 1697548504853, 1697548504911, 1697548505263, 1697548505328, 1697548505378, 1697548505439, 1697548505499, 1697548505559]"
2496,2496,552,10,[],200,llama-7b,64,1,2854.0,1.0,1,A100,1697548496007,1697548498861,120,87.0,20.0,"[28, 896, 76, 69, 64, 60, 63, 57, 55, 262, 61, 47, 47, 45, 400, 62, 51, 51, 50, 351, 59]","[1697548496035, 1697548496931, 1697548497007, 1697548497076, 1697548497140, 1697548497200, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497698, 1697548497745, 1697548497792, 1697548497837, 1697548498237, 1697548498299, 1697548498350, 1697548498401, 1697548498451, 1697548498802, 1697548498861]"
2497,2497,467,28,[],200,llama-7b,64,1,3847.0,1.0,1,A100,1697548500721,1697548504568,120,93.0,20.0,"[7, 475, 295, 63, 61, 58, 55, 55, 620, 65, 63, 62, 60, 55, 877, 62, 58, 57, 53, 41, 705]","[1697548500728, 1697548501203, 1697548501498, 1697548501561, 1697548501622, 1697548501680, 1697548501735, 1697548501790, 1697548502410, 1697548502475, 1697548502538, 1697548502600, 1697548502660, 1697548502715, 1697548503592, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504568]"
2498,2498,746,21,[],200,llama-7b,64,1,3197.0,1.0,1,A100,1697548498301,1697548501498,120,345.0,18.0,"[23, 414, 65, 58, 51, 50, 826, 61, 62, 59, 54, 52, 421, 65, 51, 50, 57, 54, 724]","[1697548498324, 1697548498738, 1697548498803, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499849, 1697548499911, 1697548499970, 1697548500024, 1697548500076, 1697548500497, 1697548500562, 1697548500613, 1697548500663, 1697548500720, 1697548500774, 1697548501498]"
2499,2499,613,50,[],200,llama-7b,64,1,4715.0,1.0,1,A100,1697548540061,1697548544776,120,90.0,20.0,"[15, 999, 53, 49, 750, 60, 60, 50, 650, 63, 61, 56, 45, 55, 692, 59, 47, 57, 55, 52, 787]","[1697548540076, 1697548541075, 1697548541128, 1697548541177, 1697548541927, 1697548541987, 1697548542047, 1697548542097, 1697548542747, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776]"
2500,2500,556,27,[],200,llama-7b,64,1,527.0,1.0,1,A100,1697548489303,1697548489830,120,9.0,1.0,"[13, 514]","[1697548489316, 1697548489830]"
2501,2501,191,28,[],200,llama-7b,64,1,3229.0,1.0,1,A100,1697548489832,1697548493061,120,85.0,20.0,"[16, 518, 41, 374, 50, 49, 41, 553, 51, 48, 40, 49, 49, 327, 54, 53, 42, 52, 50, 49, 723]","[1697548489848, 1697548490366, 1697548490407, 1697548490781, 1697548490831, 1697548490880, 1697548490921, 1697548491474, 1697548491525, 1697548491573, 1697548491613, 1697548491662, 1697548491711, 1697548492038, 1697548492092, 1697548492145, 1697548492187, 1697548492239, 1697548492289, 1697548492338, 1697548493061]"
2502,2502,75,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548534266,1697548537856,120,,,"[16, 894, 382, 59, 59, 59, 54, 669, 62, 49, 54, 315, 52, 60, 65, 64, 60, 47]","[1697548534282, 1697548535176, 1697548535558, 1697548535617, 1697548535676, 1697548535735, 1697548535789, 1697548536458, 1697548536520, 1697548536569, 1697548536623, 1697548536938, 1697548536990, 1697548537050, 1697548537115, 1697548537179, 1697548537239, 1697548537286]"
2503,2503,468,32,[],200,llama-7b,64,1,3064.0,1.0,1,A100,1697548522790,1697548525854,120,31.0,20.0,"[24, 851, 73, 52, 67, 52, 63, 64, 553, 64, 66, 65, 60, 57, 589, 67, 65, 51, 50, 65, 66]","[1697548522814, 1697548523665, 1697548523738, 1697548523790, 1697548523857, 1697548523909, 1697548523972, 1697548524036, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524844, 1697548524901, 1697548525490, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525854]"
2504,2504,273,19,[],200,llama-7b,64,1,659.0,1.0,1,A100,1697548513124,1697548513783,120,19.0,1.0,"[20, 639]","[1697548513144, 1697548513783]"
2505,2505,43,20,[],200,llama-7b,64,1,3051.0,1.0,1,A100,1697548513786,1697548516837,120,732.0,8.0,"[14, 1057, 468, 71, 69, 65, 60, 977, 270]","[1697548513800, 1697548514857, 1697548515325, 1697548515396, 1697548515465, 1697548515530, 1697548515590, 1697548516567, 1697548516837]"
2506,2506,205,11,[],200,llama-7b,64,1,3547.0,1.0,1,A100,1697548498864,1697548502411,120,87.0,20.0,"[19, 537, 368, 62, 61, 59, 55, 51, 421, 65, 50, 49, 58, 55, 724, 64, 60, 58, 55, 55, 620]","[1697548498883, 1697548499420, 1697548499788, 1697548499850, 1697548499911, 1697548499970, 1697548500025, 1697548500076, 1697548500497, 1697548500562, 1697548500612, 1697548500661, 1697548500719, 1697548500774, 1697548501498, 1697548501562, 1697548501622, 1697548501680, 1697548501735, 1697548501790, 1697548502410]"
2507,2507,899,26,[],200,llama-7b,64,1,2737.0,1.0,1,A100,1697548508166,1697548510903,120,100.0,20.0,"[9, 504, 69, 57, 54, 50, 40, 40, 579, 58, 53, 52, 54, 49, 559, 60, 59, 46, 58, 56, 230]","[1697548508175, 1697548508679, 1697548508748, 1697548508805, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509568, 1697548509626, 1697548509679, 1697548509731, 1697548509785, 1697548509834, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510672, 1697548510902]"
2508,2508,50,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526966,1697548527821,120,,,"[12, 599]","[1697548526978, 1697548527577]"
2509,2509,66,36,[],200,llama-7b,64,1,4393.0,1.0,1,A100,1697548509837,1697548514230,120,84.0,20.0,"[6, 996, 64, 57, 42, 43, 51, 51, 654, 58, 54, 52, 51, 754, 350, 58, 55, 44, 53, 834, 66]","[1697548509843, 1697548510839, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511801, 1697548511859, 1697548511913, 1697548511965, 1697548512016, 1697548512770, 1697548513120, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514164, 1697548514230]"
2510,2510,559,27,[],200,llama-7b,64,1,4561.0,1.0,1,A100,1697548510905,1697548515466,120,86.0,20.0,"[7, 647, 243, 58, 53, 52, 55, 749, 350, 59, 55, 43, 54, 834, 67, 59, 58, 58, 919, 71, 70]","[1697548510912, 1697548511559, 1697548511802, 1697548511860, 1697548511913, 1697548511965, 1697548512020, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514164, 1697548514231, 1697548514290, 1697548514348, 1697548514406, 1697548515325, 1697548515396, 1697548515466]"
2511,2511,327,28,[],200,llama-7b,64,1,2054.0,1.0,1,A100,1697548515469,1697548517523,120,563.0,10.0,"[10, 590, 498, 270, 68, 66, 65, 63, 293, 66, 65]","[1697548515479, 1697548516069, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517036, 1697548517099, 1697548517392, 1697548517458, 1697548517523]"
2512,2512,891,29,[],200,llama-7b,64,1,782.0,1.0,1,A100,1697548517526,1697548518308,120,52.0,2.0,"[30, 509, 242]","[1697548517556, 1697548518065, 1697548518307]"
2513,2513,661,30,[],200,llama-7b,64,1,1795.0,1.0,1,A100,1697548518310,1697548520105,120,161.0,10.0,"[7, 529, 69, 63, 49, 59, 56, 45, 607, 242, 69]","[1697548518317, 1697548518846, 1697548518915, 1697548518978, 1697548519027, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520105]"
2514,2514,627,21,[],200,llama-7b,64,1,3264.0,1.0,1,A100,1697548516840,1697548520104,120,93.0,20.0,"[8, 474, 71, 66, 64, 58, 54, 673, 64, 65, 58, 58, 362, 63, 50, 58, 56, 45, 607, 242, 68]","[1697548516848, 1697548517322, 1697548517393, 1697548517459, 1697548517523, 1697548517581, 1697548517635, 1697548518308, 1697548518372, 1697548518437, 1697548518495, 1697548518553, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104]"
2515,2515,744,37,[],200,llama-7b,64,1,1358.0,1.0,1,A100,1697548514233,1697548515591,120,161.0,6.0,"[18, 607, 467, 71, 70, 64, 61]","[1697548514251, 1697548514858, 1697548515325, 1697548515396, 1697548515466, 1697548515530, 1697548515591]"
2516,2516,851,46,[],200,llama-7b,64,1,498.0,1.0,1,A100,1697548552719,1697548553217,120,23.0,1.0,"[24, 474]","[1697548552743, 1697548553217]"
2517,2517,506,47,[],200,llama-7b,64,1,559.0,1.0,1,A100,1697548553222,1697548553781,120,16.0,1.0,"[18, 540]","[1697548553240, 1697548553780]"
2518,2518,911,12,[],200,llama-7b,64,1,2324.0,1.0,1,A100,1697548502413,1697548504737,120,335.0,11.0,"[17, 699, 462, 63, 58, 57, 53, 41, 703, 63, 47, 61]","[1697548502430, 1697548503129, 1697548503591, 1697548503654, 1697548503712, 1697548503769, 1697548503822, 1697548503863, 1697548504566, 1697548504629, 1697548504676, 1697548504737]"
2519,2519,316,31,[],200,llama-7b,64,1,2679.0,1.0,1,A100,1697548520108,1697548522787,120,86.0,20.0,"[26, 541, 72, 65, 63, 49, 49, 53, 405, 62, 48, 60, 46, 54, 710, 69, 67, 67, 57, 66, 50]","[1697548520134, 1697548520675, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520973, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787]"
2520,2520,706,44,[],200,llama-7b,64,1,2665.0,1.0,1,A100,1697548550343,1697548553008,120,86.0,20.0,"[10, 587, 61, 53, 52, 50, 48, 426, 63, 56, 45, 44, 55, 55, 712, 54, 65, 51, 50, 64, 64]","[1697548550353, 1697548550940, 1697548551001, 1697548551054, 1697548551106, 1697548551156, 1697548551204, 1697548551630, 1697548551693, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552714, 1697548552779, 1697548552830, 1697548552880, 1697548552944, 1697548553008]"
2521,2521,276,48,[],200,llama-7b,64,1,2060.0,1.0,1,A100,1697548553785,1697548555845,120,732.0,13.0,"[27, 1059, 251, 253, 58, 45, 45, 45, 58, 57, 44, 49, 69]","[1697548553812, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555523, 1697548555568, 1697548555626, 1697548555683, 1697548555727, 1697548555776, 1697548555845]"
2522,2522,288,43,[],200,llama-7b,64,1,2798.0,1.0,1,A100,1697548518849,1697548521647,120,93.0,20.0,"[21, 740, 184, 242, 68, 63, 62, 61, 61, 59, 336, 66, 63, 48, 49, 53, 406, 62, 48, 60, 46]","[1697548518870, 1697548519610, 1697548519794, 1697548520036, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520351, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521025, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647]"
2523,2523,400,22,[],200,llama-7b,64,1,919.0,1.0,1,A100,1697548520107,1697548521026,120,123.0,7.0,"[17, 551, 72, 65, 63, 49, 49, 53]","[1697548520124, 1697548520675, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520973, 1697548521026]"
2524,2524,54,23,[],200,llama-7b,64,1,3008.0,1.0,1,A100,1697548521028,1697548524036,120,87.0,20.0,"[13, 1104, 266, 70, 67, 67, 56, 66, 50, 317, 66, 64, 64, 62, 61, 317, 52, 67, 51, 64, 64]","[1697548521041, 1697548522145, 1697548522411, 1697548522481, 1697548522548, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524036]"
2525,2525,476,45,[],200,llama-7b,64,1,3812.0,1.0,1,A100,1697548553012,1697548556824,120,6.0,50.0,"[25, 743, 67, 60, 45, 59, 859, 251, 253, 59, 45, 45, 45, 58, 58, 43, 49, 69, 47, 36, 45, 42, 41, 39, 39, 34, 28, 32, 27, 26, 26, 27, 31, 25, 31, 25, 31, 24, 25, 25, 24, 30, 24, 24, 29, 23, 24, 24, 23, 24, 23]","[1697548553037, 1697548553780, 1697548553847, 1697548553907, 1697548553952, 1697548554011, 1697548554870, 1697548555121, 1697548555374, 1697548555433, 1697548555478, 1697548555523, 1697548555568, 1697548555626, 1697548555684, 1697548555727, 1697548555776, 1697548555845, 1697548555892, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556168, 1697548556196, 1697548556228, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556390, 1697548556421, 1697548556446, 1697548556477, 1697548556501, 1697548556526, 1697548556551, 1697548556575, 1697548556605, 1697548556629, 1697548556653, 1697548556682, 1697548556705, 1697548556729, 1697548556753, 1697548556776, 1697548556800, 1697548556823]"
2526,2526,93,32,[],200,llama-7b,64,1,3064.0,1.0,1,A100,1697548522790,1697548525854,120,88.0,20.0,"[11, 864, 73, 52, 67, 51, 64, 64, 554, 64, 65, 65, 60, 57, 589, 66, 66, 51, 50, 65, 65]","[1697548522801, 1697548523665, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524036, 1697548524590, 1697548524654, 1697548524719, 1697548524784, 1697548524844, 1697548524901, 1697548525490, 1697548525556, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853]"
2527,2527,871,44,[],200,llama-7b,64,1,1023.0,1.0,1,A100,1697548521649,1697548522672,120,123.0,6.0,"[19, 477, 265, 70, 68, 66, 57]","[1697548521668, 1697548522145, 1697548522410, 1697548522480, 1697548522548, 1697548522614, 1697548522671]"
2528,2528,639,45,[],200,llama-7b,64,1,686.0,1.0,1,A100,1697548522674,1697548523360,120,100.0,6.0,"[17, 339, 74, 66, 64, 64, 62]","[1697548522691, 1697548523030, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360]"
2529,2529,271,46,[],200,llama-7b,64,1,2490.0,1.0,1,A100,1697548523363,1697548525853,120,87.0,20.0,"[7, 296, 73, 51, 67, 52, 63, 64, 553, 64, 66, 65, 59, 58, 587, 69, 65, 51, 50, 65, 65]","[1697548523370, 1697548523666, 1697548523739, 1697548523790, 1697548523857, 1697548523909, 1697548523972, 1697548524036, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525488, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853]"
2530,2530,214,37,[],200,llama-7b,64,1,3196.0,1.0,1,A100,1697548516908,1697548520104,120,52.0,20.0,"[7, 407, 71, 66, 64, 58, 54, 673, 64, 65, 58, 58, 362, 63, 50, 58, 56, 45, 607, 242, 68]","[1697548516915, 1697548517322, 1697548517393, 1697548517459, 1697548517523, 1697548517581, 1697548517635, 1697548518308, 1697548518372, 1697548518437, 1697548518495, 1697548518553, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104]"
2531,2531,464,46,[],200,llama-7b,64,1,767.0,1.0,1,A100,1697548553014,1697548553781,120,12.0,1.0,"[33, 734]","[1697548553047, 1697548553781]"
2532,2532,233,47,[],200,llama-7b,64,1,650.0,1.0,1,A100,1697548553784,1697548554434,120,6.0,1.0,"[23, 627]","[1697548553807, 1697548554434]"
2533,2533,127,31,[],200,llama-7b,64,1,1118.0,1.0,1,A100,1697548522790,1697548523908,120,100.0,5.0,"[6, 869, 73, 52, 67, 51]","[1697548522796, 1697548523665, 1697548523738, 1697548523790, 1697548523857, 1697548523908]"
2534,2534,670,38,[],200,llama-7b,64,1,2475.0,1.0,1,A100,1697548521434,1697548523909,120,67.0,18.0,"[6, 705, 265, 71, 67, 67, 56, 66, 50, 317, 66, 64, 64, 62, 61, 318, 51, 67, 52]","[1697548521440, 1697548522145, 1697548522410, 1697548522481, 1697548522548, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523739, 1697548523790, 1697548523857, 1697548523909]"
2535,2535,144,52,[],200,llama-7b,64,1,3063.0,1.0,1,A100,1697548538984,1697548542047,120,96.0,20.0,"[6, 520, 241, 61, 56, 53, 44, 52, 42, 399, 50, 59, 47, 57, 57, 51, 350, 48, 749, 61, 60]","[1697548538990, 1697548539510, 1697548539751, 1697548539812, 1697548539868, 1697548539921, 1697548539965, 1697548540017, 1697548540059, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779, 1697548541129, 1697548541177, 1697548541926, 1697548541987, 1697548542047]"
2536,2536,826,32,[],200,llama-7b,64,1,3053.0,1.0,1,A100,1697548523911,1697548526964,120,87.0,20.0,"[14, 428, 237, 64, 65, 65, 59, 58, 588, 67, 66, 50, 51, 65, 65, 242, 48, 48, 60, 58, 655]","[1697548523925, 1697548524353, 1697548524590, 1697548524654, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525622, 1697548525672, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526309, 1697548526964]"
2537,2537,844,53,[],200,llama-7b,64,1,452.0,1.0,1,A100,1697548542050,1697548542502,120,10.0,1.0,"[14, 438]","[1697548542064, 1697548542502]"
2538,2538,501,54,[],200,llama-7b,64,1,841.0,1.0,1,A100,1697548542508,1697548543349,120,19.0,1.0,"[28, 813]","[1697548542536, 1697548543349]"
2539,2539,124,29,[],200,llama-7b,64,1,693.0,1.0,1,A100,1697548504570,1697548505263,120,83.0,2.0,"[28, 664]","[1697548504598, 1697548505262]"
2540,2540,271,55,[],200,llama-7b,64,1,4066.0,1.0,1,A100,1697548543352,1697548547418,120,87.0,20.0,"[15, 1155, 254, 56, 53, 47, 952, 71, 66, 63, 62, 61, 48, 62, 760, 65, 64, 59, 49, 58, 46]","[1697548543367, 1697548544522, 1697548544776, 1697548544832, 1697548544885, 1697548544932, 1697548545884, 1697548545955, 1697548546021, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547372, 1697548547418]"
2541,2541,825,30,[],200,llama-7b,64,1,3022.0,1.0,1,A100,1697548505265,1697548508287,120,96.0,20.0,"[7, 507, 63, 46, 54, 52, 678, 66, 58, 55, 43, 581, 65, 61, 60, 56, 56, 324, 65, 64, 61]","[1697548505272, 1697548505779, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507540, 1697548507601, 1697548507661, 1697548507717, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508287]"
2542,2542,915,38,[],200,llama-7b,64,1,569.0,1.0,1,A100,1697548520107,1697548520676,120,182.0,1.0,"[6, 562]","[1697548520113, 1697548520675]"
2543,2543,323,39,[],200,llama-7b,64,1,3054.0,1.0,1,A100,1697548523911,1697548526965,120,84.0,20.0,"[15, 427, 237, 64, 65, 65, 59, 58, 588, 67, 66, 50, 51, 65, 65, 242, 48, 48, 60, 58, 655]","[1697548523926, 1697548524353, 1697548524590, 1697548524654, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525622, 1697548525672, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526309, 1697548526964]"
2544,2544,569,39,[],200,llama-7b,64,1,684.0,1.0,1,A100,1697548520679,1697548521363,120,16.0,1.0,"[19, 665]","[1697548520698, 1697548521363]"
2545,2545,486,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526969,1697548527818,120,,,"[18, 590]","[1697548526987, 1697548527577]"
2546,2546,340,40,[],200,llama-7b,64,1,2670.0,1.0,1,A100,1697548521366,1697548524036,120,85.0,20.0,"[15, 764, 266, 70, 67, 67, 56, 66, 50, 317, 66, 64, 64, 62, 61, 317, 52, 67, 51, 64, 64]","[1697548521381, 1697548522145, 1697548522411, 1697548522481, 1697548522548, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524036]"
2547,2547,254,34,[],200,llama-7b,64,1,439.0,1.0,1,A100,1697548527826,1697548528265,120,58.0,1.0,"[18, 421]","[1697548527844, 1697548528265]"
2548,2548,485,31,[],200,llama-7b,64,1,514.0,1.0,1,A100,1697548508291,1697548508805,120,67.0,3.0,"[10, 379, 68, 56]","[1697548508301, 1697548508680, 1697548508748, 1697548508804]"
2549,2549,255,32,[],200,llama-7b,64,1,17500.0,1.0,1,A100,1697548508809,1697548526309,120,216.0,119.0,"[7, 623, 129, 57, 54, 53, 53, 50, 558, 60, 58, 46, 59, 56, 231, 57, 42, 44, 51, 50, 655, 56, 55, 52, 51, 753, 350, 59, 55, 43, 54, 834, 65, 61, 58, 57, 919, 72, 69, 65, 60, 976, 271, 68, 66, 64, 64, 293, 66, 64, 59, 53, 672, 66, 64, 59, 58, 361, 63, 50, 58, 57, 45, 607, 242, 68, 62, 62, 62, 60, 60, 336, 66, 63, 48, 49, 53, 406, 62, 48, 60, 46, 54, 709, 70, 67, 67, 57, 66, 49, 317, 67, 64, 64, 62, 61, 317, 52, 67, 51, 63, 64, 554, 64, 65, 66, 59, 57, 589, 67, 65, 51, 50, 66, 65, 242, 48, 47, 60, 59]","[1697548508816, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835, 1697548510393, 1697548510453, 1697548510511, 1697548510557, 1697548510616, 1697548510672, 1697548510903, 1697548510960, 1697548511002, 1697548511046, 1697548511097, 1697548511147, 1697548511802, 1697548511858, 1697548511913, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514164, 1697548514229, 1697548514290, 1697548514348, 1697548514405, 1697548515324, 1697548515396, 1697548515465, 1697548515530, 1697548515590, 1697548516566, 1697548516837, 1697548516905, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517634, 1697548518306, 1697548518372, 1697548518436, 1697548518495, 1697548518553, 1697548518914, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521025, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522786, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523971, 1697548524035, 1697548524589, 1697548524653, 1697548524718, 1697548524784, 1697548524843, 1697548524900, 1697548525489, 1697548525556, 1697548525621, 1697548525672, 1697548525722, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526190, 1697548526250, 1697548526309]"
2550,2550,105,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548525859,1697548527819,120,,,"[24, 827, 254, 65, 50, 64, 65, 64]","[1697548525883, 1697548526710, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2551,2551,71,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526967,1697548527822,120,,,"[15, 595]","[1697548526982, 1697548527577]"
2552,2552,478,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548537053,1697548537857,120,,,"[6, 708]","[1697548537059, 1697548537767]"
2553,2553,771,41,[],200,llama-7b,64,1,2624.0,1.0,1,A100,1697548527826,1697548530450,120,47.0,20.0,"[135, 935, 58, 52, 50, 48, 48, 39, 364, 52, 50, 40, 42, 41, 48, 348, 55, 49, 59, 59, 52]","[1697548527961, 1697548528896, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529152, 1697548529191, 1697548529555, 1697548529607, 1697548529657, 1697548529697, 1697548529739, 1697548529780, 1697548529828, 1697548530176, 1697548530231, 1697548530280, 1697548530339, 1697548530398, 1697548530450]"
2554,2554,134,37,[],200,llama-7b,64,1,2915.0,1.0,1,A100,1697548537864,1697548540779,120,86.0,20.0,"[73, 893, 54, 55, 42, 46, 45, 678, 61, 56, 53, 44, 52, 42, 400, 50, 59, 47, 57, 57, 51]","[1697548537937, 1697548538830, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539072, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779]"
2555,2555,633,32,[],200,llama-7b,64,1,2623.0,1.0,1,A100,1697548527826,1697548530449,120,90.0,20.0,"[110, 959, 60, 52, 50, 48, 47, 39, 365, 52, 49, 41, 41, 40, 50, 347, 55, 49, 59, 59, 51]","[1697548527936, 1697548528895, 1697548528955, 1697548529007, 1697548529057, 1697548529105, 1697548529152, 1697548529191, 1697548529556, 1697548529608, 1697548529657, 1697548529698, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530280, 1697548530339, 1697548530398, 1697548530449]"
2556,2556,395,38,[],200,llama-7b,64,1,4511.0,1.0,1,A100,1697548515593,1697548520104,120,88.0,20.0,"[15, 1713, 72, 65, 65, 58, 54, 672, 65, 65, 59, 57, 362, 63, 50, 57, 57, 45, 607, 242, 68]","[1697548515608, 1697548517321, 1697548517393, 1697548517458, 1697548517523, 1697548517581, 1697548517635, 1697548518307, 1697548518372, 1697548518437, 1697548518496, 1697548518553, 1697548518915, 1697548518978, 1697548519028, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104]"
2557,2557,766,28,[],200,llama-7b,64,1,1285.0,1.0,1,A100,1697548505998,1697548507283,120,11.0,1.0,"[6, 1278]","[1697548506004, 1697548507282]"
2558,2558,426,29,[],200,llama-7b,64,1,4679.0,1.0,1,A100,1697548507286,1697548511965,120,79.0,36.0,"[18, 794, 64, 64, 61, 58, 55, 42, 306, 56, 55, 50, 40, 40, 579, 57, 54, 53, 53, 50, 558, 59, 59, 47, 58, 56, 230, 57, 43, 42, 52, 51, 654, 57, 54, 53]","[1697548507304, 1697548508098, 1697548508162, 1697548508226, 1697548508287, 1697548508345, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508909, 1697548508949, 1697548508989, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835, 1697548510393, 1697548510452, 1697548510511, 1697548510558, 1697548510616, 1697548510672, 1697548510902, 1697548510959, 1697548511002, 1697548511044, 1697548511096, 1697548511147, 1697548511801, 1697548511858, 1697548511912, 1697548511965]"
2559,2559,831,38,[],200,llama-7b,64,1,807.0,1.0,1,A100,1697548540783,1697548541590,120,11.0,1.0,"[21, 786]","[1697548540804, 1697548541590]"
2560,2560,172,39,[],200,llama-7b,64,1,566.0,1.0,1,A100,1697548520109,1697548520675,120,19.0,1.0,"[30, 536]","[1697548520139, 1697548520675]"
2561,2561,464,39,[],200,llama-7b,64,1,906.0,1.0,1,A100,1697548541596,1697548542502,120,12.0,1.0,"[20, 886]","[1697548541616, 1697548542502]"
2562,2562,196,30,[],200,llama-7b,64,1,491.0,1.0,1,A100,1697548511968,1697548512459,120,13.0,1.0,"[13, 478]","[1697548511981, 1697548512459]"
2563,2563,754,40,[],200,llama-7b,64,1,1023.0,1.0,1,A100,1697548520678,1697548521701,120,88.0,7.0,"[18, 735, 62, 48, 60, 46, 54]","[1697548520696, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701]"
2564,2564,871,31,[],200,llama-7b,64,1,1945.0,1.0,1,A100,1697548512461,1697548514406,120,123.0,6.0,"[10, 1311, 382, 66, 60, 58, 58]","[1697548512471, 1697548513782, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406]"
2565,2565,231,40,[],200,llama-7b,64,1,842.0,1.0,1,A100,1697548542507,1697548543349,120,13.0,1.0,"[17, 825]","[1697548542524, 1697548543349]"
2566,2566,824,41,[],200,llama-7b,64,1,1533.0,1.0,1,A100,1697548543353,1697548544886,120,58.0,4.0,"[24, 1145, 254, 56, 53]","[1697548543377, 1697548544522, 1697548544776, 1697548544832, 1697548544885]"
2567,2567,323,24,[],200,llama-7b,64,1,2493.0,1.0,1,A100,1697548521544,1697548524037,120,84.0,20.0,"[12, 589, 266, 69, 68, 66, 57, 66, 50, 317, 66, 64, 64, 62, 61, 318, 52, 67, 51, 63, 65]","[1697548521556, 1697548522145, 1697548522411, 1697548522480, 1697548522548, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523739, 1697548523791, 1697548523858, 1697548523909, 1697548523972, 1697548524037]"
2568,2568,906,43,[],200,llama-7b,64,1,2484.0,1.0,1,A100,1697548548146,1697548550630,120,86.0,20.0,"[6, 609, 292, 61, 50, 60, 47, 57, 56, 395, 60, 48, 47, 48, 59, 55, 313, 64, 55, 49, 53]","[1697548548152, 1697548548761, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549328, 1697548549384, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550096, 1697548550409, 1697548550473, 1697548550528, 1697548550577, 1697548550630]"
2569,2569,496,52,[],200,llama-7b,64,1,2309.0,1.0,1,A100,1697548544835,1697548547144,120,335.0,11.0,"[6, 674, 368, 72, 65, 64, 63, 60, 49, 61, 760, 66]","[1697548544841, 1697548545515, 1697548545883, 1697548545955, 1697548546020, 1697548546084, 1697548546147, 1697548546207, 1697548546256, 1697548546317, 1697548547077, 1697548547143]"
2570,2570,686,44,[],200,llama-7b,64,1,927.0,1.0,1,A100,1697548550633,1697548551560,120,31.0,1.0,"[9, 918]","[1697548550642, 1697548551560]"
2571,2571,340,45,[],200,llama-7b,64,1,3811.0,1.0,1,A100,1697548551564,1697548555375,120,85.0,20.0,"[19, 703, 374, 54, 66, 50, 50, 65, 63, 275, 48, 48, 60, 60, 348, 60, 46, 58, 860, 251, 253]","[1697548551583, 1697548552286, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553008, 1697548553283, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553907, 1697548553953, 1697548554011, 1697548554871, 1697548555122, 1697548555375]"
2572,2572,524,32,[],200,llama-7b,64,1,5942.0,1.0,1,A100,1697548514408,1697548520350,120,100.0,30.0,"[16, 1644, 499, 270, 68, 65, 65, 64, 293, 66, 64, 59, 53, 673, 66, 65, 58, 57, 362, 63, 49, 58, 57, 45, 607, 242, 68, 62, 62, 62, 60]","[1697548514424, 1697548516068, 1697548516567, 1697548516837, 1697548516905, 1697548516970, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517522, 1697548517581, 1697548517634, 1697548518307, 1697548518373, 1697548518438, 1697548518496, 1697548518553, 1697548518915, 1697548518978, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350]"
2573,2573,215,40,[],200,llama-7b,64,1,345.0,1.0,1,A100,1697548540731,1697548541076,120,12.0,1.0,"[24, 320]","[1697548540755, 1697548541075]"
2574,2574,858,56,[],200,llama-7b,64,1,2360.0,1.0,1,A100,1697548547420,1697548549780,120,182.0,12.0,"[16, 709, 49, 49, 810, 61, 50, 60, 47, 58, 55, 396]","[1697548547436, 1697548548145, 1697548548194, 1697548548243, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549780]"
2575,2575,847,35,[],200,llama-7b,64,1,625.0,1.0,1,A100,1697548528272,1697548528897,120,10.0,1.0,"[11, 613]","[1697548528283, 1697548528896]"
2576,2576,658,52,[],200,llama-7b,64,1,441.0,1.0,1,A100,1697548523912,1697548524353,120,11.0,1.0,"[18, 423]","[1697548523930, 1697548524353]"
2577,2577,434,53,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548524356,1697548527820,120,,,"[12, 881, 240, 68, 65, 51, 50, 65, 66, 241, 48, 47, 60, 59, 656, 65, 49, 64, 65, 64]","[1697548524368, 1697548525249, 1697548525489, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525854, 1697548526095, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526965, 1697548527030, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2578,2578,617,36,[],200,llama-7b,64,1,2738.0,1.0,1,A100,1697548528898,1697548531636,120,87.0,20.0,"[9, 595, 54, 51, 50, 41, 41, 40, 50, 347, 55, 48, 59, 60, 51, 486, 39, 472, 65, 63, 62]","[1697548528907, 1697548529502, 1697548529556, 1697548529607, 1697548529657, 1697548529698, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530279, 1697548530338, 1697548530398, 1697548530449, 1697548530935, 1697548530974, 1697548531446, 1697548531511, 1697548531574, 1697548531636]"
2579,2579,527,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548521705,1697548527819,120,,,"[6, 1392, 68, 63, 64, 62, 62, 317, 51, 68, 50, 63, 64, 554, 64, 66, 65, 59, 58, 588, 67, 66, 51, 50, 65, 65, 242, 48, 48, 60, 59, 653, 66, 50, 64, 65, 64]","[1697548521711, 1697548523103, 1697548523171, 1697548523234, 1697548523298, 1697548523360, 1697548523422, 1697548523739, 1697548523790, 1697548523858, 1697548523908, 1697548523971, 1697548524035, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526963, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2580,2580,90,54,[],200,llama-7b,64,1,1068.0,1.0,1,A100,1697548527828,1697548528896,120,19.0,1.0,"[233, 835]","[1697548528061, 1697548528896]"
2581,2581,562,13,[],200,llama-7b,64,1,4992.0,1.0,1,A100,1697548504740,1697548509732,120,67.0,39.0,"[6, 447, 69, 66, 51, 60, 61, 59, 283, 46, 54, 52, 678, 66, 58, 55, 44, 580, 64, 62, 60, 55, 57, 324, 65, 64, 60, 59, 55, 42, 305, 57, 55, 49, 41, 40, 580, 56, 54, 52]","[1697548504746, 1697548505193, 1697548505262, 1697548505328, 1697548505379, 1697548505439, 1697548505500, 1697548505559, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507475, 1697548507539, 1697548507601, 1697548507661, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508400, 1697548508442, 1697548508747, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989, 1697548509569, 1697548509625, 1697548509679, 1697548509731]"
2582,2582,272,37,[],200,llama-7b,64,1,2625.0,1.0,1,A100,1697548531639,1697548534264,120,86.0,20.0,"[6, 474, 365, 52, 59, 64, 51, 54, 576, 64, 63, 50, 67, 54, 46, 311, 60, 45, 46, 59, 59]","[1697548531645, 1697548532119, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533584, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534264]"
2583,2583,10,43,[],200,llama-7b,64,1,2424.0,1.0,1,A100,1697548511806,1697548514230,120,563.0,9.0,"[15, 638, 311, 350, 58, 55, 44, 53, 833, 67]","[1697548511821, 1697548512459, 1697548512770, 1697548513120, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514163, 1697548514230]"
2584,2584,788,55,[],200,llama-7b,64,1,602.0,1.0,1,A100,1697548528900,1697548529502,120,31.0,1.0,"[38, 564]","[1697548528938, 1697548529502]"
2585,2585,453,56,[],200,llama-7b,64,1,602.0,1.0,1,A100,1697548529506,1697548530108,120,26.0,1.0,"[19, 582]","[1697548529525, 1697548530107]"
2586,2586,223,57,[],200,llama-7b,64,1,774.0,1.0,1,A100,1697548530111,1697548530885,120,16.0,1.0,"[22, 752]","[1697548530133, 1697548530885]"
2587,2587,48,29,[],200,llama-7b,64,1,1202.0,1.0,1,A100,1697548503594,1697548504796,120,6.0,6.0,"[22, 952, 61, 48, 60, 59]","[1697548503616, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796]"
2588,2588,594,42,[],200,llama-7b,64,1,12057.0,1.0,1,A100,1697548544890,1697548556947,120,216.0,119.0,"[32, 593, 369, 71, 65, 64, 63, 61, 48, 62, 759, 67, 63, 58, 49, 58, 45, 257, 53, 47, 371, 49, 48, 810, 61, 51, 59, 47, 58, 55, 396, 60, 48, 47, 48, 59, 56, 311, 64, 55, 49, 53, 372, 52, 52, 50, 49, 425, 62, 58, 44, 44, 55, 56, 711, 54, 66, 50, 50, 65, 64, 274, 49, 47, 60, 60, 349, 59, 46, 58, 860, 251, 253, 59, 44, 45, 45, 58, 58, 43, 50, 69, 46, 36, 46, 42, 41, 39, 39, 34, 28, 32, 27, 26, 26, 26, 32, 25, 31, 25, 31, 24, 25, 25, 24, 30, 24, 24, 29, 23, 24, 23, 24, 24, 24, 28, 24, 23, 26, 22]","[1697548544922, 1697548545515, 1697548545884, 1697548545955, 1697548546020, 1697548546084, 1697548546147, 1697548546208, 1697548546256, 1697548546318, 1697548547077, 1697548547144, 1697548547207, 1697548547265, 1697548547314, 1697548547372, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550408, 1697548550472, 1697548550527, 1697548550576, 1697548550629, 1697548551001, 1697548551053, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551691, 1697548551749, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552659, 1697548552713, 1697548552779, 1697548552829, 1697548552879, 1697548552944, 1697548553008, 1697548553282, 1697548553331, 1697548553378, 1697548553438, 1697548553498, 1697548553847, 1697548553906, 1697548553952, 1697548554010, 1697548554870, 1697548555121, 1697548555374, 1697548555433, 1697548555477, 1697548555522, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555927, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556168, 1697548556196, 1697548556228, 1697548556255, 1697548556281, 1697548556307, 1697548556333, 1697548556365, 1697548556390, 1697548556421, 1697548556446, 1697548556477, 1697548556501, 1697548556526, 1697548556551, 1697548556575, 1697548556605, 1697548556629, 1697548556653, 1697548556682, 1697548556705, 1697548556729, 1697548556752, 1697548556776, 1697548556800, 1697548556824, 1697548556852, 1697548556876, 1697548556899, 1697548556925, 1697548556947]"
2589,2589,49,38,[],200,llama-7b,64,1,1351.0,1.0,1,A100,1697548534267,1697548535618,120,109.0,3.0,"[31, 878, 382, 60]","[1697548534298, 1697548535176, 1697548535558, 1697548535618]"
2590,2590,632,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548535621,1697548537858,120,,,"[6, 646, 184, 64, 47, 55, 315, 51, 61, 65, 63, 60, 47]","[1697548535627, 1697548536273, 1697548536457, 1697548536521, 1697548536568, 1697548536623, 1697548536938, 1697548536989, 1697548537050, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
2591,2591,333,14,[],200,llama-7b,64,1,1311.0,1.0,1,A100,1697548509734,1697548511045,120,563.0,11.0,"[6, 587, 66, 60, 59, 46, 59, 56, 230, 56, 43, 43]","[1697548509740, 1697548510327, 1697548510393, 1697548510453, 1697548510512, 1697548510558, 1697548510617, 1697548510673, 1697548510903, 1697548510959, 1697548511002, 1697548511045]"
2592,2592,187,42,[],200,llama-7b,64,1,1279.0,1.0,1,A100,1697548527825,1697548529104,120,161.0,6.0,"[37, 403, 34, 656, 51, 50, 48]","[1697548527862, 1697548528265, 1697548528299, 1697548528955, 1697548529006, 1697548529056, 1697548529104]"
2593,2593,886,43,[],200,llama-7b,64,1,395.0,1.0,1,A100,1697548529108,1697548529503,120,17.0,1.0,"[13, 381]","[1697548529121, 1697548529502]"
2594,2594,542,44,[],200,llama-7b,64,1,601.0,1.0,1,A100,1697548529507,1697548530108,120,11.0,1.0,"[33, 568]","[1697548529540, 1697548530108]"
2595,2595,310,45,[],200,llama-7b,64,1,776.0,1.0,1,A100,1697548530109,1697548530885,120,26.0,1.0,"[16, 760]","[1697548530125, 1697548530885]"
2596,2596,871,46,[],200,llama-7b,64,1,808.0,1.0,1,A100,1697548530888,1697548531696,120,123.0,6.0,"[15, 465, 79, 64, 64, 62, 59]","[1697548530903, 1697548531368, 1697548531447, 1697548531511, 1697548531575, 1697548531637, 1697548531696]"
2597,2597,725,36,[],200,llama-7b,64,1,4399.0,1.0,1,A100,1697548513124,1697548517523,120,90.0,20.0,"[25, 633, 382, 66, 60, 58, 58, 918, 71, 70, 64, 61, 977, 271, 68, 65, 64, 64, 293, 67, 64]","[1697548513149, 1697548513782, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406, 1697548515324, 1697548515395, 1697548515465, 1697548515529, 1697548515590, 1697548516567, 1697548516838, 1697548516906, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517459, 1697548517523]"
2598,2598,922,15,[],200,llama-7b,64,1,4419.0,1.0,1,A100,1697548511048,1697548515467,120,91.0,20.0,"[6, 505, 243, 56, 55, 52, 55, 749, 350, 59, 55, 43, 54, 833, 68, 59, 58, 57, 920, 71, 70]","[1697548511054, 1697548511559, 1697548511802, 1697548511858, 1697548511913, 1697548511965, 1697548512020, 1697548512769, 1697548513119, 1697548513178, 1697548513233, 1697548513276, 1697548513330, 1697548514163, 1697548514231, 1697548514290, 1697548514348, 1697548514405, 1697548515325, 1697548515396, 1697548515466]"
2599,2599,400,22,[],200,llama-7b,64,1,1215.0,1.0,1,A100,1697548501500,1697548502715,120,123.0,7.0,"[7, 650, 253, 65, 64, 62, 60, 54]","[1697548501507, 1697548502157, 1697548502410, 1697548502475, 1697548502539, 1697548502601, 1697548502661, 1697548502715]"
2600,2600,640,47,[],200,llama-7b,64,1,419.0,1.0,1,A100,1697548531700,1697548532119,120,15.0,1.0,"[27, 392]","[1697548531727, 1697548532119]"
2601,2601,267,51,[],200,llama-7b,64,1,3366.0,1.0,1,A100,1697548544779,1697548548145,120,83.0,20.0,"[17, 719, 368, 71, 65, 65, 62, 61, 48, 62, 759, 67, 63, 60, 48, 57, 46, 257, 53, 47, 371]","[1697548544796, 1697548545515, 1697548545883, 1697548545954, 1697548546019, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547076, 1697548547143, 1697548547206, 1697548547266, 1697548547314, 1697548547371, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145]"
2602,2602,850,30,[],200,llama-7b,64,1,3185.0,1.0,1,A100,1697548505562,1697548508747,120,109.0,20.0,"[7, 831, 272, 66, 58, 55, 44, 580, 65, 61, 60, 56, 57, 323, 65, 65, 60, 59, 54, 42, 305]","[1697548505569, 1697548506400, 1697548506672, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507475, 1697548507540, 1697548507601, 1697548507661, 1697548507717, 1697548507774, 1697548508097, 1697548508162, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508747]"
2603,2603,296,48,[],200,llama-7b,64,1,1028.0,1.0,1,A100,1697548532122,1697548533150,120,6.0,1.0,"[25, 1003]","[1697548532147, 1697548533150]"
2604,2604,795,41,[],200,llama-7b,64,1,512.0,1.0,1,A100,1697548541079,1697548541591,120,12.0,1.0,"[14, 498]","[1697548541093, 1697548541591]"
2605,2605,565,42,[],200,llama-7b,64,1,4424.0,1.0,1,A100,1697548541596,1697548546020,120,91.0,20.0,"[25, 881, 245, 62, 61, 57, 45, 55, 692, 59, 47, 57, 55, 52, 788, 55, 54, 46, 951, 71, 66]","[1697548541621, 1697548542502, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544886, 1697548544932, 1697548545883, 1697548545954, 1697548546020]"
2606,2606,733,39,[],200,llama-7b,64,1,930.0,1.0,1,A100,1697548553504,1697548554434,120,31.0,1.0,"[41, 889]","[1697548553545, 1697548554434]"
2607,2607,897,58,[],200,llama-7b,64,1,480.0,1.0,1,A100,1697548530888,1697548531368,120,9.0,1.0,"[21, 459]","[1697548530909, 1697548531368]"
2608,2608,551,59,[],200,llama-7b,64,1,2892.0,1.0,1,A100,1697548531371,1697548534263,120,90.0,20.0,"[22, 725, 366, 52, 59, 64, 50, 55, 576, 65, 63, 49, 66, 55, 46, 311, 60, 45, 46, 59, 58]","[1697548531393, 1697548532118, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532709, 1697548532764, 1697548533340, 1697548533405, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263]"
2609,2609,266,53,[],200,llama-7b,64,1,469.0,1.0,1,A100,1697548547146,1697548547615,120,9.0,1.0,"[13, 456]","[1697548547159, 1697548547615]"
2610,2610,855,54,[],200,llama-7b,64,1,2857.0,1.0,1,A100,1697548547616,1697548550473,120,83.0,20.0,"[21, 445, 63, 50, 48, 810, 61, 51, 58, 47, 58, 55, 397, 60, 47, 47, 48, 60, 55, 312, 64]","[1697548547637, 1697548548082, 1697548548145, 1697548548195, 1697548548243, 1697548549053, 1697548549114, 1697548549165, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549780, 1697548549840, 1697548549887, 1697548549934, 1697548549982, 1697548550042, 1697548550097, 1697548550409, 1697548550473]"
2611,2611,16,45,[],200,llama-7b,64,1,295.0,1.0,1,A100,1697548536572,1697548536867,120,9.0,1.0,"[14, 281]","[1697548536586, 1697548536867]"
2612,2612,95,25,[],200,llama-7b,64,1,1203.0,1.0,1,A100,1697548524046,1697548525249,120,12.0,1.0,"[19, 1184]","[1697548524065, 1697548525249]"
2613,2613,601,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536869,1697548537856,120,,,"[30, 868]","[1697548536899, 1697548537767]"
2614,2614,676,33,[],200,llama-7b,64,1,852.0,1.0,1,A100,1697548525858,1697548526710,120,19.0,1.0,"[28, 824]","[1697548525886, 1697548526710]"
2615,2615,450,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526712,1697548527820,120,,,"[7, 858]","[1697548526719, 1697548527577]"
2616,2616,677,26,[],200,llama-7b,64,1,776.0,1.0,1,A100,1697548525253,1697548526029,120,9.0,1.0,"[20, 755]","[1697548525273, 1697548526028]"
2617,2617,104,35,[],200,llama-7b,64,1,2623.0,1.0,1,A100,1697548527826,1697548530449,120,93.0,20.0,"[160, 910, 59, 51, 51, 47, 48, 39, 364, 52, 50, 40, 41, 42, 49, 346, 56, 48, 60, 58, 51]","[1697548527986, 1697548528896, 1697548528955, 1697548529006, 1697548529057, 1697548529104, 1697548529152, 1697548529191, 1697548529555, 1697548529607, 1697548529657, 1697548529697, 1697548529738, 1697548529780, 1697548529829, 1697548530175, 1697548530231, 1697548530279, 1697548530339, 1697548530397, 1697548530448]"
2618,2618,1,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548524043,1697548527820,120,,,"[20, 1186, 240, 68, 65, 51, 50, 65, 65, 242, 48, 47, 61, 58, 656, 64, 50, 64, 65, 64]","[1697548524063, 1697548525249, 1697548525489, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526190, 1697548526251, 1697548526309, 1697548526965, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2619,2619,373,47,[],200,llama-7b,64,1,956.0,1.0,1,A100,1697548537876,1697548538832,120,15.0,1.0,"[171, 785]","[1697548538047, 1697548538832]"
2620,2620,807,36,[],200,llama-7b,64,1,2623.0,1.0,1,A100,1697548527826,1697548530449,120,90.0,20.0,"[125, 944, 60, 51, 51, 48, 47, 39, 365, 52, 49, 41, 41, 41, 49, 347, 55, 49, 59, 59, 51]","[1697548527951, 1697548528895, 1697548528955, 1697548529006, 1697548529057, 1697548529105, 1697548529152, 1697548529191, 1697548529556, 1697548529608, 1697548529657, 1697548529698, 1697548529739, 1697548529780, 1697548529829, 1697548530176, 1697548530231, 1697548530280, 1697548530339, 1697548530398, 1697548530449]"
2621,2621,410,33,[],200,llama-7b,64,1,2255.0,1.0,1,A100,1697548530454,1697548532709,120,364.0,12.0,"[34, 879, 80, 64, 64, 61, 59, 52, 736, 52, 60, 64, 50]","[1697548530488, 1697548531367, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531695, 1697548531747, 1697548532483, 1697548532535, 1697548532595, 1697548532659, 1697548532709]"
2622,2622,702,44,[],200,llama-7b,64,1,4206.0,1.0,1,A100,1697548514232,1697548518438,120,89.0,20.0,"[12, 614, 467, 71, 70, 64, 60, 976, 271, 69, 64, 66, 62, 295, 66, 64, 58, 53, 673, 66, 64]","[1697548514244, 1697548514858, 1697548515325, 1697548515396, 1697548515466, 1697548515530, 1697548515590, 1697548516566, 1697548516837, 1697548516906, 1697548516970, 1697548517036, 1697548517098, 1697548517393, 1697548517459, 1697548517523, 1697548517581, 1697548517634, 1697548518307, 1697548518373, 1697548518437]"
2623,2623,73,49,[],200,llama-7b,64,1,777.0,1.0,1,A100,1697548533154,1697548533931,120,9.0,1.0,"[19, 758]","[1697548533173, 1697548533931]"
2624,2624,655,50,[],200,llama-7b,64,1,1856.0,1.0,1,A100,1697548533934,1697548535790,120,335.0,11.0,"[14, 616, 66, 47, 59, 58, 57, 707, 59, 60, 58, 55]","[1697548533948, 1697548534564, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535677, 1697548535735, 1697548535790]"
2625,2625,556,36,[],200,llama-7b,64,1,533.0,1.0,1,A100,1697548547081,1697548547614,120,9.0,1.0,"[22, 511]","[1697548547103, 1697548547614]"
2626,2626,332,37,[],200,llama-7b,64,1,466.0,1.0,1,A100,1697548547616,1697548548082,120,39.0,1.0,"[19, 447]","[1697548547635, 1697548548082]"
2627,2627,337,38,[],200,llama-7b,64,1,851.0,1.0,1,A100,1697548525859,1697548526710,120,12.0,1.0,"[37, 814]","[1697548525896, 1697548526710]"
2628,2628,109,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526713,1697548527821,120,,,"[20, 844]","[1697548526733, 1697548527577]"
2629,2629,40,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548525857,1697548527818,120,,,"[6, 846, 254, 66, 50, 64, 65, 64]","[1697548525863, 1697548526709, 1697548526963, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2630,2630,702,40,[],200,llama-7b,64,1,2571.0,1.0,1,A100,1697548527826,1697548530397,120,89.0,20.0,"[21, 418, 33, 656, 52, 50, 48, 47, 40, 364, 52, 49, 41, 41, 41, 49, 347, 56, 48, 59, 59]","[1697548527847, 1697548528265, 1697548528298, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529656, 1697548529697, 1697548529738, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530397]"
2631,2631,699,42,[],200,llama-7b,64,1,1070.0,1.0,1,A100,1697548527827,1697548528897,120,39.0,1.0,"[212, 858]","[1697548528039, 1697548528897]"
2632,2632,820,35,[],200,llama-7b,64,1,1318.0,1.0,1,A100,1697548504570,1697548505888,120,161.0,9.0,"[18, 605, 69, 66, 51, 60, 60, 60, 283, 46]","[1697548504588, 1697548505193, 1697548505262, 1697548505328, 1697548505379, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505888]"
2633,2633,448,43,[],200,llama-7b,64,1,1439.0,1.0,1,A100,1697548528900,1697548530339,120,335.0,12.0,"[36, 566, 54, 52, 49, 41, 41, 40, 50, 347, 55, 48, 60]","[1697548528936, 1697548529502, 1697548529556, 1697548529608, 1697548529657, 1697548529698, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530279, 1697548530339]"
2634,2634,472,41,[],200,llama-7b,64,1,3183.0,1.0,1,A100,1697548530401,1697548533584,120,85.0,20.0,"[33, 451, 51, 39, 471, 66, 62, 62, 59, 53, 737, 52, 58, 64, 51, 54, 576, 64, 64, 49, 66]","[1697548530434, 1697548530885, 1697548530936, 1697548530975, 1697548531446, 1697548531512, 1697548531574, 1697548531636, 1697548531695, 1697548531748, 1697548532485, 1697548532537, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533468, 1697548533517, 1697548533583]"
2635,2635,889,29,[],200,llama-7b,64,1,2346.0,1.0,1,A100,1697548493063,1697548495409,120,86.0,20.0,"[21, 466, 62, 48, 38, 38, 310, 52, 42, 46, 433, 60, 47, 60, 55, 55, 51, 296, 49, 59, 58]","[1697548493084, 1697548493550, 1697548493612, 1697548493660, 1697548493698, 1697548493736, 1697548494046, 1697548494098, 1697548494140, 1697548494186, 1697548494619, 1697548494679, 1697548494726, 1697548494786, 1697548494841, 1697548494896, 1697548494947, 1697548495243, 1697548495292, 1697548495351, 1697548495409]"
2636,2636,475,36,[],200,llama-7b,64,1,2858.0,1.0,1,A100,1697548505890,1697548508748,120,89.0,20.0,"[6, 504, 273, 65, 58, 56, 43, 581, 63, 62, 59, 56, 57, 325, 65, 64, 60, 59, 54, 42, 306]","[1697548505896, 1697548506400, 1697548506673, 1697548506738, 1697548506796, 1697548506852, 1697548506895, 1697548507476, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508098, 1697548508163, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748]"
2637,2637,66,34,[],200,llama-7b,64,1,2846.0,1.0,1,A100,1697548532712,1697548535558,120,84.0,20.0,"[21, 416, 191, 64, 63, 50, 67, 55, 45, 311, 60, 45, 46, 59, 58, 367, 47, 59, 58, 57, 707]","[1697548532733, 1697548533149, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533584, 1697548533639, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535558]"
2638,2638,41,52,[],200,llama-7b,64,1,5291.0,1.0,1,A100,1697548548148,1697548553439,120,39.0,43.0,"[19, 594, 292, 61, 50, 60, 47, 58, 55, 395, 60, 48, 47, 48, 59, 56, 313, 63, 56, 48, 52, 372, 53, 51, 50, 49, 425, 63, 56, 45, 45, 55, 55, 711, 54, 66, 51, 50, 64, 64, 275, 48, 48, 60]","[1697548548167, 1697548548761, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550410, 1697548550473, 1697548550529, 1697548550577, 1697548550629, 1697548551001, 1697548551054, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551692, 1697548551748, 1697548551793, 1697548551838, 1697548551893, 1697548551948, 1697548552659, 1697548552713, 1697548552779, 1697548552830, 1697548552880, 1697548552944, 1697548553008, 1697548553283, 1697548553331, 1697548553379, 1697548553439]"
2639,2639,432,42,[],200,llama-7b,64,1,913.0,1.0,1,A100,1697548530454,1697548531367,120,13.0,1.0,"[29, 884]","[1697548530483, 1697548531367]"
2640,2640,225,43,[],200,llama-7b,64,1,838.0,1.0,1,A100,1697548546023,1697548546861,120,23.0,1.0,"[23, 815]","[1697548546046, 1697548546861]"
2641,2641,244,44,[],200,llama-7b,64,1,610.0,1.0,1,A100,1697548526967,1697548527577,120,9.0,1.0,"[23, 587]","[1697548526990, 1697548527577]"
2642,2642,833,45,[],200,llama-7b,64,1,1612.0,1.0,1,A100,1697548527579,1697548529191,120,563.0,8.0,"[17, 301, 401, 656, 52, 50, 48, 47, 40]","[1697548527596, 1697548527897, 1697548528298, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191]"
2643,2643,217,42,[],200,llama-7b,64,1,2979.0,1.0,1,A100,1697548533589,1697548536568,120,85.0,20.0,"[17, 325, 65, 59, 45, 46, 59, 58, 368, 46, 59, 59, 56, 707, 59, 59, 58, 55, 668, 63, 48]","[1697548533606, 1697548533931, 1697548533996, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263, 1697548534631, 1697548534677, 1697548534736, 1697548534795, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568]"
2644,2644,202,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548531370,1697548537858,120,,,"[18, 730, 366, 52, 59, 64, 50, 55, 576, 65, 63, 49, 66, 55, 47, 310, 60, 45, 46, 59, 58, 367, 46, 59, 59, 57, 707, 59, 60, 57, 55, 668, 63, 48, 55, 314, 52, 60, 66, 63, 60, 47]","[1697548531388, 1697548532118, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532709, 1697548532764, 1697548533340, 1697548533405, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533685, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534676, 1697548534735, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535677, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
2645,2645,598,46,[],200,llama-7b,64,1,2381.0,1.0,1,A100,1697548529194,1697548531575,120,345.0,12.0,"[12, 901, 69, 56, 47, 60, 59, 51, 486, 39, 473, 64, 64]","[1697548529206, 1697548530107, 1697548530176, 1697548530232, 1697548530279, 1697548530339, 1697548530398, 1697548530449, 1697548530935, 1697548530974, 1697548531447, 1697548531511, 1697548531575]"
2646,2646,460,37,[],200,llama-7b,64,1,3230.0,1.0,1,A100,1697548530454,1697548533684,120,87.0,20.0,"[29, 884, 80, 64, 64, 61, 59, 52, 736, 54, 58, 64, 51, 55, 575, 64, 64, 49, 66, 55, 46]","[1697548530483, 1697548531367, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531695, 1697548531747, 1697548532483, 1697548532537, 1697548532595, 1697548532659, 1697548532710, 1697548532765, 1697548533340, 1697548533404, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533684]"
2647,2647,924,44,[],200,llama-7b,64,1,750.0,1.0,1,A100,1697548546865,1697548547615,120,9.0,1.0,"[11, 739]","[1697548546876, 1697548547615]"
2648,2648,434,51,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548535791,1697548537862,120,,,"[14, 1061, 72, 51, 61, 65, 63, 61, 46]","[1697548535805, 1697548536866, 1697548536938, 1697548536989, 1697548537050, 1697548537115, 1697548537178, 1697548537239, 1697548537285]"
2649,2649,583,45,[],200,llama-7b,64,1,2857.0,1.0,1,A100,1697548547616,1697548550473,120,96.0,20.0,"[15, 451, 63, 50, 48, 810, 61, 51, 58, 47, 59, 55, 396, 59, 48, 47, 48, 60, 55, 312, 64]","[1697548547631, 1697548548082, 1697548548145, 1697548548195, 1697548548243, 1697548549053, 1697548549114, 1697548549165, 1697548549223, 1697548549270, 1697548549329, 1697548549384, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550042, 1697548550097, 1697548550409, 1697548550473]"
2650,2650,454,27,[],200,llama-7b,64,1,1178.0,1.0,1,A100,1697548526031,1697548527209,120,182.0,6.0,"[10, 669, 254, 65, 50, 64, 66]","[1697548526041, 1697548526710, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527209]"
2651,2651,206,38,[],200,llama-7b,64,1,877.0,1.0,1,A100,1697548533688,1697548534565,120,16.0,1.0,"[19, 857]","[1697548533707, 1697548534564]"
2652,2652,494,37,[],200,llama-7b,64,1,1559.0,1.0,1,A100,1697548517526,1697548519085,120,6.0,10.0,"[19, 520, 242, 66, 64, 59, 57, 362, 63, 49, 58]","[1697548517545, 1697548518065, 1697548518307, 1697548518373, 1697548518437, 1697548518496, 1697548518553, 1697548518915, 1697548518978, 1697548519027, 1697548519085]"
2653,2653,788,39,[],200,llama-7b,64,1,608.0,1.0,1,A100,1697548534568,1697548535176,120,31.0,1.0,"[23, 585]","[1697548534591, 1697548535176]"
2654,2654,155,38,[],200,llama-7b,64,1,2560.0,1.0,1,A100,1697548519087,1697548521647,120,90.0,20.0,"[6, 517, 185, 242, 67, 63, 62, 61, 61, 59, 337, 65, 63, 48, 49, 54, 405, 63, 47, 61, 45]","[1697548519093, 1697548519610, 1697548519795, 1697548520037, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520351, 1697548520410, 1697548520747, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521026, 1697548521431, 1697548521494, 1697548521541, 1697548521602, 1697548521647]"
2655,2655,764,35,[],200,llama-7b,64,1,711.0,1.0,1,A100,1697548535562,1697548536273,120,39.0,1.0,"[10, 701]","[1697548535572, 1697548536273]"
2656,2656,87,52,[],200,llama-7b,64,1,2853.0,1.0,1,A100,1697548537876,1697548540729,120,335.0,19.0,"[171, 784, 54, 54, 43, 45, 45, 678, 62, 55, 54, 44, 52, 41, 401, 49, 60, 47, 56, 57]","[1697548538047, 1697548538831, 1697548538885, 1697548538939, 1697548538982, 1697548539027, 1697548539072, 1697548539750, 1697548539812, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540459, 1697548540508, 1697548540568, 1697548540615, 1697548540671, 1697548540728]"
2657,2657,512,36,[],200,llama-7b,64,1,590.0,1.0,1,A100,1697548536276,1697548536866,120,11.0,1.0,"[21, 569]","[1697548536297, 1697548536866]"
2658,2658,852,39,[],200,llama-7b,64,1,2388.0,1.0,1,A100,1697548521649,1697548524037,120,100.0,20.0,"[25, 471, 266, 69, 68, 66, 57, 66, 50, 316, 67, 64, 64, 62, 62, 317, 52, 67, 51, 63, 65]","[1697548521674, 1697548522145, 1697548522411, 1697548522480, 1697548522548, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422, 1697548523739, 1697548523791, 1697548523858, 1697548523909, 1697548523972, 1697548524037]"
2659,2659,163,37,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536868,1697548537856,120,,,"[16, 883]","[1697548536884, 1697548537767]"
2660,2660,506,40,[],200,llama-7b,64,1,1205.0,1.0,1,A100,1697548524044,1697548525249,120,16.0,1.0,"[24, 1181]","[1697548524068, 1697548525249]"
2661,2661,282,41,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548525252,1697548527821,120,,,"[16, 760, 67, 48, 48, 60, 59, 655, 64, 50, 64, 65, 63]","[1697548525268, 1697548526028, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526965, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527271]"
2662,2662,865,38,[],200,llama-7b,64,1,955.0,1.0,1,A100,1697548537876,1697548538831,120,9.0,1.0,"[98, 857]","[1697548537974, 1697548538831]"
2663,2663,519,39,[],200,llama-7b,64,1,8308.0,1.0,1,A100,1697548538834,1697548547142,120,58.0,47.0,"[34, 642, 241, 60, 56, 54, 44, 52, 41, 400, 50, 59, 47, 57, 57, 51, 349, 49, 749, 60, 60, 51, 650, 62, 61, 57, 44, 55, 693, 59, 47, 57, 54, 53, 787, 55, 54, 46, 951, 72, 65, 64, 63, 61, 48, 62, 759, 66]","[1697548538868, 1697548539510, 1697548539751, 1697548539811, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779, 1697548541128, 1697548541177, 1697548541926, 1697548541986, 1697548542046, 1697548542097, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542971, 1697548543026, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543936, 1697548543989, 1697548544776, 1697548544831, 1697548544885, 1697548544931, 1697548545882, 1697548545954, 1697548546019, 1697548546083, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547076, 1697548547142]"
2664,2664,353,46,[],200,llama-7b,64,1,631.0,1.0,1,A100,1697548550475,1697548551106,120,52.0,4.0,"[16, 449, 61, 53, 51]","[1697548550491, 1697548550940, 1697548551001, 1697548551054, 1697548551105]"
2665,2665,328,60,[],200,llama-7b,64,1,1524.0,1.0,1,A100,1697548534266,1697548535790,120,109.0,6.0,"[16, 894, 382, 59, 60, 58, 55]","[1697548534282, 1697548535176, 1697548535558, 1697548535617, 1697548535677, 1697548535735, 1697548535790]"
2666,2666,378,40,[],200,llama-7b,64,1,8270.0,1.0,1,A100,1697548537876,1697548546146,120,93.0,47.0,"[212, 796, 55, 42, 46, 46, 677, 61, 55, 55, 44, 52, 41, 401, 50, 59, 47, 56, 56, 53, 348, 48, 750, 60, 59, 52, 650, 62, 61, 57, 44, 55, 693, 59, 47, 57, 54, 53, 787, 55, 54, 46, 951, 72, 65, 64, 63]","[1697548538088, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539073, 1697548539750, 1697548539811, 1697548539866, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540459, 1697548540509, 1697548540568, 1697548540615, 1697548540671, 1697548540727, 1697548540780, 1697548541128, 1697548541176, 1697548541926, 1697548541986, 1697548542045, 1697548542097, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542971, 1697548543026, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543936, 1697548543989, 1697548544776, 1697548544831, 1697548544885, 1697548544931, 1697548545882, 1697548545954, 1697548546019, 1697548546083, 1697548546146]"
2667,2667,912,61,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548535793,1697548537875,120,,,"[13, 1060, 72, 51, 61, 66, 62, 61, 46]","[1697548535806, 1697548536866, 1697548536938, 1697548536989, 1697548537050, 1697548537116, 1697548537178, 1697548537239, 1697548537285]"
2668,2668,682,62,[],200,llama-7b,64,1,8440.0,1.0,1,A100,1697548537877,1697548546317,120,244.0,50.0,"[212, 742, 53, 55, 42, 46, 46, 677, 61, 55, 55, 44, 52, 41, 401, 49, 60, 46, 57, 56, 53, 348, 49, 749, 60, 59, 52, 650, 62, 61, 57, 44, 55, 693, 59, 47, 57, 54, 53, 786, 56, 54, 46, 951, 72, 65, 64, 63, 61, 48, 62]","[1697548538089, 1697548538831, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539073, 1697548539750, 1697548539811, 1697548539866, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540459, 1697548540508, 1697548540568, 1697548540614, 1697548540671, 1697548540727, 1697548540780, 1697548541128, 1697548541177, 1697548541926, 1697548541986, 1697548542045, 1697548542097, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542971, 1697548543026, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543936, 1697548543989, 1697548544775, 1697548544831, 1697548544885, 1697548544931, 1697548545882, 1697548545954, 1697548546019, 1697548546083, 1697548546146, 1697548546207, 1697548546255, 1697548546317]"
2669,2669,26,48,[],200,llama-7b,64,1,674.0,1.0,1,A100,1697548538836,1697548539510,120,18.0,1.0,"[52, 622]","[1697548538888, 1697548539510]"
2670,2670,621,31,[],200,llama-7b,64,1,3053.0,1.0,1,A100,1697548508749,1697548511802,120,88.0,20.0,"[13, 677, 129, 57, 54, 53, 53, 50, 559, 58, 60, 47, 57, 56, 231, 57, 42, 43, 51, 51, 655]","[1697548508762, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835, 1697548510394, 1697548510452, 1697548510512, 1697548510559, 1697548510616, 1697548510672, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511802]"
2671,2671,780,25,[],200,llama-7b,64,1,2865.0,1.0,1,A100,1697548537863,1697548540728,120,85.0,20.0,"[46, 265, 27, 683, 55, 42, 46, 45, 678, 61, 56, 53, 44, 52, 42, 400, 50, 59, 47, 57, 57]","[1697548537909, 1697548538174, 1697548538201, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539072, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728]"
2672,2672,630,48,[],200,llama-7b,64,1,1070.0,1.0,1,A100,1697548527826,1697548528896,120,6.0,1.0,"[118, 951]","[1697548527944, 1697548528895]"
2673,2673,259,47,[],200,llama-7b,64,1,2686.0,1.0,1,A100,1697548531578,1697548534264,120,87.0,20.0,"[10, 531, 365, 52, 59, 64, 50, 55, 576, 64, 63, 50, 66, 55, 46, 311, 60, 45, 46, 59, 59]","[1697548531588, 1697548532119, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532709, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533583, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534264]"
2674,2674,171,23,[],200,llama-7b,64,1,1597.0,1.0,1,A100,1697548502716,1697548504313,120,6.0,1.0,"[8, 1588]","[1697548502724, 1697548504312]"
2675,2675,753,24,[],200,llama-7b,64,1,3346.0,1.0,1,A100,1697548504315,1697548507661,120,83.0,20.0,"[13, 865, 69, 65, 51, 61, 60, 61, 282, 46, 54, 52, 679, 65, 58, 55, 43, 581, 64, 62, 59]","[1697548504328, 1697548505193, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505560, 1697548505842, 1697548505888, 1697548505942, 1697548505994, 1697548506673, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660]"
2676,2676,800,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536571,1697548537856,120,,,"[18, 349, 52, 61, 65, 63, 60, 47]","[1697548536589, 1697548536938, 1697548536990, 1697548537051, 1697548537116, 1697548537179, 1697548537239, 1697548537286]"
2677,2677,531,25,[],200,llama-7b,64,1,2172.0,1.0,1,A100,1697548507664,1697548509836,120,52.0,20.0,"[18, 344, 72, 65, 63, 61, 59, 54, 42, 306, 56, 55, 49, 41, 40, 579, 57, 54, 53, 53, 50]","[1697548507682, 1697548508026, 1697548508098, 1697548508163, 1697548508226, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748, 1697548508804, 1697548508859, 1697548508908, 1697548508949, 1697548508989, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835]"
2678,2678,398,49,[],200,llama-7b,64,1,2738.0,1.0,1,A100,1697548528898,1697548531636,120,87.0,20.0,"[18, 586, 54, 51, 50, 41, 41, 40, 50, 347, 55, 48, 59, 60, 51, 486, 39, 472, 65, 64, 61]","[1697548528916, 1697548529502, 1697548529556, 1697548529607, 1697548529657, 1697548529698, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530279, 1697548530338, 1697548530398, 1697548530449, 1697548530935, 1697548530974, 1697548531446, 1697548531511, 1697548531575, 1697548531636]"
2679,2679,577,44,[],200,llama-7b,64,1,1948.0,1.0,1,A100,1697548537863,1697548539811,120,93.0,9.0,"[20, 291, 27, 683, 55, 41, 47, 45, 678, 61]","[1697548537883, 1697548538174, 1697548538201, 1697548538884, 1697548538939, 1697548538980, 1697548539027, 1697548539072, 1697548539750, 1697548539811]"
2680,2680,806,36,[],200,llama-7b,64,1,3230.0,1.0,1,A100,1697548530454,1697548533684,120,89.0,20.0,"[39, 874, 80, 64, 64, 61, 59, 52, 736, 52, 59, 64, 51, 54, 578, 63, 64, 49, 66, 55, 46]","[1697548530493, 1697548531367, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531695, 1697548531747, 1697548532483, 1697548532535, 1697548532594, 1697548532658, 1697548532709, 1697548532763, 1697548533341, 1697548533404, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533684]"
2681,2681,232,45,[],200,llama-7b,64,1,3214.0,1.0,1,A100,1697548539813,1697548543027,120,93.0,20.0,"[15, 557, 74, 50, 58, 47, 58, 56, 52, 348, 48, 751, 60, 60, 50, 650, 62, 62, 56, 45, 55]","[1697548539828, 1697548540385, 1697548540459, 1697548540509, 1697548540567, 1697548540614, 1697548540672, 1697548540728, 1697548540780, 1697548541128, 1697548541176, 1697548541927, 1697548541987, 1697548542047, 1697548542097, 1697548542747, 1697548542809, 1697548542871, 1697548542927, 1697548542972, 1697548543027]"
2682,2682,732,49,[],200,llama-7b,64,1,2475.0,1.0,1,A100,1697548539512,1697548541987,120,345.0,12.0,"[14, 859, 73, 50, 59, 47, 57, 57, 51, 350, 48, 749, 61]","[1697548539526, 1697548540385, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779, 1697548541129, 1697548541177, 1697548541926, 1697548541987]"
2683,2683,31,48,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548534267,1697548537856,120,,,"[10, 899, 382, 59, 59, 59, 55, 668, 62, 49, 54, 315, 52, 60, 65, 64, 60, 47]","[1697548534277, 1697548535176, 1697548535558, 1697548535617, 1697548535676, 1697548535735, 1697548535790, 1697548536458, 1697548536520, 1697548536569, 1697548536623, 1697548536938, 1697548536990, 1697548537050, 1697548537115, 1697548537179, 1697548537239, 1697548537286]"
2684,2684,628,30,[],200,llama-7b,64,1,1144.0,1.0,1,A100,1697548504799,1697548505943,120,732.0,10.0,"[12, 452, 65, 51, 60, 60, 60, 283, 46, 54]","[1697548504811, 1697548505263, 1697548505328, 1697548505379, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505888, 1697548505942]"
2685,2685,384,50,[],200,llama-7b,64,1,4030.0,1.0,1,A100,1697548541990,1697548546020,120,92.0,20.0,"[14, 498, 245, 63, 61, 56, 45, 55, 692, 59, 47, 57, 55, 52, 788, 55, 54, 46, 951, 71, 66]","[1697548542004, 1697548542502, 1697548542747, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544886, 1697548544932, 1697548545883, 1697548545954, 1697548546020]"
2686,2686,613,49,[],200,llama-7b,64,1,2915.0,1.0,1,A100,1697548537864,1697548540779,120,90.0,20.0,"[80, 886, 54, 55, 42, 46, 45, 678, 61, 56, 53, 44, 52, 42, 400, 50, 59, 47, 57, 57, 51]","[1697548537944, 1697548538830, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539072, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779]"
2687,2687,829,27,[],200,llama-7b,64,1,439.0,1.0,1,A100,1697548527826,1697548528265,120,20.0,1.0,"[11, 428]","[1697548527837, 1697548528265]"
2688,2688,606,28,[],200,llama-7b,64,1,626.0,1.0,1,A100,1697548528270,1697548528896,120,9.0,1.0,"[11, 615]","[1697548528281, 1697548528896]"
2689,2689,262,29,[],200,llama-7b,64,1,602.0,1.0,1,A100,1697548528901,1697548529503,120,39.0,1.0,"[45, 556]","[1697548528946, 1697548529502]"
2690,2690,2,46,[],200,llama-7b,64,1,2855.0,1.0,1,A100,1697548543029,1697548545884,120,58.0,6.0,"[15, 1478, 254, 55, 54, 46, 953]","[1697548543044, 1697548544522, 1697548544776, 1697548544831, 1697548544885, 1697548544931, 1697548545884]"
2691,2691,31,30,[],200,llama-7b,64,1,3204.0,1.0,1,A100,1697548529505,1697548532709,120,84.0,20.0,"[20, 582, 69, 56, 48, 59, 59, 52, 485, 39, 473, 64, 64, 61, 60, 51, 736, 53, 58, 64, 51]","[1697548529525, 1697548530107, 1697548530176, 1697548530232, 1697548530280, 1697548530339, 1697548530398, 1697548530450, 1697548530935, 1697548530974, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531696, 1697548531747, 1697548532483, 1697548532536, 1697548532594, 1697548532658, 1697548532709]"
2692,2692,160,51,[],200,llama-7b,64,1,838.0,1.0,1,A100,1697548546023,1697548546861,120,13.0,1.0,"[28, 810]","[1697548546051, 1697548546861]"
2693,2693,245,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548525858,1697548527819,120,,,"[15, 836, 255, 65, 50, 64, 65, 64]","[1697548525873, 1697548526709, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2694,2694,743,52,[],200,llama-7b,64,1,1331.0,1.0,1,A100,1697548546864,1697548548195,120,123.0,6.0,"[17, 734, 59, 53, 47, 370, 50]","[1697548546881, 1697548547615, 1697548547674, 1697548547727, 1697548547774, 1697548548144, 1697548548194]"
2695,2695,866,20,[],200,llama-7b,64,1,2625.0,1.0,1,A100,1697548527826,1697548530451,120,93.0,20.0,"[140, 930, 58, 52, 50, 48, 47, 40, 364, 52, 50, 40, 41, 42, 49, 347, 55, 49, 59, 59, 52]","[1697548527966, 1697548528896, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529657, 1697548529697, 1697548529738, 1697548529780, 1697548529829, 1697548530176, 1697548530231, 1697548530280, 1697548530339, 1697548530398, 1697548530450]"
2696,2696,518,53,[],200,llama-7b,64,1,563.0,1.0,1,A100,1697548548198,1697548548761,120,23.0,1.0,"[11, 552]","[1697548548209, 1697548548761]"
2697,2697,859,56,[],200,llama-7b,64,1,713.0,1.0,1,A100,1697548537055,1697548537768,120,23.0,1.0,"[6, 707]","[1697548537061, 1697548537768]"
2698,2698,827,34,[],200,llama-7b,64,1,2625.0,1.0,1,A100,1697548527826,1697548530451,120,96.0,20.0,"[150, 920, 58, 52, 50, 48, 47, 40, 364, 52, 50, 40, 41, 42, 49, 347, 55, 49, 59, 59, 53]","[1697548527976, 1697548528896, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529657, 1697548529697, 1697548529738, 1697548529780, 1697548529829, 1697548530176, 1697548530231, 1697548530280, 1697548530339, 1697548530398, 1697548530451]"
2699,2699,629,57,[],200,llama-7b,64,1,23279.0,1.0,1,A100,1697548537777,1697548561056,120,457.0,381.0,"[7, 152, 265, 683, 55, 41, 46, 46, 678, 61, 55, 54, 44, 52, 41, 401, 50, 59, 47, 56, 57, 52, 349, 48, 750, 60, 59, 51, 651, 62, 61, 57, 44, 55, 693, 59, 46, 58, 54, 52, 787, 56, 54, 46, 951, 72, 65, 64, 63, 61, 48, 62, 759, 66, 64, 59, 49, 57, 46, 256, 54, 47, 370, 50, 48, 810, 61, 51, 59, 47, 58, 55, 395, 60, 48, 47, 48, 59, 56, 312, 64, 55, 49, 53, 371, 53, 52, 50, 48, 426, 62, 57, 45, 44, 55, 56, 711, 54, 66, 50, 50, 65, 64, 274, 49, 47, 60, 60, 349, 59, 46, 58, 860, 251, 253, 59, 44, 45, 45, 58, 58, 43, 50, 69, 46, 36, 46, 42, 41, 39, 39, 34, 28, 32, 27, 26, 26, 27, 31, 26, 30, 25, 31, 24, 25, 25, 24, 30, 24, 24, 29, 23, 24, 23, 24, 24, 23, 29, 24, 23, 26, 22, 24, 21, 20, 21, 20, 21, 20, 21, 23, 20, 19, 20, 20, 22, 19, 20, 19, 22, 18, 19, 19, 18, 19, 19, 18, 19, 18, 19, 19, 19, 19, 19, 18, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 21, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 19, 18, 18, 18, 19, 18, 18, 18, 18, 18, 19, 18, 18, 19, 18, 18, 18, 19, 18, 19, 18, 18, 19, 18, 18, 19, 18, 18, 19, 18, 19, 18, 19, 18, 18, 19, 18, 19, 18, 19, 18, 18, 19, 18, 19, 18, 19, 18, 19, 18, 19, 19, 18, 19, 18, 19, 18, 19, 19, 18, 19, 18, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 18, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 19, 18, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19]","[1697548537784, 1697548537936, 1697548538201, 1697548538884, 1697548538939, 1697548538980, 1697548539026, 1697548539072, 1697548539750, 1697548539811, 1697548539866, 1697548539920, 1697548539964, 1697548540016, 1697548540057, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540670, 1697548540727, 1697548540779, 1697548541128, 1697548541176, 1697548541926, 1697548541986, 1697548542045, 1697548542096, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542971, 1697548543026, 1697548543719, 1697548543778, 1697548543824, 1697548543882, 1697548543936, 1697548543988, 1697548544775, 1697548544831, 1697548544885, 1697548544931, 1697548545882, 1697548545954, 1697548546019, 1697548546083, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547076, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547371, 1697548547417, 1697548547673, 1697548547727, 1697548547774, 1697548548144, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549778, 1697548549838, 1697548549886, 1697548549933, 1697548549981, 1697548550040, 1697548550096, 1697548550408, 1697548550472, 1697548550527, 1697548550576, 1697548550629, 1697548551000, 1697548551053, 1697548551105, 1697548551155, 1697548551203, 1697548551629, 1697548551691, 1697548551748, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552659, 1697548552713, 1697548552779, 1697548552829, 1697548552879, 1697548552944, 1697548553008, 1697548553282, 1697548553331, 1697548553378, 1697548553438, 1697548553498, 1697548553847, 1697548553906, 1697548553952, 1697548554010, 1697548554870, 1697548555121, 1697548555374, 1697548555433, 1697548555477, 1697548555522, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555927, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556168, 1697548556196, 1697548556228, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556391, 1697548556421, 1697548556446, 1697548556477, 1697548556501, 1697548556526, 1697548556551, 1697548556575, 1697548556605, 1697548556629, 1697548556653, 1697548556682, 1697548556705, 1697548556729, 1697548556752, 1697548556776, 1697548556800, 1697548556823, 1697548556852, 1697548556876, 1697548556899, 1697548556925, 1697548556947, 1697548556971, 1697548556992, 1697548557012, 1697548557033, 1697548557053, 1697548557074, 1697548557094, 1697548557115, 1697548557138, 1697548557158, 1697548557177, 1697548557197, 1697548557217, 1697548557239, 1697548557258, 1697548557278, 1697548557297, 1697548557319, 1697548557337, 1697548557356, 1697548557375, 1697548557393, 1697548557412, 1697548557431, 1697548557449, 1697548557468, 1697548557486, 1697548557505, 1697548557524, 1697548557543, 1697548557562, 1697548557581, 1697548557599, 1697548557618, 1697548557637, 1697548557656, 1697548557674, 1697548557693, 1697548557712, 1697548557731, 1697548557750, 1697548557769, 1697548557788, 1697548557807, 1697548557825, 1697548557844, 1697548557863, 1697548557882, 1697548557901, 1697548557920, 1697548557939, 1697548557958, 1697548557977, 1697548557996, 1697548558014, 1697548558033, 1697548558052, 1697548558071, 1697548558090, 1697548558109, 1697548558128, 1697548558147, 1697548558166, 1697548558185, 1697548558206, 1697548558224, 1697548558242, 1697548558260, 1697548558278, 1697548558295, 1697548558313, 1697548558331, 1697548558349, 1697548558367, 1697548558385, 1697548558403, 1697548558421, 1697548558439, 1697548558457, 1697548558475, 1697548558493, 1697548558511, 1697548558529, 1697548558547, 1697548558565, 1697548558583, 1697548558601, 1697548558619, 1697548558637, 1697548558656, 1697548558674, 1697548558692, 1697548558710, 1697548558728, 1697548558746, 1697548558764, 1697548558783, 1697548558801, 1697548558819, 1697548558837, 1697548558855, 1697548558873, 1697548558891, 1697548558909, 1697548558928, 1697548558946, 1697548558964, 1697548558982, 1697548559000, 1697548559019, 1697548559037, 1697548559055, 1697548559073, 1697548559091, 1697548559110, 1697548559128, 1697548559146, 1697548559164, 1697548559183, 1697548559201, 1697548559219, 1697548559237, 1697548559255, 1697548559273, 1697548559292, 1697548559310, 1697548559328, 1697548559347, 1697548559365, 1697548559383, 1697548559401, 1697548559420, 1697548559438, 1697548559457, 1697548559475, 1697548559493, 1697548559512, 1697548559530, 1697548559548, 1697548559567, 1697548559585, 1697548559603, 1697548559622, 1697548559640, 1697548559659, 1697548559677, 1697548559696, 1697548559714, 1697548559732, 1697548559751, 1697548559769, 1697548559788, 1697548559806, 1697548559825, 1697548559843, 1697548559861, 1697548559880, 1697548559898, 1697548559917, 1697548559935, 1697548559954, 1697548559972, 1697548559991, 1697548560009, 1697548560028, 1697548560047, 1697548560065, 1697548560084, 1697548560102, 1697548560121, 1697548560139, 1697548560158, 1697548560177, 1697548560195, 1697548560214, 1697548560232, 1697548560251, 1697548560269, 1697548560288, 1697548560307, 1697548560325, 1697548560344, 1697548560363, 1697548560381, 1697548560400, 1697548560419, 1697548560437, 1697548560456, 1697548560474, 1697548560493, 1697548560512, 1697548560530, 1697548560549, 1697548560568, 1697548560586, 1697548560605, 1697548560624, 1697548560642, 1697548560661, 1697548560680, 1697548560699, 1697548560717, 1697548560736, 1697548560755, 1697548560773, 1697548560792, 1697548560811, 1697548560830, 1697548560849, 1697548560867, 1697548560886, 1697548560905, 1697548560924, 1697548560943, 1697548560961, 1697548560980, 1697548560999, 1697548561018, 1697548561037, 1697548561056]"
2700,2700,390,50,[],200,llama-7b,64,1,4103.0,1.0,1,A100,1697548540783,1697548544886,120,84.0,20.0,"[11, 796, 337, 60, 59, 51, 651, 61, 61, 57, 44, 55, 694, 58, 47, 58, 54, 52, 788, 55, 53]","[1697548540794, 1697548541590, 1697548541927, 1697548541987, 1697548542046, 1697548542097, 1697548542748, 1697548542809, 1697548542870, 1697548542927, 1697548542971, 1697548543026, 1697548543720, 1697548543778, 1697548543825, 1697548543883, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544885]"
2701,2701,587,47,[],200,llama-7b,64,1,973.0,1.0,1,A100,1697548545888,1697548546861,120,13.0,1.0,"[16, 957]","[1697548545904, 1697548546861]"
2702,2702,357,48,[],200,llama-7b,64,1,4828.0,1.0,1,A100,1697548546864,1697548551692,120,52.0,33.0,"[22, 729, 59, 53, 47, 370, 50, 48, 810, 62, 50, 60, 47, 58, 54, 396, 60, 48, 47, 48, 59, 56, 312, 63, 56, 48, 53, 372, 52, 52, 50, 49, 425, 63]","[1697548546886, 1697548547615, 1697548547674, 1697548547727, 1697548547774, 1697548548144, 1697548548194, 1697548548242, 1697548549052, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549383, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472, 1697548550528, 1697548550576, 1697548550629, 1697548551001, 1697548551053, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551692]"
2703,2703,566,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548535179,1697548537857,120,,,"[18, 1076, 184, 64, 47, 55, 315, 51, 60, 66, 64, 59, 47]","[1697548535197, 1697548536273, 1697548536457, 1697548536521, 1697548536568, 1697548536623, 1697548536938, 1697548536989, 1697548537049, 1697548537115, 1697548537179, 1697548537238, 1697548537285]"
2704,2704,436,26,[],200,llama-7b,64,1,4046.0,1.0,1,A100,1697548540731,1697548544777,120,86.0,20.0,"[18, 326, 54, 48, 750, 59, 60, 51, 651, 62, 61, 56, 45, 54, 693, 59, 47, 57, 55, 52, 788]","[1697548540749, 1697548541075, 1697548541129, 1697548541177, 1697548541927, 1697548541986, 1697548542046, 1697548542097, 1697548542748, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543026, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544777]"
2705,2705,150,54,[],200,llama-7b,64,1,1016.0,1.0,1,A100,1697548548764,1697548549780,120,216.0,2.0,"[18, 998]","[1697548548782, 1697548549780]"
2706,2706,848,55,[],200,llama-7b,64,1,558.0,1.0,1,A100,1697548549783,1697548550341,120,47.0,1.0,"[23, 535]","[1697548549806, 1697548550341]"
2707,2707,221,41,[],200,llama-7b,64,1,6017.0,1.0,1,A100,1697548537865,1697548543882,120,364.0,36.0,"[74, 891, 54, 55, 42, 46, 45, 678, 61, 56, 53, 44, 52, 42, 400, 50, 59, 47, 57, 57, 51, 349, 48, 750, 60, 59, 52, 650, 62, 61, 57, 44, 55, 693, 59, 47, 57]","[1697548537939, 1697548538830, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539072, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779, 1697548541128, 1697548541176, 1697548541926, 1697548541986, 1697548542045, 1697548542097, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542971, 1697548543026, 1697548543719, 1697548543778, 1697548543825, 1697548543882]"
2708,2708,915,38,[],200,llama-7b,64,1,677.0,1.0,1,A100,1697548548084,1697548548761,120,182.0,1.0,"[21, 656]","[1697548548105, 1697548548761]"
2709,2709,683,39,[],200,llama-7b,64,1,1017.0,1.0,1,A100,1697548548763,1697548549780,120,874.0,2.0,"[14, 1002]","[1697548548777, 1697548549779]"
2710,2710,343,40,[],200,llama-7b,64,1,2932.0,1.0,1,A100,1697548549782,1697548552714,120,84.0,20.0,"[9, 549, 69, 64, 56, 48, 53, 370, 54, 51, 50, 49, 426, 63, 56, 44, 45, 55, 55, 712, 54]","[1697548549791, 1697548550340, 1697548550409, 1697548550473, 1697548550529, 1697548550577, 1697548550630, 1697548551000, 1697548551054, 1697548551105, 1697548551155, 1697548551204, 1697548551630, 1697548551693, 1697548551749, 1697548551793, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552714]"
2711,2711,508,56,[],200,llama-7b,64,1,2665.0,1.0,1,A100,1697548550344,1697548553009,120,86.0,20.0,"[23, 573, 61, 53, 51, 51, 48, 425, 64, 56, 45, 44, 55, 55, 712, 55, 65, 50, 50, 64, 65]","[1697548550367, 1697548550940, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551204, 1697548551629, 1697548551693, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552715, 1697548552780, 1697548552830, 1697548552880, 1697548552944, 1697548553009]"
2712,2712,58,50,[],200,llama-7b,64,1,480.0,1.0,1,A100,1697548531639,1697548532119,120,15.0,1.0,"[21, 459]","[1697548531660, 1697548532119]"
2713,2713,451,45,[],200,llama-7b,64,1,406.0,1.0,1,A100,1697548518440,1697548518846,120,286.0,1.0,"[13, 393]","[1697548518453, 1697548518846]"
2714,2714,690,16,[],200,llama-7b,64,1,598.0,1.0,1,A100,1697548515471,1697548516069,120,39.0,1.0,"[23, 575]","[1697548515494, 1697548516069]"
2715,2715,350,17,[],200,llama-7b,64,1,1249.0,1.0,1,A100,1697548516072,1697548517321,120,216.0,1.0,"[10, 1239]","[1697548516082, 1697548517321]"
2716,2716,104,46,[],200,llama-7b,64,1,2798.0,1.0,1,A100,1697548518849,1697548521647,120,93.0,20.0,"[26, 735, 184, 243, 67, 63, 62, 61, 60, 60, 336, 66, 63, 48, 49, 53, 406, 62, 48, 60, 46]","[1697548518875, 1697548519610, 1697548519794, 1697548520037, 1697548520104, 1697548520167, 1697548520229, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521025, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647]"
2717,2717,120,18,[],200,llama-7b,64,1,740.0,1.0,1,A100,1697548517325,1697548518065,120,17.0,1.0,"[6, 733]","[1697548517331, 1697548518064]"
2718,2718,704,19,[],200,llama-7b,64,1,778.0,1.0,1,A100,1697548518068,1697548518846,120,14.0,1.0,"[15, 763]","[1697548518083, 1697548518846]"
2719,2719,863,42,[],200,llama-7b,64,1,1071.0,1.0,1,A100,1697548527826,1697548528897,120,10.0,1.0,"[147, 923]","[1697548527973, 1697548528896]"
2720,2720,654,30,[],200,llama-7b,64,1,2267.0,1.0,1,A100,1697548512023,1697548514290,120,47.0,4.0,"[15, 1744, 382, 66, 60]","[1697548512038, 1697548513782, 1697548514164, 1697548514230, 1697548514290]"
2721,2721,611,43,[],200,llama-7b,64,1,602.0,1.0,1,A100,1697548528900,1697548529502,120,14.0,1.0,"[41, 561]","[1697548528941, 1697548529502]"
2722,2722,320,40,[],200,llama-7b,64,1,5092.0,1.0,1,A100,1697548550476,1697548555568,120,109.0,36.0,"[34, 492, 52, 51, 51, 49, 424, 63, 57, 44, 44, 56, 55, 712, 55, 65, 50, 50, 65, 64, 274, 48, 48, 60, 60, 348, 59, 46, 59, 860, 250, 254, 58, 44, 45, 46]","[1697548550510, 1697548551002, 1697548551054, 1697548551105, 1697548551156, 1697548551205, 1697548551629, 1697548551692, 1697548551749, 1697548551793, 1697548551837, 1697548551893, 1697548551948, 1697548552660, 1697548552715, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009, 1697548553283, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554871, 1697548555121, 1697548555375, 1697548555433, 1697548555477, 1697548555522, 1697548555568]"
2723,2723,424,31,[],200,llama-7b,64,1,4146.0,1.0,1,A100,1697548514292,1697548518438,120,88.0,20.0,"[6, 560, 467, 71, 70, 64, 61, 976, 270, 68, 65, 65, 64, 293, 66, 66, 58, 52, 673, 66, 65]","[1697548514298, 1697548514858, 1697548515325, 1697548515396, 1697548515466, 1697548515530, 1697548515591, 1697548516567, 1697548516837, 1697548516905, 1697548516970, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517524, 1697548517582, 1697548517634, 1697548518307, 1697548518373, 1697548518438]"
2724,2724,290,40,[],200,llama-7b,64,1,470.0,1.0,1,A100,1697548547145,1697548547615,120,14.0,1.0,"[6, 464]","[1697548547151, 1697548547615]"
2725,2725,264,44,[],200,llama-7b,64,1,3205.0,1.0,1,A100,1697548529504,1697548532709,120,86.0,20.0,"[6, 597, 69, 56, 47, 60, 59, 51, 486, 39, 473, 64, 64, 61, 59, 52, 736, 53, 58, 64, 51]","[1697548529510, 1697548530107, 1697548530176, 1697548530232, 1697548530279, 1697548530339, 1697548530398, 1697548530449, 1697548530935, 1697548530974, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531695, 1697548531747, 1697548532483, 1697548532536, 1697548532594, 1697548532658, 1697548532709]"
2726,2726,100,44,[],200,llama-7b,64,1,2368.0,1.0,1,A100,1697548530342,1697548532710,120,732.0,14.0,"[6, 537, 51, 38, 473, 64, 65, 61, 60, 50, 737, 52, 59, 64, 51]","[1697548530348, 1697548530885, 1697548530936, 1697548530974, 1697548531447, 1697548531511, 1697548531576, 1697548531637, 1697548531697, 1697548531747, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710]"
2727,2727,405,31,[],200,llama-7b,64,1,2803.0,1.0,1,A100,1697548505945,1697548508748,120,87.0,20.0,"[16, 439, 273, 65, 58, 55, 44, 581, 63, 62, 59, 56, 57, 325, 65, 64, 60, 59, 54, 42, 306]","[1697548505961, 1697548506400, 1697548506673, 1697548506738, 1697548506796, 1697548506851, 1697548506895, 1697548507476, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508098, 1697548508163, 1697548508227, 1697548508287, 1697548508346, 1697548508400, 1697548508442, 1697548508748]"
2728,2728,793,53,[],200,llama-7b,64,1,5585.0,1.0,1,A100,1697548540732,1697548546317,120,92.0,31.0,"[23, 374, 48, 750, 59, 60, 51, 651, 62, 61, 56, 44, 55, 694, 58, 47, 57, 55, 52, 788, 55, 53, 46, 952, 71, 65, 64, 63, 61, 48, 62]","[1697548540755, 1697548541129, 1697548541177, 1697548541927, 1697548541986, 1697548542046, 1697548542097, 1697548542748, 1697548542810, 1697548542871, 1697548542927, 1697548542971, 1697548543026, 1697548543720, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544885, 1697548544931, 1697548545883, 1697548545954, 1697548546019, 1697548546083, 1697548546146, 1697548546207, 1697548546255, 1697548546317]"
2729,2729,13,47,[],200,llama-7b,64,1,2391.0,1.0,1,A100,1697548551108,1697548553499,120,90.0,20.0,"[6, 446, 70, 62, 57, 45, 44, 55, 56, 711, 54, 66, 50, 50, 64, 64, 275, 48, 47, 61, 60]","[1697548551114, 1697548551560, 1697548551630, 1697548551692, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551949, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552944, 1697548553008, 1697548553283, 1697548553331, 1697548553378, 1697548553439, 1697548553499]"
2730,2730,757,51,[],200,llama-7b,64,1,1027.0,1.0,1,A100,1697548532123,1697548533150,120,20.0,1.0,"[29, 998]","[1697548532152, 1697548533150]"
2731,2731,416,52,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548533153,1697548537867,120,,,"[20, 758, 64, 60, 46, 46, 58, 58, 367, 47, 58, 59, 56, 708, 59, 59, 58, 55, 668, 63, 48, 55, 314, 52, 60, 66, 63, 61, 46]","[1697548533173, 1697548533931, 1697548533995, 1697548534055, 1697548534101, 1697548534147, 1697548534205, 1697548534263, 1697548534630, 1697548534677, 1697548534735, 1697548534794, 1697548534850, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537239, 1697548537285]"
2732,2732,750,20,[],200,llama-7b,64,1,2624.0,1.0,1,A100,1697548527826,1697548530450,120,88.0,20.0,"[138, 932, 58, 52, 50, 48, 48, 39, 364, 52, 50, 40, 41, 42, 48, 348, 55, 49, 59, 59, 52]","[1697548527964, 1697548528896, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529152, 1697548529191, 1697548529555, 1697548529607, 1697548529657, 1697548529697, 1697548529738, 1697548529780, 1697548529828, 1697548530176, 1697548530231, 1697548530280, 1697548530339, 1697548530398, 1697548530450]"
2733,2733,758,24,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548524042,1697548527820,120,,,"[11, 1196, 240, 67, 66, 51, 50, 65, 65, 242, 48, 48, 60, 58, 655, 65, 50, 64, 65, 64]","[1697548524053, 1697548525249, 1697548525489, 1697548525556, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526309, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2734,2734,456,20,[],200,llama-7b,64,1,2799.0,1.0,1,A100,1697548518849,1697548521648,120,90.0,20.0,"[21, 740, 185, 242, 67, 63, 61, 62, 60, 60, 336, 66, 63, 48, 49, 54, 405, 62, 48, 60, 46]","[1697548518870, 1697548519610, 1697548519795, 1697548520037, 1697548520104, 1697548520167, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647]"
2735,2735,186,26,[],200,llama-7b,64,1,4510.0,1.0,1,A100,1697548509838,1697548514348,120,123.0,22.0,"[27, 974, 64, 57, 42, 43, 52, 50, 654, 58, 53, 53, 51, 754, 349, 59, 55, 44, 53, 834, 66, 60, 58]","[1697548509865, 1697548510839, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511097, 1697548511147, 1697548511801, 1697548511859, 1697548511912, 1697548511965, 1697548512016, 1697548512770, 1697548513119, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514164, 1697548514230, 1697548514290, 1697548514348]"
2736,2736,291,33,[],200,llama-7b,64,1,4392.0,1.0,1,A100,1697548509838,1697548514230,120,79.0,20.0,"[22, 1043, 57, 42, 43, 52, 50, 654, 58, 53, 53, 51, 754, 351, 57, 55, 44, 53, 834, 66]","[1697548509860, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511097, 1697548511147, 1697548511801, 1697548511859, 1697548511912, 1697548511965, 1697548512016, 1697548512770, 1697548513121, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514164, 1697548514230]"
2737,2737,173,32,[],200,llama-7b,64,1,2533.0,1.0,1,A100,1697548518441,1697548520974,120,96.0,20.0,"[30, 375, 69, 63, 50, 58, 56, 45, 607, 242, 69, 61, 63, 61, 60, 60, 336, 66, 63, 48, 51]","[1697548518471, 1697548518846, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520105, 1697548520166, 1697548520229, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520974]"
2738,2738,193,53,[],200,llama-7b,64,1,2903.0,1.0,1,A100,1697548537877,1697548540780,120,79.0,20.0,"[207, 800, 55, 42, 46, 45, 678, 61, 55, 55, 44, 52, 41, 401, 50, 59, 47, 56, 58, 51]","[1697548538084, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539072, 1697548539750, 1697548539811, 1697548539866, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540459, 1697548540509, 1697548540568, 1697548540615, 1697548540671, 1697548540729, 1697548540780]"
2739,2739,808,45,[],200,llama-7b,64,1,625.0,1.0,1,A100,1697548532715,1697548533340,120,286.0,2.0,"[33, 402, 190]","[1697548532748, 1697548533150, 1697548533340]"
2740,2740,275,32,[],200,llama-7b,64,1,1373.0,1.0,1,A100,1697548511805,1697548513178,120,161.0,4.0,"[10, 644, 311, 350, 58]","[1697548511815, 1697548512459, 1697548512770, 1697548513120, 1697548513178]"
2741,2741,106,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548527211,1697548527818,120,,,"[14, 352]","[1697548527225, 1697548527577]"
2742,2742,52,33,[],200,llama-7b,64,1,1226.0,1.0,1,A100,1697548513180,1697548514406,120,58.0,6.0,"[7, 595, 382, 66, 60, 58, 58]","[1697548513187, 1697548513782, 1697548514164, 1697548514230, 1697548514290, 1697548514348, 1697548514406]"
2743,2743,591,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548532712,1697548537867,120,,,"[16, 422, 190, 64, 63, 50, 65, 57, 45, 311, 60, 46, 45, 59, 58, 367, 46, 59, 59, 57, 707, 59, 59, 58, 55, 668, 63, 48, 55, 314, 52, 60, 66, 63, 60, 47]","[1697548532728, 1697548533150, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533582, 1697548533639, 1697548533684, 1697548533995, 1697548534055, 1697548534101, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534676, 1697548534735, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
2744,2744,462,46,[],200,llama-7b,64,1,587.0,1.0,1,A100,1697548533344,1697548533931,120,52.0,1.0,"[6, 581]","[1697548533350, 1697548533931]"
2745,2745,812,29,[],200,llama-7b,64,1,1070.0,1.0,1,A100,1697548527826,1697548528896,120,16.0,1.0,"[130, 940]","[1697548527956, 1697548528896]"
2746,2746,465,30,[],200,llama-7b,64,1,709.0,1.0,1,A100,1697548528899,1697548529608,120,364.0,3.0,"[29, 574, 54, 52]","[1697548528928, 1697548529502, 1697548529556, 1697548529608]"
2747,2747,233,47,[],200,llama-7b,64,1,633.0,1.0,1,A100,1697548533932,1697548534565,120,6.0,1.0,"[6, 627]","[1697548533938, 1697548534565]"
2748,2748,237,31,[],200,llama-7b,64,1,3100.0,1.0,1,A100,1697548529610,1697548532710,120,87.0,20.0,"[12, 486, 69, 55, 48, 59, 60, 51, 486, 38, 473, 64, 65, 60, 60, 51, 737, 52, 59, 63, 51]","[1697548529622, 1697548530108, 1697548530177, 1697548530232, 1697548530280, 1697548530339, 1697548530399, 1697548530450, 1697548530936, 1697548530974, 1697548531447, 1697548531511, 1697548531576, 1697548531636, 1697548531696, 1697548531747, 1697548532484, 1697548532536, 1697548532595, 1697548532658, 1697548532709]"
2749,2749,824,48,[],200,llama-7b,64,1,1110.0,1.0,1,A100,1697548534567,1697548535677,120,58.0,4.0,"[19, 590, 382, 60, 59]","[1697548534586, 1697548535176, 1697548535558, 1697548535618, 1697548535677]"
2750,2750,593,49,[],200,llama-7b,64,1,1436.0,1.0,1,A100,1697548535680,1697548537116,120,335.0,9.0,"[9, 584, 184, 64, 47, 55, 315, 51, 61, 66]","[1697548535689, 1697548536273, 1697548536457, 1697548536521, 1697548536568, 1697548536623, 1697548536938, 1697548536989, 1697548537050, 1697548537116]"
2751,2751,252,50,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548537118,1697548537857,120,,,"[9, 641]","[1697548537127, 1697548537768]"
2752,2752,21,51,[],200,llama-7b,64,1,956.0,1.0,1,A100,1697548537876,1697548538832,120,15.0,1.0,"[161, 794]","[1697548538037, 1697548538831]"
2753,2753,578,52,[],200,llama-7b,64,1,674.0,1.0,1,A100,1697548538836,1697548539510,120,31.0,1.0,"[50, 624]","[1697548538886, 1697548539510]"
2754,2754,350,53,[],200,llama-7b,64,1,873.0,1.0,1,A100,1697548539512,1697548540385,120,216.0,1.0,"[21, 852]","[1697548539533, 1697548540385]"
2755,2755,822,32,[],200,llama-7b,64,1,2845.0,1.0,1,A100,1697548532713,1697548535558,120,88.0,20.0,"[25, 411, 191, 64, 63, 50, 66, 56, 45, 311, 60, 45, 46, 59, 58, 367, 46, 59, 59, 57, 707]","[1697548532738, 1697548533149, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533583, 1697548533639, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534676, 1697548534735, 1697548534794, 1697548534851, 1697548535558]"
2756,2756,3,54,[],200,llama-7b,64,1,4388.0,1.0,1,A100,1697548540388,1697548544776,120,89.0,20.0,"[21, 666, 54, 48, 750, 59, 61, 50, 650, 63, 61, 56, 45, 55, 692, 59, 47, 57, 55, 52, 787]","[1697548540409, 1697548541075, 1697548541129, 1697548541177, 1697548541927, 1697548541986, 1697548542047, 1697548542097, 1697548542747, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776]"
2757,2757,412,25,[],200,llama-7b,64,1,1730.0,1.0,1,A100,1697548527826,1697548529556,120,244.0,9.0,"[67, 372, 34, 656, 52, 50, 48, 46, 40, 365]","[1697548527893, 1697548528265, 1697548528299, 1697548528955, 1697548529007, 1697548529057, 1697548529105, 1697548529151, 1697548529191, 1697548529556]"
2758,2758,159,26,[],200,llama-7b,64,1,550.0,1.0,1,A100,1697548529558,1697548530108,120,31.0,1.0,"[6, 544]","[1697548529564, 1697548530108]"
2759,2759,860,27,[],200,llama-7b,64,1,3473.0,1.0,1,A100,1697548530111,1697548533584,120,85.0,20.0,"[19, 755, 51, 38, 473, 64, 65, 60, 60, 51, 737, 52, 59, 64, 50, 55, 576, 64, 64, 49, 63]","[1697548530130, 1697548530885, 1697548530936, 1697548530974, 1697548531447, 1697548531511, 1697548531576, 1697548531636, 1697548531696, 1697548531747, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532709, 1697548532764, 1697548533340, 1697548533404, 1697548533468, 1697548533517, 1697548533580]"
2760,2760,119,27,[],200,llama-7b,64,1,294.0,1.0,1,A100,1697548536573,1697548536867,120,31.0,1.0,"[18, 276]","[1697548536591, 1697548536867]"
2761,2761,826,28,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536869,1697548537856,120,,,"[25, 873]","[1697548536894, 1697548537767]"
2762,2762,592,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548535561,1697548537857,120,,,"[16, 696, 184, 64, 47, 55, 315, 51, 60, 66, 64, 59, 47]","[1697548535577, 1697548536273, 1697548536457, 1697548536521, 1697548536568, 1697548536623, 1697548536938, 1697548536989, 1697548537049, 1697548537115, 1697548537179, 1697548537238, 1697548537285]"
2763,2763,775,54,[],200,llama-7b,64,1,807.0,1.0,1,A100,1697548540784,1697548541591,120,17.0,1.0,"[70, 737]","[1697548540854, 1697548541591]"
2764,2764,302,33,[],200,llama-7b,64,1,2435.0,1.0,1,A100,1697548520353,1697548522788,120,85.0,20.0,"[15, 307, 72, 66, 62, 49, 49, 53, 405, 62, 48, 60, 46, 54, 710, 70, 66, 67, 57, 66, 50]","[1697548520368, 1697548520675, 1697548520747, 1697548520813, 1697548520875, 1697548520924, 1697548520973, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522481, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787]"
2765,2765,521,55,[],200,llama-7b,64,1,904.0,1.0,1,A100,1697548541598,1697548542502,120,18.0,1.0,"[33, 871]","[1697548541631, 1697548542502]"
2766,2766,32,41,[],200,llama-7b,64,1,1164.0,1.0,1,A100,1697548546150,1697548547314,120,140.0,6.0,"[6, 705, 217, 65, 66, 57, 48]","[1697548546156, 1697548546861, 1697548547078, 1697548547143, 1697548547209, 1697548547266, 1697548547314]"
2767,2767,719,43,[],200,llama-7b,64,1,1242.0,1.0,1,A100,1697548530454,1697548531696,120,182.0,6.0,"[81, 912, 64, 64, 61, 60]","[1697548530535, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531696]"
2768,2768,489,44,[],200,llama-7b,64,1,4090.0,1.0,1,A100,1697548531699,1697548535789,120,79.0,30.0,"[11, 774, 52, 59, 64, 51, 54, 576, 64, 63, 50, 67, 54, 46, 311, 60, 45, 46, 59, 59, 366, 46, 59, 59, 57, 707, 59, 59, 58, 55]","[1697548531710, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533584, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534264, 1697548534630, 1697548534676, 1697548534735, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789]"
2769,2769,635,34,[],200,llama-7b,64,1,1660.0,1.0,1,A100,1697548514409,1697548516069,120,23.0,1.0,"[15, 1645]","[1697548514424, 1697548516069]"
2770,2770,409,35,[],200,llama-7b,64,1,4902.0,1.0,1,A100,1697548516071,1697548520973,120,109.0,30.0,"[18, 1232, 72, 66, 64, 58, 54, 672, 65, 65, 58, 58, 362, 63, 50, 58, 56, 45, 607, 242, 68, 62, 62, 62, 60, 60, 336, 66, 63, 48, 49]","[1697548516089, 1697548517321, 1697548517393, 1697548517459, 1697548517523, 1697548517581, 1697548517635, 1697548518307, 1697548518372, 1697548518437, 1697548518495, 1697548518553, 1697548518915, 1697548518978, 1697548519028, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972]"
2771,2771,361,32,[],200,llama-7b,64,1,1874.0,1.0,1,A100,1697548537876,1697548539750,120,67.0,7.0,"[262, 693, 53, 55, 42, 46, 46, 677]","[1697548538138, 1697548538831, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539073, 1697548539750]"
2772,2772,201,27,[],200,llama-7b,64,1,3365.0,1.0,1,A100,1697548544780,1697548548145,120,67.0,20.0,"[26, 709, 368, 71, 66, 64, 63, 60, 49, 61, 759, 67, 63, 60, 48, 57, 46, 257, 53, 47, 371]","[1697548544806, 1697548545515, 1697548545883, 1697548545954, 1697548546020, 1697548546084, 1697548546147, 1697548546207, 1697548546256, 1697548546317, 1697548547076, 1697548547143, 1697548547206, 1697548547266, 1697548547314, 1697548547371, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145]"
2773,2773,16,33,[],200,llama-7b,64,1,632.0,1.0,1,A100,1697548539754,1697548540386,120,9.0,1.0,"[13, 619]","[1697548539767, 1697548540386]"
2774,2774,625,55,[],200,llama-7b,64,1,526.0,1.0,1,A100,1697548550475,1697548551001,120,364.0,2.0,"[20, 445, 61]","[1697548550495, 1697548550940, 1697548551001]"
2775,2775,715,34,[],200,llama-7b,64,1,686.0,1.0,1,A100,1697548540389,1697548541075,120,20.0,1.0,"[26, 660]","[1697548540415, 1697548541075]"
2776,2776,733,42,[],200,llama-7b,64,1,300.0,1.0,1,A100,1697548547315,1697548547615,120,31.0,1.0,"[7, 293]","[1697548547322, 1697548547615]"
2777,2777,288,56,[],200,llama-7b,64,1,2498.0,1.0,1,A100,1697548551002,1697548553500,120,93.0,20.0,"[8, 550, 70, 62, 57, 45, 44, 55, 56, 711, 54, 65, 51, 49, 65, 64, 275, 48, 47, 61, 61]","[1697548551010, 1697548551560, 1697548551630, 1697548551692, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551949, 1697548552660, 1697548552714, 1697548552779, 1697548552830, 1697548552879, 1697548552944, 1697548553008, 1697548553283, 1697548553331, 1697548553378, 1697548553439, 1697548553500]"
2778,2778,376,35,[],200,llama-7b,64,1,3806.0,1.0,1,A100,1697548541080,1697548544886,120,87.0,20.0,"[14, 497, 336, 60, 59, 51, 651, 61, 61, 58, 43, 56, 693, 59, 45, 58, 54, 53, 788, 55, 54]","[1697548541094, 1697548541591, 1697548541927, 1697548541987, 1697548542046, 1697548542097, 1697548542748, 1697548542809, 1697548542870, 1697548542928, 1697548542971, 1697548543027, 1697548543720, 1697548543779, 1697548543824, 1697548543882, 1697548543936, 1697548543989, 1697548544777, 1697548544832, 1697548544886]"
2779,2779,389,43,[],200,llama-7b,64,1,462.0,1.0,1,A100,1697548547619,1697548548081,120,8.0,1.0,"[27, 435]","[1697548547646, 1697548548081]"
2780,2780,158,44,[],200,llama-7b,64,1,2546.0,1.0,1,A100,1697548548084,1697548550630,120,85.0,20.0,"[26, 650, 292, 62, 51, 59, 47, 57, 56, 395, 60, 47, 48, 47, 60, 55, 313, 64, 55, 49, 53]","[1697548548110, 1697548548760, 1697548549052, 1697548549114, 1697548549165, 1697548549224, 1697548549271, 1697548549328, 1697548549384, 1697548549779, 1697548549839, 1697548549886, 1697548549934, 1697548549981, 1697548550041, 1697548550096, 1697548550409, 1697548550473, 1697548550528, 1697548550577, 1697548550630]"
2781,2781,791,28,[],200,llama-7b,64,1,7866.0,1.0,1,A100,1697548548149,1697548556015,120,182.0,64.0,"[27, 877, 61, 50, 60, 47, 58, 55, 395, 60, 48, 47, 48, 59, 56, 311, 64, 55, 50, 52, 372, 53, 51, 50, 49, 425, 63, 56, 45, 44, 56, 55, 711, 54, 66, 51, 50, 64, 64, 275, 48, 48, 59, 61, 348, 59, 46, 59, 859, 251, 254, 58, 44, 45, 45, 58, 58, 43, 50, 69, 46, 37, 45, 42]","[1697548548176, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550408, 1697548550472, 1697548550527, 1697548550577, 1697548550629, 1697548551001, 1697548551054, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551692, 1697548551748, 1697548551793, 1697548551837, 1697548551893, 1697548551948, 1697548552659, 1697548552713, 1697548552779, 1697548552830, 1697548552880, 1697548552944, 1697548553008, 1697548553283, 1697548553331, 1697548553379, 1697548553438, 1697548553499, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554870, 1697548555121, 1697548555375, 1697548555433, 1697548555477, 1697548555522, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015]"
2782,2782,277,57,[],200,llama-7b,64,1,768.0,1.0,1,A100,1697548553013,1697548553781,120,18.0,1.0,"[29, 739]","[1697548553042, 1697548553781]"
2783,2783,628,57,[],200,llama-7b,64,1,1373.0,1.0,1,A100,1697548549783,1697548551156,120,732.0,10.0,"[18, 608, 64, 56, 48, 54, 370, 53, 52, 50]","[1697548549801, 1697548550409, 1697548550473, 1697548550529, 1697548550577, 1697548550631, 1697548551001, 1697548551054, 1697548551106, 1697548551156]"
2784,2784,289,58,[],200,llama-7b,64,1,2340.0,1.0,1,A100,1697548551159,1697548553499,120,89.0,20.0,"[12, 389, 70, 63, 56, 45, 44, 55, 56, 711, 54, 66, 50, 50, 64, 64, 275, 48, 47, 61, 60]","[1697548551171, 1697548551560, 1697548551630, 1697548551693, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551949, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552944, 1697548553008, 1697548553283, 1697548553331, 1697548553378, 1697548553439, 1697548553499]"
2785,2785,549,30,[],200,llama-7b,64,1,2286.0,1.0,1,A100,1697548495412,1697548497698,120,93.0,20.0,"[11, 304, 65, 55, 53, 51, 52, 378, 40, 41, 50, 49, 447, 69, 63, 65, 58, 57, 55, 262, 60]","[1697548495423, 1697548495727, 1697548495792, 1697548495847, 1697548495900, 1697548495951, 1697548496003, 1697548496381, 1697548496421, 1697548496462, 1697548496512, 1697548496561, 1697548497008, 1697548497077, 1697548497140, 1697548497205, 1697548497263, 1697548497320, 1697548497375, 1697548497637, 1697548497697]"
2786,2786,448,54,[],200,llama-7b,64,1,2951.0,1.0,1,A100,1697548546320,1697548549271,120,335.0,12.0,"[15, 1279, 60, 53, 47, 370, 50, 48, 810, 62, 50, 59, 48]","[1697548546335, 1697548547614, 1697548547674, 1697548547727, 1697548547774, 1697548548144, 1697548548194, 1697548548242, 1697548549052, 1697548549114, 1697548549164, 1697548549223, 1697548549271]"
2787,2787,60,59,[],200,llama-7b,64,1,3074.0,1.0,1,A100,1697548553502,1697548556576,120,93.0,36.0,"[18, 1351, 251, 253, 58, 45, 44, 45, 59, 57, 43, 50, 69, 46, 37, 45, 42, 41, 40, 38, 35, 27, 33, 26, 26, 26, 27, 31, 26, 30, 25, 31, 24, 25, 25, 24]","[1697548553520, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555522, 1697548555567, 1697548555626, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556096, 1697548556134, 1697548556169, 1697548556196, 1697548556229, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556391, 1697548556421, 1697548556446, 1697548556477, 1697548556501, 1697548556526, 1697548556551, 1697548556575]"
2788,2788,319,31,[],200,llama-7b,64,1,470.0,1.0,1,A100,1697548497701,1697548498171,120,31.0,1.0,"[22, 448]","[1697548497723, 1697548498171]"
2789,2789,479,29,[],200,llama-7b,64,1,5962.0,1.0,1,A100,1697548537863,1697548543825,120,140.0,36.0,"[15, 296, 27, 683, 55, 41, 47, 45, 678, 61, 55, 54, 44, 52, 41, 401, 50, 59, 47, 57, 56, 52, 349, 48, 750, 60, 59, 52, 650, 62, 61, 57, 44, 55, 693, 59, 47]","[1697548537878, 1697548538174, 1697548538201, 1697548538884, 1697548538939, 1697548538980, 1697548539027, 1697548539072, 1697548539750, 1697548539811, 1697548539866, 1697548539920, 1697548539964, 1697548540016, 1697548540057, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540727, 1697548540779, 1697548541128, 1697548541176, 1697548541926, 1697548541986, 1697548542045, 1697548542097, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542971, 1697548543026, 1697548543719, 1697548543778, 1697548543825]"
2790,2790,218,55,[],200,llama-7b,64,1,768.0,1.0,1,A100,1697548549274,1697548550042,120,109.0,7.0,"[11, 495, 59, 48, 47, 48, 60]","[1697548549285, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550042]"
2791,2791,748,45,[],200,llama-7b,64,1,2312.0,1.0,1,A100,1697548550633,1697548552945,120,182.0,14.0,"[15, 981, 63, 57, 44, 44, 55, 56, 712, 53, 67, 50, 50, 65]","[1697548550648, 1697548551629, 1697548551692, 1697548551749, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552660, 1697548552713, 1697548552780, 1697548552830, 1697548552880, 1697548552945]"
2792,2792,518,46,[],200,llama-7b,64,1,267.0,1.0,1,A100,1697548552950,1697548553217,120,23.0,1.0,"[7, 260]","[1697548552957, 1697548553217]"
2793,2793,176,47,[],200,llama-7b,64,1,628.0,1.0,1,A100,1697548553219,1697548553847,120,216.0,2.0,"[6, 622]","[1697548553225, 1697548553847]"
2794,2794,895,56,[],200,llama-7b,64,1,295.0,1.0,1,A100,1697548550046,1697548550341,120,15.0,1.0,"[18, 277]","[1697548550064, 1697548550341]"
2795,2795,875,48,[],200,llama-7b,64,1,2598.0,1.0,1,A100,1697548553849,1697548556447,120,31.0,31.0,"[6, 579, 437, 251, 253, 58, 45, 45, 45, 58, 58, 43, 49, 69, 47, 36, 45, 42, 41, 39, 39, 35, 27, 33, 26, 26, 26, 27, 31, 26, 30, 26]","[1697548553855, 1697548554434, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555523, 1697548555568, 1697548555626, 1697548555684, 1697548555727, 1697548555776, 1697548555845, 1697548555892, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556169, 1697548556196, 1697548556229, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556391, 1697548556421, 1697548556447]"
2796,2796,547,57,[],200,llama-7b,64,1,596.0,1.0,1,A100,1697548550344,1697548550940,120,12.0,1.0,"[24, 572]","[1697548550368, 1697548550940]"
2797,2797,709,55,[],200,llama-7b,64,1,16980.0,1.0,1,A100,1697548544786,1697548561766,120,457.0,381.0,"[15, 714, 368, 71, 66, 64, 62, 61, 48, 62, 760, 66, 63, 60, 48, 57, 46, 257, 53, 47, 371, 49, 48, 810, 61, 51, 59, 47, 58, 55, 396, 60, 47, 48, 47, 60, 56, 311, 64, 55, 49, 53, 372, 53, 51, 50, 49, 425, 62, 57, 45, 44, 55, 56, 711, 54, 66, 50, 50, 65, 64, 274, 49, 47, 60, 60, 349, 59, 46, 58, 860, 251, 253, 59, 44, 45, 45, 58, 58, 43, 50, 69, 46, 36, 46, 42, 41, 39, 39, 34, 28, 32, 27, 26, 26, 26, 32, 25, 31, 25, 31, 24, 25, 24, 25, 30, 24, 24, 29, 23, 24, 23, 24, 24, 23, 29, 24, 23, 26, 22, 24, 21, 20, 21, 20, 21, 20, 21, 23, 20, 19, 20, 20, 22, 19, 20, 19, 22, 18, 19, 19, 18, 19, 19, 18, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 21, 18, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 19, 18, 18, 18, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 19, 18, 19, 18, 18, 18, 19, 18, 18, 18, 19, 18, 19, 18, 18, 19, 18, 18, 19, 18, 19, 18, 18, 19, 18, 19, 18, 19, 18, 19, 18, 18, 19, 18, 19, 18, 19, 18, 19, 19, 18, 19, 18, 19, 18, 19, 18, 19, 19, 18, 19, 18, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 18, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 19, 18, 19, 19, 18, 19, 19, 19, 19, 19, 18, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 17, 17, 17, 17, 17, 17, 16, 17, 17, 17, 17, 17, 17, 17, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16]","[1697548544801, 1697548545515, 1697548545883, 1697548545954, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077, 1697548547143, 1697548547206, 1697548547266, 1697548547314, 1697548547371, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549779, 1697548549839, 1697548549886, 1697548549934, 1697548549981, 1697548550041, 1697548550097, 1697548550408, 1697548550472, 1697548550527, 1697548550576, 1697548550629, 1697548551001, 1697548551054, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551691, 1697548551748, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552659, 1697548552713, 1697548552779, 1697548552829, 1697548552879, 1697548552944, 1697548553008, 1697548553282, 1697548553331, 1697548553378, 1697548553438, 1697548553498, 1697548553847, 1697548553906, 1697548553952, 1697548554010, 1697548554870, 1697548555121, 1697548555374, 1697548555433, 1697548555477, 1697548555522, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555927, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556168, 1697548556196, 1697548556228, 1697548556255, 1697548556281, 1697548556307, 1697548556333, 1697548556365, 1697548556390, 1697548556421, 1697548556446, 1697548556477, 1697548556501, 1697548556526, 1697548556550, 1697548556575, 1697548556605, 1697548556629, 1697548556653, 1697548556682, 1697548556705, 1697548556729, 1697548556752, 1697548556776, 1697548556800, 1697548556823, 1697548556852, 1697548556876, 1697548556899, 1697548556925, 1697548556947, 1697548556971, 1697548556992, 1697548557012, 1697548557033, 1697548557053, 1697548557074, 1697548557094, 1697548557115, 1697548557138, 1697548557158, 1697548557177, 1697548557197, 1697548557217, 1697548557239, 1697548557258, 1697548557278, 1697548557297, 1697548557319, 1697548557337, 1697548557356, 1697548557375, 1697548557393, 1697548557412, 1697548557431, 1697548557449, 1697548557468, 1697548557486, 1697548557505, 1697548557524, 1697548557543, 1697548557562, 1697548557580, 1697548557599, 1697548557618, 1697548557637, 1697548557656, 1697548557674, 1697548557693, 1697548557712, 1697548557731, 1697548557750, 1697548557769, 1697548557788, 1697548557807, 1697548557825, 1697548557844, 1697548557863, 1697548557882, 1697548557901, 1697548557920, 1697548557939, 1697548557958, 1697548557977, 1697548557996, 1697548558014, 1697548558033, 1697548558052, 1697548558071, 1697548558090, 1697548558109, 1697548558128, 1697548558147, 1697548558166, 1697548558185, 1697548558206, 1697548558224, 1697548558242, 1697548558260, 1697548558278, 1697548558296, 1697548558313, 1697548558331, 1697548558349, 1697548558367, 1697548558385, 1697548558403, 1697548558421, 1697548558439, 1697548558457, 1697548558475, 1697548558493, 1697548558511, 1697548558529, 1697548558547, 1697548558565, 1697548558583, 1697548558601, 1697548558619, 1697548558637, 1697548558656, 1697548558674, 1697548558692, 1697548558710, 1697548558728, 1697548558746, 1697548558764, 1697548558782, 1697548558801, 1697548558819, 1697548558837, 1697548558855, 1697548558873, 1697548558891, 1697548558909, 1697548558928, 1697548558946, 1697548558964, 1697548558982, 1697548559000, 1697548559018, 1697548559037, 1697548559055, 1697548559073, 1697548559091, 1697548559110, 1697548559128, 1697548559146, 1697548559164, 1697548559182, 1697548559201, 1697548559219, 1697548559237, 1697548559255, 1697548559273, 1697548559292, 1697548559310, 1697548559329, 1697548559347, 1697548559365, 1697548559383, 1697548559402, 1697548559420, 1697548559438, 1697548559456, 1697548559475, 1697548559493, 1697548559512, 1697548559530, 1697548559548, 1697548559567, 1697548559585, 1697548559603, 1697548559622, 1697548559640, 1697548559659, 1697548559677, 1697548559695, 1697548559714, 1697548559732, 1697548559751, 1697548559769, 1697548559788, 1697548559806, 1697548559825, 1697548559843, 1697548559861, 1697548559880, 1697548559898, 1697548559917, 1697548559935, 1697548559954, 1697548559972, 1697548559991, 1697548560010, 1697548560028, 1697548560047, 1697548560065, 1697548560084, 1697548560102, 1697548560121, 1697548560139, 1697548560158, 1697548560177, 1697548560195, 1697548560214, 1697548560232, 1697548560251, 1697548560269, 1697548560288, 1697548560307, 1697548560325, 1697548560344, 1697548560363, 1697548560381, 1697548560400, 1697548560419, 1697548560437, 1697548560456, 1697548560474, 1697548560493, 1697548560512, 1697548560530, 1697548560549, 1697548560568, 1697548560586, 1697548560605, 1697548560624, 1697548560642, 1697548560661, 1697548560680, 1697548560699, 1697548560717, 1697548560736, 1697548560755, 1697548560773, 1697548560792, 1697548560811, 1697548560830, 1697548560849, 1697548560868, 1697548560886, 1697548560905, 1697548560924, 1697548560943, 1697548560961, 1697548560980, 1697548560999, 1697548561018, 1697548561037, 1697548561056, 1697548561075, 1697548561092, 1697548561109, 1697548561126, 1697548561143, 1697548561160, 1697548561177, 1697548561193, 1697548561210, 1697548561227, 1697548561244, 1697548561261, 1697548561278, 1697548561295, 1697548561312, 1697548561328, 1697548561345, 1697548561362, 1697548561379, 1697548561396, 1697548561413, 1697548561430, 1697548561447, 1697548561464, 1697548561481, 1697548561498, 1697548561514, 1697548561531, 1697548561548, 1697548561565, 1697548561582, 1697548561599, 1697548561616, 1697548561633, 1697548561650, 1697548561667, 1697548561684, 1697548561701, 1697548561718, 1697548561734, 1697548561750, 1697548561766]"
2798,2798,320,58,[],200,llama-7b,64,1,4904.0,1.0,1,A100,1697548550941,1697548555845,120,109.0,36.0,"[6, 683, 62, 57, 45, 44, 55, 56, 711, 53, 67, 49, 50, 65, 64, 275, 49, 47, 60, 61, 347, 59, 46, 59, 860, 251, 253, 58, 44, 45, 46, 58, 57, 43, 50, 69]","[1697548550947, 1697548551630, 1697548551692, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551949, 1697548552660, 1697548552713, 1697548552780, 1697548552829, 1697548552879, 1697548552944, 1697548553008, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553500, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555477, 1697548555522, 1697548555568, 1697548555626, 1697548555683, 1697548555726, 1697548555776, 1697548555845]"
2799,2799,599,35,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548530456,1697548537858,120,,,"[89, 822, 80, 64, 64, 61, 60, 51, 736, 53, 58, 64, 51, 55, 576, 64, 64, 49, 66, 55, 46, 311, 60, 45, 46, 59, 58, 367, 46, 59, 58, 58, 707, 59, 60, 57, 55, 668, 63, 48, 55, 314, 52, 60, 66, 63, 60, 47]","[1697548530545, 1697548531367, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531696, 1697548531747, 1697548532483, 1697548532536, 1697548532594, 1697548532658, 1697548532709, 1697548532764, 1697548533340, 1697548533404, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534676, 1697548534735, 1697548534793, 1697548534851, 1697548535558, 1697548535617, 1697548535677, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
2800,2800,291,56,[],200,llama-7b,64,1,4572.0,1.0,1,A100,1697548542505,1697548547077,120,79.0,20.0,"[16, 1198, 59, 47, 57, 55, 52, 787, 55, 54, 47, 951, 71, 66, 64, 62, 61, 48, 62, 760]","[1697548542521, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776, 1697548544831, 1697548544885, 1697548544932, 1697548545883, 1697548545954, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077]"
2801,2801,880,57,[],200,llama-7b,64,1,595.0,1.0,1,A100,1697548547079,1697548547674,120,84.0,2.0,"[15, 521, 59]","[1697548547094, 1697548547615, 1697548547674]"
2802,2802,652,58,[],200,llama-7b,64,1,407.0,1.0,1,A100,1697548547675,1697548548082,120,14.0,1.0,"[7, 399]","[1697548547682, 1697548548081]"
2803,2803,302,59,[],200,llama-7b,64,1,2545.0,1.0,1,A100,1697548548085,1697548550630,120,85.0,20.0,"[30, 645, 293, 61, 51, 59, 47, 57, 56, 395, 60, 47, 48, 48, 59, 55, 313, 64, 55, 49, 53]","[1697548548115, 1697548548760, 1697548549053, 1697548549114, 1697548549165, 1697548549224, 1697548549271, 1697548549328, 1697548549384, 1697548549779, 1697548549839, 1697548549886, 1697548549934, 1697548549982, 1697548550041, 1697548550096, 1697548550409, 1697548550473, 1697548550528, 1697548550577, 1697548550630]"
2804,2804,903,32,[],200,llama-7b,64,1,1676.0,1.0,1,A100,1697548498174,1697548499850,120,244.0,7.0,"[15, 549, 64, 59, 51, 50, 826, 62]","[1697548498189, 1697548498738, 1697548498802, 1697548498861, 1697548498912, 1697548498962, 1697548499788, 1697548499850]"
2805,2805,245,37,[],200,llama-7b,64,1,3053.0,1.0,1,A100,1697548508750,1697548511803,120,100.0,20.0,"[18, 671, 129, 57, 54, 53, 53, 50, 559, 58, 60, 47, 57, 56, 231, 57, 42, 43, 51, 51, 655]","[1697548508768, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509835, 1697548510394, 1697548510452, 1697548510512, 1697548510559, 1697548510616, 1697548510672, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511802]"
2806,2806,835,38,[],200,llama-7b,64,1,5101.0,1.0,1,A100,1697548511805,1697548516906,120,87.0,20.0,"[11, 643, 311, 350, 58, 55, 44, 53, 833, 66, 60, 59, 57, 920, 72, 70, 63, 61, 976, 270, 68]","[1697548511816, 1697548512459, 1697548512770, 1697548513120, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514163, 1697548514229, 1697548514289, 1697548514348, 1697548514405, 1697548515325, 1697548515397, 1697548515467, 1697548515530, 1697548515591, 1697548516567, 1697548516837, 1697548516905]"
2807,2807,527,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548530456,1697548537858,120,,,"[85, 906, 64, 64, 61, 60, 51, 736, 53, 59, 63, 51, 55, 577, 63, 64, 49, 66, 55, 46, 311, 60, 45, 46, 58, 59, 367, 46, 59, 58, 58, 706, 60, 60, 57, 55, 668, 63, 48, 55, 314, 52, 60, 66, 63, 60, 47]","[1697548530541, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531696, 1697548531747, 1697548532483, 1697548532536, 1697548532595, 1697548532658, 1697548532709, 1697548532764, 1697548533341, 1697548533404, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534204, 1697548534263, 1697548534630, 1697548534676, 1697548534735, 1697548534793, 1697548534851, 1697548535557, 1697548535617, 1697548535677, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
2808,2808,885,27,[],200,llama-7b,64,1,7080.0,1.0,1,A100,1697548514351,1697548521431,120,84.0,43.0,"[8, 499, 467, 71, 70, 64, 61, 976, 270, 68, 65, 65, 64, 293, 66, 66, 57, 53, 673, 66, 65, 58, 57, 362, 62, 50, 58, 57, 45, 607, 242, 68, 62, 62, 62, 60, 60, 336, 66, 63, 48, 49, 54, 405]","[1697548514359, 1697548514858, 1697548515325, 1697548515396, 1697548515466, 1697548515530, 1697548515591, 1697548516567, 1697548516837, 1697548516905, 1697548516970, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517524, 1697548517581, 1697548517634, 1697548518307, 1697548518373, 1697548518438, 1697548518496, 1697548518553, 1697548518915, 1697548518977, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521026, 1697548521431]"
2809,2809,756,33,[],200,llama-7b,64,1,387.0,1.0,1,A100,1697548520977,1697548521364,120,19.0,1.0,"[39, 348]","[1697548521016, 1697548521364]"
2810,2810,603,39,[],200,llama-7b,64,1,413.0,1.0,1,A100,1697548516909,1697548517322,120,9.0,1.0,"[19, 394]","[1697548516928, 1697548517322]"
2811,2811,234,40,[],200,llama-7b,64,1,3648.0,1.0,1,A100,1697548517325,1697548520973,120,457.0,25.0,"[17, 722, 243, 65, 65, 58, 58, 362, 63, 49, 59, 56, 45, 607, 242, 68, 62, 62, 62, 60, 60, 336, 66, 63, 49, 49]","[1697548517342, 1697548518064, 1697548518307, 1697548518372, 1697548518437, 1697548518495, 1697548518553, 1697548518915, 1697548518978, 1697548519027, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520924, 1697548520973]"
2812,2812,343,63,[],200,llama-7b,64,1,3721.0,1.0,1,A100,1697548546320,1697548550041,120,84.0,20.0,"[6, 1288, 60, 53, 47, 370, 50, 48, 810, 62, 50, 59, 48, 58, 54, 396, 60, 48, 47, 48, 59]","[1697548546326, 1697548547614, 1697548547674, 1697548547727, 1697548547774, 1697548548144, 1697548548194, 1697548548242, 1697548549052, 1697548549114, 1697548549164, 1697548549223, 1697548549271, 1697548549329, 1697548549383, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041]"
2813,2813,107,64,[],200,llama-7b,64,1,366.0,1.0,1,A100,1697548550044,1697548550410,120,216.0,2.0,"[15, 350]","[1697548550059, 1697548550409]"
2814,2814,786,44,[],200,llama-7b,64,1,2904.0,1.0,1,A100,1697548537877,1697548540781,120,87.0,20.0,"[216, 738, 53, 55, 42, 46, 46, 677, 61, 56, 54, 44, 52, 41, 401, 49, 60, 46, 57, 56, 54]","[1697548538093, 1697548538831, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539073, 1697548539750, 1697548539811, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540459, 1697548540508, 1697548540568, 1697548540614, 1697548540671, 1697548540727, 1697548540781]"
2815,2815,882,34,[],200,llama-7b,64,1,1994.0,1.0,1,A100,1697548522790,1697548524784,120,345.0,11.0,"[29, 846, 73, 52, 67, 52, 63, 64, 553, 64, 66, 65]","[1697548522819, 1697548523665, 1697548523738, 1697548523790, 1697548523857, 1697548523909, 1697548523972, 1697548524036, 1697548524589, 1697548524653, 1697548524719, 1697548524784]"
2816,2816,16,49,[],200,llama-7b,64,1,591.0,1.0,1,A100,1697548551696,1697548552287,120,9.0,1.0,"[22, 569]","[1697548551718, 1697548552287]"
2817,2817,715,50,[],200,llama-7b,64,1,924.0,1.0,1,A100,1697548552293,1697548553217,120,20.0,1.0,"[18, 905]","[1697548552311, 1697548553216]"
2818,2818,347,51,[],200,llama-7b,64,1,4076.0,1.0,1,A100,1697548553221,1697548557297,120,100.0,72.0,"[15, 611, 60, 45, 59, 860, 250, 253, 60, 44, 44, 45, 58, 58, 43, 50, 69, 46, 36, 46, 42, 41, 39, 39, 35, 27, 32, 27, 26, 26, 27, 31, 26, 30, 25, 31, 24, 25, 25, 24, 30, 24, 24, 29, 23, 24, 23, 24, 24, 23, 29, 24, 23, 26, 22, 24, 21, 20, 21, 20, 21, 20, 21, 23, 20, 19, 20, 20, 22, 19, 20, 19]","[1697548553236, 1697548553847, 1697548553907, 1697548553952, 1697548554011, 1697548554871, 1697548555121, 1697548555374, 1697548555434, 1697548555478, 1697548555522, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555927, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556169, 1697548556196, 1697548556228, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556391, 1697548556421, 1697548556446, 1697548556477, 1697548556501, 1697548556526, 1697548556551, 1697548556575, 1697548556605, 1697548556629, 1697548556653, 1697548556682, 1697548556705, 1697548556729, 1697548556752, 1697548556776, 1697548556800, 1697548556823, 1697548556852, 1697548556876, 1697548556899, 1697548556925, 1697548556947, 1697548556971, 1697548556992, 1697548557012, 1697548557033, 1697548557053, 1697548557074, 1697548557094, 1697548557115, 1697548557138, 1697548557158, 1697548557177, 1697548557197, 1697548557217, 1697548557239, 1697548557258, 1697548557278, 1697548557297]"
2819,2819,563,45,[],200,llama-7b,64,1,3993.0,1.0,1,A100,1697548540784,1697548544777,120,874.0,18.0,"[56, 751, 335, 61, 59, 51, 651, 61, 61, 58, 43, 55, 694, 58, 46, 59, 54, 52, 788]","[1697548540840, 1697548541591, 1697548541926, 1697548541987, 1697548542046, 1697548542097, 1697548542748, 1697548542809, 1697548542870, 1697548542928, 1697548542971, 1697548543026, 1697548543720, 1697548543778, 1697548543824, 1697548543883, 1697548543937, 1697548543989, 1697548544777]"
2820,2820,216,46,[],200,llama-7b,64,1,3365.0,1.0,1,A100,1697548544780,1697548548145,120,91.0,20.0,"[31, 704, 368, 71, 66, 64, 63, 60, 49, 61, 760, 66, 63, 60, 48, 57, 46, 257, 53, 47, 371]","[1697548544811, 1697548545515, 1697548545883, 1697548545954, 1697548546020, 1697548546084, 1697548546147, 1697548546207, 1697548546256, 1697548546317, 1697548547077, 1697548547143, 1697548547206, 1697548547266, 1697548547314, 1697548547371, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145]"
2821,2821,147,36,[],200,llama-7b,64,1,626.0,1.0,1,A100,1697548544889,1697548545515,120,182.0,1.0,"[20, 606]","[1697548544909, 1697548545515]"
2822,2822,730,37,[],200,llama-7b,64,1,2629.0,1.0,1,A100,1697548545516,1697548548145,120,364.0,12.0,"[7, 1338, 216, 67, 63, 58, 49, 58, 45, 256, 54, 47, 371]","[1697548545523, 1697548546861, 1697548547077, 1697548547144, 1697548547207, 1697548547265, 1697548547314, 1697548547372, 1697548547417, 1697548547673, 1697548547727, 1697548547774, 1697548548145]"
2823,2823,36,45,[],200,llama-7b,64,1,2846.0,1.0,1,A100,1697548532712,1697548535558,120,457.0,20.0,"[6, 432, 190, 64, 63, 50, 64, 57, 46, 311, 60, 46, 45, 59, 58, 367, 46, 59, 59, 57, 707]","[1697548532718, 1697548533150, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533581, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534101, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534676, 1697548534735, 1697548534794, 1697548534851, 1697548535558]"
2824,2824,921,47,[],200,llama-7b,64,1,615.0,1.0,1,A100,1697548548146,1697548548761,120,31.0,1.0,"[6, 609]","[1697548548152, 1697548548761]"
2825,2825,462,37,[],200,llama-7b,64,1,880.0,1.0,1,A100,1697548533685,1697548534565,120,52.0,1.0,"[7, 873]","[1697548533692, 1697548534565]"
2826,2826,231,38,[],200,llama-7b,64,1,607.0,1.0,1,A100,1697548534569,1697548535176,120,13.0,1.0,"[27, 580]","[1697548534596, 1697548535176]"
2827,2827,908,39,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548535179,1697548537857,120,,,"[23, 1071, 184, 64, 47, 55, 315, 51, 60, 66, 64, 59, 47]","[1697548535202, 1697548536273, 1697548536457, 1697548536521, 1697548536568, 1697548536623, 1697548536938, 1697548536989, 1697548537049, 1697548537115, 1697548537179, 1697548537238, 1697548537285]"
2828,2828,570,48,[],200,llama-7b,64,1,935.0,1.0,1,A100,1697548548765,1697548549700,120,18.0,1.0,"[30, 904]","[1697548548795, 1697548549699]"
2829,2829,506,38,[],200,llama-7b,64,1,611.0,1.0,1,A100,1697548548150,1697548548761,120,16.0,1.0,"[31, 580]","[1697548548181, 1697548548761]"
2830,2830,318,49,[],200,llama-7b,64,1,929.0,1.0,1,A100,1697548549702,1697548550631,120,6.0,6.0,"[14, 624, 69, 63, 56, 48, 54]","[1697548549716, 1697548550340, 1697548550409, 1697548550472, 1697548550528, 1697548550576, 1697548550630]"
2831,2831,561,40,[],200,llama-7b,64,1,2904.0,1.0,1,A100,1697548537875,1697548540779,120,87.0,20.0,"[89, 866, 54, 55, 42, 46, 46, 677, 61, 56, 53, 44, 52, 42, 400, 50, 59, 47, 57, 57, 51]","[1697548537964, 1697548538830, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539073, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779]"
2832,2832,158,39,[],200,llama-7b,64,1,2928.0,1.0,1,A100,1697548548764,1697548551692,120,85.0,20.0,"[26, 909, 81, 59, 48, 47, 48, 59, 56, 312, 63, 55, 50, 53, 371, 53, 51, 51, 48, 425, 63]","[1697548548790, 1697548549699, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472, 1697548550527, 1697548550577, 1697548550630, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551204, 1697548551629, 1697548551692]"
2833,2833,845,33,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526312,1697548527821,120,,,"[13, 1252]","[1697548526325, 1697548527577]"
2834,2834,612,34,[],200,llama-7b,64,1,2622.0,1.0,1,A100,1697548527827,1697548530449,120,93.0,20.0,"[229, 840, 59, 51, 51, 47, 48, 39, 365, 51, 49, 41, 42, 40, 50, 346, 56, 48, 59, 59, 52]","[1697548528056, 1697548528896, 1697548528955, 1697548529006, 1697548529057, 1697548529104, 1697548529152, 1697548529191, 1697548529556, 1697548529607, 1697548529656, 1697548529697, 1697548529739, 1697548529779, 1697548529829, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530397, 1697548530449]"
2835,2835,903,50,[],200,llama-7b,64,1,1255.0,1.0,1,A100,1697548550638,1697548551893,120,244.0,7.0,"[27, 895, 69, 63, 58, 43, 44, 56]","[1697548550665, 1697548551560, 1697548551629, 1697548551692, 1697548551750, 1697548551793, 1697548551837, 1697548551893]"
2836,2836,880,41,[],200,llama-7b,64,1,529.0,1.0,1,A100,1697548547617,1697548548146,120,84.0,2.0,"[25, 439, 64]","[1697548547642, 1697548548081, 1697548548145]"
2837,2837,649,42,[],200,llama-7b,64,1,2482.0,1.0,1,A100,1697548548148,1697548550630,120,244.0,20.0,"[29, 584, 292, 61, 50, 60, 47, 58, 55, 395, 60, 48, 47, 48, 59, 56, 312, 63, 55, 50, 53]","[1697548548177, 1697548548761, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472, 1697548550527, 1697548550577, 1697548550630]"
2838,2838,340,34,[],200,llama-7b,64,1,2904.0,1.0,1,A100,1697548537876,1697548540780,120,85.0,20.0,"[183, 773, 52, 55, 41, 47, 45, 678, 62, 55, 54, 44, 52, 40, 402, 49, 60, 47, 56, 58, 51]","[1697548538059, 1697548538832, 1697548538884, 1697548538939, 1697548538980, 1697548539027, 1697548539072, 1697548539750, 1697548539812, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540057, 1697548540459, 1697548540508, 1697548540568, 1697548540615, 1697548540671, 1697548540729, 1697548540780]"
2839,2839,629,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548535567,1697548537858,120,,,"[20, 686, 184, 64, 47, 55, 315, 51, 61, 65, 63, 60, 47]","[1697548535587, 1697548536273, 1697548536457, 1697548536521, 1697548536568, 1697548536623, 1697548536938, 1697548536989, 1697548537050, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
2840,2840,336,41,[],200,llama-7b,64,1,2026.0,1.0,1,A100,1697548540783,1697548542809,120,58.0,7.0,"[6, 801, 337, 60, 59, 51, 651, 61]","[1697548540789, 1697548541590, 1697548541927, 1697548541987, 1697548542046, 1697548542097, 1697548542748, 1697548542809]"
2841,2841,919,42,[],200,llama-7b,64,1,537.0,1.0,1,A100,1697548542812,1697548543349,120,14.0,1.0,"[15, 522]","[1697548542827, 1697548543349]"
2842,2842,533,34,[],200,llama-7b,64,1,1043.0,1.0,1,A100,1697548521368,1697548522411,120,216.0,2.0,"[38, 1004]","[1697548521406, 1697548522410]"
2843,2843,699,43,[],200,llama-7b,64,1,1167.0,1.0,1,A100,1697548543355,1697548544522,120,39.0,1.0,"[18, 1149]","[1697548543373, 1697548544522]"
2844,2844,398,47,[],200,llama-7b,64,1,2905.0,1.0,1,A100,1697548537875,1697548540780,120,87.0,20.0,"[103, 853, 54, 54, 43, 45, 46, 677, 61, 56, 53, 44, 53, 41, 400, 50, 59, 47, 57, 57, 52]","[1697548537978, 1697548538831, 1697548538885, 1697548538939, 1697548538982, 1697548539027, 1697548539073, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540017, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540780]"
2845,2845,696,65,[],200,llama-7b,64,1,2598.0,1.0,1,A100,1697548550411,1697548553009,120,83.0,20.0,"[6, 523, 61, 53, 51, 51, 48, 425, 64, 56, 45, 44, 55, 55, 712, 54, 66, 50, 50, 65, 64]","[1697548550417, 1697548550940, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551204, 1697548551629, 1697548551693, 1697548551749, 1697548551794, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009]"
2846,2846,308,43,[],200,llama-7b,64,1,2865.0,1.0,1,A100,1697548550635,1697548553500,120,87.0,20.0,"[25, 900, 69, 63, 58, 43, 44, 56, 55, 713, 52, 67, 49, 50, 66, 64, 274, 49, 47, 60, 61]","[1697548550660, 1697548551560, 1697548551629, 1697548551692, 1697548551750, 1697548551793, 1697548551837, 1697548551893, 1697548551948, 1697548552661, 1697548552713, 1697548552780, 1697548552829, 1697548552879, 1697548552945, 1697548553009, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553500]"
2847,2847,355,44,[],200,llama-7b,64,1,3620.0,1.0,1,A100,1697548544525,1697548548145,120,90.0,20.0,"[16, 973, 369, 71, 65, 64, 63, 62, 47, 62, 761, 65, 66, 57, 48, 57, 46, 256, 54, 47, 371]","[1697548544541, 1697548545514, 1697548545883, 1697548545954, 1697548546019, 1697548546083, 1697548546146, 1697548546208, 1697548546255, 1697548546317, 1697548547078, 1697548547143, 1697548547209, 1697548547266, 1697548547314, 1697548547371, 1697548547417, 1697548547673, 1697548547727, 1697548547774, 1697548548145]"
2848,2848,805,47,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548521650,1697548527819,120,,,"[28, 467, 266, 69, 68, 66, 57, 66, 50, 316, 68, 63, 64, 62, 62, 317, 52, 67, 51, 63, 65, 552, 64, 66, 65, 59, 58, 588, 67, 66, 51, 50, 65, 65, 242, 48, 48, 60, 59, 653, 66, 50, 64, 65, 63]","[1697548521678, 1697548522145, 1697548522411, 1697548522480, 1697548522548, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103, 1697548523171, 1697548523234, 1697548523298, 1697548523360, 1697548523422, 1697548523739, 1697548523791, 1697548523858, 1697548523909, 1697548523972, 1697548524037, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526963, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527271]"
2849,2849,109,21,[],200,llama-7b,64,1,2385.0,1.0,1,A100,1697548521650,1697548524035,120,90.0,20.0,"[29, 466, 266, 69, 68, 66, 57, 66, 50, 316, 68, 63, 64, 62, 62, 317, 51, 68, 51, 63, 63]","[1697548521679, 1697548522145, 1697548522411, 1697548522480, 1697548522548, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103, 1697548523171, 1697548523234, 1697548523298, 1697548523360, 1697548523422, 1697548523739, 1697548523790, 1697548523858, 1697548523909, 1697548523972, 1697548524035]"
2850,2850,466,66,[],200,llama-7b,64,1,2962.0,1.0,1,A100,1697548553012,1697548555974,120,457.0,20.0,"[15, 753, 67, 60, 45, 59, 859, 252, 252, 59, 45, 45, 45, 58, 58, 43, 49, 69, 46, 37, 45]","[1697548553027, 1697548553780, 1697548553847, 1697548553907, 1697548553952, 1697548554011, 1697548554870, 1697548555122, 1697548555374, 1697548555433, 1697548555478, 1697548555523, 1697548555568, 1697548555626, 1697548555684, 1697548555727, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973]"
2851,2851,58,57,[],200,llama-7b,64,1,929.0,1.0,1,A100,1697548553505,1697548554434,120,15.0,1.0,"[41, 888]","[1697548553546, 1697548554434]"
2852,2852,513,28,[],200,llama-7b,64,1,2976.0,1.0,1,A100,1697548533593,1697548536569,120,83.0,20.0,"[24, 314, 65, 59, 46, 45, 59, 58, 367, 47, 59, 59, 56, 707, 59, 59, 58, 55, 668, 63, 49]","[1697548533617, 1697548533931, 1697548533996, 1697548534055, 1697548534101, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534677, 1697548534736, 1697548534795, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536569]"
2853,2853,624,53,[],200,llama-7b,64,1,4743.0,1.0,1,A100,1697548553443,1697548558186,120,563.0,119.0,"[12, 326, 67, 59, 46, 58, 860, 250, 253, 60, 43, 45, 45, 58, 58, 43, 50, 69, 46, 37, 45, 42, 41, 39, 39, 35, 27, 32, 27, 26, 26, 27, 31, 26, 30, 25, 31, 24, 25, 25, 24, 30, 24, 24, 29, 23, 24, 24, 23, 24, 23, 29, 24, 23, 26, 22, 24, 21, 20, 21, 20, 21, 20, 21, 23, 20, 19, 20, 20, 22, 19, 20, 19, 22, 18, 19, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 19]","[1697548553455, 1697548553781, 1697548553848, 1697548553907, 1697548553953, 1697548554011, 1697548554871, 1697548555121, 1697548555374, 1697548555434, 1697548555477, 1697548555522, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556169, 1697548556196, 1697548556228, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556391, 1697548556421, 1697548556446, 1697548556477, 1697548556501, 1697548556526, 1697548556551, 1697548556575, 1697548556605, 1697548556629, 1697548556653, 1697548556682, 1697548556705, 1697548556729, 1697548556753, 1697548556776, 1697548556800, 1697548556823, 1697548556852, 1697548556876, 1697548556899, 1697548556925, 1697548556947, 1697548556971, 1697548556992, 1697548557012, 1697548557033, 1697548557053, 1697548557074, 1697548557094, 1697548557115, 1697548557138, 1697548557158, 1697548557177, 1697548557197, 1697548557217, 1697548557239, 1697548557258, 1697548557278, 1697548557297, 1697548557319, 1697548557337, 1697548557356, 1697548557375, 1697548557393, 1697548557412, 1697548557431, 1697548557449, 1697548557468, 1697548557487, 1697548557505, 1697548557524, 1697548557543, 1697548557562, 1697548557581, 1697548557599, 1697548557618, 1697548557637, 1697548557656, 1697548557675, 1697548557693, 1697548557712, 1697548557731, 1697548557750, 1697548557769, 1697548557788, 1697548557807, 1697548557825, 1697548557844, 1697548557863, 1697548557882, 1697548557901, 1697548557920, 1697548557939, 1697548557958, 1697548557977, 1697548557996, 1697548558015, 1697548558033, 1697548558052, 1697548558071, 1697548558090, 1697548558109, 1697548558128, 1697548558147, 1697548558166, 1697548558185]"
2854,2854,293,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536570,1697548537861,120,,,"[7, 289, 72, 52, 60, 66, 63, 60, 47]","[1697548536577, 1697548536866, 1697548536938, 1697548536990, 1697548537050, 1697548537116, 1697548537179, 1697548537239, 1697548537286]"
2855,2855,275,28,[],200,llama-7b,64,1,1181.0,1.0,1,A100,1697548527826,1697548529007,120,161.0,4.0,"[48, 391, 34, 656, 51]","[1697548527874, 1697548528265, 1697548528299, 1697548528955, 1697548529006]"
2856,2856,189,35,[],200,llama-7b,64,1,3076.0,1.0,1,A100,1697548522414,1697548525490,120,88.0,20.0,"[6, 610, 73, 67, 64, 64, 62, 62, 317, 51, 68, 50, 63, 64, 554, 64, 66, 65, 59, 58, 588]","[1697548522420, 1697548523030, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422, 1697548523739, 1697548523790, 1697548523858, 1697548523908, 1697548523971, 1697548524035, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489]"
2857,2857,875,30,[],200,llama-7b,64,1,5096.0,1.0,1,A100,1697548537876,1697548542972,120,31.0,31.0,"[176, 780, 53, 54, 41, 47, 45, 678, 62, 55, 54, 44, 52, 41, 401, 49, 60, 47, 56, 58, 51, 348, 48, 750, 60, 59, 52, 650, 62, 61, 57, 44]","[1697548538052, 1697548538832, 1697548538885, 1697548538939, 1697548538980, 1697548539027, 1697548539072, 1697548539750, 1697548539812, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540459, 1697548540508, 1697548540568, 1697548540615, 1697548540671, 1697548540729, 1697548540780, 1697548541128, 1697548541176, 1697548541926, 1697548541986, 1697548542045, 1697548542097, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542971]"
2858,2858,45,29,[],200,llama-7b,64,1,493.0,1.0,1,A100,1697548529009,1697548529502,120,19.0,1.0,"[11, 482]","[1697548529020, 1697548529502]"
2859,2859,866,58,[],200,llama-7b,64,1,2351.0,1.0,1,A100,1697548553783,1697548556134,120,93.0,20.0,"[19, 632, 437, 251, 253, 58, 45, 45, 45, 58, 57, 44, 49, 69, 47, 36, 45, 42, 41, 39, 39]","[1697548553802, 1697548554434, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555523, 1697548555568, 1697548555626, 1697548555683, 1697548555727, 1697548555776, 1697548555845, 1697548555892, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134]"
2860,2860,634,30,[],200,llama-7b,64,1,602.0,1.0,1,A100,1697548529506,1697548530108,120,13.0,1.0,"[24, 577]","[1697548529530, 1697548530107]"
2861,2861,406,31,[],200,llama-7b,64,1,1337.0,1.0,1,A100,1697548530110,1697548531447,120,244.0,4.0,"[18, 757, 51, 38, 473]","[1697548530128, 1697548530885, 1697548530936, 1697548530974, 1697548531447]"
2862,2862,56,32,[],200,llama-7b,64,1,2814.0,1.0,1,A100,1697548531450,1697548534264,120,86.0,20.0,"[6, 662, 366, 52, 59, 64, 50, 55, 576, 64, 64, 49, 66, 55, 46, 311, 60, 45, 46, 59, 58]","[1697548531456, 1697548532118, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532709, 1697548532764, 1697548533340, 1697548533404, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263]"
2863,2863,79,44,[],200,llama-7b,64,1,930.0,1.0,1,A100,1697548553504,1697548554434,120,12.0,1.0,"[37, 893]","[1697548553541, 1697548554434]"
2864,2864,813,22,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548524042,1697548527820,120,,,"[18, 1189, 240, 68, 65, 51, 50, 65, 65, 242, 48, 47, 61, 58, 656, 64, 50, 64, 65, 64]","[1697548524060, 1697548525249, 1697548525489, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526190, 1697548526251, 1697548526309, 1697548526965, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2865,2865,733,33,[],200,llama-7b,64,1,910.0,1.0,1,A100,1697548534266,1697548535176,120,31.0,1.0,"[22, 888]","[1697548534288, 1697548535176]"
2866,2866,680,33,[],200,llama-7b,64,1,1826.0,1.0,1,A100,1697548499853,1697548501679,120,123.0,11.0,"[17, 555, 72, 65, 51, 48, 59, 54, 723, 64, 60, 58]","[1697548499870, 1697548500425, 1697548500497, 1697548500562, 1697548500613, 1697548500661, 1697548500720, 1697548500774, 1697548501497, 1697548501561, 1697548501621, 1697548501679]"
2867,2867,467,23,[],200,llama-7b,64,1,2571.0,1.0,1,A100,1697548527826,1697548530397,120,93.0,20.0,"[6, 433, 33, 656, 52, 50, 48, 47, 40, 364, 52, 49, 41, 41, 41, 49, 347, 56, 48, 59, 59]","[1697548527832, 1697548528265, 1697548528298, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529656, 1697548529697, 1697548529738, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530397]"
2868,2868,333,34,[],200,llama-7b,64,1,2088.0,1.0,1,A100,1697548501681,1697548503769,120,563.0,11.0,"[7, 468, 254, 66, 63, 62, 60, 54, 876, 64, 56, 58]","[1697548501688, 1697548502156, 1697548502410, 1697548502476, 1697548502539, 1697548502601, 1697548502661, 1697548502715, 1697548503591, 1697548503655, 1697548503711, 1697548503769]"
2869,2869,105,35,[],200,llama-7b,64,1,4671.0,1.0,1,A100,1697548503771,1697548508442,120,364.0,36.0,"[9, 533, 255, 61, 48, 60, 59, 58, 57, 351, 65, 51, 61, 60, 60, 283, 45, 55, 52, 679, 65, 58, 55, 43, 581, 64, 62, 59, 56, 57, 324, 65, 64, 60, 59, 54, 43]","[1697548503780, 1697548504313, 1697548504568, 1697548504629, 1697548504677, 1697548504737, 1697548504796, 1697548504854, 1697548504911, 1697548505262, 1697548505327, 1697548505378, 1697548505439, 1697548505499, 1697548505559, 1697548505842, 1697548505887, 1697548505942, 1697548505994, 1697548506673, 1697548506738, 1697548506796, 1697548506851, 1697548506894, 1697548507475, 1697548507539, 1697548507601, 1697548507660, 1697548507716, 1697548507773, 1697548508097, 1697548508162, 1697548508226, 1697548508286, 1697548508345, 1697548508399, 1697548508442]"
2870,2870,41,51,[],200,llama-7b,64,1,6165.0,1.0,1,A100,1697548544889,1697548551054,120,39.0,43.0,"[10, 616, 368, 72, 65, 64, 63, 60, 49, 62, 759, 66, 63, 60, 48, 58, 45, 257, 53, 47, 371, 49, 48, 810, 61, 51, 59, 47, 58, 55, 396, 60, 47, 48, 47, 60, 56, 311, 64, 55, 49, 53, 372, 52]","[1697548544899, 1697548545515, 1697548545883, 1697548545955, 1697548546020, 1697548546084, 1697548546147, 1697548546207, 1697548546256, 1697548546318, 1697548547077, 1697548547143, 1697548547206, 1697548547266, 1697548547314, 1697548547372, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549779, 1697548549839, 1697548549886, 1697548549934, 1697548549981, 1697548550041, 1697548550097, 1697548550408, 1697548550472, 1697548550527, 1697548550576, 1697548550629, 1697548551001, 1697548551053]"
2871,2871,4,41,[],200,llama-7b,64,1,2446.0,1.0,1,A100,1697548520976,1697548523422,120,89.0,20.0,"[35, 353, 68, 62, 47, 60, 46, 54, 710, 70, 67, 67, 56, 66, 49, 318, 66, 64, 64, 62, 62]","[1697548521011, 1697548521364, 1697548521432, 1697548521494, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522481, 1697548522548, 1697548522615, 1697548522671, 1697548522737, 1697548522786, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422]"
2872,2872,57,48,[],200,llama-7b,64,1,805.0,1.0,1,A100,1697548540785,1697548541590,120,13.0,1.0,"[74, 731]","[1697548540859, 1697548541590]"
2873,2873,756,49,[],200,llama-7b,64,1,905.0,1.0,1,A100,1697548541597,1697548542502,120,19.0,1.0,"[29, 876]","[1697548541626, 1697548542502]"
2874,2874,417,50,[],200,llama-7b,64,1,846.0,1.0,1,A100,1697548542503,1697548543349,120,17.0,1.0,"[6, 840]","[1697548542509, 1697548543349]"
2875,2875,384,34,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548535178,1697548537857,120,,,"[13, 1082, 184, 63, 48, 55, 315, 52, 59, 66, 64, 59, 47]","[1697548535191, 1697548536273, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536938, 1697548536990, 1697548537049, 1697548537115, 1697548537179, 1697548537238, 1697548537285]"
2876,2876,673,51,[],200,llama-7b,64,1,3479.0,1.0,1,A100,1697548551896,1697548555375,120,93.0,20.0,"[6, 385, 373, 54, 66, 50, 50, 65, 64, 274, 48, 48, 60, 60, 349, 59, 46, 58, 860, 251, 253]","[1697548551902, 1697548552287, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009, 1697548553283, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553848, 1697548553907, 1697548553953, 1697548554011, 1697548554871, 1697548555122, 1697548555375]"
2877,2877,652,35,[],200,llama-7b,64,1,461.0,1.0,1,A100,1697548524788,1697548525249,120,14.0,1.0,"[16, 445]","[1697548524804, 1697548525249]"
2878,2878,312,36,[],200,llama-7b,64,1,776.0,1.0,1,A100,1697548525253,1697548526029,120,23.0,1.0,"[21, 754]","[1697548525274, 1697548526028]"
2879,2879,80,37,[],200,llama-7b,64,1,677.0,1.0,1,A100,1697548526033,1697548526710,120,13.0,1.0,"[18, 659]","[1697548526051, 1697548526710]"
2880,2880,670,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526713,1697548527821,120,,,"[15, 849]","[1697548526728, 1697548527577]"
2881,2881,632,28,[],200,llama-7b,64,1,2603.0,1.0,1,A100,1697548521434,1697548524037,120,91.0,20.0,"[6, 705, 265, 71, 67, 67, 56, 66, 50, 317, 66, 64, 64, 62, 61, 318, 51, 67, 51, 64, 65]","[1697548521440, 1697548522145, 1697548522410, 1697548522481, 1697548522548, 1697548522615, 1697548522671, 1697548522737, 1697548522787, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523421, 1697548523739, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524037]"
2882,2882,439,39,[],200,llama-7b,64,1,1230.0,1.0,1,A100,1697548527827,1697548529057,120,13.0,4.0,"[157, 912, 59, 51, 51]","[1697548527984, 1697548528896, 1697548528955, 1697548529006, 1697548529057]"
2883,2883,243,24,[],200,llama-7b,64,1,1048.0,1.0,1,A100,1697548530400,1697548531448,120,67.0,4.0,"[19, 466, 50, 39, 474]","[1697548530419, 1697548530885, 1697548530935, 1697548530974, 1697548531448]"
2884,2884,719,52,[],200,llama-7b,64,1,782.0,1.0,1,A100,1697548551056,1697548551838,120,182.0,6.0,"[6, 568, 62, 57, 45, 44]","[1697548551062, 1697548551630, 1697548551692, 1697548551749, 1697548551794, 1697548551838]"
2885,2885,828,25,[],200,llama-7b,64,1,1261.0,1.0,1,A100,1697548531449,1697548532710,120,182.0,6.0,"[6, 1029, 52, 59, 64, 50]","[1697548531455, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532709]"
2886,2886,687,36,[],200,llama-7b,64,1,3358.0,1.0,1,A100,1697548508444,1697548511802,120,96.0,20.0,"[15, 980, 129, 57, 54, 53, 53, 49, 560, 59, 59, 46, 58, 57, 230, 57, 42, 43, 51, 51, 655]","[1697548508459, 1697548509439, 1697548509568, 1697548509625, 1697548509679, 1697548509732, 1697548509785, 1697548509834, 1697548510394, 1697548510453, 1697548510512, 1697548510558, 1697548510616, 1697548510673, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511096, 1697548511147, 1697548511802]"
2887,2887,231,36,[],200,llama-7b,64,1,956.0,1.0,1,A100,1697548537876,1697548538832,120,13.0,1.0,"[188, 768]","[1697548538064, 1697548538832]"
2888,2888,597,26,[],200,llama-7b,64,1,437.0,1.0,1,A100,1697548532713,1697548533150,120,39.0,1.0,"[30, 407]","[1697548532743, 1697548533150]"
2889,2889,925,37,[],200,llama-7b,64,1,3212.0,1.0,1,A100,1697548538835,1697548542047,120,87.0,20.0,"[46, 629, 241, 61, 55, 54, 44, 52, 41, 400, 50, 59, 47, 57, 57, 51, 350, 48, 749, 61, 59]","[1697548538881, 1697548539510, 1697548539751, 1697548539812, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779, 1697548541129, 1697548541177, 1697548541926, 1697548541987, 1697548542046]"
2890,2890,254,27,[],200,llama-7b,64,1,777.0,1.0,1,A100,1697548533154,1697548533931,120,58.0,1.0,"[24, 753]","[1697548533178, 1697548533931]"
2891,2891,586,38,[],200,llama-7b,64,1,3971.0,1.0,1,A100,1697548542049,1697548546020,120,85.0,20.0,"[24, 429, 246, 62, 61, 56, 45, 55, 692, 59, 47, 57, 55, 52, 787, 56, 53, 47, 951, 71, 66]","[1697548542073, 1697548542502, 1697548542748, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776, 1697548544832, 1697548544885, 1697548544932, 1697548545883, 1697548545954, 1697548546020]"
2892,2892,24,28,[],200,llama-7b,64,1,1744.0,1.0,1,A100,1697548533933,1697548535677,120,79.0,9.0,"[10, 687, 47, 59, 58, 57, 707, 59, 60]","[1697548533943, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535677]"
2893,2893,375,53,[],200,llama-7b,64,1,2171.0,1.0,1,A100,1697548551841,1697548554012,120,874.0,17.0,"[11, 808, 54, 66, 50, 50, 65, 64, 274, 48, 48, 60, 60, 349, 59, 46, 59]","[1697548551852, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009, 1697548553283, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553848, 1697548553907, 1697548553953, 1697548554012]"
2894,2894,699,29,[],200,llama-7b,64,1,593.0,1.0,1,A100,1697548535680,1697548536273,120,39.0,1.0,"[14, 579]","[1697548535694, 1697548536273]"
2895,2895,353,30,[],200,llama-7b,64,1,774.0,1.0,1,A100,1697548536276,1697548537050,120,52.0,4.0,"[16, 574, 72, 51, 61]","[1697548536292, 1697548536866, 1697548536938, 1697548536989, 1697548537050]"
2896,2896,354,39,[],200,llama-7b,64,1,3306.0,1.0,1,A100,1697548546023,1697548549329,120,91.0,20.0,"[18, 820, 216, 67, 64, 58, 48, 58, 46, 256, 53, 47, 371, 49, 48, 810, 61, 51, 59, 48, 57]","[1697548546041, 1697548546861, 1697548547077, 1697548547144, 1697548547208, 1697548547266, 1697548547314, 1697548547372, 1697548547418, 1697548547674, 1697548547727, 1697548547774, 1697548548145, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549271, 1697548549328]"
2897,2897,63,36,[],200,llama-7b,64,1,387.0,1.0,1,A100,1697548520977,1697548521364,120,39.0,1.0,"[34, 353]","[1697548521011, 1697548521364]"
2898,2898,15,40,[],200,llama-7b,64,1,2362.0,1.0,1,A100,1697548549331,1697548551693,120,100.0,20.0,"[13, 355, 81, 59, 48, 47, 48, 60, 55, 312, 63, 56, 49, 53, 371, 53, 51, 51, 47, 427, 62]","[1697548549344, 1697548549699, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550042, 1697548550097, 1697548550409, 1697548550472, 1697548550528, 1697548550577, 1697548550630, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551203, 1697548551630, 1697548551692]"
2899,2899,95,40,[],200,llama-7b,64,1,442.0,1.0,1,A100,1697548529060,1697548529502,120,12.0,1.0,"[12, 430]","[1697548529072, 1697548529502]"
2900,2900,775,41,[],200,llama-7b,64,1,602.0,1.0,1,A100,1697548529506,1697548530108,120,17.0,1.0,"[29, 573]","[1697548529535, 1697548530108]"
2901,2901,79,60,[],200,llama-7b,64,1,920.0,1.0,1,A100,1697548550640,1697548551560,120,12.0,1.0,"[38, 882]","[1697548550678, 1697548551560]"
2902,2902,659,61,[],200,llama-7b,64,1,10922.0,1.0,1,A100,1697548551569,1697548562491,120,286.0,381.0,"[8, 709, 374, 54, 66, 50, 50, 65, 63, 275, 48, 48, 60, 60, 348, 60, 45, 59, 860, 251, 253, 58, 45, 44, 46, 58, 57, 43, 50, 69, 46, 37, 45, 42, 41, 39, 39, 34, 28, 32, 27, 26, 26, 27, 31, 26, 30, 25, 31, 24, 25, 25, 24, 30, 24, 24, 29, 23, 24, 23, 24, 24, 24, 28, 24, 23, 26, 22, 24, 21, 20, 21, 20, 21, 20, 21, 23, 19, 20, 20, 20, 22, 19, 20, 19, 22, 18, 19, 19, 18, 19, 19, 18, 19, 18, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 21, 18, 18, 18, 18, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 19, 18, 19, 18, 18, 18, 19, 18, 18, 18, 19, 18, 19, 18, 18, 19, 18, 18, 19, 18, 19, 18, 19, 18, 18, 19, 18, 19, 18, 19, 18, 18, 19, 18, 19, 18, 19, 18, 19, 18, 19, 19, 18, 19, 18, 19, 19, 18, 19, 18, 19, 18, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 18, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 19, 18, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 17, 17, 17, 17, 17, 17, 16, 17, 17, 17, 17, 17, 17, 17, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 15, 14, 14, 14, 14, 14, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 13, 14, 14, 14, 14, 14, 14, 14, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]","[1697548551577, 1697548552286, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553008, 1697548553283, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553907, 1697548553952, 1697548554011, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555522, 1697548555568, 1697548555626, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556168, 1697548556196, 1697548556228, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556391, 1697548556421, 1697548556446, 1697548556477, 1697548556501, 1697548556526, 1697548556551, 1697548556575, 1697548556605, 1697548556629, 1697548556653, 1697548556682, 1697548556705, 1697548556729, 1697548556752, 1697548556776, 1697548556800, 1697548556824, 1697548556852, 1697548556876, 1697548556899, 1697548556925, 1697548556947, 1697548556971, 1697548556992, 1697548557012, 1697548557033, 1697548557053, 1697548557074, 1697548557094, 1697548557115, 1697548557138, 1697548557157, 1697548557177, 1697548557197, 1697548557217, 1697548557239, 1697548557258, 1697548557278, 1697548557297, 1697548557319, 1697548557337, 1697548557356, 1697548557375, 1697548557393, 1697548557412, 1697548557431, 1697548557449, 1697548557468, 1697548557486, 1697548557505, 1697548557524, 1697548557543, 1697548557562, 1697548557581, 1697548557599, 1697548557618, 1697548557637, 1697548557656, 1697548557675, 1697548557693, 1697548557712, 1697548557731, 1697548557750, 1697548557769, 1697548557788, 1697548557807, 1697548557825, 1697548557844, 1697548557863, 1697548557882, 1697548557901, 1697548557920, 1697548557939, 1697548557958, 1697548557977, 1697548557996, 1697548558014, 1697548558033, 1697548558052, 1697548558071, 1697548558090, 1697548558109, 1697548558128, 1697548558147, 1697548558166, 1697548558185, 1697548558206, 1697548558224, 1697548558242, 1697548558260, 1697548558278, 1697548558295, 1697548558313, 1697548558331, 1697548558349, 1697548558367, 1697548558385, 1697548558403, 1697548558421, 1697548558439, 1697548558457, 1697548558475, 1697548558493, 1697548558511, 1697548558529, 1697548558547, 1697548558565, 1697548558583, 1697548558601, 1697548558619, 1697548558637, 1697548558656, 1697548558674, 1697548558692, 1697548558710, 1697548558728, 1697548558746, 1697548558764, 1697548558783, 1697548558801, 1697548558819, 1697548558837, 1697548558855, 1697548558873, 1697548558891, 1697548558909, 1697548558928, 1697548558946, 1697548558964, 1697548558982, 1697548559000, 1697548559019, 1697548559037, 1697548559055, 1697548559073, 1697548559091, 1697548559110, 1697548559128, 1697548559146, 1697548559164, 1697548559182, 1697548559201, 1697548559219, 1697548559237, 1697548559255, 1697548559273, 1697548559292, 1697548559310, 1697548559329, 1697548559347, 1697548559365, 1697548559383, 1697548559402, 1697548559420, 1697548559438, 1697548559456, 1697548559475, 1697548559493, 1697548559512, 1697548559530, 1697548559548, 1697548559567, 1697548559585, 1697548559603, 1697548559622, 1697548559640, 1697548559659, 1697548559677, 1697548559696, 1697548559714, 1697548559732, 1697548559751, 1697548559769, 1697548559788, 1697548559806, 1697548559825, 1697548559843, 1697548559861, 1697548559880, 1697548559898, 1697548559917, 1697548559935, 1697548559954, 1697548559972, 1697548559991, 1697548560009, 1697548560028, 1697548560047, 1697548560065, 1697548560084, 1697548560102, 1697548560121, 1697548560140, 1697548560158, 1697548560177, 1697548560195, 1697548560214, 1697548560232, 1697548560251, 1697548560269, 1697548560288, 1697548560307, 1697548560325, 1697548560344, 1697548560363, 1697548560381, 1697548560400, 1697548560419, 1697548560437, 1697548560456, 1697548560474, 1697548560493, 1697548560512, 1697548560530, 1697548560549, 1697548560568, 1697548560586, 1697548560605, 1697548560624, 1697548560642, 1697548560661, 1697548560680, 1697548560699, 1697548560717, 1697548560736, 1697548560755, 1697548560773, 1697548560792, 1697548560811, 1697548560830, 1697548560849, 1697548560867, 1697548560886, 1697548560905, 1697548560924, 1697548560943, 1697548560961, 1697548560980, 1697548560999, 1697548561018, 1697548561037, 1697548561056, 1697548561075, 1697548561092, 1697548561109, 1697548561126, 1697548561143, 1697548561160, 1697548561177, 1697548561193, 1697548561210, 1697548561227, 1697548561244, 1697548561261, 1697548561278, 1697548561295, 1697548561312, 1697548561328, 1697548561345, 1697548561362, 1697548561379, 1697548561396, 1697548561413, 1697548561430, 1697548561447, 1697548561464, 1697548561481, 1697548561498, 1697548561515, 1697548561531, 1697548561548, 1697548561565, 1697548561582, 1697548561599, 1697548561616, 1697548561633, 1697548561650, 1697548561667, 1697548561684, 1697548561701, 1697548561718, 1697548561734, 1697548561750, 1697548561766, 1697548561781, 1697548561795, 1697548561809, 1697548561823, 1697548561837, 1697548561851, 1697548561864, 1697548561878, 1697548561892, 1697548561906, 1697548561920, 1697548561934, 1697548561948, 1697548561962, 1697548561976, 1697548561990, 1697548562004, 1697548562018, 1697548562032, 1697548562045, 1697548562059, 1697548562073, 1697548562087, 1697548562101, 1697548562115, 1697548562129, 1697548562143, 1697548562156, 1697548562170, 1697548562184, 1697548562198, 1697548562212, 1697548562226, 1697548562240, 1697548562254, 1697548562268, 1697548562282, 1697548562296, 1697548562310, 1697548562323, 1697548562337, 1697548562351, 1697548562365, 1697548562379, 1697548562393, 1697548562407, 1697548562421, 1697548562435, 1697548562449, 1697548562463, 1697548562477, 1697548562491]"
2903,2903,713,41,[],200,llama-7b,64,1,1314.0,1.0,1,A100,1697548551695,1697548553009,120,874.0,8.0,"[18, 947, 54, 66, 50, 50, 65, 64]","[1697548551713, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009]"
2904,2904,369,42,[],200,llama-7b,64,1,2715.0,1.0,1,A100,1697548553012,1697548555727,120,216.0,15.0,"[20, 748, 67, 60, 45, 59, 859, 252, 252, 59, 45, 45, 45, 58, 58, 43]","[1697548553032, 1697548553780, 1697548553847, 1697548553907, 1697548553952, 1697548554011, 1697548554870, 1697548555122, 1697548555374, 1697548555433, 1697548555478, 1697548555523, 1697548555568, 1697548555626, 1697548555684, 1697548555727]"
2905,2905,113,41,[],200,llama-7b,64,1,498.0,1.0,1,A100,1697548552719,1697548553217,120,13.0,1.0,"[19, 479]","[1697548552738, 1697548553217]"
2906,2906,717,48,[],200,llama-7b,64,1,2632.0,1.0,1,A100,1697548553502,1697548556134,120,89.0,20.0,"[20, 911, 438, 251, 253, 58, 45, 44, 45, 59, 57, 43, 50, 69, 46, 37, 45, 42, 41, 40, 38]","[1697548553522, 1697548554433, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555522, 1697548555567, 1697548555626, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556096, 1697548556134]"
2907,2907,921,35,[],200,llama-7b,64,1,807.0,1.0,1,A100,1697548540784,1697548541591,120,31.0,1.0,"[35, 772]","[1697548540819, 1697548541591]"
2908,2908,698,36,[],200,llama-7b,64,1,1378.0,1.0,1,A100,1697548541594,1697548542972,120,182.0,6.0,"[32, 876, 245, 62, 61, 57, 45]","[1697548541626, 1697548542502, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542972]"
2909,2909,351,37,[],200,llama-7b,64,1,964.0,1.0,1,A100,1697548542974,1697548543938,120,216.0,6.0,"[11, 364, 371, 58, 47, 58, 54]","[1697548542985, 1697548543349, 1697548543720, 1697548543778, 1697548543825, 1697548543883, 1697548543937]"
2910,2910,887,36,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548525493,1697548527817,120,,,"[21, 515, 66, 48, 48, 60, 59, 655, 64, 50, 64, 65, 63]","[1697548525514, 1697548526029, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526965, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527271]"
2911,2911,124,45,[],200,llama-7b,64,1,907.0,1.0,1,A100,1697548548146,1697548549053,120,83.0,2.0,"[10, 897]","[1697548548156, 1697548549053]"
2912,2912,714,46,[],200,llama-7b,64,1,2636.0,1.0,1,A100,1697548549056,1697548551692,120,83.0,20.0,"[6, 637, 81, 59, 48, 47, 48, 59, 56, 312, 63, 56, 49, 53, 371, 53, 51, 51, 48, 425, 63]","[1697548549062, 1697548549699, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472, 1697548550528, 1697548550577, 1697548550630, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551204, 1697548551629, 1697548551692]"
2913,2913,651,31,[],200,llama-7b,64,1,744.0,1.0,1,A100,1697548542976,1697548543720,120,457.0,2.0,"[15, 358, 371]","[1697548542991, 1697548543349, 1697548543720]"
2914,2914,305,32,[],200,llama-7b,64,1,3695.0,1.0,1,A100,1697548543723,1697548547418,120,86.0,20.0,"[6, 793, 254, 56, 53, 47, 950, 73, 64, 65, 62, 62, 48, 62, 760, 64, 65, 59, 48, 58, 46]","[1697548543729, 1697548544522, 1697548544776, 1697548544832, 1697548544885, 1697548544932, 1697548545882, 1697548545955, 1697548546019, 1697548546084, 1697548546146, 1697548546208, 1697548546256, 1697548546318, 1697548547078, 1697548547142, 1697548547207, 1697548547266, 1697548547314, 1697548547372, 1697548547418]"
2915,2915,38,34,[],200,llama-7b,64,1,4205.0,1.0,1,A100,1697548514232,1697548518437,120,88.0,20.0,"[6, 620, 467, 71, 70, 64, 60, 977, 270, 69, 64, 66, 62, 295, 66, 64, 58, 53, 672, 67, 64]","[1697548514238, 1697548514858, 1697548515325, 1697548515396, 1697548515466, 1697548515530, 1697548515590, 1697548516567, 1697548516837, 1697548516906, 1697548516970, 1697548517036, 1697548517098, 1697548517393, 1697548517459, 1697548517523, 1697548517581, 1697548517634, 1697548518306, 1697548518373, 1697548518437]"
2916,2916,284,29,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548524042,1697548527820,120,,,"[13, 1434, 67, 66, 50, 51, 65, 65, 242, 48, 48, 60, 58, 655, 65, 50, 64, 65, 64]","[1697548524055, 1697548525489, 1697548525556, 1697548525622, 1697548525672, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526309, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2917,2917,482,47,[],200,llama-7b,64,1,3681.0,1.0,1,A100,1697548551694,1697548555375,120,91.0,20.0,"[9, 584, 373, 54, 66, 50, 50, 65, 64, 274, 48, 48, 60, 60, 348, 60, 46, 58, 860, 251, 253]","[1697548551703, 1697548552287, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009, 1697548553283, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553907, 1697548553953, 1697548554011, 1697548554871, 1697548555122, 1697548555375]"
2918,2918,273,35,[],200,llama-7b,64,1,914.0,1.0,1,A100,1697548530454,1697548531368,120,19.0,1.0,"[39, 875]","[1697548530493, 1697548531368]"
2919,2919,739,35,[],200,llama-7b,64,1,406.0,1.0,1,A100,1697548518440,1697548518846,120,216.0,1.0,"[21, 385]","[1697548518461, 1697548518846]"
2920,2920,76,33,[],200,llama-7b,64,1,2358.0,1.0,1,A100,1697548547422,1697548549780,120,364.0,12.0,"[39, 620, 64, 50, 48, 810, 61, 51, 59, 47, 58, 55, 396]","[1697548547461, 1697548548081, 1697548548145, 1697548548195, 1697548548243, 1697548549053, 1697548549114, 1697548549165, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549780]"
2921,2921,39,36,[],200,llama-7b,64,1,747.0,1.0,1,A100,1697548531371,1697548532118,120,8.0,1.0,"[32, 715]","[1697548531403, 1697548532118]"
2922,2922,666,34,[],200,llama-7b,64,1,2931.0,1.0,1,A100,1697548549783,1697548552714,120,84.0,20.0,"[11, 546, 69, 64, 56, 48, 53, 371, 53, 52, 49, 49, 426, 63, 56, 44, 45, 55, 56, 711, 54]","[1697548549794, 1697548550340, 1697548550409, 1697548550473, 1697548550529, 1697548550577, 1697548550630, 1697548551001, 1697548551054, 1697548551106, 1697548551155, 1697548551204, 1697548551630, 1697548551693, 1697548551749, 1697548551793, 1697548551838, 1697548551893, 1697548551949, 1697548552660, 1697548552714]"
2923,2923,623,37,[],200,llama-7b,64,1,1283.0,1.0,1,A100,1697548532121,1697548533404,120,140.0,3.0,"[6, 1213, 64]","[1697548532127, 1697548533340, 1697548533404]"
2924,2924,189,51,[],200,llama-7b,64,1,4064.0,1.0,1,A100,1697548543354,1697548547418,120,88.0,20.0,"[14, 1154, 254, 56, 53, 47, 952, 71, 65, 64, 62, 61, 48, 62, 760, 65, 64, 59, 49, 58, 46]","[1697548543368, 1697548544522, 1697548544776, 1697548544832, 1697548544885, 1697548544932, 1697548545884, 1697548545955, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547372, 1697548547418]"
2925,2925,371,38,[],200,llama-7b,64,1,522.0,1.0,1,A100,1697548533409,1697548533931,120,13.0,1.0,"[6, 516]","[1697548533415, 1697548533931]"
2926,2926,25,39,[],200,llama-7b,64,1,630.0,1.0,1,A100,1697548533935,1697548534565,120,12.0,1.0,"[14, 616]","[1697548533949, 1697548534565]"
2927,2927,465,48,[],200,llama-7b,64,1,1181.0,1.0,1,A100,1697548527826,1697548529007,120,364.0,3.0,"[113, 956, 60, 52]","[1697548527939, 1697548528895, 1697548528955, 1697548529007]"
2928,2928,729,40,[],200,llama-7b,64,1,992.0,1.0,1,A100,1697548534566,1697548535558,120,874.0,2.0,"[13, 979]","[1697548534579, 1697548535558]"
2929,2929,234,49,[],200,llama-7b,64,1,3585.0,1.0,1,A100,1697548529010,1697548532595,120,457.0,25.0,"[15, 477, 54, 52, 49, 41, 41, 40, 50, 347, 56, 47, 60, 59, 52, 485, 39, 473, 64, 64, 61, 59, 52, 736, 53, 58]","[1697548529025, 1697548529502, 1697548529556, 1697548529608, 1697548529657, 1697548529698, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530232, 1697548530279, 1697548530339, 1697548530398, 1697548530450, 1697548530935, 1697548530974, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531695, 1697548531747, 1697548532483, 1697548532536, 1697548532594]"
2930,2930,771,52,[],200,llama-7b,64,1,3052.0,1.0,1,A100,1697548547420,1697548550472,120,47.0,20.0,"[21, 640, 64, 49, 49, 810, 61, 50, 60, 47, 58, 55, 396, 59, 48, 47, 48, 59, 56, 312, 63]","[1697548547441, 1697548548081, 1697548548145, 1697548548194, 1697548548243, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549384, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472]"
2931,2931,383,41,[],200,llama-7b,64,1,709.0,1.0,1,A100,1697548535564,1697548536273,120,15.0,1.0,"[28, 681]","[1697548535592, 1697548536273]"
2932,2932,838,40,[],200,llama-7b,64,1,3681.0,1.0,1,A100,1697548551694,1697548555375,120,90.0,20.0,"[9, 583, 374, 54, 66, 50, 50, 65, 64, 274, 48, 48, 60, 60, 348, 60, 46, 58, 860, 251, 253]","[1697548551703, 1697548552286, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009, 1697548553283, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553907, 1697548553953, 1697548554011, 1697548554871, 1697548555122, 1697548555375]"
2933,2933,159,42,[],200,llama-7b,64,1,590.0,1.0,1,A100,1697548536277,1697548536867,120,31.0,1.0,"[26, 563]","[1697548536303, 1697548536866]"
2934,2934,129,31,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548537057,1697548537857,120,,,"[7, 704]","[1697548537064, 1697548537768]"
2935,2935,747,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536870,1697548537856,120,,,"[19, 878]","[1697548536889, 1697548537767]"
2936,2936,517,44,[],200,llama-7b,64,1,961.0,1.0,1,A100,1697548537869,1697548538830,120,15.0,1.0,"[85, 876]","[1697548537954, 1697548538830]"
2937,2937,514,53,[],200,llama-7b,64,1,2534.0,1.0,1,A100,1697548550475,1697548553009,120,85.0,20.0,"[25, 440, 61, 53, 51, 51, 49, 424, 64, 56, 44, 45, 55, 55, 712, 55, 65, 50, 50, 65, 64]","[1697548550500, 1697548550940, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551205, 1697548551629, 1697548551693, 1697548551749, 1697548551793, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552715, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009]"
2938,2938,177,45,[],200,llama-7b,64,1,675.0,1.0,1,A100,1697548538835,1697548539510,120,14.0,1.0,"[48, 627]","[1697548538883, 1697548539510]"
2939,2939,874,46,[],200,llama-7b,64,1,8681.0,1.0,1,A100,1697548539514,1697548548195,120,140.0,50.0,"[24, 847, 74, 50, 58, 47, 57, 57, 51, 350, 48, 750, 60, 60, 50, 650, 62, 61, 57, 45, 54, 693, 59, 47, 57, 55, 52, 787, 55, 54, 46, 952, 71, 65, 64, 63, 61, 48, 62, 759, 66, 64, 59, 49, 57, 46, 256, 53, 48, 370, 51]","[1697548539538, 1697548540385, 1697548540459, 1697548540509, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779, 1697548541129, 1697548541177, 1697548541927, 1697548541987, 1697548542047, 1697548542097, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542972, 1697548543026, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776, 1697548544831, 1697548544885, 1697548544931, 1697548545883, 1697548545954, 1697548546019, 1697548546083, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547076, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547371, 1697548547417, 1697548547673, 1697548547726, 1697548547774, 1697548548144, 1697548548195]"
2940,2940,284,54,[],200,llama-7b,64,1,3322.0,1.0,1,A100,1697548553012,1697548556334,120,90.0,31.0,"[20, 815, 60, 45, 59, 859, 252, 252, 59, 45, 45, 45, 58, 58, 43, 49, 69, 46, 37, 45, 42, 41, 39, 39, 34, 28, 32, 27, 26, 26, 27]","[1697548553032, 1697548553847, 1697548553907, 1697548553952, 1697548554011, 1697548554870, 1697548555122, 1697548555374, 1697548555433, 1697548555478, 1697548555523, 1697548555568, 1697548555626, 1697548555684, 1697548555727, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556168, 1697548556196, 1697548556228, 1697548556255, 1697548556281, 1697548556307, 1697548556334]"
2941,2941,123,38,[],200,llama-7b,64,1,580.0,1.0,1,A100,1697548543942,1697548544522,120,14.0,1.0,"[6, 574]","[1697548543948, 1697548544522]"
2942,2942,716,39,[],200,llama-7b,64,1,5254.0,1.0,1,A100,1697548544525,1697548549779,120,79.0,30.0,"[21, 1337, 71, 65, 65, 62, 61, 48, 62, 759, 67, 63, 60, 48, 57, 46, 257, 53, 47, 371, 49, 48, 810, 61, 51, 59, 47, 58, 55, 396]","[1697548544546, 1697548545883, 1697548545954, 1697548546019, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547076, 1697548547143, 1697548547206, 1697548547266, 1697548547314, 1697548547371, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549779]"
2943,2943,64,30,[],200,llama-7b,64,1,2624.0,1.0,1,A100,1697548527825,1697548530449,120,89.0,20.0,"[78, 993, 59, 52, 50, 48, 47, 40, 364, 52, 49, 41, 41, 40, 50, 347, 55, 48, 60, 59, 51]","[1697548527903, 1697548528896, 1697548528955, 1697548529007, 1697548529057, 1697548529105, 1697548529152, 1697548529192, 1697548529556, 1697548529608, 1697548529657, 1697548529698, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530279, 1697548530339, 1697548530398, 1697548530449]"
2944,2944,392,36,[],200,llama-7b,64,1,761.0,1.0,1,A100,1697548518849,1697548519610,120,20.0,1.0,"[16, 745]","[1697548518865, 1697548519610]"
2945,2945,247,30,[],200,llama-7b,64,1,17867.0,1.0,1,A100,1697548543834,1697548561701,120,216.0,381.0,"[10, 678, 255, 55, 54, 46, 951, 71, 65, 65, 62, 62, 47, 63, 760, 65, 64, 59, 48, 58, 46, 255, 54, 47, 370, 50, 48, 810, 61, 51, 59, 47, 58, 55, 395, 61, 47, 47, 48, 60, 55, 312, 64, 55, 50, 52, 372, 52, 52, 50, 49, 425, 62, 57, 45, 44, 55, 56, 711, 54, 66, 50, 50, 65, 64, 274, 49, 47, 60, 60, 349, 59, 46, 58, 860, 251, 253, 59, 44, 45, 45, 58, 58, 43, 50, 69, 46, 36, 46, 42, 41, 39, 38, 35, 28, 32, 27, 26, 26, 27, 31, 26, 30, 25, 30, 25, 25, 25, 24, 30, 24, 24, 29, 23, 24, 24, 23, 24, 23, 29, 24, 23, 26, 22, 24, 21, 20, 21, 20, 21, 20, 21, 23, 20, 19, 20, 20, 22, 20, 19, 19, 22, 18, 19, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 21, 18, 18, 18, 17, 19, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 18, 18, 19, 18, 18, 18, 18, 19, 18, 18, 18, 18, 19, 18, 18, 18, 19, 18, 18, 18, 18, 18, 19, 18, 18, 19, 18, 18, 19, 18, 18, 18, 19, 18, 19, 18, 18, 19, 18, 18, 19, 18, 18, 19, 18, 19, 18, 19, 18, 19, 18, 19, 18, 18, 19, 18, 19, 18, 19, 18, 19, 18, 19, 19, 18, 19, 18, 19, 18, 19, 19, 18, 19, 18, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 18, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 18, 19, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 18, 19, 19, 19, 19, 19, 19, 17, 17, 17, 17, 17, 17, 16, 17, 17, 17, 17, 17, 17, 17, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17]","[1697548543844, 1697548544522, 1697548544777, 1697548544832, 1697548544886, 1697548544932, 1697548545883, 1697548545954, 1697548546019, 1697548546084, 1697548546146, 1697548546208, 1697548546255, 1697548546318, 1697548547078, 1697548547143, 1697548547207, 1697548547266, 1697548547314, 1697548547372, 1697548547418, 1697548547673, 1697548547727, 1697548547774, 1697548548144, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549778, 1697548549839, 1697548549886, 1697548549933, 1697548549981, 1697548550041, 1697548550096, 1697548550408, 1697548550472, 1697548550527, 1697548550577, 1697548550629, 1697548551001, 1697548551053, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551691, 1697548551748, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552659, 1697548552713, 1697548552779, 1697548552829, 1697548552879, 1697548552944, 1697548553008, 1697548553282, 1697548553331, 1697548553378, 1697548553438, 1697548553498, 1697548553847, 1697548553906, 1697548553952, 1697548554010, 1697548554870, 1697548555121, 1697548555374, 1697548555433, 1697548555477, 1697548555522, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555927, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556133, 1697548556168, 1697548556196, 1697548556228, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556391, 1697548556421, 1697548556446, 1697548556476, 1697548556501, 1697548556526, 1697548556551, 1697548556575, 1697548556605, 1697548556629, 1697548556653, 1697548556682, 1697548556705, 1697548556729, 1697548556753, 1697548556776, 1697548556800, 1697548556823, 1697548556852, 1697548556876, 1697548556899, 1697548556925, 1697548556947, 1697548556971, 1697548556992, 1697548557012, 1697548557033, 1697548557053, 1697548557074, 1697548557094, 1697548557115, 1697548557138, 1697548557158, 1697548557177, 1697548557197, 1697548557217, 1697548557239, 1697548557259, 1697548557278, 1697548557297, 1697548557319, 1697548557337, 1697548557356, 1697548557375, 1697548557393, 1697548557412, 1697548557431, 1697548557449, 1697548557468, 1697548557487, 1697548557505, 1697548557524, 1697548557543, 1697548557562, 1697548557581, 1697548557599, 1697548557618, 1697548557637, 1697548557656, 1697548557675, 1697548557693, 1697548557712, 1697548557731, 1697548557750, 1697548557769, 1697548557788, 1697548557806, 1697548557825, 1697548557844, 1697548557863, 1697548557882, 1697548557901, 1697548557920, 1697548557939, 1697548557958, 1697548557977, 1697548557996, 1697548558014, 1697548558033, 1697548558052, 1697548558071, 1697548558090, 1697548558109, 1697548558128, 1697548558147, 1697548558166, 1697548558185, 1697548558206, 1697548558224, 1697548558242, 1697548558260, 1697548558277, 1697548558296, 1697548558313, 1697548558331, 1697548558349, 1697548558367, 1697548558385, 1697548558403, 1697548558421, 1697548558439, 1697548558457, 1697548558475, 1697548558493, 1697548558511, 1697548558529, 1697548558547, 1697548558565, 1697548558583, 1697548558601, 1697548558619, 1697548558637, 1697548558656, 1697548558674, 1697548558692, 1697548558710, 1697548558728, 1697548558746, 1697548558764, 1697548558782, 1697548558801, 1697548558819, 1697548558837, 1697548558855, 1697548558873, 1697548558891, 1697548558909, 1697548558928, 1697548558946, 1697548558964, 1697548558982, 1697548559000, 1697548559019, 1697548559037, 1697548559055, 1697548559073, 1697548559091, 1697548559110, 1697548559128, 1697548559146, 1697548559164, 1697548559183, 1697548559201, 1697548559219, 1697548559237, 1697548559255, 1697548559273, 1697548559292, 1697548559310, 1697548559328, 1697548559347, 1697548559365, 1697548559383, 1697548559402, 1697548559420, 1697548559438, 1697548559456, 1697548559475, 1697548559493, 1697548559512, 1697548559530, 1697548559548, 1697548559567, 1697548559585, 1697548559603, 1697548559622, 1697548559640, 1697548559658, 1697548559677, 1697548559695, 1697548559714, 1697548559732, 1697548559751, 1697548559769, 1697548559788, 1697548559806, 1697548559825, 1697548559843, 1697548559861, 1697548559880, 1697548559898, 1697548559917, 1697548559935, 1697548559954, 1697548559972, 1697548559991, 1697548560009, 1697548560028, 1697548560047, 1697548560065, 1697548560084, 1697548560102, 1697548560121, 1697548560139, 1697548560158, 1697548560177, 1697548560195, 1697548560214, 1697548560232, 1697548560251, 1697548560269, 1697548560288, 1697548560307, 1697548560325, 1697548560344, 1697548560363, 1697548560381, 1697548560400, 1697548560419, 1697548560437, 1697548560456, 1697548560474, 1697548560493, 1697548560512, 1697548560530, 1697548560549, 1697548560568, 1697548560586, 1697548560605, 1697548560624, 1697548560642, 1697548560661, 1697548560680, 1697548560698, 1697548560717, 1697548560736, 1697548560755, 1697548560773, 1697548560792, 1697548560811, 1697548560830, 1697548560849, 1697548560867, 1697548560886, 1697548560905, 1697548560924, 1697548560943, 1697548560961, 1697548560980, 1697548560999, 1697548561018, 1697548561037, 1697548561056, 1697548561075, 1697548561092, 1697548561109, 1697548561126, 1697548561143, 1697548561160, 1697548561177, 1697548561193, 1697548561210, 1697548561227, 1697548561244, 1697548561261, 1697548561278, 1697548561295, 1697548561312, 1697548561328, 1697548561345, 1697548561362, 1697548561379, 1697548561396, 1697548561413, 1697548561430, 1697548561447, 1697548561464, 1697548561481, 1697548561497, 1697548561514, 1697548561531, 1697548561548, 1697548561565, 1697548561582, 1697548561599, 1697548561616, 1697548561633, 1697548561650, 1697548561667, 1697548561684, 1697548561701]"
2946,2946,435,35,[],200,llama-7b,64,1,3339.0,1.0,1,A100,1697548552717,1697548556056,120,563.0,27.0,"[16, 483, 67, 49, 47, 60, 60, 348, 59, 46, 59, 860, 251, 253, 58, 45, 44, 46, 58, 57, 44, 49, 69, 46, 37, 45, 42, 41]","[1697548552733, 1697548553216, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555522, 1697548555568, 1697548555626, 1697548555683, 1697548555727, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056]"
2947,2947,714,32,[],200,llama-7b,64,1,2910.0,1.0,1,A100,1697548537870,1697548540780,120,83.0,20.0,"[103, 858, 53, 56, 42, 45, 46, 677, 62, 55, 53, 44, 53, 41, 400, 50, 59, 47, 57, 57, 52]","[1697548537973, 1697548538831, 1697548538884, 1697548538940, 1697548538982, 1697548539027, 1697548539073, 1697548539750, 1697548539812, 1697548539867, 1697548539920, 1697548539964, 1697548540017, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540780]"
2948,2948,298,22,[],200,llama-7b,64,1,954.0,1.0,1,A100,1697548537877,1697548538831,120,17.0,1.0,"[197, 757]","[1697548538074, 1697548538831]"
2949,2949,622,47,[],200,llama-7b,64,1,564.0,1.0,1,A100,1697548548197,1697548548761,120,20.0,1.0,"[12, 552]","[1697548548209, 1697548548761]"
2950,2950,881,23,[],200,llama-7b,64,1,1131.0,1.0,1,A100,1697548538834,1697548539965,120,58.0,6.0,"[19, 656, 242, 60, 56, 53, 44]","[1697548538853, 1697548539509, 1697548539751, 1697548539811, 1697548539867, 1697548539920, 1697548539964]"
2951,2951,276,48,[],200,llama-7b,64,1,1867.0,1.0,1,A100,1697548548763,1697548550630,120,732.0,13.0,"[15, 1001, 60, 48, 47, 48, 59, 56, 312, 63, 55, 50, 53]","[1697548548778, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472, 1697548550527, 1697548550577, 1697548550630]"
2952,2952,483,33,[],200,llama-7b,64,1,4102.0,1.0,1,A100,1697548540784,1697548544886,120,84.0,20.0,"[55, 752, 335, 61, 59, 51, 651, 61, 61, 58, 43, 55, 694, 58, 47, 58, 54, 52, 788, 55, 54]","[1697548540839, 1697548541591, 1697548541926, 1697548541987, 1697548542046, 1697548542097, 1697548542748, 1697548542809, 1697548542870, 1697548542928, 1697548542971, 1697548543026, 1697548543720, 1697548543778, 1697548543825, 1697548543883, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544886]"
2953,2953,628,24,[],200,llama-7b,64,1,1209.0,1.0,1,A100,1697548539968,1697548541177,120,732.0,10.0,"[6, 485, 50, 59, 46, 58, 56, 52, 348, 49]","[1697548539974, 1697548540459, 1697548540509, 1697548540568, 1697548540614, 1697548540672, 1697548540728, 1697548540780, 1697548541128, 1697548541177]"
2954,2954,47,49,[],200,llama-7b,64,1,2866.0,1.0,1,A100,1697548550634,1697548553500,120,90.0,20.0,"[29, 897, 69, 63, 58, 43, 44, 56, 55, 713, 52, 67, 49, 50, 66, 63, 275, 49, 47, 60, 61]","[1697548550663, 1697548551560, 1697548551629, 1697548551692, 1697548551750, 1697548551793, 1697548551837, 1697548551893, 1697548551948, 1697548552661, 1697548552713, 1697548552780, 1697548552829, 1697548552879, 1697548552945, 1697548553008, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553500]"
2955,2955,276,25,[],200,llama-7b,64,1,2808.0,1.0,1,A100,1697548541181,1697548543989,120,732.0,13.0,"[7, 1561, 60, 61, 58, 44, 55, 692, 60, 45, 58, 55, 52]","[1697548541188, 1697548542749, 1697548542809, 1697548542870, 1697548542928, 1697548542972, 1697548543027, 1697548543719, 1697548543779, 1697548543824, 1697548543882, 1697548543937, 1697548543989]"
2956,2956,429,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548530111,1697548537857,120,,,"[24, 750, 51, 38, 473, 64, 65, 61, 59, 51, 737, 52, 59, 64, 51, 54, 576, 64, 63, 50, 63, 58, 46, 311, 60, 45, 46, 59, 57, 368, 46, 59, 58, 57, 708, 59, 59, 58, 55, 668, 63, 48, 55, 314, 52, 60, 65, 64, 60, 47]","[1697548530135, 1697548530885, 1697548530936, 1697548530974, 1697548531447, 1697548531511, 1697548531576, 1697548531637, 1697548531696, 1697548531747, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533580, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534262, 1697548534630, 1697548534676, 1697548534735, 1697548534793, 1697548534850, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537114, 1697548537178, 1697548537238, 1697548537285]"
2957,2957,53,26,[],200,llama-7b,64,1,8722.0,1.0,1,A100,1697548543991,1697548552713,120,216.0,55.0,"[9, 1514, 369, 71, 65, 65, 62, 62, 47, 62, 761, 65, 64, 59, 48, 58, 46, 255, 54, 47, 370, 50, 48, 810, 61, 51, 59, 47, 58, 55, 395, 61, 47, 47, 48, 60, 55, 312, 64, 55, 49, 53, 372, 53, 51, 50, 49, 425, 62, 57, 45, 44, 55, 56, 711, 54]","[1697548544000, 1697548545514, 1697548545883, 1697548545954, 1697548546019, 1697548546084, 1697548546146, 1697548546208, 1697548546255, 1697548546317, 1697548547078, 1697548547143, 1697548547207, 1697548547266, 1697548547314, 1697548547372, 1697548547418, 1697548547673, 1697548547727, 1697548547774, 1697548548144, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549778, 1697548549839, 1697548549886, 1697548549933, 1697548549981, 1697548550041, 1697548550096, 1697548550408, 1697548550472, 1697548550527, 1697548550576, 1697548550629, 1697548551001, 1697548551054, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551691, 1697548551748, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552659, 1697548552713]"
2958,2958,633,50,[],200,llama-7b,64,1,2631.0,1.0,1,A100,1697548553503,1697548556134,120,90.0,20.0,"[32, 899, 437, 251, 253, 58, 45, 44, 46, 58, 57, 43, 50, 69, 46, 37, 45, 42, 41, 39, 39]","[1697548553535, 1697548554434, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555522, 1697548555568, 1697548555626, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134]"
2959,2959,145,34,[],200,llama-7b,64,1,1429.0,1.0,1,A100,1697548544889,1697548546318,120,161.0,9.0,"[25, 601, 369, 71, 65, 64, 63, 61, 48, 62]","[1697548544914, 1697548545515, 1697548545884, 1697548545955, 1697548546020, 1697548546084, 1697548546147, 1697548546208, 1697548546256, 1697548546318]"
2960,2960,823,50,[],200,llama-7b,64,1,2961.0,1.0,1,A100,1697548532597,1697548535558,120,90.0,20.0,"[16, 537, 190, 64, 63, 50, 64, 58, 45, 311, 60, 46, 45, 59, 58, 367, 46, 60, 58, 57, 707]","[1697548532613, 1697548533150, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533581, 1697548533639, 1697548533684, 1697548533995, 1697548534055, 1697548534101, 1697548534146, 1697548534205, 1697548534263, 1697548534630, 1697548534676, 1697548534736, 1697548534794, 1697548534851, 1697548535558]"
2961,2961,845,35,[],200,llama-7b,64,1,7178.0,1.0,1,A100,1697548546321,1697548553499,120,244.0,50.0,"[19, 1274, 60, 53, 47, 370, 50, 48, 810, 62, 50, 59, 48, 58, 54, 396, 60, 48, 47, 48, 59, 56, 311, 64, 56, 48, 53, 372, 52, 52, 50, 49, 425, 62, 57, 45, 44, 55, 56, 711, 54, 66, 50, 50, 65, 64, 274, 49, 48, 59, 61]","[1697548546340, 1697548547614, 1697548547674, 1697548547727, 1697548547774, 1697548548144, 1697548548194, 1697548548242, 1697548549052, 1697548549114, 1697548549164, 1697548549223, 1697548549271, 1697548549329, 1697548549383, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550408, 1697548550472, 1697548550528, 1697548550576, 1697548550629, 1697548551001, 1697548551053, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551691, 1697548551748, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552659, 1697548552713, 1697548552779, 1697548552829, 1697548552879, 1697548552944, 1697548553008, 1697548553282, 1697548553331, 1697548553379, 1697548553438, 1697548553499]"
2962,2962,170,37,[],200,llama-7b,64,1,2868.0,1.0,1,A100,1697548519613,1697548522481,120,335.0,15.0,"[19, 1043, 72, 65, 63, 49, 48, 54, 405, 63, 47, 60, 46, 54, 710, 69]","[1697548519632, 1697548520675, 1697548520747, 1697548520812, 1697548520875, 1697548520924, 1697548520972, 1697548521026, 1697548521431, 1697548521494, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522411, 1697548522480]"
2963,2963,161,35,[],200,llama-7b,64,1,1209.0,1.0,1,A100,1697548537864,1697548539073,120,109.0,7.0,"[38, 299, 683, 55, 42, 46, 45]","[1697548537902, 1697548538201, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539072]"
2964,2964,702,42,[],200,llama-7b,64,1,2754.0,1.0,1,A100,1697548553220,1697548555974,120,89.0,20.0,"[6, 555, 66, 60, 45, 59, 859, 251, 253, 59, 45, 45, 44, 59, 58, 42, 50, 69, 47, 36, 46]","[1697548553226, 1697548553781, 1697548553847, 1697548553907, 1697548553952, 1697548554011, 1697548554870, 1697548555121, 1697548555374, 1697548555433, 1697548555478, 1697548555523, 1697548555567, 1697548555626, 1697548555684, 1697548555726, 1697548555776, 1697548555845, 1697548555892, 1697548555928, 1697548555974]"
2965,2965,592,42,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548523426,1697548527819,120,,,"[23, 903, 237, 64, 66, 65, 59, 58, 588, 67, 66, 50, 50, 66, 65, 242, 48, 48, 60, 59, 654, 65, 50, 64, 65, 64]","[1697548523449, 1697548524352, 1697548524589, 1697548524653, 1697548524719, 1697548524784, 1697548524843, 1697548524901, 1697548525489, 1697548525556, 1697548525622, 1697548525672, 1697548525722, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310, 1697548526964, 1697548527029, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
2966,2966,435,37,[],200,llama-7b,64,1,5776.0,1.0,1,A100,1697548511805,1697548517581,120,563.0,27.0,"[6, 648, 311, 350, 58, 55, 44, 53, 833, 66, 60, 59, 57, 920, 71, 71, 63, 61, 976, 270, 68, 66, 64, 64, 293, 66, 64, 59]","[1697548511811, 1697548512459, 1697548512770, 1697548513120, 1697548513178, 1697548513233, 1697548513277, 1697548513330, 1697548514163, 1697548514229, 1697548514289, 1697548514348, 1697548514405, 1697548515325, 1697548515396, 1697548515467, 1697548515530, 1697548515591, 1697548516567, 1697548516837, 1697548516905, 1697548516971, 1697548517035, 1697548517099, 1697548517392, 1697548517458, 1697548517522, 1697548517581]"
2967,2967,743,36,[],200,llama-7b,64,1,1596.0,1.0,1,A100,1697548539075,1697548540671,120,123.0,6.0,"[12, 1298, 73, 50, 59, 47, 57]","[1697548539087, 1697548540385, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671]"
2968,2968,144,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548535792,1697548537866,120,,,"[18, 1056, 72, 51, 61, 66, 62, 61, 46]","[1697548535810, 1697548536866, 1697548536938, 1697548536989, 1697548537050, 1697548537116, 1697548537178, 1697548537239, 1697548537285]"
2969,2969,515,37,[],200,llama-7b,64,1,402.0,1.0,1,A100,1697548540674,1697548541076,120,11.0,1.0,"[6, 396]","[1697548540680, 1697548541076]"
2970,2970,176,38,[],200,llama-7b,64,1,849.0,1.0,1,A100,1697548541078,1697548541927,120,216.0,2.0,"[10, 839]","[1697548541088, 1697548541927]"
2971,2971,877,39,[],200,llama-7b,64,1,4089.0,1.0,1,A100,1697548541931,1697548546020,120,85.0,20.0,"[6, 565, 245, 62, 62, 56, 45, 55, 692, 59, 47, 57, 55, 52, 788, 55, 55, 45, 951, 71, 66]","[1697548541937, 1697548542502, 1697548542747, 1697548542809, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544887, 1697548544932, 1697548545883, 1697548545954, 1697548546020]"
2972,2972,548,37,[],200,llama-7b,64,1,2573.0,1.0,1,A100,1697548527825,1697548530398,120,86.0,20.0,"[39, 401, 34, 655, 52, 50, 48, 47, 40, 364, 52, 49, 41, 42, 40, 49, 347, 56, 48, 59, 59]","[1697548527864, 1697548528265, 1697548528299, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529656, 1697548529697, 1697548529739, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530397]"
2973,2973,636,27,[],200,llama-7b,64,1,499.0,1.0,1,A100,1697548552718,1697548553217,120,31.0,1.0,"[20, 479]","[1697548552738, 1697548553217]"
2974,2974,505,36,[],200,llama-7b,64,1,2832.0,1.0,1,A100,1697548553502,1697548556334,120,100.0,27.0,"[23, 1346, 251, 252, 59, 45, 44, 46, 58, 57, 43, 50, 69, 46, 37, 45, 42, 41, 40, 38, 35, 27, 33, 26, 26, 26, 27]","[1697548553525, 1697548554871, 1697548555122, 1697548555374, 1697548555433, 1697548555478, 1697548555522, 1697548555568, 1697548555626, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556096, 1697548556134, 1697548556169, 1697548556196, 1697548556229, 1697548556255, 1697548556281, 1697548556307, 1697548556334]"
2975,2975,407,28,[],200,llama-7b,64,1,561.0,1.0,1,A100,1697548553220,1697548553781,120,16.0,1.0,"[15, 545]","[1697548553235, 1697548553780]"
2976,2976,68,29,[],200,llama-7b,64,1,650.0,1.0,1,A100,1697548553784,1697548554434,120,12.0,1.0,"[15, 635]","[1697548553799, 1697548554434]"
2977,2977,647,31,[],200,llama-7b,64,1,3231.0,1.0,1,A100,1697548530453,1697548533684,120,83.0,20.0,"[25, 889, 80, 64, 64, 61, 59, 52, 738, 52, 58, 64, 51, 55, 575, 64, 63, 50, 67, 54, 46]","[1697548530478, 1697548531367, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531695, 1697548531747, 1697548532485, 1697548532537, 1697548532595, 1697548532659, 1697548532710, 1697548532765, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533584, 1697548533638, 1697548533684]"
2978,2978,746,38,[],200,llama-7b,64,1,2361.0,1.0,1,A100,1697548522483,1697548524844,120,345.0,18.0,"[10, 537, 73, 67, 64, 64, 62, 62, 316, 52, 67, 51, 63, 64, 554, 65, 65, 65, 60]","[1697548522493, 1697548523030, 1697548523103, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523971, 1697548524035, 1697548524589, 1697548524654, 1697548524719, 1697548524784, 1697548524844]"
2979,2979,368,43,[],200,llama-7b,64,1,2622.0,1.0,1,A100,1697548527827,1697548530449,120,88.0,20.0,"[222, 847, 59, 51, 51, 47, 48, 39, 364, 52, 49, 42, 40, 41, 50, 346, 56, 48, 59, 59, 52]","[1697548528049, 1697548528896, 1697548528955, 1697548529006, 1697548529057, 1697548529104, 1697548529152, 1697548529191, 1697548529555, 1697548529607, 1697548529656, 1697548529698, 1697548529738, 1697548529779, 1697548529829, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530397, 1697548530449]"
2980,2980,527,21,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548530454,1697548537858,120,,,"[48, 945, 64, 64, 61, 60, 51, 736, 52, 59, 64, 51, 54, 578, 64, 63, 49, 66, 55, 46, 311, 60, 45, 46, 58, 59, 367, 46, 59, 58, 57, 708, 59, 60, 57, 55, 668, 63, 48, 55, 314, 52, 60, 66, 63, 60, 47]","[1697548530502, 1697548531447, 1697548531511, 1697548531575, 1697548531636, 1697548531696, 1697548531747, 1697548532483, 1697548532535, 1697548532594, 1697548532658, 1697548532709, 1697548532763, 1697548533341, 1697548533405, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534204, 1697548534263, 1697548534630, 1697548534676, 1697548534735, 1697548534793, 1697548534850, 1697548535558, 1697548535617, 1697548535677, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
2981,2981,207,38,[],200,llama-7b,64,1,481.0,1.0,1,A100,1697548517584,1697548518065,120,10.0,1.0,"[11, 470]","[1697548517595, 1697548518065]"
2982,2982,595,51,[],200,llama-7b,64,1,712.0,1.0,1,A100,1697548535561,1697548536273,120,8.0,1.0,"[21, 691]","[1697548535582, 1697548536273]"
2983,2983,257,52,[],200,llama-7b,64,1,590.0,1.0,1,A100,1697548536277,1697548536867,120,14.0,1.0,"[30, 559]","[1697548536307, 1697548536866]"
2984,2984,789,39,[],200,llama-7b,64,1,6716.0,1.0,1,A100,1697548518068,1697548524784,120,6.0,50.0,"[25, 753, 69, 63, 49, 59, 56, 45, 607, 242, 68, 62, 63, 62, 59, 61, 335, 66, 63, 49, 49, 53, 405, 62, 48, 60, 46, 54, 709, 70, 67, 67, 57, 66, 50, 316, 67, 64, 63, 63, 61, 317, 52, 67, 51, 64, 63, 554, 64, 66, 65]","[1697548518093, 1697548518846, 1697548518915, 1697548518978, 1697548519027, 1697548519086, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520229, 1697548520291, 1697548520350, 1697548520411, 1697548520746, 1697548520812, 1697548520875, 1697548520924, 1697548520973, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787, 1697548523103, 1697548523170, 1697548523234, 1697548523297, 1697548523360, 1697548523421, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523972, 1697548524035, 1697548524589, 1697548524653, 1697548524719, 1697548524784]"
2985,2985,28,53,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536869,1697548537856,120,,,"[15, 883]","[1697548536884, 1697548537767]"
2986,2986,55,32,[],200,llama-7b,64,1,688.0,1.0,1,A100,1697548508752,1697548509440,120,12.0,1.0,"[26, 661]","[1697548508778, 1697548509439]"
2987,2987,743,37,[],200,llama-7b,64,1,1306.0,1.0,1,A100,1697548521366,1697548522672,120,123.0,6.0,"[25, 754, 266, 70, 67, 66, 57]","[1697548521391, 1697548522145, 1697548522411, 1697548522481, 1697548522548, 1697548522614, 1697548522671]"
2988,2988,851,46,[],200,llama-7b,64,1,954.0,1.0,1,A100,1697548537877,1697548538831,120,23.0,1.0,"[202, 752]","[1697548538079, 1697548538831]"
2989,2989,504,47,[],200,llama-7b,64,1,3212.0,1.0,1,A100,1697548538834,1697548542046,120,58.0,20.0,"[22, 653, 242, 60, 56, 53, 44, 52, 42, 401, 49, 59, 47, 57, 56, 52, 349, 49, 749, 60, 60]","[1697548538856, 1697548539509, 1697548539751, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540459, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540727, 1697548540779, 1697548541128, 1697548541177, 1697548541926, 1697548541986, 1697548542046]"
2990,2990,419,32,[],200,llama-7b,64,1,3490.0,1.0,1,A100,1697548533689,1697548537179,120,88.0,20.0,"[23, 852, 66, 47, 59, 58, 57, 707, 59, 59, 58, 55, 668, 63, 49, 54, 314, 52, 61, 65, 64]","[1697548533712, 1697548534564, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536569, 1697548536623, 1697548536937, 1697548536989, 1697548537050, 1697548537115, 1697548537179]"
2991,2991,523,39,[],200,llama-7b,64,1,1465.0,1.0,1,A100,1697548524845,1697548526310,120,345.0,13.0,"[13, 391, 241, 67, 66, 50, 50, 65, 66, 241, 48, 48, 60, 59]","[1697548524858, 1697548525249, 1697548525490, 1697548525557, 1697548525623, 1697548525673, 1697548525723, 1697548525788, 1697548525854, 1697548526095, 1697548526143, 1697548526191, 1697548526251, 1697548526310]"
2992,2992,177,40,[],200,llama-7b,64,1,1263.0,1.0,1,A100,1697548526314,1697548527577,120,14.0,1.0,"[6, 1257]","[1697548526320, 1697548527577]"
2993,2993,877,41,[],200,llama-7b,64,1,2817.0,1.0,1,A100,1697548527580,1697548530397,120,85.0,20.0,"[12, 304, 402, 656, 52, 50, 48, 47, 40, 364, 52, 49, 41, 41, 41, 49, 347, 56, 48, 59, 59]","[1697548527592, 1697548527896, 1697548528298, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529656, 1697548529697, 1697548529738, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530397]"
2994,2994,79,33,[],200,llama-7b,64,1,585.0,1.0,1,A100,1697548537183,1697548537768,120,12.0,1.0,"[17, 568]","[1697548537200, 1697548537768]"
2995,2995,538,42,[],200,llama-7b,64,1,3185.0,1.0,1,A100,1697548530400,1697548533585,120,89.0,20.0,"[12, 473, 50, 39, 473, 64, 65, 61, 60, 51, 736, 52, 59, 64, 51, 54, 576, 64, 63, 50, 67]","[1697548530412, 1697548530885, 1697548530935, 1697548530974, 1697548531447, 1697548531511, 1697548531576, 1697548531637, 1697548531697, 1697548531748, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533584]"
2996,2996,778,34,[],200,llama-7b,64,1,8483.0,1.0,1,A100,1697548537772,1697548546255,120,16.0,50.0,"[15, 149, 265, 683, 55, 41, 46, 46, 678, 60, 56, 54, 44, 52, 41, 401, 50, 59, 47, 57, 56, 52, 349, 48, 750, 60, 59, 52, 650, 62, 61, 57, 44, 55, 693, 59, 46, 58, 54, 53, 786, 56, 54, 46, 951, 72, 65, 64, 63, 61, 48]","[1697548537787, 1697548537936, 1697548538201, 1697548538884, 1697548538939, 1697548538980, 1697548539026, 1697548539072, 1697548539750, 1697548539810, 1697548539866, 1697548539920, 1697548539964, 1697548540016, 1697548540057, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540727, 1697548540779, 1697548541128, 1697548541176, 1697548541926, 1697548541986, 1697548542045, 1697548542097, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542971, 1697548543026, 1697548543719, 1697548543778, 1697548543824, 1697548543882, 1697548543936, 1697548543989, 1697548544775, 1697548544831, 1697548544885, 1697548544931, 1697548545882, 1697548545954, 1697548546019, 1697548546083, 1697548546146, 1697548546207, 1697548546255]"
2997,2997,250,48,[],200,llama-7b,64,1,451.0,1.0,1,A100,1697548542051,1697548542502,120,31.0,1.0,"[18, 433]","[1697548542069, 1697548542502]"
2998,2998,832,49,[],200,llama-7b,64,1,843.0,1.0,1,A100,1697548542506,1697548543349,120,15.0,1.0,"[13, 829]","[1697548542519, 1697548543348]"
2999,2999,612,50,[],200,llama-7b,64,1,4066.0,1.0,1,A100,1697548543352,1697548547418,120,93.0,20.0,"[20, 1150, 254, 56, 53, 47, 952, 71, 64, 65, 62, 61, 49, 61, 760, 65, 64, 59, 49, 58, 46]","[1697548543372, 1697548544522, 1697548544776, 1697548544832, 1697548544885, 1697548544932, 1697548545884, 1697548545955, 1697548546019, 1697548546084, 1697548546146, 1697548546207, 1697548546256, 1697548546317, 1697548547077, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547372, 1697548547418]"
3000,3000,307,43,[],200,llama-7b,64,1,338.0,1.0,1,A100,1697548533593,1697548533931,120,26.0,1.0,"[15, 323]","[1697548533608, 1697548533931]"
3001,3001,866,44,[],200,llama-7b,64,1,3245.0,1.0,1,A100,1697548533934,1697548537179,120,93.0,20.0,"[8, 622, 66, 47, 59, 58, 57, 708, 58, 59, 59, 54, 669, 62, 49, 54, 315, 51, 61, 65, 64]","[1697548533942, 1697548534564, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535559, 1697548535617, 1697548535676, 1697548535735, 1697548535789, 1697548536458, 1697548536520, 1697548536569, 1697548536623, 1697548536938, 1697548536989, 1697548537050, 1697548537115, 1697548537179]"
3002,3002,486,40,[],200,llama-7b,64,1,2932.0,1.0,1,A100,1697548549782,1697548552714,120,14.0,20.0,"[7, 551, 69, 63, 57, 47, 54, 370, 54, 51, 50, 49, 426, 63, 56, 44, 45, 55, 55, 712, 54]","[1697548549789, 1697548550340, 1697548550409, 1697548550472, 1697548550529, 1697548550576, 1697548550630, 1697548551000, 1697548551054, 1697548551105, 1697548551155, 1697548551204, 1697548551630, 1697548551693, 1697548551749, 1697548551793, 1697548551838, 1697548551893, 1697548551948, 1697548552660, 1697548552714]"
3003,3003,756,33,[],200,llama-7b,64,1,885.0,1.0,1,A100,1697548509442,1697548510327,120,19.0,1.0,"[10, 875]","[1697548509452, 1697548510327]"
3004,3004,503,34,[],200,llama-7b,64,1,3900.0,1.0,1,A100,1697548510330,1697548514230,120,109.0,20.0,"[7, 502, 64, 57, 42, 43, 52, 51, 653, 58, 53, 53, 51, 753, 350, 59, 56, 43, 53, 834, 66]","[1697548510337, 1697548510839, 1697548510903, 1697548510960, 1697548511002, 1697548511045, 1697548511097, 1697548511148, 1697548511801, 1697548511859, 1697548511912, 1697548511965, 1697548512016, 1697548512769, 1697548513119, 1697548513178, 1697548513234, 1697548513277, 1697548513330, 1697548514164, 1697548514230]"
3005,3005,398,38,[],200,llama-7b,64,1,2816.0,1.0,1,A100,1697548522674,1697548525490,120,87.0,20.0,"[10, 346, 74, 66, 64, 64, 62, 62, 316, 52, 67, 51, 63, 64, 555, 64, 65, 65, 60, 57, 589]","[1697548522684, 1697548523030, 1697548523104, 1697548523170, 1697548523234, 1697548523298, 1697548523360, 1697548523422, 1697548523738, 1697548523790, 1697548523857, 1697548523908, 1697548523971, 1697548524035, 1697548524590, 1697548524654, 1697548524719, 1697548524784, 1697548524844, 1697548524901, 1697548525490]"
3006,3006,611,54,[],200,llama-7b,64,1,962.0,1.0,1,A100,1697548537869,1697548538831,120,14.0,1.0,"[110, 852]","[1697548537979, 1697548538831]"
3007,3007,358,55,[],200,llama-7b,64,1,978.0,1.0,1,A100,1697548538834,1697548539812,120,216.0,3.0,"[30, 645, 242, 60]","[1697548538864, 1697548539509, 1697548539751, 1697548539811]"
3008,3008,169,39,[],200,llama-7b,64,1,536.0,1.0,1,A100,1697548525493,1697548526029,120,10.0,1.0,"[11, 525]","[1697548525504, 1697548526029]"
3009,3009,758,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548526032,1697548527820,120,,,"[18, 660, 254, 66, 49, 64, 66, 63]","[1697548526050, 1697548526710, 1697548526964, 1697548527030, 1697548527079, 1697548527143, 1697548527209, 1697548527272]"
3010,3010,639,45,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548537181,1697548537857,120,,,"[11, 576]","[1697548537192, 1697548537768]"
3011,3011,317,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548530401,1697548537858,120,,,"[33, 451, 51, 39, 471, 66, 62, 62, 59, 53, 737, 52, 58, 64, 51, 54, 576, 64, 63, 50, 68, 53, 46, 311, 60, 45, 46, 58, 58, 368, 46, 59, 59, 56, 708, 59, 60, 57, 55, 668, 63, 48, 55, 314, 52, 60, 66, 63, 60, 47]","[1697548530434, 1697548530885, 1697548530936, 1697548530975, 1697548531446, 1697548531512, 1697548531574, 1697548531636, 1697548531695, 1697548531748, 1697548532485, 1697548532537, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533585, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534204, 1697548534262, 1697548534630, 1697548534676, 1697548534735, 1697548534794, 1697548534850, 1697548535558, 1697548535617, 1697548535677, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
3012,3012,300,46,[],200,llama-7b,64,1,960.0,1.0,1,A100,1697548537870,1697548538830,120,9.0,1.0,"[75, 885]","[1697548537945, 1697548538830]"
3013,3013,526,41,[],200,llama-7b,64,1,2571.0,1.0,1,A100,1697548527827,1697548530398,120,89.0,20.0,"[61, 377, 34, 656, 52, 50, 48, 46, 40, 365, 52, 49, 40, 42, 40, 50, 347, 55, 48, 59, 60]","[1697548527888, 1697548528265, 1697548528299, 1697548528955, 1697548529007, 1697548529057, 1697548529105, 1697548529151, 1697548529191, 1697548529556, 1697548529608, 1697548529657, 1697548529697, 1697548529739, 1697548529779, 1697548529829, 1697548530176, 1697548530231, 1697548530279, 1697548530338, 1697548530398]"
3014,3014,72,47,[],200,llama-7b,64,1,3213.0,1.0,1,A100,1697548538833,1697548542046,120,84.0,20.0,"[14, 662, 241, 61, 56, 53, 44, 52, 42, 401, 49, 59, 47, 58, 55, 52, 349, 49, 749, 60, 60]","[1697548538847, 1697548539509, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540459, 1697548540508, 1697548540567, 1697548540614, 1697548540672, 1697548540727, 1697548540779, 1697548541128, 1697548541177, 1697548541926, 1697548541986, 1697548542046]"
3015,3015,438,35,[],200,llama-7b,64,1,602.0,1.0,1,A100,1697548546259,1697548546861,120,9.0,1.0,"[17, 585]","[1697548546276, 1697548546861]"
3016,3016,654,48,[],200,llama-7b,64,1,823.0,1.0,1,A100,1697548542048,1697548542871,120,47.0,4.0,"[20, 434, 246, 62, 61]","[1697548542068, 1697548542502, 1697548542748, 1697548542810, 1697548542871]"
3017,3017,207,36,[],200,llama-7b,64,1,750.0,1.0,1,A100,1697548546864,1697548547614,120,10.0,1.0,"[7, 743]","[1697548546871, 1697548547614]"
3018,3018,431,49,[],200,llama-7b,64,1,4332.0,1.0,1,A100,1697548542874,1697548547206,120,732.0,22.0,"[7, 468, 371, 58, 47, 58, 54, 52, 787, 55, 54, 47, 951, 71, 66, 64, 62, 61, 48, 62, 760, 65, 64]","[1697548542881, 1697548543349, 1697548543720, 1697548543778, 1697548543825, 1697548543883, 1697548543937, 1697548543989, 1697548544776, 1697548544831, 1697548544885, 1697548544932, 1697548545883, 1697548545954, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077, 1697548547142, 1697548547206]"
3019,3019,763,37,[],200,llama-7b,64,1,463.0,1.0,1,A100,1697548547618,1697548548081,120,20.0,1.0,"[23, 440]","[1697548547641, 1697548548081]"
3020,3020,920,42,[],200,llama-7b,64,1,1002.0,1.0,1,A100,1697548543884,1697548544886,120,96.0,4.0,"[14, 624, 255, 55, 54]","[1697548543898, 1697548544522, 1697548544777, 1697548544832, 1697548544886]"
3021,3021,533,38,[],200,llama-7b,64,1,969.0,1.0,1,A100,1697548548084,1697548549053,120,216.0,2.0,"[31, 938]","[1697548548115, 1697548549053]"
3022,3022,188,39,[],200,llama-7b,64,1,2636.0,1.0,1,A100,1697548549056,1697548551692,120,85.0,20.0,"[6, 637, 81, 59, 48, 47, 48, 59, 56, 312, 63, 56, 49, 53, 371, 53, 51, 51, 48, 425, 63]","[1697548549062, 1697548549699, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472, 1697548550528, 1697548550577, 1697548550630, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551204, 1697548551629, 1697548551692]"
3023,3023,583,43,[],200,llama-7b,64,1,3256.0,1.0,1,A100,1697548544889,1697548548145,120,96.0,20.0,"[15, 611, 369, 71, 65, 64, 63, 61, 48, 62, 759, 66, 63, 60, 48, 58, 45, 257, 53, 47, 371]","[1697548544904, 1697548545515, 1697548545884, 1697548545955, 1697548546020, 1697548546084, 1697548546147, 1697548546208, 1697548546256, 1697548546318, 1697548547077, 1697548547143, 1697548547206, 1697548547266, 1697548547314, 1697548547372, 1697548547417, 1697548547674, 1697548547727, 1697548547774, 1697548548145]"
3024,3024,144,41,[],200,llama-7b,64,1,3010.0,1.0,1,A100,1697548552717,1697548555727,120,96.0,20.0,"[6, 493, 67, 49, 47, 60, 60, 348, 59, 46, 59, 860, 251, 253, 58, 45, 44, 46, 58, 57, 44]","[1697548552723, 1697548553216, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555522, 1697548555568, 1697548555626, 1697548555683, 1697548555727]"
3025,3025,351,44,[],200,llama-7b,64,1,1124.0,1.0,1,A100,1697548548147,1697548549271,120,216.0,6.0,"[19, 595, 292, 61, 50, 60, 47]","[1697548548166, 1697548548761, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271]"
3026,3026,11,56,[],200,llama-7b,64,1,3057.0,1.0,1,A100,1697548539814,1697548542871,120,732.0,17.0,"[12, 560, 73, 50, 59, 46, 57, 57, 51, 349, 49, 749, 61, 60, 50, 650, 62, 62]","[1697548539826, 1697548540386, 1697548540459, 1697548540509, 1697548540568, 1697548540614, 1697548540671, 1697548540728, 1697548540779, 1697548541128, 1697548541177, 1697548541926, 1697548541987, 1697548542047, 1697548542097, 1697548542747, 1697548542809, 1697548542871]"
3027,3027,147,54,[],200,llama-7b,64,1,1792.0,1.0,1,A100,1697548554014,1697548555806,120,182.0,1.0,"[9, 1783]","[1697548554023, 1697548555806]"
3028,3028,186,42,[],200,llama-7b,64,1,3283.0,1.0,1,A100,1697548530401,1697548533684,120,123.0,22.0,"[23, 461, 51, 39, 471, 66, 62, 63, 60, 51, 736, 52, 59, 64, 51, 54, 576, 64, 63, 50, 67, 54, 46]","[1697548530424, 1697548530885, 1697548530936, 1697548530975, 1697548531446, 1697548531512, 1697548531574, 1697548531637, 1697548531697, 1697548531748, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533584, 1697548533638, 1697548533684]"
3029,3029,901,39,[],200,llama-7b,64,1,955.0,1.0,1,A100,1697548537876,1697548538831,120,17.0,1.0,"[198, 756]","[1697548538074, 1697548538830]"
3030,3030,671,40,[],200,llama-7b,64,1,676.0,1.0,1,A100,1697548538834,1697548539510,120,12.0,1.0,"[27, 648]","[1697548538861, 1697548539509]"
3031,3031,303,41,[],200,llama-7b,64,1,3515.0,1.0,1,A100,1697548539512,1697548543027,120,88.0,20.0,"[16, 857, 74, 49, 59, 47, 57, 57, 51, 350, 48, 749, 61, 59, 51, 650, 62, 61, 57, 45, 54]","[1697548539528, 1697548540385, 1697548540459, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779, 1697548541129, 1697548541177, 1697548541926, 1697548541987, 1697548542046, 1697548542097, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542972, 1697548543026]"
3032,3032,205,43,[],200,llama-7b,64,1,2864.0,1.0,1,A100,1697548537863,1697548540727,120,87.0,20.0,"[21, 290, 26, 684, 55, 41, 47, 45, 678, 61, 55, 54, 44, 52, 42, 400, 50, 59, 47, 57, 56]","[1697548537884, 1697548538174, 1697548538200, 1697548538884, 1697548538939, 1697548538980, 1697548539027, 1697548539072, 1697548539750, 1697548539811, 1697548539866, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540727]"
3033,3033,786,44,[],200,llama-7b,64,1,4047.0,1.0,1,A100,1697548540730,1697548544777,120,87.0,20.0,"[13, 332, 54, 48, 750, 59, 60, 51, 651, 62, 61, 56, 45, 55, 692, 59, 47, 57, 55, 52, 787]","[1697548540743, 1697548541075, 1697548541129, 1697548541177, 1697548541927, 1697548541986, 1697548542046, 1697548542097, 1697548542748, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776]"
3034,3034,157,35,[],200,llama-7b,64,1,8553.0,1.0,1,A100,1697548514234,1697548522787,120,563.0,55.0,"[14, 610, 467, 71, 70, 64, 60, 976, 271, 69, 64, 65, 63, 295, 65, 65, 58, 53, 673, 66, 64, 59, 57, 361, 64, 49, 58, 57, 45, 607, 242, 68, 62, 62, 62, 60, 60, 336, 66, 63, 48, 49, 54, 405, 62, 48, 60, 46, 54, 709, 70, 67, 67, 57, 66, 50]","[1697548514248, 1697548514858, 1697548515325, 1697548515396, 1697548515466, 1697548515530, 1697548515590, 1697548516566, 1697548516837, 1697548516906, 1697548516970, 1697548517035, 1697548517098, 1697548517393, 1697548517458, 1697548517523, 1697548517581, 1697548517634, 1697548518307, 1697548518373, 1697548518437, 1697548518496, 1697548518553, 1697548518914, 1697548518978, 1697548519027, 1697548519085, 1697548519142, 1697548519187, 1697548519794, 1697548520036, 1697548520104, 1697548520166, 1697548520228, 1697548520290, 1697548520350, 1697548520410, 1697548520746, 1697548520812, 1697548520875, 1697548520923, 1697548520972, 1697548521026, 1697548521431, 1697548521493, 1697548521541, 1697548521601, 1697548521647, 1697548521701, 1697548522410, 1697548522480, 1697548522547, 1697548522614, 1697548522671, 1697548522737, 1697548522787]"
3035,3035,84,50,[],200,llama-7b,64,1,398.0,1.0,1,A100,1697548547217,1697548547615,120,26.0,1.0,"[16, 382]","[1697548547233, 1697548547615]"
3036,3036,789,51,[],200,llama-7b,64,1,6336.0,1.0,1,A100,1697548547616,1697548553952,120,6.0,50.0,"[13, 452, 64, 50, 48, 810, 61, 51, 58, 48, 58, 55, 396, 59, 48, 47, 48, 59, 56, 312, 63, 56, 48, 53, 372, 53, 51, 50, 49, 425, 63, 56, 45, 44, 55, 56, 711, 54, 66, 51, 49, 65, 64, 274, 49, 48, 59, 61, 348, 59, 46]","[1697548547629, 1697548548081, 1697548548145, 1697548548195, 1697548548243, 1697548549053, 1697548549114, 1697548549165, 1697548549223, 1697548549271, 1697548549329, 1697548549384, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472, 1697548550528, 1697548550576, 1697548550629, 1697548551001, 1697548551054, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551692, 1697548551748, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552659, 1697548552713, 1697548552779, 1697548552830, 1697548552879, 1697548552944, 1697548553008, 1697548553282, 1697548553331, 1697548553379, 1697548553438, 1697548553499, 1697548553847, 1697548553906, 1697548553952]"
3037,3037,268,51,[],200,llama-7b,64,1,660.0,1.0,1,A100,1697548547422,1697548548082,120,19.0,1.0,"[44, 615]","[1697548547466, 1697548548081]"
3038,3038,12,45,[],200,llama-7b,64,1,425.0,1.0,1,A100,1697548549275,1697548549700,120,11.0,1.0,"[13, 412]","[1697548549288, 1697548549700]"
3039,3039,710,46,[],200,llama-7b,64,1,635.0,1.0,1,A100,1697548549705,1697548550340,120,14.0,1.0,"[23, 612]","[1697548549728, 1697548550340]"
3040,3040,37,52,[],200,llama-7b,64,1,675.0,1.0,1,A100,1697548548086,1697548548761,120,20.0,1.0,"[34, 641]","[1697548548120, 1697548548761]"
3041,3041,366,47,[],200,llama-7b,64,1,862.0,1.0,1,A100,1697548550342,1697548551204,120,85.0,6.0,"[8, 589, 62, 53, 52, 50, 48]","[1697548550350, 1697548550939, 1697548551001, 1697548551054, 1697548551106, 1697548551156, 1697548551204]"
3042,3042,627,53,[],200,llama-7b,64,1,2928.0,1.0,1,A100,1697548548764,1697548551692,120,93.0,20.0,"[20, 915, 81, 59, 48, 47, 48, 59, 56, 312, 63, 55, 50, 53, 371, 53, 51, 51, 48, 425, 63]","[1697548548784, 1697548549699, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472, 1697548550527, 1697548550577, 1697548550630, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551204, 1697548551629, 1697548551692]"
3043,3043,114,48,[],200,llama-7b,64,1,4167.0,1.0,1,A100,1697548551208,1697548555375,120,88.0,20.0,"[16, 1062, 374, 54, 66, 50, 50, 64, 64, 275, 48, 47, 60, 61, 348, 60, 45, 59, 860, 251, 253]","[1697548551224, 1697548552286, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552944, 1697548553008, 1697548553283, 1697548553331, 1697548553378, 1697548553438, 1697548553499, 1697548553847, 1697548553907, 1697548553952, 1697548554011, 1697548554871, 1697548555122, 1697548555375]"
3044,3044,74,42,[],200,llama-7b,64,1,4388.0,1.0,1,A100,1697548543029,1697548547417,120,88.0,20.0,"[7, 1486, 254, 55, 54, 47, 951, 72, 65, 64, 62, 61, 48, 62, 760, 65, 64, 59, 49, 57, 46]","[1697548543036, 1697548544522, 1697548544776, 1697548544831, 1697548544885, 1697548544932, 1697548545883, 1697548545955, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547371, 1697548547417]"
3045,3045,563,45,[],200,llama-7b,64,1,2948.0,1.0,1,A100,1697548544779,1697548547727,120,874.0,18.0,"[17, 719, 368, 71, 65, 65, 62, 61, 48, 62, 759, 67, 63, 60, 48, 57, 46, 257, 53]","[1697548544796, 1697548545515, 1697548545883, 1697548545954, 1697548546019, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547076, 1697548547143, 1697548547206, 1697548547266, 1697548547314, 1697548547371, 1697548547417, 1697548547674, 1697548547727]"
3046,3046,216,46,[],200,llama-7b,64,1,2743.0,1.0,1,A100,1697548547730,1697548550473,120,91.0,20.0,"[13, 338, 64, 50, 47, 811, 60, 52, 58, 47, 58, 55, 397, 59, 47, 47, 48, 59, 56, 313, 64]","[1697548547743, 1697548548081, 1697548548145, 1697548548195, 1697548548242, 1697548549053, 1697548549113, 1697548549165, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549780, 1697548549839, 1697548549886, 1697548549933, 1697548549981, 1697548550040, 1697548550096, 1697548550409, 1697548550473]"
3047,3047,395,54,[],200,llama-7b,64,1,3681.0,1.0,1,A100,1697548551694,1697548555375,120,88.0,20.0,"[19, 574, 373, 54, 66, 50, 50, 65, 64, 274, 48, 48, 60, 60, 348, 60, 46, 58, 860, 251, 253]","[1697548551713, 1697548552287, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009, 1697548553283, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553907, 1697548553953, 1697548554011, 1697548554871, 1697548555122, 1697548555375]"
3048,3048,178,22,[],200,llama-7b,64,1,955.0,1.0,1,A100,1697548537876,1697548538831,120,11.0,1.0,"[193, 761]","[1697548538069, 1697548538830]"
3049,3049,856,23,[],200,llama-7b,64,1,11262.0,1.0,1,A100,1697548538835,1697548550097,120,286.0,72.0,"[41, 634, 241, 61, 55, 54, 44, 52, 41, 400, 50, 59, 47, 57, 57, 51, 350, 48, 749, 61, 59, 51, 650, 62, 61, 57, 45, 54, 693, 59, 47, 57, 55, 52, 787, 56, 53, 46, 951, 72, 65, 64, 63, 61, 48, 62, 759, 66, 64, 59, 49, 57, 46, 256, 53, 48, 370, 50, 48, 810, 61, 51, 59, 47, 58, 55, 395, 60, 48, 47, 48, 59, 56]","[1697548538876, 1697548539510, 1697548539751, 1697548539812, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779, 1697548541129, 1697548541177, 1697548541926, 1697548541987, 1697548542046, 1697548542097, 1697548542747, 1697548542809, 1697548542870, 1697548542927, 1697548542972, 1697548543026, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776, 1697548544832, 1697548544885, 1697548544931, 1697548545882, 1697548545954, 1697548546019, 1697548546083, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547076, 1697548547142, 1697548547206, 1697548547265, 1697548547314, 1697548547371, 1697548547417, 1697548547673, 1697548547726, 1697548547774, 1697548548144, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549778, 1697548549838, 1697548549886, 1697548549933, 1697548549981, 1697548550040, 1697548550096]"
3050,3050,438,52,[],200,llama-7b,64,1,478.0,1.0,1,A100,1697548553956,1697548554434,120,9.0,1.0,"[15, 463]","[1697548553971, 1697548554434]"
3051,3051,657,43,[],200,llama-7b,64,1,660.0,1.0,1,A100,1697548547421,1697548548081,120,10.0,1.0,"[35, 625]","[1697548547456, 1697548548081]"
3052,3052,434,44,[],200,llama-7b,64,1,2546.0,1.0,1,A100,1697548548084,1697548550630,120,85.0,20.0,"[26, 650, 292, 61, 52, 59, 47, 57, 56, 395, 60, 47, 48, 47, 60, 55, 313, 64, 55, 49, 53]","[1697548548110, 1697548548760, 1697548549052, 1697548549113, 1697548549165, 1697548549224, 1697548549271, 1697548549328, 1697548549384, 1697548549779, 1697548549839, 1697548549886, 1697548549934, 1697548549981, 1697548550041, 1697548550096, 1697548550409, 1697548550473, 1697548550528, 1697548550577, 1697548550630]"
3053,3053,716,57,[],200,llama-7b,64,1,5271.0,1.0,1,A100,1697548542874,1697548548145,120,79.0,30.0,"[6, 840, 58, 47, 58, 54, 52, 787, 55, 54, 47, 951, 71, 66, 64, 62, 61, 48, 62, 760, 65, 64, 60, 48, 57, 46, 256, 54, 47, 370]","[1697548542880, 1697548543720, 1697548543778, 1697548543825, 1697548543883, 1697548543937, 1697548543989, 1697548544776, 1697548544831, 1697548544885, 1697548544932, 1697548545883, 1697548545954, 1697548546020, 1697548546084, 1697548546146, 1697548546207, 1697548546255, 1697548546317, 1697548547077, 1697548547142, 1697548547206, 1697548547266, 1697548547314, 1697548547371, 1697548547417, 1697548547673, 1697548547727, 1697548547774, 1697548548144]"
3054,3054,885,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548533687,1697548537855,120,,,"[15, 862, 66, 47, 59, 58, 57, 707, 59, 59, 58, 55, 668, 63, 49, 54, 314, 52, 61, 65, 63, 61, 46]","[1697548533702, 1697548534564, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536569, 1697548536623, 1697548536937, 1697548536989, 1697548537050, 1697548537115, 1697548537178, 1697548537239, 1697548537285]"
3055,3055,22,44,[],200,llama-7b,64,1,914.0,1.0,1,A100,1697548530453,1697548531367,120,16.0,1.0,"[20, 894]","[1697548530473, 1697548531367]"
3056,3056,537,44,[],200,llama-7b,64,1,2865.0,1.0,1,A100,1697548537862,1697548540727,120,83.0,20.0,"[17, 295, 27, 683, 55, 41, 46, 46, 678, 61, 55, 54, 44, 52, 42, 400, 50, 59, 47, 57, 56]","[1697548537879, 1697548538174, 1697548538201, 1697548538884, 1697548538939, 1697548538980, 1697548539026, 1697548539072, 1697548539750, 1697548539811, 1697548539866, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540727]"
3057,3057,728,45,[],200,llama-7b,64,1,747.0,1.0,1,A100,1697548531371,1697548532118,120,20.0,1.0,"[16, 731]","[1697548531387, 1697548532118]"
3058,3058,380,46,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548532122,1697548537858,120,,,"[10, 1208, 64, 64, 49, 67, 54, 46, 311, 60, 45, 46, 59, 57, 368, 46, 59, 59, 57, 707, 59, 59, 58, 55, 668, 63, 48, 55, 314, 52, 60, 66, 63, 60, 47]","[1697548532132, 1697548533340, 1697548533404, 1697548533468, 1697548533517, 1697548533584, 1697548533638, 1697548533684, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534262, 1697548534630, 1697548534676, 1697548534735, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568, 1697548536623, 1697548536937, 1697548536989, 1697548537049, 1697548537115, 1697548537178, 1697548537238, 1697548537285]"
3059,3059,537,40,[],200,llama-7b,64,1,3305.0,1.0,1,A100,1697548546023,1697548549328,120,83.0,20.0,"[11, 827, 216, 67, 63, 59, 48, 58, 45, 256, 54, 47, 371, 49, 48, 810, 61, 51, 59, 47, 58]","[1697548546034, 1697548546861, 1697548547077, 1697548547144, 1697548547207, 1697548547266, 1697548547314, 1697548547372, 1697548547417, 1697548547673, 1697548547727, 1697548547774, 1697548548145, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549270, 1697548549328]"
3060,3060,917,47,[],200,llama-7b,64,1,526.0,1.0,1,A100,1697548550475,1697548551001,120,123.0,2.0,"[21, 444, 61]","[1697548550496, 1697548550940, 1697548551001]"
3061,3061,574,48,[],200,llama-7b,64,1,627.0,1.0,1,A100,1697548551003,1697548551630,120,364.0,2.0,"[10, 547, 70]","[1697548551013, 1697548551560, 1697548551630]"
3062,3062,306,41,[],200,llama-7b,64,1,652.0,1.0,1,A100,1697548549331,1697548549983,120,140.0,6.0,"[13, 436, 59, 48, 47, 48]","[1697548549344, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982]"
3063,3063,344,49,[],200,llama-7b,64,1,655.0,1.0,1,A100,1697548551632,1697548552287,120,13.0,1.0,"[6, 649]","[1697548551638, 1697548552287]"
3064,3064,89,50,[],200,llama-7b,64,1,3436.0,1.0,1,A100,1697548552290,1697548555726,120,52.0,20.0,"[16, 910, 67, 49, 47, 60, 60, 349, 59, 45, 59, 860, 251, 253, 58, 45, 44, 46, 58, 57, 43]","[1697548552306, 1697548553216, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553499, 1697548553848, 1697548553907, 1697548553952, 1697548554011, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555522, 1697548555568, 1697548555626, 1697548555683, 1697548555726]"
3065,3065,54,42,[],200,llama-7b,64,1,2730.0,1.0,1,A100,1697548549985,1697548552715,120,87.0,20.0,"[6, 349, 69, 64, 56, 48, 56, 368, 53, 52, 50, 48, 426, 63, 56, 44, 45, 55, 56, 711, 54]","[1697548549991, 1697548550340, 1697548550409, 1697548550473, 1697548550529, 1697548550577, 1697548550633, 1697548551001, 1697548551054, 1697548551106, 1697548551156, 1697548551204, 1697548551630, 1697548551693, 1697548551749, 1697548551793, 1697548551838, 1697548551893, 1697548551949, 1697548552660, 1697548552714]"
3066,3066,638,43,[],200,llama-7b,64,1,3009.0,1.0,1,A100,1697548552718,1697548555727,120,88.0,20.0,"[10, 488, 67, 49, 47, 60, 60, 348, 59, 46, 59, 860, 251, 253, 58, 45, 44, 46, 58, 58, 43]","[1697548552728, 1697548553216, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555522, 1697548555568, 1697548555626, 1697548555684, 1697548555727]"
3067,3067,863,36,[],200,llama-7b,64,1,875.0,1.0,1,A100,1697548522790,1697548523665,120,10.0,1.0,"[16, 859]","[1697548522806, 1697548523665]"
3068,3068,515,37,[],200,llama-7b,64,1,684.0,1.0,1,A100,1697548523669,1697548524353,120,11.0,1.0,"[20, 664]","[1697548523689, 1697548524353]"
3069,3069,291,38,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548524356,1697548527820,120,,,"[12, 1121, 68, 65, 51, 50, 65, 65, 242, 48, 47, 60, 59, 656, 65, 49, 64, 65, 64]","[1697548524368, 1697548525489, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525853, 1697548526095, 1697548526143, 1697548526190, 1697548526250, 1697548526309, 1697548526965, 1697548527030, 1697548527079, 1697548527143, 1697548527208, 1697548527272]"
3070,3070,566,40,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 4.52 GiB is free. Process 1412106 has 34.87 GiB memory in use. Of the allocated memory 21.31 GiB is allocated by PyTorch, and 11.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548524785,1697548527821,120,,,"[13, 451, 241, 67, 65, 51, 50, 65, 66, 241, 48, 48, 59, 59, 656, 64, 50, 65, 64, 63]","[1697548524798, 1697548525249, 1697548525490, 1697548525557, 1697548525622, 1697548525673, 1697548525723, 1697548525788, 1697548525854, 1697548526095, 1697548526143, 1697548526191, 1697548526250, 1697548526309, 1697548526965, 1697548527029, 1697548527079, 1697548527144, 1697548527208, 1697548527271]"
3071,3071,625,24,[],200,llama-7b,64,1,902.0,1.0,1,A100,1697548550099,1697548551001,120,364.0,2.0,"[7, 834, 61]","[1697548550106, 1697548550940, 1697548551001]"
3072,3072,277,25,[],200,llama-7b,64,1,555.0,1.0,1,A100,1697548551005,1697548551560,120,18.0,1.0,"[16, 539]","[1697548551021, 1697548551560]"
3073,3073,54,26,[],200,llama-7b,64,1,3814.0,1.0,1,A100,1697548551561,1697548555375,120,87.0,20.0,"[6, 720, 373, 54, 66, 50, 50, 64, 64, 274, 49, 48, 60, 60, 348, 60, 45, 59, 860, 251, 253]","[1697548551567, 1697548552287, 1697548552660, 1697548552714, 1697548552780, 1697548552830, 1697548552880, 1697548552944, 1697548553008, 1697548553282, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553907, 1697548553952, 1697548554011, 1697548554871, 1697548555122, 1697548555375]"
3074,3074,877,39,[],200,llama-7b,64,1,2571.0,1.0,1,A100,1697548527826,1697548530397,120,85.0,20.0,"[28, 411, 34, 655, 52, 50, 48, 47, 40, 364, 52, 49, 41, 41, 41, 49, 347, 56, 48, 59, 59]","[1697548527854, 1697548528265, 1697548528299, 1697548528954, 1697548529006, 1697548529056, 1697548529104, 1697548529151, 1697548529191, 1697548529555, 1697548529607, 1697548529656, 1697548529697, 1697548529738, 1697548529779, 1697548529828, 1697548530175, 1697548530231, 1697548530279, 1697548530338, 1697548530397]"
3075,3075,219,41,[],200,llama-7b,64,1,2623.0,1.0,1,A100,1697548527826,1697548530449,120,90.0,20.0,"[218, 852, 59, 51, 51, 47, 48, 39, 364, 52, 49, 41, 41, 41, 50, 346, 56, 48, 61, 57, 52]","[1697548528044, 1697548528896, 1697548528955, 1697548529006, 1697548529057, 1697548529104, 1697548529152, 1697548529191, 1697548529555, 1697548529607, 1697548529656, 1697548529697, 1697548529738, 1697548529779, 1697548529829, 1697548530175, 1697548530231, 1697548530279, 1697548530340, 1697548530397, 1697548530449]"
3076,3076,649,40,[],200,llama-7b,64,1,3184.0,1.0,1,A100,1697548530400,1697548533584,120,244.0,20.0,"[12, 473, 51, 38, 473, 64, 65, 61, 60, 50, 737, 52, 59, 64, 51, 54, 576, 64, 63, 50, 66]","[1697548530412, 1697548530885, 1697548530936, 1697548530974, 1697548531447, 1697548531511, 1697548531576, 1697548531637, 1697548531697, 1697548531747, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532710, 1697548532764, 1697548533340, 1697548533404, 1697548533467, 1697548533517, 1697548533583]"
3077,3077,313,45,[],200,llama-7b,64,1,344.0,1.0,1,A100,1697548540731,1697548541075,120,20.0,1.0,"[18, 326]","[1697548540749, 1697548541075]"
3078,3078,896,46,[],200,llama-7b,64,1,512.0,1.0,1,A100,1697548541079,1697548541591,120,15.0,1.0,"[10, 502]","[1697548541089, 1697548541591]"
3079,3079,365,58,[],200,llama-7b,64,1,613.0,1.0,1,A100,1697548548148,1697548548761,120,23.0,1.0,"[23, 590]","[1697548548171, 1697548548761]"
3080,3080,642,47,[],200,llama-7b,64,1,4425.0,1.0,1,A100,1697548541594,1697548546019,120,89.0,20.0,"[17, 891, 245, 62, 61, 58, 44, 55, 692, 60, 46, 57, 55, 52, 788, 55, 54, 46, 951, 71, 65]","[1697548541611, 1697548542502, 1697548542747, 1697548542809, 1697548542870, 1697548542928, 1697548542972, 1697548543027, 1697548543719, 1697548543779, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544886, 1697548544932, 1697548545883, 1697548545954, 1697548546019]"
3081,3081,143,59,[],200,llama-7b,64,1,1813.0,1.0,1,A100,1697548548764,1697548550577,120,6.0,12.0,"[28, 907, 81, 59, 48, 47, 48, 59, 56, 312, 63, 55, 50]","[1697548548792, 1697548549699, 1697548549780, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472, 1697548550527, 1697548550577]"
3082,3082,85,45,[],200,llama-7b,64,1,2866.0,1.0,1,A100,1697548550634,1697548553500,120,88.0,20.0,"[24, 901, 70, 63, 58, 43, 44, 56, 55, 713, 52, 67, 49, 50, 66, 64, 274, 49, 47, 60, 60]","[1697548550658, 1697548551559, 1697548551629, 1697548551692, 1697548551750, 1697548551793, 1697548551837, 1697548551893, 1697548551948, 1697548552661, 1697548552713, 1697548552780, 1697548552829, 1697548552879, 1697548552945, 1697548553009, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553499]"
3083,3083,309,41,[],200,llama-7b,64,1,2978.0,1.0,1,A100,1697548533590,1697548536568,120,52.0,20.0,"[13, 328, 64, 60, 46, 46, 58, 58, 367, 47, 59, 58, 57, 707, 59, 59, 58, 55, 668, 63, 48]","[1697548533603, 1697548533931, 1697548533995, 1697548534055, 1697548534101, 1697548534147, 1697548534205, 1697548534263, 1697548534630, 1697548534677, 1697548534736, 1697548534794, 1697548534851, 1697548535558, 1697548535617, 1697548535676, 1697548535734, 1697548535789, 1697548536457, 1697548536520, 1697548536568]"
3084,3084,894,40,[],200,llama-7b,64,1,592.0,1.0,1,A100,1697548551695,1697548552287,120,14.0,1.0,"[23, 569]","[1697548551718, 1697548552287]"
3085,3085,548,41,[],200,llama-7b,64,1,3435.0,1.0,1,A100,1697548552292,1697548555727,120,86.0,20.0,"[20, 904, 67, 49, 47, 60, 60, 349, 59, 45, 59, 860, 251, 253, 58, 45, 44, 46, 58, 57, 43]","[1697548552312, 1697548553216, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553499, 1697548553848, 1697548553907, 1697548553952, 1697548554011, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555522, 1697548555568, 1697548555626, 1697548555683, 1697548555726]"
3086,3086,726,60,[],200,llama-7b,64,1,5515.0,1.0,1,A100,1697548550580,1697548556095,120,67.0,47.0,"[7, 353, 62, 52, 52, 50, 49, 424, 63, 57, 44, 44, 56, 55, 712, 53, 67, 50, 50, 65, 64, 274, 48, 48, 60, 60, 348, 59, 46, 59, 860, 250, 254, 58, 44, 45, 46, 57, 58, 43, 50, 69, 46, 37, 45, 42, 41, 39]","[1697548550587, 1697548550940, 1697548551002, 1697548551054, 1697548551106, 1697548551156, 1697548551205, 1697548551629, 1697548551692, 1697548551749, 1697548551793, 1697548551837, 1697548551893, 1697548551948, 1697548552660, 1697548552713, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009, 1697548553283, 1697548553331, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554871, 1697548555121, 1697548555375, 1697548555433, 1697548555477, 1697548555522, 1697548555568, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095]"
3087,3087,412,48,[],200,llama-7b,64,1,1652.0,1.0,1,A100,1697548546022,1697548547674,120,244.0,9.0,"[8, 831, 216, 67, 63, 58, 49, 58, 45, 256]","[1697548546030, 1697548546861, 1697548547077, 1697548547144, 1697548547207, 1697548547265, 1697548547314, 1697548547372, 1697548547417, 1697548547673]"
3088,3088,791,46,[],200,llama-7b,64,1,3714.0,1.0,1,A100,1697548553503,1697548557217,120,182.0,64.0,"[37, 1331, 251, 253, 58, 45, 45, 45, 58, 57, 44, 49, 69, 46, 37, 45, 42, 41, 39, 39, 35, 27, 33, 26, 26, 26, 27, 31, 26, 30, 25, 31, 24, 25, 25, 24, 30, 24, 24, 29, 23, 24, 24, 23, 24, 23, 29, 24, 23, 26, 22, 24, 21, 20, 21, 20, 21, 20, 21, 23, 20, 19, 20, 20]","[1697548553540, 1697548554871, 1697548555122, 1697548555375, 1697548555433, 1697548555478, 1697548555523, 1697548555568, 1697548555626, 1697548555683, 1697548555727, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556169, 1697548556196, 1697548556229, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556391, 1697548556421, 1697548556446, 1697548556477, 1697548556501, 1697548556526, 1697548556551, 1697548556575, 1697548556605, 1697548556629, 1697548556653, 1697548556682, 1697548556705, 1697548556729, 1697548556753, 1697548556776, 1697548556800, 1697548556823, 1697548556852, 1697548556876, 1697548556899, 1697548556925, 1697548556947, 1697548556971, 1697548556992, 1697548557012, 1697548557033, 1697548557053, 1697548557074, 1697548557094, 1697548557115, 1697548557138, 1697548557158, 1697548557177, 1697548557197, 1697548557217]"
3089,3089,156,47,[],200,llama-7b,64,1,2905.0,1.0,1,A100,1697548537875,1697548540780,120,86.0,20.0,"[93, 863, 53, 55, 42, 46, 46, 677, 61, 56, 53, 44, 52, 42, 400, 50, 59, 47, 57, 57, 51]","[1697548537968, 1697548538831, 1697548538884, 1697548538939, 1697548538981, 1697548539027, 1697548539073, 1697548539750, 1697548539811, 1697548539867, 1697548539920, 1697548539964, 1697548540016, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540728, 1697548540779]"
3090,3090,924,42,[],200,llama-7b,64,1,914.0,1.0,1,A100,1697548530454,1697548531368,120,9.0,1.0,"[44, 870]","[1697548530498, 1697548531368]"
3091,3091,79,42,[],200,llama-7b,64,1,294.0,1.0,1,A100,1697548536573,1697548536867,120,12.0,1.0,"[26, 268]","[1697548536599, 1697548536867]"
3092,3092,581,43,[],200,llama-7b,64,1,2893.0,1.0,1,A100,1697548531370,1697548534263,120,47.0,20.0,"[12, 736, 366, 52, 59, 64, 50, 55, 576, 65, 63, 49, 66, 55, 47, 310, 60, 45, 46, 59, 58]","[1697548531382, 1697548532118, 1697548532484, 1697548532536, 1697548532595, 1697548532659, 1697548532709, 1697548532764, 1697548533340, 1697548533405, 1697548533468, 1697548533517, 1697548533583, 1697548533638, 1697548533685, 1697548533995, 1697548534055, 1697548534100, 1697548534146, 1697548534205, 1697548534263]"
3093,3093,638,43,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548536869,1697548537856,120,,,"[10, 888]","[1697548536879, 1697548537767]"
3094,3094,739,48,[],200,llama-7b,64,1,808.0,1.0,1,A100,1697548540783,1697548541591,120,216.0,1.0,"[65, 743]","[1697548540848, 1697548541591]"
3095,3095,507,49,[],200,llama-7b,64,1,4423.0,1.0,1,A100,1697548541597,1697548546020,120,83.0,20.0,"[24, 881, 245, 62, 61, 58, 44, 55, 692, 59, 47, 57, 55, 52, 788, 55, 54, 46, 951, 71, 65]","[1697548541621, 1697548542502, 1697548542747, 1697548542809, 1697548542870, 1697548542928, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544886, 1697548544932, 1697548545883, 1697548545954, 1697548546019]"
3096,3096,408,44,[],200,llama-7b,64,1,311.0,1.0,1,A100,1697548537863,1697548538174,120,16.0,1.0,"[41, 270]","[1697548537904, 1697548538174]"
3097,3097,68,45,[],200,llama-7b,64,1,657.0,1.0,1,A100,1697548538175,1697548538832,120,12.0,1.0,"[7, 649]","[1697548538182, 1697548538831]"
3098,3098,357,44,"['{""code"":2,""details"":[],""message"":""Request failed during generation: Unexpected <class \'torch.cuda.OutOfMemoryError\'>: CUDA out of memory. Tried to allocate 6.29 GiB. GPU 0 has a total capacty of 39.39 GiB of which 5.56 GiB is free. Process 1412106 has 33.82 GiB memory in use. Of the allocated memory 20.49 GiB is allocated by PyTorch, and 11.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF""}']",408,llama-7b,64,1,,,1,A100,1697548534266,1697548537856,120,,,"[27, 883, 382, 59, 60, 58, 55, 668, 62, 49, 54, 315, 52, 60, 66, 63, 60, 47]","[1697548534293, 1697548535176, 1697548535558, 1697548535617, 1697548535677, 1697548535735, 1697548535790, 1697548536458, 1697548536520, 1697548536569, 1697548536623, 1697548536938, 1697548536990, 1697548537050, 1697548537116, 1697548537179, 1697548537239, 1697548537286]"
3099,3099,13,45,[],200,llama-7b,64,1,2904.0,1.0,1,A100,1697548537876,1697548540780,120,90.0,20.0,"[188, 768, 52, 55, 41, 47, 45, 679, 61, 55, 54, 44, 52, 40, 402, 49, 60, 47, 56, 58, 51]","[1697548538064, 1697548538832, 1697548538884, 1697548538939, 1697548538980, 1697548539027, 1697548539072, 1697548539751, 1697548539812, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540057, 1697548540459, 1697548540508, 1697548540568, 1697548540615, 1697548540671, 1697548540729, 1697548540780]"
3100,3100,72,49,[],200,llama-7b,64,1,2798.0,1.0,1,A100,1697548547675,1697548550473,120,84.0,20.0,"[7, 399, 64, 50, 47, 811, 61, 51, 58, 47, 58, 55, 397, 60, 47, 47, 48, 60, 55, 312, 64]","[1697548547682, 1697548548081, 1697548548145, 1697548548195, 1697548548242, 1697548549053, 1697548549114, 1697548549165, 1697548549223, 1697548549270, 1697548549328, 1697548549383, 1697548549780, 1697548549840, 1697548549887, 1697548549934, 1697548549982, 1697548550042, 1697548550097, 1697548550409, 1697548550473]"
3101,3101,255,50,[],200,llama-7b,64,1,11091.0,1.0,1,A100,1697548546024,1697548557115,120,216.0,119.0,"[32, 805, 216, 67, 63, 59, 48, 58, 46, 256, 53, 47, 371, 49, 48, 810, 61, 51, 59, 48, 57, 55, 396, 60, 48, 47, 48, 59, 56, 312, 63, 56, 48, 53, 372, 52, 52, 50, 49, 425, 62, 57, 45, 44, 55, 56, 711, 54, 66, 50, 50, 65, 64, 274, 49, 48, 59, 61, 348, 59, 46, 59, 859, 251, 253, 59, 44, 45, 45, 58, 58, 43, 50, 69, 46, 37, 45, 42, 41, 39, 39, 34, 28, 33, 26, 26, 26, 27, 31, 25, 31, 25, 31, 24, 25, 25, 24, 30, 24, 24, 29, 23, 24, 23, 24, 24, 24, 28, 24, 23, 26, 22, 24, 21, 20, 21, 20, 21, 20, 21]","[1697548546056, 1697548546861, 1697548547077, 1697548547144, 1697548547207, 1697548547266, 1697548547314, 1697548547372, 1697548547418, 1697548547674, 1697548547727, 1697548547774, 1697548548145, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549271, 1697548549328, 1697548549383, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472, 1697548550528, 1697548550576, 1697548550629, 1697548551001, 1697548551053, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551691, 1697548551748, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552659, 1697548552713, 1697548552779, 1697548552829, 1697548552879, 1697548552944, 1697548553008, 1697548553282, 1697548553331, 1697548553379, 1697548553438, 1697548553499, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554870, 1697548555121, 1697548555374, 1697548555433, 1697548555477, 1697548555522, 1697548555567, 1697548555625, 1697548555683, 1697548555726, 1697548555776, 1697548555845, 1697548555891, 1697548555928, 1697548555973, 1697548556015, 1697548556056, 1697548556095, 1697548556134, 1697548556168, 1697548556196, 1697548556229, 1697548556255, 1697548556281, 1697548556307, 1697548556334, 1697548556365, 1697548556390, 1697548556421, 1697548556446, 1697548556477, 1697548556501, 1697548556526, 1697548556551, 1697548556575, 1697548556605, 1697548556629, 1697548556653, 1697548556682, 1697548556705, 1697548556729, 1697548556752, 1697548556776, 1697548556800, 1697548556824, 1697548556852, 1697548556876, 1697548556899, 1697548556925, 1697548556947, 1697548556971, 1697548556992, 1697548557012, 1697548557033, 1697548557053, 1697548557074, 1697548557094, 1697548557115]"
3102,3102,711,46,[],200,llama-7b,64,1,1262.0,1.0,1,A100,1697548540784,1697548542046,120,457.0,4.0,"[34, 773, 335, 61, 59]","[1697548540818, 1697548541591, 1697548541926, 1697548541987, 1697548542046]"
3103,3103,343,47,[],200,llama-7b,64,1,3971.0,1.0,1,A100,1697548542049,1697548546020,120,84.0,20.0,"[10, 443, 245, 63, 61, 56, 45, 55, 692, 59, 47, 57, 55, 52, 787, 56, 54, 46, 951, 71, 66]","[1697548542059, 1697548542502, 1697548542747, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544776, 1697548544832, 1697548544886, 1697548544932, 1697548545883, 1697548545954, 1697548546020]"
3104,3104,773,50,[],200,llama-7b,64,1,2533.0,1.0,1,A100,1697548550476,1697548553009,120,90.0,20.0,"[25, 439, 61, 53, 51, 51, 49, 424, 62, 58, 44, 44, 56, 55, 712, 55, 65, 50, 50, 65, 64]","[1697548550501, 1697548550940, 1697548551001, 1697548551054, 1697548551105, 1697548551156, 1697548551205, 1697548551629, 1697548551691, 1697548551749, 1697548551793, 1697548551837, 1697548551893, 1697548551948, 1697548552660, 1697548552715, 1697548552780, 1697548552830, 1697548552880, 1697548552945, 1697548553009]"
3105,3105,769,46,[],200,llama-7b,64,1,3212.0,1.0,1,A100,1697548538834,1697548542046,120,47.0,20.0,"[32, 643, 242, 60, 56, 54, 44, 52, 41, 400, 50, 59, 47, 57, 56, 52, 349, 49, 749, 60, 60]","[1697548538866, 1697548539509, 1697548539751, 1697548539811, 1697548539867, 1697548539921, 1697548539965, 1697548540017, 1697548540058, 1697548540458, 1697548540508, 1697548540567, 1697548540614, 1697548540671, 1697548540727, 1697548540779, 1697548541128, 1697548541177, 1697548541926, 1697548541986, 1697548542046]"
3106,3106,430,51,[],200,llama-7b,64,1,769.0,1.0,1,A100,1697548553012,1697548553781,120,15.0,1.0,"[30, 739]","[1697548553042, 1697548553781]"
3107,3107,207,52,[],200,llama-7b,64,1,650.0,1.0,1,A100,1697548553784,1697548554434,120,10.0,1.0,"[10, 640]","[1697548553794, 1697548554434]"
3108,3108,110,48,[],200,llama-7b,64,1,1187.0,1.0,1,A100,1697548546022,1697548547209,120,96.0,4.0,"[7, 832, 216, 67, 63]","[1697548546029, 1697548546861, 1697548547077, 1697548547144, 1697548547207]"
3109,3109,701,49,[],200,llama-7b,64,1,5665.0,1.0,1,A100,1697548547215,1697548552880,120,58.0,43.0,"[11, 389, 59, 53, 48, 370, 49, 49, 810, 61, 50, 60, 47, 58, 54, 396, 60, 48, 47, 48, 59, 56, 312, 63, 56, 48, 53, 372, 53, 51, 50, 49, 425, 62, 57, 45, 44, 55, 56, 711, 54, 66, 50, 50]","[1697548547226, 1697548547615, 1697548547674, 1697548547727, 1697548547775, 1697548548145, 1697548548194, 1697548548243, 1697548549053, 1697548549114, 1697548549164, 1697548549224, 1697548549271, 1697548549329, 1697548549383, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550409, 1697548550472, 1697548550528, 1697548550576, 1697548550629, 1697548551001, 1697548551054, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551691, 1697548551748, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552659, 1697548552713, 1697548552779, 1697548552829, 1697548552879]"
3110,3110,466,50,[],200,llama-7b,64,1,2845.0,1.0,1,A100,1697548552882,1697548555727,120,457.0,20.0,"[7, 328, 66, 49, 47, 60, 60, 348, 59, 46, 59, 860, 251, 252, 59, 45, 45, 45, 58, 57, 44]","[1697548552889, 1697548553217, 1697548553283, 1697548553332, 1697548553379, 1697548553439, 1697548553499, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554871, 1697548555122, 1697548555374, 1697548555433, 1697548555478, 1697548555523, 1697548555568, 1697548555626, 1697548555683, 1697548555727]"
3111,3111,419,47,[],200,llama-7b,64,1,3972.0,1.0,1,A100,1697548542048,1697548546020,120,88.0,20.0,"[6, 448, 246, 62, 61, 56, 45, 55, 692, 59, 47, 57, 55, 52, 788, 55, 54, 46, 951, 71, 66]","[1697548542054, 1697548542502, 1697548542748, 1697548542810, 1697548542871, 1697548542927, 1697548542972, 1697548543027, 1697548543719, 1697548543778, 1697548543825, 1697548543882, 1697548543937, 1697548543989, 1697548544777, 1697548544832, 1697548544886, 1697548544932, 1697548545883, 1697548545954, 1697548546020]"
3112,3112,195,48,[],200,llama-7b,64,1,9351.0,1.0,1,A100,1697548546023,1697548555374,120,286.0,64.0,"[16, 822, 216, 67, 63, 59, 48, 58, 46, 256, 53, 47, 371, 49, 48, 810, 61, 51, 59, 48, 57, 55, 396, 60, 48, 47, 48, 59, 56, 311, 64, 55, 49, 53, 372, 52, 52, 50, 49, 425, 62, 57, 45, 44, 55, 56, 711, 54, 66, 50, 50, 65, 64, 274, 49, 47, 60, 60, 349, 59, 46, 59, 859, 251, 253]","[1697548546039, 1697548546861, 1697548547077, 1697548547144, 1697548547207, 1697548547266, 1697548547314, 1697548547372, 1697548547418, 1697548547674, 1697548547727, 1697548547774, 1697548548145, 1697548548194, 1697548548242, 1697548549052, 1697548549113, 1697548549164, 1697548549223, 1697548549271, 1697548549328, 1697548549383, 1697548549779, 1697548549839, 1697548549887, 1697548549934, 1697548549982, 1697548550041, 1697548550097, 1697548550408, 1697548550472, 1697548550527, 1697548550576, 1697548550629, 1697548551001, 1697548551053, 1697548551105, 1697548551155, 1697548551204, 1697548551629, 1697548551691, 1697548551748, 1697548551793, 1697548551837, 1697548551892, 1697548551948, 1697548552659, 1697548552713, 1697548552779, 1697548552829, 1697548552879, 1697548552944, 1697548553008, 1697548553282, 1697548553331, 1697548553378, 1697548553438, 1697548553498, 1697548553847, 1697548553906, 1697548553952, 1697548554011, 1697548554870, 1697548555121, 1697548555374]"
